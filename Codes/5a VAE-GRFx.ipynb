{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778863dc",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0fc7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "import os\n",
    "import skfda\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import itertools \n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib as mpl\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0fa96e",
   "metadata": {},
   "source": [
    "# Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73a0610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wf(x):\n",
    "    return '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Data/' + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "250c5bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_GRF_stance_N = pd.read_csv(wf('V_GRF_stance_N.csv'), header = None)\n",
    "ML_GRF_stance_N = pd.read_csv(wf('ML_GRF_stance_N.csv'), header = None)\n",
    "AP_GRF_stance_N = pd.read_csv(wf('AP_GRF_stance_N.csv'), header = None)\n",
    "ID_info = pd.read_csv(wf('IDinfo.csv'), header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13bf172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_GRF_stance_N_matrix = V_GRF_stance_N.to_numpy()\n",
    "ML_GRF_stance_N_matrix = ML_GRF_stance_N.to_numpy()\n",
    "AP_GRF_stance_N_matrix = AP_GRF_stance_N.to_numpy()\n",
    "ID_info_matrix = ID_info.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3a5be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a list of grid points\n",
    "grid_points_100 = list(range(1,101))\n",
    "grid_points_2990 = list(range(1,2991))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d666c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_GRF_stance_N_fd = skfda.FDataGrid(data_matrix=V_GRF_stance_N_matrix,grid_points=grid_points_100)\n",
    "V_GRF_stance_N_mean = skfda.exploratory.stats.mean(V_GRF_stance_N_fd)\n",
    "\n",
    "ML_GRF_stance_N_fd = skfda.FDataGrid(data_matrix=ML_GRF_stance_N_matrix,grid_points=grid_points_100)\n",
    "ML_GRF_stance_N_mean = skfda.exploratory.stats.mean(ML_GRF_stance_N_fd)\n",
    "\n",
    "AP_GRF_stance_N_fd = skfda.FDataGrid(data_matrix=AP_GRF_stance_N_matrix,grid_points=grid_points_100)\n",
    "AP_GRF_stance_N_mean = skfda.exploratory.stats.mean(AP_GRF_stance_N_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49df8445",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_GRF_stance_N_mean_array = np.zeros((15696,100))\n",
    "for j in range(0,15696):\n",
    "    for i in range(0,100):\n",
    "        val = V_GRF_stance_N_mean.data_matrix[0,i,0]\n",
    "        V_GRF_stance_N_mean_array[j,i] = val\n",
    "    \n",
    "ML_GRF_stance_N_mean_array = np.zeros((15696,100))\n",
    "for j in range(0,15696):\n",
    "    for i in range(0,100):\n",
    "        val = ML_GRF_stance_N_mean.data_matrix[0,i,0]\n",
    "        ML_GRF_stance_N_mean_array[j,i] = val\n",
    "    \n",
    "AP_GRF_stance_N_mean_array = np.zeros((15696,100))\n",
    "for j in range(0,15696):\n",
    "    for i in range(0,100):\n",
    "        val = AP_GRF_stance_N_mean.data_matrix[0,i,0]\n",
    "        AP_GRF_stance_N_mean_array[j,i] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b9e7a",
   "metadata": {},
   "source": [
    "# Defining VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf3f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoVAE(nn.Module):\n",
    "    def __init__(self,nfeat=100, ncode=5, alpha=0, lambd=10000, nhidden=128, nhidden2=35, dropout=0.2):\n",
    "        super(InfoVAE, self).__init__()\n",
    "        \n",
    "        self.ncode = int(ncode)\n",
    "        self.alpha = float(alpha)\n",
    "        self.lambd = float(lambd)\n",
    "        \n",
    "        self.encd = nn.Linear(nfeat, nhidden)\n",
    "        self.d1 = nn.Dropout(p=dropout)\n",
    "        self.enc2 = nn.Linear(nhidden, nhidden2)\n",
    "        self.d2 = nn.Dropout(p=dropout)\n",
    "        self.mu = nn.Linear(nhidden2, ncode)\n",
    "        self.lv = nn.Linear(nhidden2, ncode)\n",
    "        \n",
    "        self.decd = nn.Linear(ncode, nhidden2)\n",
    "        self.d3 = nn.Dropout(p=dropout)\n",
    "        self.dec2 = nn.Linear(nhidden2, nhidden)\n",
    "        self.d4 = nn.Dropout(p=dropout)\n",
    "        self.outp = nn.Linear(nhidden, nfeat)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.d1(F.leaky_relu(self.encd(x)))\n",
    "        x = self.d2(F.leaky_relu(self.enc2(x)))\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.lv(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.d3(F.leaky_relu(self.decd(x)))\n",
    "        x = self.d4(F.leaky_relu(self.dec2(x)))\n",
    "        x = self.outp(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    # https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n",
    "    def compute_kernel(self, x, y):\n",
    "        x_size = x.size(0)\n",
    "        y_size = y.size(0)\n",
    "        dim = x.size(1)\n",
    "        x = x.unsqueeze(1) # (x_size, 1, dim)\n",
    "        y = y.unsqueeze(0) # (1, y_size, dim)\n",
    "        tiled_x = x.expand(x_size, y_size, dim)\n",
    "        tiled_y = y.expand(x_size, y_size, dim)\n",
    "        # The example code divides by (dim) here, making <kernel_input> ~ 1/dim\n",
    "        # excluding (dim) makes <kernel_input> ~ 1\n",
    "        kernel_input = (tiled_x - tiled_y).pow(2).mean(2)#/float(dim)\n",
    "        return torch.exp(-kernel_input) # (x_size, y_size)\n",
    "    \n",
    "    # https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n",
    "    def compute_mmd(self, x, y):\n",
    "        xx_kernel = self.compute_kernel(x,x)\n",
    "        yy_kernel = self.compute_kernel(y,y)\n",
    "        xy_kernel = self.compute_kernel(x,y)\n",
    "        return torch.mean(xx_kernel) + torch.mean(yy_kernel) - 2*torch.mean(xy_kernel)\n",
    "    \n",
    "    def loss(self, x, epoch):\n",
    "        recon_x, mu, logvar = self.forward(x)\n",
    "        MSE = torch.sum(0.5 *  (x - recon_x).pow(2))\n",
    "        \n",
    "        # KL divergence (Kingma and Welling, https://arxiv.org/abs/1312.6114, Appendix B)\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        #return MSE + self.beta*KLD, MSE\n",
    "                \n",
    "        # https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n",
    "        true_samples = Variable(torch.randn(200, self.ncode), requires_grad=False)\n",
    "        z = self.reparameterize(mu, logvar) #duplicate call\n",
    "        # compute MMD ~ 1, so upweight to match KLD which is ~ n_batch x n_code\n",
    "        MMD = self.compute_mmd(true_samples,z) * x.size(0) * self.ncode\n",
    "        return MSE + (1-self.alpha)*KLD + (self.lambd+self.alpha-1)*MMD, MSE, KLD, MMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6391eac9",
   "metadata": {},
   "source": [
    "# Preparing Test and Train datasets for model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf5caf4",
   "metadata": {},
   "source": [
    "# MLGRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3099c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfrac = 0.75\n",
    "ntrain = int(ML_GRF_stance_N_matrix.shape[0] * trainfrac)\n",
    "nvalid = ML_GRF_stance_N_matrix.shape[0] - ntrain\n",
    "nfeat = ML_GRF_stance_N_matrix.shape[1]\n",
    "np.random.seed(20190425) # make validation set deterministic\n",
    "permutation = np.random.permutation(ML_GRF_stance_N_matrix.shape[0])\n",
    "np.random.seed()\n",
    "trainidx = permutation[0:ntrain]\n",
    "valididx = permutation[-1-nvalid:-1]\n",
    "\n",
    "train_mlgrf = ML_GRF_stance_N_matrix[trainidx,:]\n",
    "valid_mlgrf = ML_GRF_stance_N_matrix[valididx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2c9f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.tensor(train_mlgrf, dtype=torch.float32)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "valdloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.tensor(valid_mlgrf, dtype=torch.float32)),\n",
    "    batch_size=nvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea9f88",
   "metadata": {},
   "source": [
    "## Defining Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc0c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(): #model, optimizer, epoch, min_valid_loss, badepochs\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_logL = 0\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        spectrum = data[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss, logL, KLD, MMD = model.loss(spectrum, epoch)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        train_logL += logL.item()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(dataloader.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_logL = 0\n",
    "        valid_KLD = 0\n",
    "        valid_MMD = 0\n",
    "\n",
    "        for valddata in valdloader:\n",
    "            spectrum = valddata[0]\n",
    "            loss, logL, KLD, MMD = model.loss(spectrum, epoch)\n",
    "            valid_loss += loss.item()\n",
    "            valid_logL += logL.item()\n",
    "            valid_KLD += KLD.item()\n",
    "            valid_MMD += MMD.item()\n",
    "        \n",
    "        valid_loss /= len(valdloader.dataset)\n",
    "        valid_logL /= -len(valdloader.dataset)\n",
    "        valid_KLD  /= len(valdloader.dataset)\n",
    "        valid_MMD  /= len(valdloader.dataset)\n",
    "    return valid_loss, valid_logL, valid_KLD, valid_MMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97b38cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, precision=1e-3, patience=10):\n",
    "        self.precision = precision\n",
    "        self.patience = patience\n",
    "        self.badepochs = 0\n",
    "        self.min_valid_loss = float('inf')\n",
    "        \n",
    "    def step(self, valid_loss):\n",
    "        if valid_loss < self.min_valid_loss*(1-self.precision):\n",
    "            self.badepochs = 0\n",
    "            self.min_valid_loss = valid_loss\n",
    "        else:\n",
    "            self.badepochs += 1\n",
    "        return not (self.badepochs == self.patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a724bb",
   "metadata": {},
   "source": [
    "# Setting training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcfac0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "log_interval = 10\n",
    "mdl_ncode_l = range(2,11,2)\n",
    "n_config_l = range(100)\n",
    "test_list = [n_config_l,mdl_ncode_l]\n",
    "\n",
    "mdl_MSE = np.zeros((100, 5))\n",
    "mdl_KLD = np.zeros((100, 5))\n",
    "mdl_MMD = np.zeros((100, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5ee1882",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/GRFx'\n",
    "# Parent Directory path\n",
    "parent_dir = '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/'\n",
    "# Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851fb58a",
   "metadata": {},
   "source": [
    "# Training 100 models for 5 different encoder shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "853f5e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config 0, alpha = 0.0, lambda = 2694.9, dropout = 0.00; 2 hidden layers with 17, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.32e+03 logL: -7.06e+03 KL: 7.92e+01 MMD: 8.09e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 7.99e+03 logL: -5.74e+03 KL: 6.68e+01 MMD: 8.10e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 7.23e+03 logL: -5.19e+03 KL: 9.19e+01 MMD: 7.25e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 6.63e+03 logL: -4.66e+03 KL: 8.49e+01 MMD: 7.02e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 5.42e+03 logL: -4.83e+03 KL: 5.97e+00 MMD: 2.14e-01\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.05e+04 logL: -2.92e+04 KL: 1.16e+02 MMD: 4.31e-01\n",
      "Stopping\n",
      "====> Epoch: 63 VALIDATION Loss: 3.01e+04 logL: -2.87e+04 KL: 1.16e+02 MMD: 4.72e-01\n",
      "config 0, alpha = 0.0, lambda = 9.3, dropout = 0.00; 2 hidden layers with 27, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.65e+03 logL: -5.48e+03 KL: 1.66e+02 MMD: 1.20e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.28e+03 logL: -5.19e+03 KL: 8.50e+01 MMD: 1.32e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.80e+03 logL: -3.72e+03 KL: 7.15e+01 MMD: 1.23e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.68e+03 logL: -2.60e+03 KL: 7.32e+01 MMD: 1.01e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.60e+03 logL: -2.54e+03 KL: 4.44e+01 MMD: 1.13e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.55e+03 logL: -2.51e+03 KL: 3.42e+01 MMD: 1.10e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.55e+03 logL: -2.51e+03 KL: 2.91e+01 MMD: 1.12e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.54e+03 logL: -2.51e+03 KL: 2.62e+01 MMD: 9.89e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.90e+03 logL: -1.86e+03 KL: 3.20e+01 MMD: 1.14e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.86e+03 logL: -1.82e+03 KL: 3.10e+01 MMD: 1.01e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 2.85e+01 MMD: 1.11e+00\n",
      "Epoch 00119: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.82e+03 logL: -1.79e+03 KL: 2.72e+01 MMD: 1.07e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.82e+03 logL: -1.79e+03 KL: 2.71e+01 MMD: 9.33e-01\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 136 VALIDATION Loss: 1.82e+03 logL: -1.79e+03 KL: 2.70e+01 MMD: 9.68e-01\n",
      "config 0, alpha = 0.0, lambda = 787.7, dropout = 0.00; 2 hidden layers with 84, 65 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.18e+03 logL: -1.00e+03 KL: 1.30e+02 MMD: 1.34e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.70e+03 logL: -1.03e+03 KL: 3.50e+01 MMD: 8.05e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.10e+03 logL: -1.02e+03 KL: 3.10e+01 MMD: 6.93e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.03e+03 logL: -9.68e+02 KL: 3.07e+01 MMD: 4.55e-02\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.01e+03 logL: -9.43e+02 KL: 3.07e+01 MMD: 4.36e-02\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.01e+03 logL: -9.40e+02 KL: 3.05e+01 MMD: 4.79e-02\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.00e+03 logL: -9.39e+02 KL: 3.05e+01 MMD: 4.43e-02\n",
      "Stopping\n",
      "====> Epoch: 71 VALIDATION Loss: 1.01e+03 logL: -9.39e+02 KL: 3.05e+01 MMD: 4.86e-02\n",
      "config 0, alpha = 0.0, lambda = 85.9, dropout = 0.00; 2 hidden layers with 10, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.22e+03 logL: -2.94e+03 KL: 1.27e+02 MMD: 1.80e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.11e+03 logL: -1.87e+03 KL: 9.03e+01 MMD: 1.82e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.05e+03 logL: -1.84e+03 KL: 6.41e+01 MMD: 1.76e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.02e+03 logL: -1.84e+03 KL: 4.89e+01 MMD: 1.59e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.96e+03 logL: -1.83e+03 KL: 3.94e+01 MMD: 1.04e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.93e+03 logL: -1.83e+03 KL: 3.61e+01 MMD: 7.51e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.90e+03 logL: -1.82e+03 KL: 3.48e+01 MMD: 5.66e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.89e+03 logL: -1.82e+03 KL: 3.39e+01 MMD: 4.56e-01\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.87e+03 logL: -1.80e+03 KL: 3.38e+01 MMD: 4.78e-01\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00098: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.88e+03 logL: -1.80e+03 KL: 3.38e+01 MMD: 5.19e-01\n",
      "Stopping\n",
      "====> Epoch: 102 VALIDATION Loss: 1.87e+03 logL: -1.80e+03 KL: 3.38e+01 MMD: 4.32e-01\n",
      "config 0, alpha = 0.0, lambda = 63.7, dropout = 0.00; 2 hidden layers with 147, 46 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.40e+02 logL: -6.28e+02 KL: 9.04e+01 MMD: 1.94e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.64e+02 logL: -5.11e+02 KL: 6.08e+01 MMD: 1.47e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.41e+02 logL: -4.43e+02 KL: 5.04e+01 MMD: 7.58e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 4.91e+02 logL: -4.22e+02 KL: 4.68e+01 MMD: 3.60e-01\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.71e+02 logL: -4.05e+02 KL: 4.73e+01 MMD: 3.00e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 4.66e+02 logL: -4.04e+02 KL: 4.71e+01 MMD: 2.33e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 4.64e+02 logL: -4.02e+02 KL: 4.72e+01 MMD: 2.32e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 4.63e+02 logL: -4.03e+02 KL: 4.68e+01 MMD: 2.17e-01\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 4.61e+02 logL: -4.01e+02 KL: 4.66e+01 MMD: 2.16e-01\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 98 VALIDATION Loss: 4.62e+02 logL: -4.00e+02 KL: 4.68e+01 MMD: 2.36e-01\n",
      "config 1, alpha = 0.0, lambda = 73.6, dropout = 0.00; 2 hidden layers with 7, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.54e+03 logL: -5.35e+03 KL: 1.28e+02 MMD: 7.84e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.72e+03 logL: -3.57e+03 KL: 8.76e+01 MMD: 7.40e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.35e+03 logL: -3.23e+03 KL: 5.52e+01 MMD: 8.06e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.18e+03 logL: -3.09e+03 KL: 3.87e+01 MMD: 7.30e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.19e+03 logL: -3.11e+03 KL: 3.32e+01 MMD: 7.36e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.12e+03 logL: -3.04e+03 KL: 2.52e+01 MMD: 7.12e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.11e+03 logL: -3.03e+03 KL: 2.14e+01 MMD: 7.53e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.13e+03 logL: -3.06e+03 KL: 1.85e+01 MMD: 6.04e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.13e+03 logL: -3.06e+03 KL: 1.72e+01 MMD: 6.55e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.05e+03 logL: -2.99e+03 KL: 1.39e+01 MMD: 5.37e-01\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 109 VALIDATION Loss: 3.08e+03 logL: -3.03e+03 KL: 1.28e+01 MMD: 4.64e-01\n",
      "config 1, alpha = 0.0, lambda = 46.8, dropout = 0.00; 2 hidden layers with 183, 98 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.95e+03 logL: -1.83e+03 KL: 6.47e+01 MMD: 1.14e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.80e+03 logL: -1.70e+03 KL: 4.68e+01 MMD: 1.15e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.69e+03 logL: -1.60e+03 KL: 3.85e+01 MMD: 9.86e-01\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.60e+03 logL: -1.53e+03 KL: 3.25e+01 MMD: 9.59e-01\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.58e+03 logL: -1.51e+03 KL: 3.16e+01 MMD: 8.49e-01\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 59 VALIDATION Loss: 1.59e+03 logL: -1.52e+03 KL: 3.15e+01 MMD: 8.68e-01\n",
      "config 1, alpha = 0.0, lambda = 1.3, dropout = 0.00; 2 hidden layers with 61, 41 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.46e+03 logL: -1.36e+03 KL: 1.02e+02 MMD: 1.37e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.02e+03 logL: -9.56e+02 KL: 6.48e+01 MMD: 1.35e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.99e+02 logL: -9.50e+02 KL: 4.88e+01 MMD: 1.29e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.85e+02 logL: -9.42e+02 KL: 4.27e+01 MMD: 1.23e+00\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 9.68e+02 logL: -9.25e+02 KL: 4.18e+01 MMD: 1.30e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.66e+02 logL: -9.25e+02 KL: 4.06e+01 MMD: 1.31e+00\n",
      "Stopping\n",
      "====> Epoch: 61 VALIDATION Loss: 9.65e+02 logL: -9.25e+02 KL: 4.04e+01 MMD: 1.27e+00\n",
      "config 1, alpha = 0.0, lambda = 629.5, dropout = 0.00; 2 hidden layers with 14, 10 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 5.42e+03 logL: -4.20e+03 KL: 1.55e+02 MMD: 1.69e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.76e+03 logL: -3.63e+03 KL: 7.74e+01 MMD: 1.66e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.82e+03 logL: -2.60e+03 KL: 3.36e+01 MMD: 3.03e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.71e+03 logL: -2.55e+03 KL: 3.35e+01 MMD: 1.97e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.50e+03 logL: -2.36e+03 KL: 3.27e+01 MMD: 1.71e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.56e+03 logL: -1.43e+03 KL: 3.36e+01 MMD: 1.49e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.43e+03 logL: -1.32e+03 KL: 3.01e+01 MMD: 1.23e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.40e+03 logL: -1.30e+03 KL: 2.86e+01 MMD: 1.03e-01\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.38e+03 logL: -1.28e+03 KL: 2.73e+01 MMD: 1.23e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.34e+03 logL: -1.28e+03 KL: 2.70e+01 MMD: 5.97e-02\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 104 VALIDATION Loss: 1.35e+03 logL: -1.28e+03 KL: 2.71e+01 MMD: 7.49e-02\n",
      "config 1, alpha = 0.0, lambda = 202.5, dropout = 0.00; 2 hidden layers with 101, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.33e+03 logL: -4.75e+03 KL: 1.16e+02 MMD: 2.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.26e+03 logL: -3.69e+03 KL: 1.09e+02 MMD: 2.26e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.11e+03 logL: -3.66e+03 KL: 6.35e+01 MMD: 1.90e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.77e+03 logL: -3.63e+03 KL: 3.05e+01 MMD: 5.52e-01\n",
      "====> Epoch: 50 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 56 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "config 2, alpha = 0.0, lambda = 155.9, dropout = 0.00; 2 hidden layers with 22, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.81e+03 logL: -3.63e+03 KL: 6.48e+01 MMD: 7.60e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.70e+03 logL: -3.55e+03 KL: 3.88e+01 MMD: 7.28e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.46e+03 logL: -3.33e+03 KL: 3.05e+01 MMD: 6.60e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.26e+03 logL: -3.17e+03 KL: 2.11e+01 MMD: 4.07e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.19e+03 logL: -3.12e+03 KL: 1.77e+01 MMD: 3.03e-01\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.15e+03 logL: -3.09e+03 KL: 1.62e+01 MMD: 2.70e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.10e+03 logL: -3.05e+03 KL: 1.58e+01 MMD: 2.19e-01\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.12e+03 logL: -3.07e+03 KL: 1.60e+01 MMD: 2.23e-01\n",
      "Stopping\n",
      "====> Epoch: 80 VALIDATION Loss: 3.12e+03 logL: -3.07e+03 KL: 1.60e+01 MMD: 2.23e-01\n",
      "config 2, alpha = 0.0, lambda = 16181.5, dropout = 0.00; 2 hidden layers with 9, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.00e+03 logL: -6.08e+03 KL: 4.58e+00 MMD: 5.68e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.54e+03 logL: -5.21e+03 KL: 5.61e+00 MMD: 1.97e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.79e+03 logL: -4.41e+03 KL: 6.84e+00 MMD: 2.29e-02\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 4.52e+03 logL: -4.19e+03 KL: 7.79e+00 MMD: 1.94e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 4.57e+03 logL: -4.15e+03 KL: 8.27e+00 MMD: 2.52e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 4.45e+03 logL: -3.97e+03 KL: 8.69e+00 MMD: 2.90e-02\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 4.30e+03 logL: -3.92e+03 KL: 8.81e+00 MMD: 2.33e-02\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 4.46e+03 logL: -3.90e+03 KL: 8.85e+00 MMD: 3.40e-02\n",
      "config 2, alpha = 0.0, lambda = 10.0, dropout = 0.00; 2 hidden layers with 25, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.41e+03 logL: -4.17e+03 KL: 2.29e+02 MMD: 1.33e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.74e+03 logL: -3.64e+03 KL: 8.46e+01 MMD: 1.60e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.65e+03 logL: -2.56e+03 KL: 7.25e+01 MMD: 1.56e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.00e+03 logL: -1.92e+03 KL: 6.36e+01 MMD: 1.53e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.89e+03 logL: -1.83e+03 KL: 4.87e+01 MMD: 1.39e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.86e+03 logL: -1.81e+03 KL: 4.26e+01 MMD: 1.46e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 3.96e+01 MMD: 1.39e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 4.21e+01 MMD: 1.28e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.93e+02 logL: -9.42e+02 KL: 4.09e+01 MMD: 1.11e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.89e+02 logL: -9.40e+02 KL: 3.65e+01 MMD: 1.39e+00\n",
      "Stopping\n",
      "====> Epoch: 107 VALIDATION Loss: 9.85e+02 logL: -9.40e+02 KL: 3.41e+01 MMD: 1.21e+00\n",
      "config 2, alpha = 0.0, lambda = 6.4, dropout = 0.00; 2 hidden layers with 29, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.07e+03 logL: -3.90e+03 KL: 1.63e+02 MMD: 1.74e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.74e+03 logL: -2.62e+03 KL: 1.07e+02 MMD: 1.87e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.58e+03 logL: -2.51e+03 KL: 5.92e+01 MMD: 1.94e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.88e+03 logL: -1.81e+03 KL: 5.57e+01 MMD: 1.78e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.35e+03 logL: -1.29e+03 KL: 5.28e+01 MMD: 1.68e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.34e+03 logL: -1.29e+03 KL: 4.61e+01 MMD: 1.69e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 4.10e+01 MMD: 1.42e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 3.82e+01 MMD: 1.56e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.89e+02 logL: -9.39e+02 KL: 4.23e+01 MMD: 1.50e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.89e+02 logL: -9.41e+02 KL: 3.98e+01 MMD: 1.40e+00\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 9.78e+02 logL: -9.31e+02 KL: 3.87e+01 MMD: 1.44e+00\n",
      "Epoch 00110: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 114 VALIDATION Loss: 9.76e+02 logL: -9.30e+02 KL: 3.86e+01 MMD: 1.34e+00\n",
      "config 2, alpha = 0.0, lambda = 14.1, dropout = 0.00; 2 hidden layers with 38, 32 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.19e+03 logL: -1.02e+03 KL: 1.43e+02 MMD: 1.98e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.75e+02 logL: -7.72e+02 KL: 7.86e+01 MMD: 1.91e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.38e+02 logL: -6.44e+02 KL: 6.75e+01 MMD: 2.02e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.81e+02 logL: -5.99e+02 KL: 5.90e+01 MMD: 1.79e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.78e+02 logL: -5.02e+02 KL: 5.43e+01 MMD: 1.66e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.31e+02 logL: -4.58e+02 KL: 5.22e+01 MMD: 1.61e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.81e+02 logL: -4.14e+02 KL: 4.98e+01 MMD: 1.36e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.83e+02 logL: -4.20e+02 KL: 4.61e+01 MMD: 1.25e+00\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 4.65e+02 logL: -4.04e+02 KL: 4.46e+01 MMD: 1.23e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.65e+02 logL: -4.04e+02 KL: 4.40e+01 MMD: 1.33e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 4.64e+02 logL: -4.06e+02 KL: 4.34e+01 MMD: 1.13e+00\n",
      "Stopping\n",
      "====> Epoch: 115 VALIDATION Loss: 4.61e+02 logL: -4.05e+02 KL: 4.31e+01 MMD: 1.02e+00\n",
      "config 3, alpha = 0.0, lambda = 15.3, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.60e+03 logL: -5.49e+03 KL: 9.83e+01 MMD: 6.85e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.31e+03 logL: -5.24e+03 KL: 5.79e+01 MMD: 6.68e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 4.82e+03 logL: -4.77e+03 KL: 3.35e+01 MMD: 7.91e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 4.59e+03 logL: -4.56e+03 KL: 2.52e+01 MMD: 7.29e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 4.04e+03 logL: -4.00e+03 KL: 3.48e+01 MMD: 8.54e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.72e+03 logL: -3.67e+03 KL: 4.33e+01 MMD: 8.34e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.68e+03 logL: -3.64e+03 KL: 3.43e+01 MMD: 7.51e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.67e+03 logL: -3.63e+03 KL: 2.80e+01 MMD: 8.44e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.66e+03 logL: -3.62e+03 KL: 2.39e+01 MMD: 7.77e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.65e+03 logL: -3.62e+03 KL: 2.14e+01 MMD: 8.70e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 3.64e+03 logL: -3.61e+03 KL: 1.93e+01 MMD: 8.67e-01\n",
      "Epoch 00119: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 120 VALIDATION Loss: 3.64e+03 logL: -3.61e+03 KL: 1.76e+01 MMD: 7.83e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 3.63e+03 logL: -3.61e+03 KL: 1.73e+01 MMD: 7.67e-01\n",
      "Stopping\n",
      "====> Epoch: 131 VALIDATION Loss: 3.64e+03 logL: -3.61e+03 KL: 1.73e+01 MMD: 8.04e-01\n",
      "config 3, alpha = 0.0, lambda = 59255.7, dropout = 0.00; 2 hidden layers with 90, 32 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.05e+03 logL: -5.16e+03 KL: 4.76e+00 MMD: 1.50e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 6.41e+03 logL: -4.75e+03 KL: 6.98e+00 MMD: 2.78e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 5.81e+03 logL: -4.59e+03 KL: 7.52e+00 MMD: 2.05e-02\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 5.04e+03 logL: -4.41e+03 KL: 8.18e+00 MMD: 1.04e-02\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 44 VALIDATION Loss: 5.41e+03 logL: -4.39e+03 KL: 8.26e+00 MMD: 1.71e-02\n",
      "config 3, alpha = 0.0, lambda = 48.7, dropout = 0.00; 2 hidden layers with 26, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.88e+03 logL: -3.71e+03 KL: 9.47e+01 MMD: 1.77e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.69e+03 logL: -2.54e+03 KL: 6.81e+01 MMD: 1.57e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.15e+03 logL: -2.00e+03 KL: 7.67e+01 MMD: 1.56e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.01e+03 logL: -1.88e+03 KL: 4.99e+01 MMD: 1.49e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.93e+03 logL: -1.83e+03 KL: 4.07e+01 MMD: 1.27e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.38e+03 logL: -1.28e+03 KL: 4.35e+01 MMD: 1.18e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.37e+03 logL: -1.29e+03 KL: 3.56e+01 MMD: 9.49e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.34e+03 logL: -1.27e+03 KL: 3.17e+01 MMD: 7.51e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.33e+03 logL: -1.27e+03 KL: 3.03e+01 MMD: 6.06e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 2.87e+01 MMD: 4.12e-01\n",
      "Epoch 00106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 2.83e+01 MMD: 3.46e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 2.81e+01 MMD: 3.97e-01\n",
      "Stopping\n",
      "====> Epoch: 120 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 2.81e+01 MMD: 3.97e-01\n",
      "config 3, alpha = 0.0, lambda = 1467.9, dropout = 0.00; 2 hidden layers with 117, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.54e+03 logL: -1.92e+03 KL: 1.54e+02 MMD: 1.69e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.03e+03 logL: -1.47e+03 KL: 3.69e+01 MMD: 3.60e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.20e+03 logL: -9.45e+02 KL: 4.04e+01 MMD: 1.46e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.27e+02 logL: -7.72e+02 KL: 4.15e+01 MMD: 7.72e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 7.74e+02 logL: -6.45e+02 KL: 4.12e+01 MMD: 5.93e-02\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 7.27e+02 logL: -6.13e+02 KL: 4.13e+01 MMD: 4.96e-02\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 69 VALIDATION Loss: 7.37e+02 logL: -6.08e+02 KL: 4.09e+01 MMD: 6.04e-02\n",
      "config 3, alpha = 0.0, lambda = 31.3, dropout = 0.00; 2 hidden layers with 15, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.77e+03 logL: -3.60e+03 KL: 1.11e+02 MMD: 2.17e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.67e+03 logL: -2.53e+03 KL: 7.52e+01 MMD: 2.23e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.65e+03 logL: -1.51e+03 KL: 7.74e+01 MMD: 2.10e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.42e+03 logL: -1.30e+03 KL: 6.21e+01 MMD: 1.91e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.09e+03 logL: -9.64e+02 KL: 6.78e+01 MMD: 1.91e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.06e+03 logL: -9.55e+02 KL: 5.25e+01 MMD: 1.70e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.04e+03 logL: -9.51e+02 KL: 4.78e+01 MMD: 1.45e+00\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.02e+03 logL: -9.41e+02 KL: 4.55e+01 MMD: 1.28e+00\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.02e+03 logL: -9.40e+02 KL: 4.49e+01 MMD: 1.30e+00\n",
      "Stopping\n",
      "====> Epoch: 93 VALIDATION Loss: 1.02e+03 logL: -9.40e+02 KL: 4.50e+01 MMD: 1.32e+00\n",
      "config 4, alpha = 0.0, lambda = 43546.8, dropout = 0.00; 2 hidden layers with 32, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.11e+03 logL: -4.06e+03 KL: 1.01e+01 MMD: 2.40e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.80e+03 logL: -3.54e+03 KL: 1.09e+01 MMD: 5.77e-03\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.76e+03 logL: -3.48e+03 KL: 1.12e+01 MMD: 6.14e-03\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 4.11e+03 logL: -3.44e+03 KL: 1.21e+01 MMD: 1.50e-02\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.68e+03 logL: -3.44e+03 KL: 1.23e+01 MMD: 5.19e-03\n",
      "Stopping\n",
      "====> Epoch: 51 VALIDATION Loss: 3.63e+03 logL: -3.44e+03 KL: 1.23e+01 MMD: 4.06e-03\n",
      "config 4, alpha = 0.0, lambda = 1201.5, dropout = 0.00; 2 hidden layers with 13, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.92e+03 logL: -5.49e+03 KL: 9.24e+01 MMD: 1.12e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.29e+03 logL: -4.98e+03 KL: 8.65e+01 MMD: 1.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.24e+03 logL: -3.92e+03 KL: 2.36e+01 MMD: 2.41e-01\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 4.43e+03 logL: -4.02e+03 KL: 2.35e+01 MMD: 3.25e-01\n",
      "Stopping\n",
      "====> Epoch: 40 VALIDATION Loss: 4.43e+03 logL: -4.02e+03 KL: 2.35e+01 MMD: 3.25e-01\n",
      "config 4, alpha = 0.0, lambda = 398.3, dropout = 0.00; 2 hidden layers with 21, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.32e+03 logL: -3.66e+03 KL: 1.38e+02 MMD: 1.31e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.21e+03 logL: -2.54e+03 KL: 8.12e+01 MMD: 1.47e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.84e+03 logL: -2.35e+03 KL: 3.98e+01 MMD: 1.15e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.00e+03 logL: -1.86e+03 KL: 3.01e+01 MMD: 2.65e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.93e+03 logL: -1.85e+03 KL: 2.97e+01 MMD: 1.37e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.90e+03 logL: -1.82e+03 KL: 2.91e+01 MMD: 1.26e-01\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.88e+03 logL: -1.81e+03 KL: 2.87e+01 MMD: 9.62e-02\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.88e+03 logL: -1.81e+03 KL: 2.86e+01 MMD: 1.07e-01\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 89 VALIDATION Loss: 1.88e+03 logL: -1.81e+03 KL: 2.86e+01 MMD: 1.12e-01\n",
      "config 4, alpha = 0.0, lambda = 1.5, dropout = 0.00; 2 hidden layers with 10, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.79e+03 logL: -3.68e+03 KL: 1.13e+02 MMD: 2.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.65e+03 logL: -2.57e+03 KL: 7.83e+01 MMD: 2.13e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.22e+03 logL: -2.15e+03 KL: 7.32e+01 MMD: 1.75e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.90e+03 logL: -1.83e+03 KL: 6.14e+01 MMD: 2.02e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.86e+03 logL: -1.80e+03 KL: 5.36e+01 MMD: 1.92e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+03 logL: -1.29e+03 KL: 5.59e+01 MMD: 1.80e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.34e+03 logL: -1.29e+03 KL: 4.98e+01 MMD: 1.92e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.33e+03 logL: -1.29e+03 KL: 4.64e+01 MMD: 1.95e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.33e+03 logL: -1.28e+03 KL: 4.39e+01 MMD: 2.04e+00\n",
      "Epoch 00099: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.31e+03 logL: -1.27e+03 KL: 4.17e+01 MMD: 1.84e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.31e+03 logL: -1.27e+03 KL: 4.15e+01 MMD: 1.84e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.31e+03 logL: -1.27e+03 KL: 4.06e+01 MMD: 1.90e+00\n",
      "Stopping\n",
      "====> Epoch: 120 VALIDATION Loss: 1.31e+03 logL: -1.27e+03 KL: 4.06e+01 MMD: 1.90e+00\n",
      "config 4, alpha = 0.0, lambda = 5.9, dropout = 0.00; 2 hidden layers with 109, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.96e+03 logL: -1.86e+03 KL: 9.37e+01 MMD: 2.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.05e+03 logL: -9.68e+02 KL: 7.31e+01 MMD: 2.20e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.01e+03 logL: -9.43e+02 KL: 5.99e+01 MMD: 2.10e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.00e+03 logL: -9.38e+02 KL: 5.53e+01 MMD: 1.86e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 50 VALIDATION Loss: 1.00e+03 logL: -9.44e+02 KL: 5.09e+01 MMD: 1.65e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.82e+02 logL: -9.27e+02 KL: 4.76e+01 MMD: 1.60e+00\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.70e+02 logL: -9.15e+02 KL: 4.72e+01 MMD: 1.45e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.66e+02 logL: -9.12e+02 KL: 4.67e+01 MMD: 1.54e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 8.46e+02 logL: -7.68e+02 KL: 6.73e+01 MMD: 2.22e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 8.16e+02 logL: -7.46e+02 KL: 6.02e+01 MMD: 2.12e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 8.07e+02 logL: -7.41e+02 KL: 5.56e+01 MMD: 2.13e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 8.02e+02 logL: -7.38e+02 KL: 5.29e+01 MMD: 2.11e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 7.97e+02 logL: -7.37e+02 KL: 5.09e+01 MMD: 1.90e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 7.96e+02 logL: -7.38e+02 KL: 4.92e+01 MMD: 1.85e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 7.94e+02 logL: -7.37e+02 KL: 4.82e+01 MMD: 1.85e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 7.92e+02 logL: -7.37e+02 KL: 4.69e+01 MMD: 1.68e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 7.89e+02 logL: -7.35e+02 KL: 4.59e+01 MMD: 1.65e+00\n",
      "Epoch 00171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00178: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 180 VALIDATION Loss: 7.88e+02 logL: -7.34e+02 KL: 4.58e+01 MMD: 1.78e+00\n",
      "Stopping\n",
      "====> Epoch: 182 VALIDATION Loss: 7.88e+02 logL: -7.34e+02 KL: 4.58e+01 MMD: 1.65e+00\n",
      "config 5, alpha = 0.0, lambda = 18293.1, dropout = 0.00; 2 hidden layers with 25, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.85e+03 logL: -5.41e+03 KL: 6.67e+00 MMD: 2.34e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.96e+03 logL: -4.73e+03 KL: 7.15e+00 MMD: 1.23e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.88e+03 logL: -4.72e+03 KL: 8.19e+00 MMD: 8.25e-03\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 4.80e+03 logL: -4.59e+03 KL: 8.86e+00 MMD: 1.11e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 4.74e+03 logL: -4.59e+03 KL: 8.39e+00 MMD: 7.98e-03\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 59 VALIDATION Loss: 4.69e+03 logL: -4.57e+03 KL: 8.64e+00 MMD: 6.20e-03\n",
      "config 5, alpha = 0.0, lambda = 3336.4, dropout = 0.00; 2 hidden layers with 50, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.36e+03 logL: -4.03e+03 KL: 1.34e+02 MMD: 9.57e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.18e+03 logL: -3.00e+03 KL: 1.99e+01 MMD: 4.90e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.77e+03 logL: -2.59e+03 KL: 1.94e+01 MMD: 4.84e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.63e+03 logL: -2.54e+03 KL: 1.84e+01 MMD: 2.19e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 2.64e+03 logL: -2.52e+03 KL: 1.92e+01 MMD: 3.00e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.96e+03 logL: -1.85e+03 KL: 2.53e+01 MMD: 2.57e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 1.98e+03 logL: -1.83e+03 KL: 2.33e+01 MMD: 3.62e-02\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.87e+03 logL: -1.80e+03 KL: 2.25e+01 MMD: 1.51e-02\n",
      "Stopping\n",
      "====> Epoch: 85 VALIDATION Loss: 1.89e+03 logL: -1.80e+03 KL: 2.22e+01 MMD: 2.10e-02\n",
      "config 5, alpha = 0.0, lambda = 589.4, dropout = 0.00; 2 hidden layers with 51, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.77e+03 logL: -1.84e+03 KL: 9.52e+01 MMD: 1.41e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.88e+03 logL: -1.34e+03 KL: 3.86e+01 MMD: 8.48e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.57e+03 logL: -1.44e+03 KL: 3.09e+01 MMD: 1.71e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.45e+03 logL: -1.35e+03 KL: 2.98e+01 MMD: 1.20e-01\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.34e+03 logL: -1.26e+03 KL: 2.96e+01 MMD: 7.93e-02\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.34e+03 logL: -1.26e+03 KL: 2.94e+01 MMD: 9.08e-02\n",
      "Stopping\n",
      "====> Epoch: 60 VALIDATION Loss: 1.34e+03 logL: -1.26e+03 KL: 2.94e+01 MMD: 9.08e-02\n",
      "config 5, alpha = 0.0, lambda = 573.5, dropout = 0.00; 2 hidden layers with 75, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.00e+03 logL: -3.92e+03 KL: 1.43e+02 MMD: 1.63e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.53e+03 logL: -3.55e+03 KL: 6.91e+01 MMD: 1.59e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.51e+03 logL: -3.34e+03 KL: 3.43e+01 MMD: 2.49e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.30e+03 logL: -3.13e+03 KL: 4.11e+01 MMD: 2.24e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.80e+03 logL: -2.65e+03 KL: 3.83e+01 MMD: 2.04e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 2.60e+03 logL: -2.48e+03 KL: 3.16e+01 MMD: 1.58e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.97e+03 logL: -1.87e+03 KL: 2.69e+01 MMD: 1.41e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.93e+03 logL: -1.82e+03 KL: 2.67e+01 MMD: 1.44e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.84e+03 logL: -1.75e+03 KL: 2.54e+01 MMD: 1.05e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.41e+03 logL: -1.30e+03 KL: 3.82e+01 MMD: 1.20e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.37e+03 logL: -1.30e+03 KL: 2.84e+01 MMD: 8.83e-02\n",
      "====> Epoch: 120 VALIDATION Loss: 1.36e+03 logL: -1.30e+03 KL: 2.45e+01 MMD: 6.48e-02\n",
      "Epoch 00122: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 126 VALIDATION Loss: 1.37e+03 logL: -1.32e+03 KL: 2.44e+01 MMD: 4.78e-02\n",
      "config 5, alpha = 0.0, lambda = 1695.5, dropout = 0.00; 2 hidden layers with 33, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.76e+03 logL: -4.47e+03 KL: 7.58e+01 MMD: 7.15e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.17e+03 logL: -2.94e+03 KL: 2.95e+01 MMD: 1.16e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.57e+03 logL: -1.38e+03 KL: 2.87e+01 MMD: 9.40e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.47e+03 logL: -1.34e+03 KL: 2.82e+01 MMD: 6.24e-02\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.43e+03 logL: -1.29e+03 KL: 2.79e+01 MMD: 6.17e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.43e+03 logL: -1.30e+03 KL: 2.76e+01 MMD: 6.13e-02\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.40e+03 logL: -1.29e+03 KL: 2.75e+01 MMD: 5.37e-02\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.44e+03 logL: -1.28e+03 KL: 2.75e+01 MMD: 7.81e-02\n",
      "Stopping\n",
      "====> Epoch: 82 VALIDATION Loss: 1.41e+03 logL: -1.28e+03 KL: 2.75e+01 MMD: 5.64e-02\n",
      "config 6, alpha = 0.0, lambda = 14302.0, dropout = 0.00; 2 hidden layers with 7, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.56e+03 logL: -6.14e+03 KL: 1.90e+00 MMD: 2.95e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.37e+03 logL: -5.16e+03 KL: 3.96e+00 MMD: 1.43e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.77e+03 logL: -4.60e+03 KL: 4.87e+00 MMD: 1.19e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 4.73e+03 logL: -4.55e+03 KL: 5.78e+00 MMD: 1.25e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 4.73e+03 logL: -4.54e+03 KL: 6.08e+00 MMD: 1.30e-02\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 57 VALIDATION Loss: 4.73e+03 logL: -4.53e+03 KL: 6.36e+00 MMD: 1.40e-02\n",
      "config 6, alpha = 0.0, lambda = 128.1, dropout = 0.00; 2 hidden layers with 154, 106 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.94e+03 logL: -1.78e+03 KL: 4.63e+01 MMD: 9.52e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.75e+03 logL: -1.65e+03 KL: 2.53e+01 MMD: 6.51e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.65e+03 logL: -1.60e+03 KL: 2.30e+01 MMD: 1.53e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.61e+03 logL: -1.57e+03 KL: 2.17e+01 MMD: 1.15e-01\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.56e+03 logL: -1.52e+03 KL: 2.06e+01 MMD: 9.43e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.54e+03 logL: -1.51e+03 KL: 2.03e+01 MMD: 6.83e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 1.54e+03 logL: -1.51e+03 KL: 2.00e+01 MMD: 5.98e-02\n",
      "Stopping\n",
      "====> Epoch: 76 VALIDATION Loss: 1.54e+03 logL: -1.51e+03 KL: 1.99e+01 MMD: 9.68e-02\n",
      "config 6, alpha = 0.0, lambda = 937.9, dropout = 0.00; 2 hidden layers with 86, 78 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.84e+03 logL: -1.45e+03 KL: 1.37e+02 MMD: 1.34e+00\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 2.67e+03 logL: -1.33e+03 KL: 7.91e+01 MMD: 1.34e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.38e+03 logL: -1.16e+03 KL: 7.17e+01 MMD: 1.23e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 2.30e+03 logL: -1.06e+03 KL: 5.88e+01 MMD: 1.26e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.82e+03 logL: -1.16e+03 KL: 3.60e+01 MMD: 6.65e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.31e+03 logL: -1.11e+03 KL: 3.31e+01 MMD: 1.79e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.16e+03 logL: -1.07e+03 KL: 3.35e+01 MMD: 6.48e-02\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.12e+03 logL: -1.03e+03 KL: 3.39e+01 MMD: 5.93e-02\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 88 VALIDATION Loss: 1.14e+03 logL: -1.03e+03 KL: 3.40e+01 MMD: 8.11e-02\n",
      "config 6, alpha = 0.0, lambda = 914.9, dropout = 0.00; 2 hidden layers with 84, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.95e+03 logL: -5.26e+03 KL: 1.39e+02 MMD: 1.70e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.54e+03 logL: -3.89e+03 KL: 1.11e+02 MMD: 1.69e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.27e+03 logL: -2.89e+03 KL: 3.05e+01 MMD: 3.86e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.84e+03 logL: -2.64e+03 KL: 3.23e+01 MMD: 1.80e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.14e+03 logL: -1.97e+03 KL: 3.16e+01 MMD: 1.49e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 2.03e+03 logL: -1.87e+03 KL: 2.94e+01 MMD: 1.39e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.48e+03 logL: -1.35e+03 KL: 3.15e+01 MMD: 1.10e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.43e+03 logL: -1.32e+03 KL: 3.03e+01 MMD: 8.93e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 1.39e+03 logL: -1.29e+03 KL: 2.88e+01 MMD: 8.20e-02\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.35e+03 logL: -1.26e+03 KL: 2.86e+01 MMD: 6.95e-02\n",
      "Epoch 00108: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.36e+03 logL: -1.25e+03 KL: 2.84e+01 MMD: 8.97e-02\n",
      "Stopping\n",
      "====> Epoch: 112 VALIDATION Loss: 1.36e+03 logL: -1.25e+03 KL: 2.84e+01 MMD: 8.44e-02\n",
      "config 6, alpha = 0.0, lambda = 39.1, dropout = 0.00; 2 hidden layers with 24, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.95e+03 logL: -2.72e+03 KL: 1.57e+02 MMD: 2.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.84e+03 logL: -1.67e+03 KL: 9.97e+01 MMD: 1.95e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.16e+03 logL: -1.00e+03 KL: 8.10e+01 MMD: 2.01e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.09e+03 logL: -9.53e+02 KL: 6.30e+01 MMD: 1.85e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.89e+02 logL: -7.70e+02 KL: 5.78e+01 MMD: 1.61e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.57e+02 logL: -7.57e+02 KL: 5.09e+01 MMD: 1.29e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.34e+02 logL: -7.46e+02 KL: 4.74e+01 MMD: 1.07e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 7.00e+02 logL: -6.19e+02 KL: 4.54e+01 MMD: 9.19e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 6.80e+02 logL: -6.06e+02 KL: 4.44e+01 MMD: 7.80e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 6.69e+02 logL: -6.05e+02 KL: 4.24e+01 MMD: 5.51e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 6.65e+02 logL: -6.04e+02 KL: 4.20e+01 MMD: 4.83e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 6.55e+02 logL: -5.97e+02 KL: 4.15e+01 MMD: 4.52e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 6.54e+02 logL: -5.99e+02 KL: 4.10e+01 MMD: 3.63e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 6.67e+02 logL: -6.13e+02 KL: 4.01e+01 MMD: 3.80e-01\n",
      "Epoch 00148: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 6.41e+02 logL: -5.90e+02 KL: 4.00e+01 MMD: 3.07e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 6.41e+02 logL: -5.90e+02 KL: 3.99e+01 MMD: 2.94e-01\n",
      "Stopping\n",
      "====> Epoch: 166 VALIDATION Loss: 6.44e+02 logL: -5.90e+02 KL: 4.00e+01 MMD: 3.72e-01\n",
      "config 7, alpha = 0.0, lambda = 80874.1, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.39e+03 logL: -7.74e+03 KL: 9.98e-01 MMD: 8.06e-03\n",
      "====> Epoch: 20 VALIDATION Loss: 7.76e+03 logL: -7.13e+03 KL: 1.50e+00 MMD: 7.73e-03\n",
      "====> Epoch: 30 VALIDATION Loss: 6.80e+03 logL: -5.62e+03 KL: 2.26e+00 MMD: 1.45e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 6.32e+03 logL: -5.25e+03 KL: 2.97e+00 MMD: 1.32e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 6.11e+03 logL: -5.02e+03 KL: 3.48e+00 MMD: 1.34e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 5.19e+03 logL: -4.71e+03 KL: 3.87e+00 MMD: 5.83e-03\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.67e+03 logL: -4.71e+03 KL: 3.79e+00 MMD: 1.17e-02\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 5.09e+03 logL: -4.73e+03 KL: 3.86e+00 MMD: 4.44e-03\n",
      "config 7, alpha = 0.0, lambda = 48000.9, dropout = 0.00; 2 hidden layers with 9, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.47e+03 logL: -8.56e+03 KL: 1.22e+00 MMD: 1.89e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 6.10e+03 logL: -5.53e+03 KL: 2.91e+00 MMD: 1.19e-02\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.28e+03 logL: -4.73e+03 KL: 3.79e+00 MMD: 1.13e-02\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 5.14e+03 logL: -4.68e+03 KL: 3.90e+00 MMD: 9.33e-03\n",
      "Stopping\n",
      "====> Epoch: 41 VALIDATION Loss: 5.75e+03 logL: -4.69e+03 KL: 3.91e+00 MMD: 2.20e-02\n",
      "config 7, alpha = 0.0, lambda = 9216.0, dropout = 0.00; 2 hidden layers with 102, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.44e+03 logL: -6.10e+03 KL: 7.09e+01 MMD: 2.46e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.97e+03 logL: -3.19e+03 KL: 1.53e+01 MMD: 8.29e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.04e+03 logL: -2.71e+03 KL: 1.49e+01 MMD: 3.45e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.13e+03 logL: -2.73e+03 KL: 1.36e+01 MMD: 4.13e-02\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.96e+03 logL: -2.51e+03 KL: 1.79e+01 MMD: 4.76e-02\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 55 VALIDATION Loss: 2.77e+03 logL: -2.50e+03 KL: 1.80e+01 MMD: 2.81e-02\n",
      "config 7, alpha = 0.0, lambda = 30.7, dropout = 0.00; 2 hidden layers with 85, 26 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.68e+03 logL: -2.52e+03 KL: 1.00e+02 MMD: 1.93e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.15e+03 logL: -1.01e+03 KL: 9.07e+01 MMD: 1.71e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.58e+02 logL: -8.42e+02 KL: 6.53e+01 MMD: 1.72e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.52e+02 logL: -7.58e+02 KL: 5.25e+01 MMD: 1.38e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.20e+02 logL: -7.35e+02 KL: 4.73e+01 MMD: 1.30e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.25e+02 logL: -6.51e+02 KL: 4.39e+01 MMD: 1.03e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.75e+02 logL: -6.11e+02 KL: 4.09e+01 MMD: 7.81e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 6.69e+02 logL: -6.14e+02 KL: 3.93e+01 MMD: 5.38e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 6.62e+02 logL: -6.06e+02 KL: 3.79e+01 MMD: 6.11e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 6.42e+02 logL: -5.93e+02 KL: 3.68e+01 MMD: 4.02e-01\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 6.31e+02 logL: -5.84e+02 KL: 3.60e+01 MMD: 3.72e-01\n",
      "Epoch 00119: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 6.28e+02 logL: -5.83e+02 KL: 3.56e+01 MMD: 3.11e-01\n",
      "Stopping\n",
      "====> Epoch: 123 VALIDATION Loss: 6.27e+02 logL: -5.83e+02 KL: 3.56e+01 MMD: 2.82e-01\n",
      "config 7, alpha = 0.0, lambda = 2119.1, dropout = 0.00; 2 hidden layers with 21, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.51e+03 logL: -1.34e+03 KL: 1.50e+02 MMD: 1.90e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.75e+03 logL: -1.25e+03 KL: 5.42e+01 MMD: 2.11e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.38e+03 logL: -9.71e+02 KL: 5.15e+01 MMD: 1.70e-01\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.20e+03 logL: -8.30e+02 KL: 5.23e+01 MMD: 1.49e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.13e+03 logL: -7.91e+02 KL: 5.13e+01 MMD: 1.36e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.02e+03 logL: -7.70e+02 KL: 5.09e+01 MMD: 9.36e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 1.05e+03 logL: -7.75e+02 KL: 5.08e+01 MMD: 1.06e-01\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.94e+02 logL: -7.42e+02 KL: 5.05e+01 MMD: 9.54e-02\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 84 VALIDATION Loss: 9.98e+02 logL: -7.41e+02 KL: 5.05e+01 MMD: 9.75e-02\n",
      "config 8, alpha = 0.0, lambda = 54708.3, dropout = 0.00; 2 hidden layers with 58, 43 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 5.69e+03 logL: -4.09e+03 KL: 9.17e+00 MMD: 2.91e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.14e+03 logL: -3.53e+03 KL: 1.13e+01 MMD: 1.10e-02\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 4.01e+03 logL: -3.42e+03 KL: 1.27e+01 MMD: 1.06e-02\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 39 VALIDATION Loss: 3.63e+03 logL: -3.39e+03 KL: 1.29e+01 MMD: 4.11e-03\n",
      "config 8, alpha = 0.0, lambda = 11140.2, dropout = 0.00; 2 hidden layers with 67, 37 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.66e+03 logL: -3.01e+03 KL: 1.78e+01 MMD: 5.61e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.37e+03 logL: -2.77e+03 KL: 1.69e+01 MMD: 5.22e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.78e+03 logL: -2.60e+03 KL: 1.69e+01 MMD: 1.50e-02\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.72e+03 logL: -2.51e+03 KL: 1.78e+01 MMD: 1.71e-02\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.73e+03 logL: -2.50e+03 KL: 1.79e+01 MMD: 1.95e-02\n",
      "Stopping\n",
      "====> Epoch: 51 VALIDATION Loss: 2.72e+03 logL: -2.50e+03 KL: 1.80e+01 MMD: 1.80e-02\n",
      "config 8, alpha = 0.0, lambda = 408.1, dropout = 0.00; 2 hidden layers with 13, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.46e+03 logL: -3.72e+03 KL: 1.45e+02 MMD: 1.47e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.24e+03 logL: -2.58e+03 KL: 9.79e+01 MMD: 1.38e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.12e+03 logL: -2.57e+03 KL: 4.96e+01 MMD: 1.24e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.75e+03 logL: -2.60e+03 KL: 3.09e+01 MMD: 2.97e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.66e+03 logL: -2.53e+03 KL: 3.07e+01 MMD: 2.25e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 2.10e+03 logL: -2.01e+03 KL: 2.79e+01 MMD: 1.39e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 2.04e+03 logL: -1.97e+03 KL: 2.66e+01 MMD: 1.10e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.97e+03 logL: -1.90e+03 KL: 2.63e+01 MMD: 1.26e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.92e+03 logL: -1.84e+03 KL: 2.58e+01 MMD: 1.25e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.89e+03 logL: -1.82e+03 KL: 2.40e+01 MMD: 1.17e-01\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.88e+03 logL: -1.81e+03 KL: 2.38e+01 MMD: 1.13e-01\n",
      "Epoch 00110: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00117: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 2.38e+01 MMD: 9.61e-02\n",
      "Stopping\n",
      "====> Epoch: 121 VALIDATION Loss: 1.88e+03 logL: -1.81e+03 KL: 2.38e+01 MMD: 1.17e-01\n",
      "config 8, alpha = 0.0, lambda = 1576.1, dropout = 0.00; 2 hidden layers with 30, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.35e+03 logL: -3.55e+03 KL: 1.68e+02 MMD: 1.67e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.76e+03 logL: -1.87e+03 KL: 1.12e+02 MMD: 1.76e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.14e+03 logL: -1.84e+03 KL: 3.97e+01 MMD: 1.65e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.62e+03 logL: -1.35e+03 KL: 3.82e+01 MMD: 1.47e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.30e+03 logL: -1.07e+03 KL: 3.64e+01 MMD: 1.24e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.23e+03 logL: -1.02e+03 KL: 3.51e+01 MMD: 1.11e-01\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.09e+03 logL: -9.50e+02 KL: 3.46e+01 MMD: 6.91e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 1.12e+03 logL: -9.52e+02 KL: 3.46e+01 MMD: 8.54e-02\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.07e+03 logL: -9.39e+02 KL: 3.43e+01 MMD: 6.19e-02\n",
      "Stopping\n",
      "====> Epoch: 93 VALIDATION Loss: 1.07e+03 logL: -9.39e+02 KL: 3.42e+01 MMD: 6.42e-02\n",
      "config 8, alpha = 0.0, lambda = 2.4, dropout = 0.00; 2 hidden layers with 33, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.64e+03 logL: -2.53e+03 KL: 1.04e+02 MMD: 1.95e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+03 logL: -1.30e+03 KL: 8.47e+01 MMD: 2.00e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.36e+03 logL: -1.29e+03 KL: 6.39e+01 MMD: 2.12e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.04e+03 logL: -9.75e+02 KL: 6.44e+01 MMD: 2.01e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.00e+03 logL: -9.42e+02 KL: 5.62e+01 MMD: 2.00e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.97e+02 logL: -9.44e+02 KL: 5.09e+01 MMD: 1.90e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.87e+02 logL: -9.36e+02 KL: 4.83e+01 MMD: 1.55e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.85e+02 logL: -9.37e+02 KL: 4.58e+01 MMD: 1.78e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.73e+02 logL: -9.29e+02 KL: 4.19e+01 MMD: 1.50e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.82e+02 logL: -9.40e+02 KL: 4.02e+01 MMD: 1.63e+00\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 9.56e+02 logL: -9.14e+02 KL: 3.95e+01 MMD: 1.61e+00\n",
      "Epoch 00113: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 9.54e+02 logL: -9.13e+02 KL: 3.94e+01 MMD: 1.58e+00\n",
      "Stopping\n",
      "====> Epoch: 124 VALIDATION Loss: 9.55e+02 logL: -9.13e+02 KL: 3.94e+01 MMD: 1.63e+00\n",
      "config 9, alpha = 0.0, lambda = 303.7, dropout = 0.00; 2 hidden layers with 28, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.91e+03 logL: -3.63e+03 KL: 7.88e+01 MMD: 6.74e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.87e+03 logL: -3.59e+03 KL: 4.90e+01 MMD: 7.77e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.68e+03 logL: -3.42e+03 KL: 4.28e+01 MMD: 7.22e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.51e+03 logL: -3.25e+03 KL: 3.00e+01 MMD: 7.69e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.43e+03 logL: -3.23e+03 KL: 2.03e+01 MMD: 5.82e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.32e+03 logL: -3.19e+03 KL: 1.50e+01 MMD: 3.82e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.26e+03 logL: -3.17e+03 KL: 1.25e+01 MMD: 2.61e-01\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.17e+03 logL: -3.11e+03 KL: 1.19e+01 MMD: 1.60e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.17e+03 logL: -3.11e+03 KL: 1.17e+01 MMD: 1.72e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.16e+03 logL: -3.11e+03 KL: 1.15e+01 MMD: 1.33e-01\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 104 VALIDATION Loss: 3.16e+03 logL: -3.10e+03 KL: 1.15e+01 MMD: 1.44e-01\n",
      "config 9, alpha = 0.0, lambda = 15490.6, dropout = 0.00; 2 hidden layers with 30, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.07e+04 logL: -5.95e+03 KL: 3.46e+02 MMD: 9.32e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 8.38e+03 logL: -6.81e+03 KL: 2.30e+01 MMD: 9.98e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 5.56e+03 logL: -4.84e+03 KL: 8.19e+00 MMD: 4.54e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 4.65e+03 logL: -4.15e+03 KL: 1.02e+01 MMD: 3.16e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 4.19e+03 logL: -3.79e+03 KL: 1.01e+01 MMD: 2.53e-02\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.05e+03 logL: -3.73e+03 KL: 1.03e+01 MMD: 2.01e-02\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: 4.14e+03 logL: -3.71e+03 KL: 1.04e+01 MMD: 2.71e-02\n",
      "config 9, alpha = 0.0, lambda = 9632.9, dropout = 0.00; 2 hidden layers with 13, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.36e+03 logL: -4.90e+03 KL: 1.12e+01 MMD: 4.66e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.29e+03 logL: -2.95e+03 KL: 1.92e+01 MMD: 3.29e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.95e+03 logL: -2.66e+03 KL: 1.58e+01 MMD: 2.90e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.97e+03 logL: -2.59e+03 KL: 1.59e+01 MMD: 3.72e-02\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.84e+03 logL: -2.57e+03 KL: 1.66e+01 MMD: 2.58e-02\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 59 VALIDATION Loss: 2.87e+03 logL: -2.57e+03 KL: 1.65e+01 MMD: 3.01e-02\n",
      "config 9, alpha = 0.0, lambda = 22625.4, dropout = 0.00; 2 hidden layers with 29, 26 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.87e+03 logL: -4.73e+03 KL: 1.11e+01 MMD: 5.00e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.04e+03 logL: -2.97e+03 KL: 1.38e+01 MMD: 4.68e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.60e+03 logL: -2.85e+03 KL: 1.48e+01 MMD: 3.25e-02\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 3.62e+03 logL: -2.64e+03 KL: 1.64e+01 MMD: 4.24e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 48 VALIDATION Loss: 3.32e+03 logL: -2.62e+03 KL: 1.66e+01 MMD: 3.02e-02\n",
      "config 9, alpha = 0.0, lambda = 4.2, dropout = 0.00; 2 hidden layers with 93, 47 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.00e+03 logL: -9.02e+02 KL: 9.67e+01 MMD: 1.87e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.58e+02 logL: -5.77e+02 KL: 7.47e+01 MMD: 1.78e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.68e+02 logL: -5.02e+02 KL: 6.02e+01 MMD: 1.78e+00\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 5.51e+02 logL: -4.90e+02 KL: 5.52e+01 MMD: 1.72e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.43e+02 logL: -4.84e+02 KL: 5.43e+01 MMD: 1.55e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.77e+02 logL: -4.16e+02 KL: 5.58e+01 MMD: 1.65e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.66e+02 logL: -4.07e+02 KL: 5.35e+01 MMD: 1.55e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.60e+02 logL: -4.03e+02 KL: 5.23e+01 MMD: 1.48e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.62e+02 logL: -4.05e+02 KL: 5.12e+01 MMD: 1.57e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.56e+02 logL: -4.00e+02 KL: 5.06e+01 MMD: 1.51e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 4.54e+02 logL: -3.99e+02 KL: 4.95e+01 MMD: 1.47e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.52e+02 logL: -3.99e+02 KL: 4.85e+01 MMD: 1.33e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 4.51e+02 logL: -3.99e+02 KL: 4.77e+01 MMD: 1.28e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 4.49e+02 logL: -3.98e+02 KL: 4.70e+01 MMD: 1.26e+00\n",
      "Stopping\n",
      "====> Epoch: 146 VALIDATION Loss: 4.49e+02 logL: -3.98e+02 KL: 4.68e+01 MMD: 1.19e+00\n",
      "config 10, alpha = 0.0, lambda = 3.7, dropout = 0.00; 2 hidden layers with 6, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.54e+03 logL: -5.38e+03 KL: 1.57e+02 MMD: 8.45e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.73e+03 logL: -3.64e+03 KL: 8.64e+01 MMD: 7.02e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.68e+03 logL: -3.63e+03 KL: 4.30e+01 MMD: 8.42e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.66e+03 logL: -3.62e+03 KL: 2.91e+01 MMD: 7.74e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.65e+03 logL: -3.62e+03 KL: 2.27e+01 MMD: 8.21e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.64e+03 logL: -3.62e+03 KL: 1.89e+01 MMD: 8.02e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.63e+03 logL: -3.61e+03 KL: 1.67e+01 MMD: 7.63e-01\n",
      "Stopping\n",
      "====> Epoch: 73 VALIDATION Loss: 3.63e+03 logL: -3.61e+03 KL: 1.61e+01 MMD: 7.81e-01\n",
      "config 10, alpha = 0.0, lambda = 78.4, dropout = 0.00; 2 hidden layers with 20, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.81e+03 logL: -3.62e+03 KL: 9.38e+01 MMD: 1.21e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.68e+03 logL: -2.53e+03 KL: 6.26e+01 MMD: 1.12e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.64e+03 logL: -2.51e+03 KL: 4.24e+01 MMD: 1.14e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.97e+03 logL: -1.83e+03 KL: 5.44e+01 MMD: 1.07e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.95e+03 logL: -1.82e+03 KL: 3.87e+01 MMD: 1.17e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.92e+03 logL: -1.82e+03 KL: 3.10e+01 MMD: 9.77e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.90e+03 logL: -1.81e+03 KL: 2.60e+01 MMD: 7.44e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.88e+03 logL: -1.82e+03 KL: 2.24e+01 MMD: 5.11e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 2.08e+01 MMD: 3.39e-01\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.00e+01 MMD: 3.80e-01\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 107 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.00e+01 MMD: 3.52e-01\n",
      "config 10, alpha = 0.0, lambda = 659.9, dropout = 0.00; 2 hidden layers with 14, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.84e+03 logL: -3.77e+03 KL: 1.63e+02 MMD: 1.38e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.71e+03 logL: -2.59e+03 KL: 1.07e+02 MMD: 1.52e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.34e+03 logL: -2.58e+03 KL: 3.70e+01 MMD: 1.10e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.66e+03 logL: -2.54e+03 KL: 2.91e+01 MMD: 1.44e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.10e+03 logL: -1.99e+03 KL: 2.75e+01 MMD: 1.16e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 2.00e+03 logL: -1.90e+03 KL: 2.65e+01 MMD: 1.12e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.98e+03 logL: -1.87e+03 KL: 2.47e+01 MMD: 1.30e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.91e+03 logL: -1.84e+03 KL: 2.02e+01 MMD: 7.99e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 1.87e+03 logL: -1.82e+03 KL: 2.00e+01 MMD: 5.44e-02\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 1.93e+01 MMD: 5.83e-02\n",
      "====> Epoch: 110 VALIDATION Loss: 1.86e+03 logL: -1.81e+03 KL: 1.93e+01 MMD: 4.84e-02\n",
      "Epoch 00111: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 112 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 1.93e+01 MMD: 4.74e-02\n",
      "config 10, alpha = 0.0, lambda = 9127.8, dropout = 0.00; 2 hidden layers with 135, 74 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.92e+03 logL: -1.80e+03 KL: 4.11e+01 MMD: 1.18e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.83e+03 logL: -1.15e+03 KL: 3.61e+01 MMD: 7.06e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 1.48e+03 logL: -9.78e+02 KL: 3.87e+01 MMD: 5.10e-02\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.49e+03 logL: -8.93e+02 KL: 4.17e+01 MMD: 6.09e-02\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.36e+03 logL: -8.49e+02 KL: 4.21e+01 MMD: 5.14e-02\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.34e+03 logL: -8.45e+02 KL: 4.26e+01 MMD: 4.96e-02\n",
      "Stopping\n",
      "====> Epoch: 61 VALIDATION Loss: 1.19e+03 logL: -8.46e+02 KL: 4.26e+01 MMD: 3.35e-02\n",
      "config 10, alpha = 0.0, lambda = 10685.2, dropout = 0.00; 2 hidden layers with 27, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.21e+04 logL: -5.98e+04 KL: 2.53e+01 MMD: 2.14e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.59e+04 logL: -1.19e+04 KL: 3.66e+01 MMD: 3.68e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 7.03e+03 logL: -5.77e+03 KL: 2.61e+01 MMD: 1.16e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 6.39e+03 logL: -5.63e+03 KL: 2.51e+01 MMD: 6.87e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 6.22e+03 logL: -5.46e+03 KL: 2.39e+01 MMD: 6.91e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 5.50e+03 logL: -4.75e+03 KL: 2.28e+01 MMD: 6.72e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 4.85e+03 logL: -4.17e+03 KL: 2.23e+01 MMD: 6.11e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 4.45e+03 logL: -3.65e+03 KL: 2.24e+01 MMD: 7.26e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 3.72e+03 logL: -3.00e+03 KL: 2.53e+01 MMD: 6.51e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 3.34e+03 logL: -2.56e+03 KL: 2.61e+01 MMD: 7.11e-02\n",
      "====> Epoch: 110 VALIDATION Loss: 2.71e+03 logL: -2.22e+03 KL: 2.58e+01 MMD: 4.31e-02\n",
      "====> Epoch: 120 VALIDATION Loss: 2.79e+03 logL: -1.99e+03 KL: 2.51e+01 MMD: 7.22e-02\n",
      "====> Epoch: 130 VALIDATION Loss: 2.38e+03 logL: -1.84e+03 KL: 2.47e+01 MMD: 4.90e-02\n",
      "====> Epoch: 140 VALIDATION Loss: 2.66e+03 logL: -1.70e+03 KL: 2.45e+01 MMD: 8.70e-02\n",
      "Epoch 00140: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 2.26e+03 logL: -1.66e+03 KL: 2.48e+01 MMD: 5.37e-02\n",
      "Epoch 00154: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 158 VALIDATION Loss: 2.14e+03 logL: -1.64e+03 KL: 2.48e+01 MMD: 4.42e-02\n",
      "config 11, alpha = 0.0, lambda = 6176.2, dropout = 0.00; 2 hidden layers with 145, 49 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.14e+03 logL: -3.60e+03 KL: 1.73e+02 MMD: 7.07e-01\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 7.83e+03 logL: -3.40e+03 KL: 1.46e+02 MMD: 6.93e-01\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 7.64e+03 logL: -3.38e+03 KL: 1.43e+02 MMD: 6.66e-01\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 35 VALIDATION Loss: 7.66e+03 logL: -3.38e+03 KL: 1.44e+02 MMD: 6.70e-01\n",
      "config 11, alpha = 0.0, lambda = 1382.6, dropout = 0.00; 2 hidden layers with 13, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.96e+03 logL: -5.48e+03 KL: 7.01e+01 MMD: 1.02e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.49e+03 logL: -4.10e+03 KL: 5.41e+01 MMD: 9.65e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.99e+03 logL: -3.83e+03 KL: 1.28e+01 MMD: 1.09e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 3.73e+03 logL: -3.65e+03 KL: 1.20e+01 MMD: 4.79e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.68e+03 logL: -3.62e+03 KL: 1.21e+01 MMD: 3.89e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.54e+03 logL: -3.47e+03 KL: 1.22e+01 MMD: 3.66e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 2.63e+03 logL: -2.56e+03 KL: 1.62e+01 MMD: 3.52e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 2.59e+03 logL: -2.54e+03 KL: 1.60e+01 MMD: 2.39e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 2.60e+03 logL: -2.53e+03 KL: 1.53e+01 MMD: 4.37e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 2.55e+03 logL: -2.50e+03 KL: 1.49e+01 MMD: 2.06e-02\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.55e+03 logL: -2.49e+03 KL: 1.46e+01 MMD: 2.99e-02\n",
      "Epoch 00119: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 2.53e+03 logL: -2.49e+03 KL: 1.46e+01 MMD: 1.56e-02\n",
      "Stopping\n",
      "====> Epoch: 123 VALIDATION Loss: 2.55e+03 logL: -2.49e+03 KL: 1.45e+01 MMD: 3.30e-02\n",
      "config 11, alpha = 0.0, lambda = 242.0, dropout = 0.00; 2 hidden layers with 187, 110 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.35e+03 logL: -9.74e+02 KL: 7.40e+01 MMD: 1.25e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.21e+03 logL: -9.57e+02 KL: 3.74e+01 MMD: 8.73e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.01e+03 logL: -9.43e+02 KL: 3.28e+01 MMD: 1.49e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.83e+02 logL: -9.35e+02 KL: 3.25e+01 MMD: 6.28e-02\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 9.58e+02 logL: -9.11e+02 KL: 3.12e+01 MMD: 6.75e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 9.50e+02 logL: -9.08e+02 KL: 3.08e+01 MMD: 4.42e-02\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 67 VALIDATION Loss: 9.53e+02 logL: -9.06e+02 KL: 3.07e+01 MMD: 6.75e-02\n",
      "config 11, alpha = 0.0, lambda = 21728.5, dropout = 0.00; 2 hidden layers with 13, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.33e+04 logL: -1.14e+04 KL: 7.19e+00 MMD: 8.77e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 8.59e+03 logL: -7.24e+03 KL: 6.13e+00 MMD: 6.17e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 6.64e+03 logL: -5.61e+03 KL: 6.42e+00 MMD: 4.74e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 5.93e+03 logL: -5.24e+03 KL: 6.89e+00 MMD: 3.15e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 5.53e+03 logL: -4.91e+03 KL: 6.91e+00 MMD: 2.84e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 4.83e+03 logL: -4.28e+03 KL: 7.69e+00 MMD: 2.50e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 4.89e+03 logL: -3.93e+03 KL: 9.16e+00 MMD: 4.38e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 4.64e+03 logL: -3.78e+03 KL: 9.26e+00 MMD: 3.90e-02\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 84 VALIDATION Loss: 4.75e+03 logL: -3.77e+03 KL: 9.40e+00 MMD: 4.48e-02\n",
      "config 11, alpha = 0.0, lambda = 2341.9, dropout = 0.00; 2 hidden layers with 12, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.81e+03 logL: -5.41e+03 KL: 2.05e+01 MMD: 1.60e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.98e+03 logL: -3.76e+03 KL: 2.19e+01 MMD: 8.44e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.86e+03 logL: -3.66e+03 KL: 2.14e+01 MMD: 7.58e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.97e+03 logL: -2.74e+03 KL: 1.91e+01 MMD: 8.96e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 2.73e+03 logL: -2.55e+03 KL: 1.72e+01 MMD: 6.76e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 2.69e+03 logL: -2.53e+03 KL: 1.62e+01 MMD: 6.00e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 2.72e+03 logL: -2.56e+03 KL: 1.51e+01 MMD: 5.94e-02\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 2.67e+03 logL: -2.52e+03 KL: 1.48e+01 MMD: 5.76e-02\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 2.66e+03 logL: -2.51e+03 KL: 1.49e+01 MMD: 5.77e-02\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 2.66e+03 logL: -2.51e+03 KL: 1.49e+01 MMD: 5.59e-02\n",
      "config 12, alpha = 0.0, lambda = 119.0, dropout = 0.00; 2 hidden layers with 5, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.79e+03 logL: -5.50e+03 KL: 1.98e+02 MMD: 7.24e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.48e+03 logL: -5.29e+03 KL: 8.08e+01 MMD: 8.94e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.00e+03 logL: -4.83e+03 KL: 6.26e+01 MMD: 9.11e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.89e+03 logL: -3.72e+03 KL: 8.95e+01 MMD: 7.21e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.80e+03 logL: -3.65e+03 KL: 5.78e+01 MMD: 7.62e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.77e+03 logL: -3.63e+03 KL: 4.46e+01 MMD: 7.73e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.76e+03 logL: -3.63e+03 KL: 3.59e+01 MMD: 7.63e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.74e+03 logL: -3.62e+03 KL: 3.04e+01 MMD: 7.48e-01\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.72e+03 logL: -3.61e+03 KL: 2.70e+01 MMD: 7.43e-01\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 3.72e+03 logL: -3.60e+03 KL: 2.63e+01 MMD: 7.85e-01\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 109 VALIDATION Loss: 3.71e+03 logL: -3.61e+03 KL: 2.62e+01 MMD: 6.98e-01\n",
      "config 12, alpha = 0.0, lambda = 1.0, dropout = 0.00; 2 hidden layers with 58, 53 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.90e+03 logL: -1.84e+03 KL: 6.82e+01 MMD: 1.04e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 4.32e+01 MMD: 1.08e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.84e+03 logL: -1.80e+03 KL: 3.65e+01 MMD: 1.07e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.66e+03 logL: -1.62e+03 KL: 3.33e+01 MMD: 1.00e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.76e+03 logL: -1.73e+03 KL: 3.02e+01 MMD: 1.09e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.74e+03 logL: -1.71e+03 KL: 2.69e+01 MMD: 1.06e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.76e+03 logL: -1.74e+03 KL: 2.63e+01 MMD: 1.19e+00\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.54e+03 logL: -1.51e+03 KL: 2.53e+01 MMD: 8.58e-01\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 88 VALIDATION Loss: 1.54e+03 logL: -1.52e+03 KL: 2.50e+01 MMD: 9.40e-01\n",
      "config 12, alpha = 0.0, lambda = 7.8, dropout = 0.00; 2 hidden layers with 14, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.56e+03 logL: -5.42e+03 KL: 1.30e+02 MMD: 1.67e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.84e+03 logL: -3.74e+03 KL: 9.19e+01 MMD: 1.54e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.70e+03 logL: -3.63e+03 KL: 4.99e+01 MMD: 1.59e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.60e+03 logL: -2.53e+03 KL: 5.83e+01 MMD: 1.50e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.91e+03 logL: -1.85e+03 KL: 5.25e+01 MMD: 1.41e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.87e+03 logL: -1.82e+03 KL: 4.38e+01 MMD: 1.37e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.86e+03 logL: -1.81e+03 KL: 3.89e+01 MMD: 1.39e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.34e+03 logL: -1.29e+03 KL: 4.57e+01 MMD: 1.24e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.31e+03 logL: -1.27e+03 KL: 3.94e+01 MMD: 1.17e+00\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 3.70e+01 MMD: 1.23e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 3.64e+01 MMD: 1.26e+00\n",
      "Epoch 00111: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 115 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 3.64e+01 MMD: 1.24e+00\n",
      "config 12, alpha = 0.0, lambda = 2.0, dropout = 0.00; 2 hidden layers with 23, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.63e+03 logL: -2.51e+03 KL: 1.14e+02 MMD: 1.73e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.92e+03 logL: -1.85e+03 KL: 6.62e+01 MMD: 1.71e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.36e+03 logL: -1.30e+03 KL: 6.29e+01 MMD: 1.84e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.02e+03 logL: -9.60e+02 KL: 5.75e+01 MMD: 1.69e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.02e+03 logL: -9.69e+02 KL: 5.03e+01 MMD: 1.59e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.08e+02 logL: -7.56e+02 KL: 5.04e+01 MMD: 1.52e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.11e+02 logL: -6.60e+02 KL: 4.94e+01 MMD: 1.43e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.77e+02 logL: -6.30e+02 KL: 4.53e+01 MMD: 1.62e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.54e+02 logL: -6.10e+02 KL: 4.24e+01 MMD: 1.52e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.43e+02 logL: -6.01e+02 KL: 4.06e+01 MMD: 1.33e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 110 VALIDATION Loss: 6.38e+02 logL: -5.98e+02 KL: 3.87e+01 MMD: 1.21e+00\n",
      "Epoch 00114: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 6.30e+02 logL: -5.91e+02 KL: 3.78e+01 MMD: 1.37e+00\n",
      "Stopping\n",
      "====> Epoch: 126 VALIDATION Loss: 6.30e+02 logL: -5.90e+02 KL: 3.80e+01 MMD: 1.29e+00\n",
      "config 12, alpha = 0.0, lambda = 2.3, dropout = 0.00; 2 hidden layers with 32, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.43e+03 logL: -1.31e+03 KL: 1.15e+02 MMD: 1.87e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.03e+03 logL: -9.58e+02 KL: 7.46e+01 MMD: 1.79e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.24e+02 logL: -7.56e+02 KL: 6.53e+01 MMD: 1.63e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.00e+02 logL: -7.42e+02 KL: 5.55e+01 MMD: 1.75e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.13e+02 logL: -6.58e+02 KL: 5.32e+01 MMD: 1.72e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.85e+02 logL: -6.33e+02 KL: 4.92e+01 MMD: 1.67e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.69e+02 logL: -6.20e+02 KL: 4.64e+01 MMD: 1.52e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.54e+02 logL: -6.06e+02 KL: 4.55e+01 MMD: 1.61e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.42e+02 logL: -5.97e+02 KL: 4.33e+01 MMD: 1.49e+00\n",
      "Stopping\n",
      "====> Epoch: 94 VALIDATION Loss: 6.43e+02 logL: -5.99e+02 KL: 4.27e+01 MMD: 1.23e+00\n",
      "config 13, alpha = 0.0, lambda = 2424.9, dropout = 0.00; 2 hidden layers with 15, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.18e+03 logL: -5.51e+03 KL: 1.12e+02 MMD: 6.40e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 6.89e+03 logL: -5.25e+03 KL: 7.93e+01 MMD: 6.42e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 6.36e+06 logL: -6.30e+06 KL: 5.93e+04 MMD: 7.12e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 4.63e+03 logL: -4.55e+03 KL: 4.64e+00 MMD: 2.79e-02\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 5.78e+03 logL: -5.45e+03 KL: 1.27e+01 MMD: 1.32e-01\n",
      "Stopping\n",
      "====> Epoch: 50 VALIDATION Loss: 5.78e+03 logL: -5.45e+03 KL: 1.27e+01 MMD: 1.32e-01\n",
      "config 13, alpha = 0.0, lambda = 2.9, dropout = 0.00; 2 hidden layers with 10, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.93e+03 logL: -4.83e+03 KL: 9.79e+01 MMD: 1.48e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.72e+03 logL: -3.66e+03 KL: 5.74e+01 MMD: 1.28e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.70e+03 logL: -3.66e+03 KL: 3.93e+01 MMD: 1.43e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.68e+03 logL: -3.65e+03 KL: 3.07e+01 MMD: 1.48e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.65e+03 logL: -3.62e+03 KL: 2.59e+01 MMD: 1.55e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.58e+03 logL: -2.54e+03 KL: 4.01e+01 MMD: 1.12e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.55e+03 logL: -2.51e+03 KL: 3.21e+01 MMD: 1.07e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.54e+03 logL: -2.51e+03 KL: 2.82e+01 MMD: 1.19e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.53e+03 logL: -2.50e+03 KL: 2.56e+01 MMD: 1.11e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.54e+03 logL: -2.51e+03 KL: 2.36e+01 MMD: 1.14e+00\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.52e+03 logL: -2.50e+03 KL: 2.32e+01 MMD: 1.05e+00\n",
      "Stopping\n",
      "====> Epoch: 114 VALIDATION Loss: 2.52e+03 logL: -2.50e+03 KL: 2.31e+01 MMD: 1.08e+00\n",
      "config 13, alpha = 0.0, lambda = 1.6, dropout = 0.00; 2 hidden layers with 8, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.75e+03 logL: -3.66e+03 KL: 9.15e+01 MMD: 1.83e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.62e+03 logL: -2.55e+03 KL: 7.09e+01 MMD: 1.59e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.59e+03 logL: -2.54e+03 KL: 4.83e+01 MMD: 1.65e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.58e+03 logL: -2.53e+03 KL: 4.16e+01 MMD: 1.86e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.56e+03 logL: -2.52e+03 KL: 3.71e+01 MMD: 1.67e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.53e+03 logL: -2.50e+03 KL: 3.54e+01 MMD: 1.86e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.88e+03 logL: -1.84e+03 KL: 4.11e+01 MMD: 1.61e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.86e+03 logL: -1.82e+03 KL: 3.69e+01 MMD: 1.59e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 3.47e+01 MMD: 1.45e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 3.28e+01 MMD: 1.57e+00\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 108 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 3.18e+01 MMD: 1.52e+00\n",
      "config 13, alpha = 0.0, lambda = 1412.4, dropout = 0.00; 2 hidden layers with 11, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.43e+03 logL: -3.86e+03 KL: 1.98e+02 MMD: 1.69e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.49e+03 logL: -2.02e+03 KL: 1.34e+02 MMD: 1.65e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.47e+03 logL: -2.04e+03 KL: 3.64e+01 MMD: 2.78e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.18e+03 logL: -1.92e+03 KL: 3.81e+01 MMD: 1.53e-01\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.01e+03 logL: -1.76e+03 KL: 3.80e+01 MMD: 1.54e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.88e+03 logL: -1.64e+03 KL: 3.87e+01 MMD: 1.40e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.75e+03 logL: -1.51e+03 KL: 3.91e+01 MMD: 1.40e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.64e+03 logL: -1.43e+03 KL: 3.86e+01 MMD: 1.22e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.61e+03 logL: -1.39e+03 KL: 3.85e+01 MMD: 1.27e-01\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.58e+03 logL: -1.35e+03 KL: 3.86e+01 MMD: 1.36e-01\n",
      "Epoch 00107: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.53e+03 logL: -1.35e+03 KL: 3.87e+01 MMD: 1.00e-01\n",
      "Epoch 00116: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.56e+03 logL: -1.35e+03 KL: 3.86e+01 MMD: 1.20e-01\n",
      "Stopping\n",
      "====> Epoch: 120 VALIDATION Loss: 1.56e+03 logL: -1.35e+03 KL: 3.86e+01 MMD: 1.20e-01\n",
      "config 13, alpha = 0.0, lambda = 2.6, dropout = 0.00; 2 hidden layers with 157, 53 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.32e+03 logL: -1.22e+03 KL: 9.71e+01 MMD: 2.00e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.97e+02 logL: -6.20e+02 KL: 7.40e+01 MMD: 1.88e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.95e+02 logL: -5.30e+02 KL: 6.20e+01 MMD: 1.85e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.85e+02 logL: -4.25e+02 KL: 5.75e+01 MMD: 1.73e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.73e+02 logL: -4.17e+02 KL: 5.33e+01 MMD: 1.66e+00\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.54e+02 logL: -4.00e+02 KL: 5.19e+01 MMD: 1.58e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.54e+02 logL: -4.00e+02 KL: 5.08e+01 MMD: 1.72e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.52e+02 logL: -4.00e+02 KL: 4.98e+01 MMD: 1.63e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.50e+02 logL: -3.99e+02 KL: 4.84e+01 MMD: 1.58e+00\n",
      "Epoch 00098: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 4.47e+02 logL: -3.97e+02 KL: 4.79e+01 MMD: 1.52e+00\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 109 VALIDATION Loss: 4.47e+02 logL: -3.97e+02 KL: 4.79e+01 MMD: 1.60e+00\n",
      "config 14, alpha = 0.0, lambda = 6392.9, dropout = 0.00; 2 hidden layers with 160, 53 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.00e+03 logL: -3.65e+03 KL: 1.66e+02 MMD: 6.54e-01\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 8.00e+03 logL: -3.59e+03 KL: 1.52e+02 MMD: 6.66e-01\n",
      "Stopping\n",
      "====> Epoch: 20 VALIDATION Loss: 8.00e+03 logL: -3.59e+03 KL: 1.52e+02 MMD: 6.66e-01\n",
      "config 14, alpha = 0.0, lambda = 2353.9, dropout = 0.00; 2 hidden layers with 27, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.02e+03 logL: -5.63e+03 KL: 6.68e+01 MMD: 9.90e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.35e+03 logL: -4.11e+03 KL: 1.28e+01 MMD: 9.58e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.77e+03 logL: -3.67e+03 KL: 1.23e+01 MMD: 4.05e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.68e+03 logL: -3.61e+03 KL: 1.31e+01 MMD: 2.35e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.48e+03 logL: -3.42e+03 KL: 1.32e+01 MMD: 2.04e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.34e+03 logL: -3.24e+03 KL: 1.51e+01 MMD: 3.72e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 2.72e+03 logL: -2.63e+03 KL: 3.00e+01 MMD: 2.89e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 2.67e+03 logL: -2.57e+03 KL: 2.82e+01 MMD: 3.14e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 2.63e+03 logL: -2.52e+03 KL: 2.64e+01 MMD: 3.34e-02\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 100 VALIDATION Loss: 2.58e+03 logL: -2.52e+03 KL: 2.61e+01 MMD: 1.53e-02\n",
      "Epoch 00107: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.58e+03 logL: -2.51e+03 KL: 2.60e+01 MMD: 1.69e-02\n",
      "Stopping\n",
      "====> Epoch: 110 VALIDATION Loss: 2.58e+03 logL: -2.51e+03 KL: 2.60e+01 MMD: 1.69e-02\n",
      "config 14, alpha = 0.0, lambda = 336.1, dropout = 0.00; 2 hidden layers with 37, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.10e+03 logL: -5.48e+03 KL: 1.13e+02 MMD: 1.50e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.47e+03 logL: -3.85e+03 KL: 1.22e+02 MMD: 1.48e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.25e+03 logL: -3.68e+03 KL: 7.77e+01 MMD: 1.48e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.09e+03 logL: -3.65e+03 KL: 3.49e+01 MMD: 1.21e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.83e+03 logL: -3.74e+03 KL: 1.73e+01 MMD: 2.21e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.66e+03 logL: -3.60e+03 KL: 1.90e+01 MMD: 1.35e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 2.63e+03 logL: -2.56e+03 KL: 2.38e+01 MMD: 1.38e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 2.54e+03 logL: -2.49e+03 KL: 2.25e+01 MMD: 1.02e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 2.51e+03 logL: -2.45e+03 KL: 2.20e+01 MMD: 1.00e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 2.47e+03 logL: -2.42e+03 KL: 2.12e+01 MMD: 9.72e-02\n",
      "====> Epoch: 110 VALIDATION Loss: 2.44e+03 logL: -2.38e+03 KL: 2.05e+01 MMD: 1.25e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 2.34e+03 logL: -2.29e+03 KL: 1.99e+01 MMD: 9.43e-02\n",
      "====> Epoch: 130 VALIDATION Loss: 1.94e+03 logL: -1.88e+03 KL: 2.21e+01 MMD: 1.19e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 1.83e+03 logL: -1.79e+03 KL: 1.98e+01 MMD: 7.14e-02\n",
      "====> Epoch: 150 VALIDATION Loss: 1.44e+03 logL: -1.35e+03 KL: 2.54e+01 MMD: 1.69e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 1.33e+03 logL: -1.28e+03 KL: 2.39e+01 MMD: 7.75e-02\n",
      "====> Epoch: 170 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 2.25e+01 MMD: 5.94e-02\n",
      "====> Epoch: 180 VALIDATION Loss: 1.30e+03 logL: -1.27e+03 KL: 2.21e+01 MMD: 4.61e-02\n",
      "Epoch 00180: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00189: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 190 VALIDATION Loss: 1.29e+03 logL: -1.25e+03 KL: 2.19e+01 MMD: 5.50e-02\n",
      "Stopping\n",
      "====> Epoch: 193 VALIDATION Loss: 1.29e+03 logL: -1.25e+03 KL: 2.19e+01 MMD: 4.33e-02\n",
      "config 14, alpha = 0.0, lambda = 76250.5, dropout = 0.00; 2 hidden layers with 187, 147 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.37e+03 logL: -4.94e+03 KL: 3.96e+00 MMD: 3.17e-02\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 1.54e+04 logL: -9.78e+03 KL: 4.06e+00 MMD: 7.31e-02\n",
      "Stopping\n",
      "====> Epoch: 22 VALIDATION Loss: 1.32e+04 logL: -9.57e+03 KL: 4.05e+00 MMD: 4.72e-02\n",
      "config 14, alpha = 0.0, lambda = 11104.7, dropout = 0.00; 2 hidden layers with 70, 60 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.00e+03 logL: -3.23e+03 KL: 1.27e+01 MMD: 6.87e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.16e+03 logL: -2.60e+03 KL: 1.43e+01 MMD: 4.92e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.76e+03 logL: -2.04e+03 KL: 1.93e+01 MMD: 6.30e-02\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.44e+03 logL: -1.85e+03 KL: 2.10e+01 MMD: 5.08e-02\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 49 VALIDATION Loss: 2.32e+03 logL: -1.84e+03 KL: 2.17e+01 MMD: 4.16e-02\n",
      "config 15, alpha = 0.0, lambda = 3.5, dropout = 0.00; 2 hidden layers with 130, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.54e+03 logL: -3.49e+03 KL: 5.25e+01 MMD: 7.14e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.18e+03 logL: -3.15e+03 KL: 3.32e+01 MMD: 7.11e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.12e+03 logL: -3.09e+03 KL: 2.54e+01 MMD: 7.56e-01\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 3.08e+03 logL: -3.05e+03 KL: 2.41e+01 MMD: 7.05e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.04e+03 logL: -3.01e+03 KL: 2.30e+01 MMD: 7.40e-01\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 59 VALIDATION Loss: 3.04e+03 logL: -3.01e+03 KL: 2.21e+01 MMD: 7.86e-01\n",
      "config 15, alpha = 0.0, lambda = 136.0, dropout = 0.00; 2 hidden layers with 159, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.22e+03 logL: -2.87e+03 KL: 1.97e+02 MMD: 1.12e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.26e+03 logL: -2.01e+03 KL: 1.14e+02 MMD: 1.05e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.06e+03 logL: -1.85e+03 KL: 6.59e+01 MMD: 1.06e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.01e+03 logL: -1.82e+03 KL: 4.88e+01 MMD: 1.05e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.95e+03 logL: -1.80e+03 KL: 3.58e+01 MMD: 8.27e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.90e+03 logL: -1.79e+03 KL: 2.63e+01 MMD: 5.96e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.85e+03 logL: -1.78e+03 KL: 2.26e+01 MMD: 3.09e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.79e+03 logL: -1.74e+03 KL: 2.24e+01 MMD: 1.75e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.73e+03 logL: -1.70e+03 KL: 2.21e+01 MMD: 1.01e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.70e+03 logL: -1.67e+03 KL: 2.19e+01 MMD: 5.96e-02\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.65e+03 logL: -1.62e+03 KL: 2.11e+01 MMD: 7.17e-02\n",
      "Stopping\n",
      "====> Epoch: 118 VALIDATION Loss: 1.65e+03 logL: -1.62e+03 KL: 2.10e+01 MMD: 8.10e-02\n",
      "config 15, alpha = 0.0, lambda = 3708.5, dropout = 0.00; 2 hidden layers with 87, 59 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.87e+03 logL: -2.62e+03 KL: 1.80e+01 MMD: 6.29e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 2.13e+03 logL: -1.92e+03 KL: 2.15e+01 MMD: 5.11e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 1.45e+03 logL: -1.34e+03 KL: 2.38e+01 MMD: 2.44e-02\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.38e+03 logL: -1.28e+03 KL: 2.54e+01 MMD: 2.27e-02\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.39e+03 logL: -1.26e+03 KL: 2.50e+01 MMD: 2.78e-02\n",
      "Stopping\n",
      "====> Epoch: 50 VALIDATION Loss: 1.39e+03 logL: -1.26e+03 KL: 2.50e+01 MMD: 2.78e-02\n",
      "config 15, alpha = 0.0, lambda = 2.7, dropout = 0.00; 2 hidden layers with 15, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.90e+03 logL: -2.74e+03 KL: 1.57e+02 MMD: 1.73e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.94e+03 logL: -1.86e+03 KL: 7.75e+01 MMD: 1.74e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.89e+03 logL: -1.83e+03 KL: 5.50e+01 MMD: 1.94e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.88e+03 logL: -1.83e+03 KL: 4.76e+01 MMD: 1.88e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.86e+03 logL: -1.82e+03 KL: 4.28e+01 MMD: 1.74e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 3.98e+01 MMD: 1.84e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+03 logL: -1.28e+03 KL: 5.80e+01 MMD: 1.79e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.33e+03 logL: -1.28e+03 KL: 4.63e+01 MMD: 1.87e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 4.13e+01 MMD: 1.87e+00\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 4.08e+01 MMD: 1.88e+00\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 102 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 4.07e+01 MMD: 1.85e+00\n",
      "config 15, alpha = 0.0, lambda = 3.0, dropout = 0.00; 2 hidden layers with 38, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.06e+03 logL: -9.56e+02 KL: 1.01e+02 MMD: 1.97e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.24e+02 logL: -7.53e+02 KL: 6.78e+01 MMD: 1.89e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.86e+02 logL: -6.21e+02 KL: 6.12e+01 MMD: 1.85e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.62e+02 logL: -5.01e+02 KL: 5.80e+01 MMD: 1.81e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.49e+02 logL: -4.94e+02 KL: 5.12e+01 MMD: 1.82e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.78e+02 logL: -4.24e+02 KL: 5.12e+01 MMD: 1.60e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.68e+02 logL: -4.18e+02 KL: 4.67e+01 MMD: 1.61e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.68e+02 logL: -4.20e+02 KL: 4.57e+01 MMD: 1.43e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.51e+02 logL: -4.04e+02 KL: 4.43e+01 MMD: 1.40e+00\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 4.45e+02 logL: -3.99e+02 KL: 4.33e+01 MMD: 1.36e+00\n",
      "Stopping\n",
      "====> Epoch: 104 VALIDATION Loss: 4.46e+02 logL: -3.99e+02 KL: 4.33e+01 MMD: 1.66e+00\n",
      "config 16, alpha = 0.0, lambda = 5718.2, dropout = 0.00; 2 hidden layers with 17, 5 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 8.96e+03 logL: -5.70e+03 KL: 7.41e+01 MMD: 5.58e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.30e+03 logL: -4.89e+03 KL: 8.74e+00 MMD: 7.14e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.88e+03 logL: -3.75e+03 KL: 1.52e+01 MMD: 1.86e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.92e+03 logL: -3.73e+03 KL: 1.39e+01 MMD: 3.04e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.76e+03 logL: -3.66e+03 KL: 1.27e+01 MMD: 1.56e-02\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.72e+03 logL: -3.61e+03 KL: 1.23e+01 MMD: 1.82e-02\n",
      "Stopping\n",
      "====> Epoch: 61 VALIDATION Loss: 3.67e+03 logL: -3.61e+03 KL: 1.23e+01 MMD: 8.73e-03\n",
      "config 16, alpha = 0.0, lambda = 5.7, dropout = 0.00; 2 hidden layers with 38, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.04e+03 logL: -1.95e+03 KL: 8.49e+01 MMD: 9.77e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.87e+03 logL: -1.82e+03 KL: 4.49e+01 MMD: 1.00e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.84e+03 logL: -1.80e+03 KL: 3.49e+01 MMD: 1.08e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.81e+03 logL: -1.77e+03 KL: 3.16e+01 MMD: 9.51e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.78e+03 logL: -1.74e+03 KL: 3.01e+01 MMD: 9.57e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.72e+03 logL: -1.69e+03 KL: 2.80e+01 MMD: 8.67e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.66e+03 logL: -1.62e+03 KL: 2.69e+01 MMD: 9.29e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.63e+03 logL: -1.60e+03 KL: 2.54e+01 MMD: 9.29e-01\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.59e+03 logL: -1.56e+03 KL: 2.48e+01 MMD: 9.18e-01\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 97 VALIDATION Loss: 1.59e+03 logL: -1.56e+03 KL: 2.48e+01 MMD: 8.89e-01\n",
      "config 16, alpha = 0.0, lambda = 8935.0, dropout = 0.00; 2 hidden layers with 16, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.45e+03 logL: -5.01e+03 KL: 5.69e+00 MMD: 4.91e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.07e+03 logL: -3.74e+03 KL: 1.23e+01 MMD: 3.54e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.07e+03 logL: -3.68e+03 KL: 1.22e+01 MMD: 4.19e-02\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 3.90e+03 logL: -3.65e+03 KL: 1.21e+01 MMD: 2.65e-02\n",
      "Stopping\n",
      "====> Epoch: 42 VALIDATION Loss: 3.94e+03 logL: -3.65e+03 KL: 1.21e+01 MMD: 3.06e-02\n",
      "config 16, alpha = 0.0, lambda = 290.5, dropout = 0.00; 2 hidden layers with 200, 62 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.26e+03 logL: -7.33e+02 KL: 9.25e+01 MMD: 1.51e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.32e+02 logL: -6.68e+02 KL: 4.58e+01 MMD: 7.52e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 7.51e+02 logL: -6.69e+02 KL: 4.23e+01 MMD: 1.39e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 6.92e+02 logL: -6.29e+02 KL: 4.09e+01 MMD: 7.68e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 7.04e+02 logL: -6.44e+02 KL: 3.99e+01 MMD: 6.69e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 7.36e+02 logL: -6.73e+02 KL: 3.78e+01 MMD: 8.52e-02\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 6.50e+02 logL: -5.88e+02 KL: 3.80e+01 MMD: 8.40e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 6.43e+02 logL: -5.89e+02 KL: 3.79e+01 MMD: 5.65e-02\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 6.43e+02 logL: -5.88e+02 KL: 3.69e+01 MMD: 6.32e-02\n",
      "config 16, alpha = 0.0, lambda = 247.4, dropout = 0.00; 2 hidden layers with 112, 44 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.91e+03 logL: -5.27e+03 KL: 1.76e+02 MMD: 1.91e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.30e+03 logL: -2.63e+03 KL: 1.68e+02 MMD: 2.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.07e+03 logL: -1.49e+03 KL: 1.34e+02 MMD: 1.84e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.65e+03 logL: -1.10e+03 KL: 1.09e+02 MMD: 1.79e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.35e+03 logL: -8.59e+02 KL: 8.23e+01 MMD: 1.64e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.14e+03 logL: -8.08e+02 KL: 5.55e+01 MMD: 1.11e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.54e+02 logL: -7.21e+02 KL: 5.14e+01 MMD: 3.32e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.11e+02 logL: -7.00e+02 KL: 4.88e+01 MMD: 2.55e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 6.80e+02 logL: -5.79e+02 KL: 4.79e+01 MMD: 2.18e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 5.96e+02 logL: -5.05e+02 KL: 4.66e+01 MMD: 1.83e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 5.44e+02 logL: -4.61e+02 KL: 4.63e+01 MMD: 1.48e-01\n",
      "Epoch 00118: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 5.03e+02 logL: -4.27e+02 KL: 4.52e+01 MMD: 1.25e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 5.06e+02 logL: -4.27e+02 KL: 4.50e+01 MMD: 1.38e-01\n",
      "Epoch 00133: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 140 VALIDATION Loss: 4.90e+02 logL: -4.22e+02 KL: 4.52e+01 MMD: 9.08e-02\n",
      "Epoch 00141: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 145 VALIDATION Loss: 4.92e+02 logL: -4.22e+02 KL: 4.52e+01 MMD: 9.98e-02\n",
      "config 17, alpha = 0.0, lambda = 88414.9, dropout = 0.00; 2 hidden layers with 82, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.91e+03 logL: -5.46e+03 KL: 5.35e+00 MMD: 1.64e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.67e+03 logL: -5.08e+03 KL: 6.61e+00 MMD: 6.65e-03\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.21e+03 logL: -4.69e+03 KL: 8.60e+00 MMD: 5.75e-03\n",
      "====> Epoch: 40 VALIDATION Loss: 5.37e+03 logL: -4.61e+03 KL: 9.50e+00 MMD: 8.40e-03\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.90e+03 logL: -4.59e+03 KL: 9.30e+00 MMD: 3.34e-03\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 59 VALIDATION Loss: 5.19e+03 logL: -4.59e+03 KL: 9.27e+00 MMD: 6.66e-03\n",
      "config 17, alpha = 0.0, lambda = 6400.8, dropout = 0.00; 2 hidden layers with 79, 35 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.63e+03 logL: -2.11e+03 KL: 2.39e+01 MMD: 7.67e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 2.35e+03 logL: -1.98e+03 KL: 2.41e+01 MMD: 5.36e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.23e+03 logL: -1.90e+03 KL: 2.38e+01 MMD: 4.88e-02\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.97e+03 logL: -1.81e+03 KL: 2.35e+01 MMD: 2.11e-02\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.89e+03 logL: -1.80e+03 KL: 2.31e+01 MMD: 1.07e-02\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.95e+03 logL: -1.80e+03 KL: 2.32e+01 MMD: 2.05e-02\n",
      "Stopping\n",
      "====> Epoch: 60 VALIDATION Loss: 1.95e+03 logL: -1.80e+03 KL: 2.32e+01 MMD: 2.05e-02\n",
      "config 17, alpha = 0.0, lambda = 97.5, dropout = 0.00; 2 hidden layers with 41, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.82e+03 logL: -2.53e+03 KL: 1.49e+02 MMD: 1.42e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.55e+03 logL: -1.32e+03 KL: 9.63e+01 MMD: 1.37e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.47e+03 logL: -1.29e+03 KL: 6.07e+01 MMD: 1.28e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.13e+03 logL: -9.49e+02 KL: 5.18e+01 MMD: 1.32e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.10e+03 logL: -9.56e+02 KL: 3.85e+01 MMD: 1.10e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.05e+03 logL: -9.56e+02 KL: 3.16e+01 MMD: 6.84e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.01e+03 logL: -9.53e+02 KL: 2.99e+01 MMD: 2.68e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.00e+03 logL: -9.59e+02 KL: 2.85e+01 MMD: 1.73e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.84e+02 logL: -9.40e+02 KL: 2.77e+01 MMD: 1.69e-01\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 9.65e+02 logL: -9.27e+02 KL: 2.73e+01 MMD: 1.08e-01\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 107 VALIDATION Loss: 9.64e+02 logL: -9.26e+02 KL: 2.73e+01 MMD: 1.12e-01\n",
      "config 17, alpha = 0.0, lambda = 4.0, dropout = 0.00; 2 hidden layers with 16, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.77e+03 logL: -2.64e+03 KL: 1.25e+02 MMD: 1.82e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.03e+03 logL: -1.94e+03 KL: 7.69e+01 MMD: 1.71e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.90e+03 logL: -1.84e+03 KL: 5.63e+01 MMD: 1.91e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+03 logL: -1.29e+03 KL: 6.35e+01 MMD: 1.78e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.33e+03 logL: -1.28e+03 KL: 5.07e+01 MMD: 1.80e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 4.81e+01 MMD: 1.73e+00\n",
      "Stopping\n",
      "====> Epoch: 64 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 4.80e+01 MMD: 1.76e+00\n",
      "config 17, alpha = 0.0, lambda = 7.6, dropout = 0.00; 2 hidden layers with 141, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.21e+03 logL: -3.08e+03 KL: 1.21e+02 MMD: 2.10e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.43e+03 logL: -1.32e+03 KL: 9.37e+01 MMD: 2.00e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.05e+03 logL: -9.61e+02 KL: 7.39e+01 MMD: 2.00e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.37e+02 logL: -7.56e+02 KL: 6.67e+01 MMD: 2.04e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.20e+02 logL: -7.49e+02 KL: 5.91e+01 MMD: 1.89e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.16e+02 logL: -7.48e+02 KL: 5.62e+01 MMD: 1.73e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.26e+02 logL: -7.64e+02 KL: 5.26e+01 MMD: 1.54e+00\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 7.91e+02 logL: -7.29e+02 KL: 5.22e+01 MMD: 1.55e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 7.88e+02 logL: -7.27e+02 KL: 5.11e+01 MMD: 1.48e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 7.87e+02 logL: -7.28e+02 KL: 4.95e+01 MMD: 1.43e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 7.83e+02 logL: -7.25e+02 KL: 4.85e+01 MMD: 1.33e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 7.79e+02 logL: -7.23e+02 KL: 4.78e+01 MMD: 1.34e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 7.70e+02 logL: -7.14e+02 KL: 4.73e+01 MMD: 1.43e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 6.78e+02 logL: -6.22e+02 KL: 4.79e+01 MMD: 1.33e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 6.59e+02 logL: -6.02e+02 KL: 4.71e+01 MMD: 1.43e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 6.52e+02 logL: -5.97e+02 KL: 4.64e+01 MMD: 1.36e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 6.47e+02 logL: -5.92e+02 KL: 4.63e+01 MMD: 1.26e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 6.46e+02 logL: -5.92e+02 KL: 4.57e+01 MMD: 1.31e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 6.43e+02 logL: -5.90e+02 KL: 4.51e+01 MMD: 1.24e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 6.42e+02 logL: -5.89e+02 KL: 4.47e+01 MMD: 1.30e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 6.42e+02 logL: -5.89e+02 KL: 4.47e+01 MMD: 1.30e+00\n",
      "config 18, alpha = 0.0, lambda = 69516.8, dropout = 0.00; 2 hidden layers with 13, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.26e+03 logL: -5.36e+03 KL: 7.21e+00 MMD: 1.28e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.60e+03 logL: -3.98e+03 KL: 1.15e+01 MMD: 2.31e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 7.94e+03 logL: -3.71e+03 KL: 1.29e+01 MMD: 6.06e-02\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 3.91e+03 logL: -3.51e+03 KL: 1.25e+01 MMD: 5.64e-03\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 49 VALIDATION Loss: 4.06e+03 logL: -3.49e+03 KL: 1.28e+01 MMD: 7.94e-03\n",
      "config 18, alpha = 0.0, lambda = 3.4, dropout = 0.00; 2 hidden layers with 135, 32 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.97e+03 logL: -1.88e+03 KL: 8.15e+01 MMD: 1.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.88e+03 logL: -1.82e+03 KL: 5.06e+01 MMD: 1.11e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.87e+03 logL: -1.82e+03 KL: 4.22e+01 MMD: 1.10e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.83e+03 logL: -1.79e+03 KL: 3.75e+01 MMD: 9.95e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.76e+03 logL: -1.72e+03 KL: 3.65e+01 MMD: 9.63e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.68e+03 logL: -1.64e+03 KL: 3.49e+01 MMD: 9.85e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.62e+03 logL: -1.59e+03 KL: 3.27e+01 MMD: 9.25e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.59e+03 logL: -1.56e+03 KL: 3.11e+01 MMD: 1.06e+00\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.55e+03 logL: -1.52e+03 KL: 3.08e+01 MMD: 9.11e-01\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.54e+03 logL: -1.51e+03 KL: 3.08e+01 MMD: 9.40e-01\n",
      "Epoch 00108: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.54e+03 logL: -1.51e+03 KL: 3.08e+01 MMD: 8.64e-01\n",
      "Stopping\n",
      "====> Epoch: 112 VALIDATION Loss: 1.54e+03 logL: -1.51e+03 KL: 3.08e+01 MMD: 8.45e-01\n",
      "config 18, alpha = 0.0, lambda = 26.5, dropout = 0.00; 2 hidden layers with 11, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.92e+03 logL: -2.74e+03 KL: 1.43e+02 MMD: 1.43e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.99e+03 logL: -1.87e+03 KL: 7.88e+01 MMD: 1.47e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.93e+03 logL: -1.84e+03 KL: 5.45e+01 MMD: 1.47e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.42e+03 logL: -1.33e+03 KL: 5.80e+01 MMD: 1.37e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.36e+03 logL: -1.28e+03 KL: 4.71e+01 MMD: 1.37e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+03 logL: -1.28e+03 KL: 4.09e+01 MMD: 1.22e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.34e+03 logL: -1.27e+03 KL: 3.61e+01 MMD: 1.23e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.33e+03 logL: -1.27e+03 KL: 3.33e+01 MMD: 1.09e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.33e+03 logL: -1.27e+03 KL: 3.13e+01 MMD: 1.05e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 2.93e+01 MMD: 9.70e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.01e+03 logL: -9.53e+02 KL: 3.12e+01 MMD: 1.13e+00\n",
      "Epoch 00119: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 9.81e+02 logL: -9.29e+02 KL: 2.97e+01 MMD: 8.73e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 9.79e+02 logL: -9.29e+02 KL: 2.94e+01 MMD: 8.33e-01\n",
      "Epoch 00134: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 137 VALIDATION Loss: 9.80e+02 logL: -9.28e+02 KL: 2.92e+01 MMD: 8.93e-01\n",
      "config 18, alpha = 0.0, lambda = 1.2, dropout = 0.00; 2 hidden layers with 30, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.97e+03 logL: -1.87e+03 KL: 9.69e+01 MMD: 1.60e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.37e+03 logL: -1.30e+03 KL: 6.72e+01 MMD: 1.66e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.01e+03 logL: -9.56e+02 KL: 5.69e+01 MMD: 1.67e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.06e+02 logL: -7.52e+02 KL: 5.32e+01 MMD: 1.55e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.00e+02 logL: -7.53e+02 KL: 4.68e+01 MMD: 1.55e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.90e+02 logL: -7.46e+02 KL: 4.32e+01 MMD: 1.46e+00\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 7.81e+02 logL: -7.39e+02 KL: 4.23e+01 MMD: 1.67e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 7.81e+02 logL: -7.39e+02 KL: 4.18e+01 MMD: 1.42e+00\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 84 VALIDATION Loss: 7.80e+02 logL: -7.38e+02 KL: 4.15e+01 MMD: 1.55e+00\n",
      "config 18, alpha = 0.0, lambda = 17.5, dropout = 0.00; 2 hidden layers with 173, 73 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.94e+02 logL: -8.45e+02 KL: 1.16e+02 MMD: 2.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.37e+02 logL: -6.22e+02 KL: 8.46e+01 MMD: 1.85e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.56e+02 logL: -4.58e+02 KL: 7.01e+01 MMD: 1.69e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.29e+02 logL: -4.43e+02 KL: 5.98e+01 MMD: 1.60e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.00e+02 logL: -4.23e+02 KL: 5.27e+01 MMD: 1.47e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.81e+02 logL: -4.13e+02 KL: 4.95e+01 MMD: 1.15e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.76e+02 logL: -4.13e+02 KL: 4.72e+01 MMD: 9.86e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 4.78e+02 logL: -4.19e+02 KL: 4.48e+01 MMD: 8.52e-01\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 4.49e+02 logL: -3.95e+02 KL: 4.37e+01 MMD: 6.37e-01\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 4.49e+02 logL: -3.94e+02 KL: 4.34e+01 MMD: 6.92e-01\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 106 VALIDATION Loss: 4.48e+02 logL: -3.94e+02 KL: 4.32e+01 MMD: 6.73e-01\n",
      "config 19, alpha = 0.0, lambda = 109.2, dropout = 0.00; 2 hidden layers with 57, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.78e+03 logL: -3.63e+03 KL: 7.34e+01 MMD: 7.43e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.72e+03 logL: -3.59e+03 KL: 4.08e+01 MMD: 8.23e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.51e+03 logL: -3.38e+03 KL: 3.53e+01 MMD: 8.09e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.29e+03 logL: -3.18e+03 KL: 2.55e+01 MMD: 7.78e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 50 VALIDATION Loss: 3.26e+03 logL: -3.16e+03 KL: 2.10e+01 MMD: 6.80e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.19e+03 logL: -3.11e+03 KL: 1.73e+01 MMD: 5.35e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.16e+03 logL: -3.10e+03 KL: 1.51e+01 MMD: 4.10e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.13e+03 logL: -3.08e+03 KL: 1.36e+01 MMD: 3.09e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.11e+03 logL: -3.07e+03 KL: 1.28e+01 MMD: 2.52e-01\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 3.07e+03 logL: -3.04e+03 KL: 1.25e+01 MMD: 2.15e-01\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 3.07e+03 logL: -3.04e+03 KL: 1.23e+01 MMD: 1.85e-01\n",
      "Stopping\n",
      "====> Epoch: 113 VALIDATION Loss: 3.07e+03 logL: -3.04e+03 KL: 1.24e+01 MMD: 2.07e-01\n",
      "config 19, alpha = 0.0, lambda = 6.6, dropout = 0.00; 2 hidden layers with 26, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.70e+03 logL: -3.63e+03 KL: 6.58e+01 MMD: 1.23e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.92e+03 logL: -1.85e+03 KL: 5.84e+01 MMD: 9.81e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.86e+03 logL: -1.82e+03 KL: 3.97e+01 MMD: 1.03e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 3.35e+01 MMD: 1.00e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 3.01e+01 MMD: 1.06e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.82e+03 logL: -1.79e+03 KL: 2.81e+01 MMD: 9.98e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.81e+03 logL: -1.78e+03 KL: 2.68e+01 MMD: 9.04e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.78e+03 logL: -1.75e+03 KL: 2.72e+01 MMD: 9.71e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.78e+03 logL: -1.75e+03 KL: 2.68e+01 MMD: 9.00e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.71e+03 logL: -1.68e+03 KL: 2.42e+01 MMD: 9.02e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.66e+03 logL: -1.63e+03 KL: 2.35e+01 MMD: 8.35e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.64e+03 logL: -1.61e+03 KL: 2.32e+01 MMD: 8.04e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 1.64e+03 logL: -1.61e+03 KL: 2.26e+01 MMD: 7.72e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 1.61e+03 logL: -1.58e+03 KL: 2.20e+01 MMD: 7.90e-01\n",
      "Epoch 00148: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 1.62e+03 logL: -1.59e+03 KL: 2.30e+01 MMD: 7.65e-01\n",
      "Stopping\n",
      "====> Epoch: 150 VALIDATION Loss: 1.62e+03 logL: -1.59e+03 KL: 2.30e+01 MMD: 7.65e-01\n",
      "config 19, alpha = 0.0, lambda = 13.7, dropout = 0.00; 2 hidden layers with 131, 24 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.56e+03 logL: -1.44e+03 KL: 1.04e+02 MMD: 1.31e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.02e+03 logL: -9.48e+02 KL: 5.96e+01 MMD: 1.33e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.01e+03 logL: -9.50e+02 KL: 4.57e+01 MMD: 1.26e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.03e+03 logL: -9.74e+02 KL: 4.19e+01 MMD: 1.24e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.80e+02 logL: -9.26e+02 KL: 3.94e+01 MMD: 1.13e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.79e+02 logL: -9.29e+02 KL: 3.62e+01 MMD: 1.05e+00\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.61e+02 logL: -9.11e+02 KL: 3.54e+01 MMD: 1.10e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.58e+02 logL: -9.10e+02 KL: 3.46e+01 MMD: 1.05e+00\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 9.56e+02 logL: -9.09e+02 KL: 3.47e+01 MMD: 9.77e-01\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 93 VALIDATION Loss: 9.57e+02 logL: -9.09e+02 KL: 3.47e+01 MMD: 1.06e+00\n",
      "config 19, alpha = 0.0, lambda = 5.2, dropout = 0.00; 2 hidden layers with 28, 28 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.66e+03 logL: -1.54e+03 KL: 1.15e+02 MMD: 1.60e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.02e+03 logL: -9.43e+02 KL: 7.30e+01 MMD: 1.59e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.19e+02 logL: -7.52e+02 KL: 6.05e+01 MMD: 1.55e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.68e+02 logL: -6.05e+02 KL: 5.67e+01 MMD: 1.58e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.61e+02 logL: -6.04e+02 KL: 4.98e+01 MMD: 1.53e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.49e+02 logL: -5.98e+02 KL: 4.53e+01 MMD: 1.45e+00\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 6.41e+02 logL: -5.91e+02 KL: 4.36e+01 MMD: 1.47e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.40e+02 logL: -5.91e+02 KL: 4.30e+01 MMD: 1.44e+00\n",
      "Stopping\n",
      "====> Epoch: 89 VALIDATION Loss: 6.39e+02 logL: -5.90e+02 KL: 4.25e+01 MMD: 1.45e+00\n",
      "config 19, alpha = 0.0, lambda = 3.7, dropout = 0.00; 2 hidden layers with 34, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.73e+03 logL: -2.60e+03 KL: 1.26e+02 MMD: 2.31e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.41e+03 logL: -1.31e+03 KL: 8.96e+01 MMD: 2.09e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.19e+03 logL: -1.11e+03 KL: 7.45e+01 MMD: 1.99e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.01e+03 logL: -9.47e+02 KL: 6.18e+01 MMD: 2.03e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.68e+02 logL: -8.01e+02 KL: 6.21e+01 MMD: 1.93e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.11e+02 logL: -7.51e+02 KL: 5.49e+01 MMD: 1.92e+00\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 7.96e+02 logL: -7.39e+02 KL: 5.24e+01 MMD: 1.73e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 7.94e+02 logL: -7.38e+02 KL: 5.15e+01 MMD: 2.03e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 7.87e+02 logL: -7.32e+02 KL: 5.00e+01 MMD: 1.87e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 7.67e+02 logL: -7.12e+02 KL: 5.00e+01 MMD: 1.82e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 7.16e+02 logL: -6.60e+02 KL: 5.08e+01 MMD: 1.78e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 6.94e+02 logL: -6.40e+02 KL: 5.00e+01 MMD: 1.75e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 6.81e+02 logL: -6.27e+02 KL: 4.90e+01 MMD: 1.78e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 6.71e+02 logL: -6.18e+02 KL: 4.79e+01 MMD: 1.80e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 6.61e+02 logL: -6.10e+02 KL: 4.72e+01 MMD: 1.72e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 6.54e+02 logL: -6.03e+02 KL: 4.65e+01 MMD: 1.60e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 6.49e+02 logL: -5.99e+02 KL: 4.60e+01 MMD: 1.67e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 6.47e+02 logL: -5.97e+02 KL: 4.51e+01 MMD: 1.77e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 6.44e+02 logL: -5.95e+02 KL: 4.45e+01 MMD: 1.66e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 6.44e+02 logL: -5.96e+02 KL: 4.40e+01 MMD: 1.53e+00\n",
      "Epoch 00200: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 6.44e+02 logL: -5.96e+02 KL: 4.40e+01 MMD: 1.53e+00\n",
      "config 20, alpha = 0.0, lambda = 2462.5, dropout = 0.00; 2 hidden layers with 36, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.46e+03 logL: -3.56e+03 KL: 1.06e+02 MMD: 7.27e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.12e+03 logL: -3.26e+03 KL: 7.65e+01 MMD: 7.25e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 4.88e+03 logL: -3.25e+03 KL: 5.05e+01 MMD: 6.42e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.31e+03 logL: -3.19e+03 KL: 1.65e+01 MMD: 4.26e-02\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.28e+03 logL: -3.13e+03 KL: 1.47e+01 MMD: 5.33e-02\n",
      "Epoch 00058: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.29e+03 logL: -3.13e+03 KL: 1.45e+01 MMD: 5.77e-02\n",
      "Stopping\n",
      "====> Epoch: 62 VALIDATION Loss: 3.29e+03 logL: -3.13e+03 KL: 1.45e+01 MMD: 5.80e-02\n",
      "config 20, alpha = 0.0, lambda = 40358.0, dropout = 0.00; 2 hidden layers with 87, 82 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.01e+03 logL: -3.43e+03 KL: 1.31e+01 MMD: 1.41e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.25e+03 logL: -3.01e+03 KL: 1.57e+01 MMD: 3.02e-02\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.05e+03 logL: -2.48e+03 KL: 1.78e+01 MMD: 1.37e-02\n",
      "Stopping\n",
      "====> Epoch: 33 VALIDATION Loss: 3.30e+03 logL: -2.48e+03 KL: 1.79e+01 MMD: 1.97e-02\n",
      "config 20, alpha = 0.0, lambda = 28.1, dropout = 0.00; 2 hidden layers with 96, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.44e+03 logL: -1.32e+03 KL: 8.60e+01 MMD: 1.33e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.07e+03 logL: -9.71e+02 KL: 5.89e+01 MMD: 1.32e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.02e+03 logL: -9.41e+02 KL: 4.44e+01 MMD: 1.33e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.95e+02 logL: -9.26e+02 KL: 3.69e+01 MMD: 1.18e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 9.75e+02 logL: -9.15e+02 KL: 3.40e+01 MMD: 9.25e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.71e+02 logL: -9.16e+02 KL: 3.33e+01 MMD: 8.28e-01\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: 9.70e+02 logL: -9.14e+02 KL: 3.28e+01 MMD: 8.64e-01\n",
      "config 20, alpha = 0.0, lambda = 67.7, dropout = 0.00; 2 hidden layers with 24, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.61e+03 logL: -2.37e+03 KL: 1.17e+02 MMD: 1.70e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.18e+03 logL: -9.82e+02 KL: 8.97e+01 MMD: 1.67e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.12e+03 logL: -9.50e+02 KL: 6.31e+01 MMD: 1.55e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.09e+03 logL: -9.46e+02 KL: 5.07e+01 MMD: 1.38e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.85e+02 logL: -7.54e+02 KL: 4.51e+01 MMD: 1.28e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.54e+02 logL: -7.53e+02 KL: 3.95e+01 MMD: 9.25e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.32e+02 logL: -7.56e+02 KL: 3.78e+01 MMD: 5.78e-01\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 8.17e+02 logL: -7.46e+02 KL: 3.64e+01 MMD: 5.27e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 8.13e+02 logL: -7.45e+02 KL: 3.64e+01 MMD: 4.72e-01\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 98 VALIDATION Loss: 8.10e+02 logL: -7.44e+02 KL: 3.61e+01 MMD: 4.46e-01\n",
      "config 20, alpha = 0.0, lambda = 58810.3, dropout = 0.00; 2 hidden layers with 99, 74 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.83e+03 logL: -4.06e+03 KL: 8.60e+00 MMD: 4.68e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 6.69e+03 logL: -3.55e+03 KL: 8.68e+00 MMD: 5.32e-02\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 6.15e+03 logL: -3.39e+03 KL: 1.02e+01 MMD: 4.66e-02\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 34 VALIDATION Loss: 6.03e+03 logL: -3.39e+03 KL: 1.03e+01 MMD: 4.48e-02\n",
      "config 21, alpha = 0.0, lambda = 1888.6, dropout = 0.00; 2 hidden layers with 23, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.63e+03 logL: -4.17e+03 KL: 2.40e+02 MMD: 6.47e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.15e+03 logL: -3.72e+03 KL: 1.15e+02 MMD: 6.97e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.10e+03 logL: -3.65e+03 KL: 9.09e+01 MMD: 7.17e-01\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 5.05e+03 logL: -3.64e+03 KL: 8.77e+01 MMD: 7.03e-01\n",
      "config 21, alpha = 0.0, lambda = 8.7, dropout = 0.00; 2 hidden layers with 18, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.52e+03 logL: -5.34e+03 KL: 1.80e+02 MMD: 1.12e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.82e+03 logL: -3.70e+03 KL: 1.07e+02 MMD: 1.16e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.66e+03 logL: -2.57e+03 KL: 7.90e+01 MMD: 1.04e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.59e+03 logL: -2.53e+03 KL: 5.26e+01 MMD: 1.10e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.57e+03 logL: -2.52e+03 KL: 4.06e+01 MMD: 1.25e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.57e+03 logL: -2.53e+03 KL: 3.26e+01 MMD: 1.11e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.52e+03 logL: -2.48e+03 KL: 3.15e+01 MMD: 1.17e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.86e+03 logL: -1.81e+03 KL: 3.85e+01 MMD: 9.93e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 3.37e+01 MMD: 9.52e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.84e+03 logL: -1.80e+03 KL: 3.06e+01 MMD: 1.02e+00\n",
      "Stopping\n",
      "====> Epoch: 106 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 2.94e+01 MMD: 1.05e+00\n",
      "config 21, alpha = 0.0, lambda = 8.5, dropout = 0.00; 2 hidden layers with 11, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.24e+03 logL: -4.00e+03 KL: 2.31e+02 MMD: 1.47e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.80e+03 logL: -3.70e+03 KL: 8.66e+01 MMD: 1.53e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.72e+03 logL: -2.65e+03 KL: 6.73e+01 MMD: 1.49e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.27e+03 logL: -2.20e+03 KL: 5.76e+01 MMD: 1.49e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.97e+03 logL: -1.90e+03 KL: 5.33e+01 MMD: 1.37e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.89e+03 logL: -1.84e+03 KL: 4.68e+01 MMD: 1.41e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+03 logL: -1.29e+03 KL: 5.44e+01 MMD: 1.30e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.36e+03 logL: -1.30e+03 KL: 4.53e+01 MMD: 1.31e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.33e+03 logL: -1.28e+03 KL: 4.19e+01 MMD: 1.41e+00\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 4.09e+01 MMD: 1.17e+00\n",
      "Stopping\n",
      "====> Epoch: 103 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 4.07e+01 MMD: 1.39e+00\n",
      "config 21, alpha = 0.0, lambda = 343.6, dropout = 0.00; 2 hidden layers with 34, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.51e+03 logL: -2.80e+03 KL: 1.18e+02 MMD: 1.73e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.00e+03 logL: -1.33e+03 KL: 1.00e+02 MMD: 1.68e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.77e+03 logL: -1.39e+03 KL: 4.65e+01 MMD: 9.65e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.14e+03 logL: -9.95e+02 KL: 4.24e+01 MMD: 3.07e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.08e+03 logL: -9.63e+02 KL: 4.17e+01 MMD: 2.08e-01\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.05e+03 logL: -9.47e+02 KL: 4.07e+01 MMD: 1.68e-01\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 64 VALIDATION Loss: 1.04e+03 logL: -9.45e+02 KL: 4.07e+01 MMD: 1.67e-01\n",
      "config 21, alpha = 0.0, lambda = 6507.8, dropout = 0.00; 2 hidden layers with 98, 24 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.51e+03 logL: -2.82e+03 KL: 2.38e+01 MMD: 1.03e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 2.43e+03 logL: -1.98e+03 KL: 2.89e+01 MMD: 6.51e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.37e+03 logL: -1.88e+03 KL: 2.87e+01 MMD: 7.11e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.93e+03 logL: -1.39e+03 KL: 2.90e+01 MMD: 7.78e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.70e+03 logL: -1.32e+03 KL: 2.94e+01 MMD: 5.31e-02\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.74e+03 logL: -1.28e+03 KL: 2.99e+01 MMD: 6.62e-02\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.56e+03 logL: -1.28e+03 KL: 3.00e+01 MMD: 3.88e-02\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.56e+03 logL: -1.28e+03 KL: 3.00e+01 MMD: 3.83e-02\n",
      "Stopping\n",
      "====> Epoch: 82 VALIDATION Loss: 1.57e+03 logL: -1.28e+03 KL: 3.00e+01 MMD: 4.03e-02\n",
      "config 22, alpha = 0.0, lambda = 17.4, dropout = 0.00; 2 hidden layers with 16, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.51e+03 logL: -5.36e+03 KL: 1.40e+02 MMD: 7.91e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.99e+03 logL: -4.93e+03 KL: 5.00e+01 MMD: 8.19e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.79e+03 logL: -3.71e+03 KL: 6.48e+01 MMD: 7.86e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.68e+03 logL: -3.62e+03 KL: 4.52e+01 MMD: 8.01e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.63e+03 logL: -3.59e+03 KL: 3.41e+01 MMD: 7.65e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.51e+03 logL: -3.47e+03 KL: 3.24e+01 MMD: 6.83e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.43e+03 logL: -3.39e+03 KL: 3.20e+01 MMD: 7.94e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.40e+03 logL: -3.36e+03 KL: 2.90e+01 MMD: 6.91e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.39e+03 logL: -3.35e+03 KL: 2.78e+01 MMD: 7.67e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.34e+03 logL: -3.30e+03 KL: 2.53e+01 MMD: 7.91e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 3.29e+03 logL: -3.26e+03 KL: 2.43e+01 MMD: 7.00e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 3.26e+03 logL: -3.23e+03 KL: 2.15e+01 MMD: 7.64e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 3.24e+03 logL: -3.21e+03 KL: 2.04e+01 MMD: 7.33e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 3.31e+03 logL: -3.28e+03 KL: 1.88e+01 MMD: 6.96e-01\n",
      "Epoch 00142: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 3.17e+03 logL: -3.14e+03 KL: 1.85e+01 MMD: 7.40e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 3.16e+03 logL: -3.13e+03 KL: 1.80e+01 MMD: 6.64e-01\n",
      "Epoch 00166: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 170 VALIDATION Loss: 3.16e+03 logL: -3.13e+03 KL: 1.78e+01 MMD: 7.86e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping\n",
      "====> Epoch: 178 VALIDATION Loss: 3.16e+03 logL: -3.13e+03 KL: 1.78e+01 MMD: 6.46e-01\n",
      "config 22, alpha = 0.0, lambda = 10301.4, dropout = 0.00; 2 hidden layers with 68, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.12e+03 logL: -6.56e+03 KL: 5.85e+00 MMD: 5.41e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.16e+03 logL: -4.67e+03 KL: 7.17e+00 MMD: 4.75e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.34e+03 logL: -3.87e+03 KL: 1.04e+01 MMD: 4.40e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.23e+03 logL: -2.79e+03 KL: 1.78e+01 MMD: 4.09e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.01e+03 logL: -2.63e+03 KL: 1.79e+01 MMD: 3.51e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 2.99e+03 logL: -2.63e+03 KL: 1.86e+01 MMD: 3.34e-02\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 65 VALIDATION Loss: 2.81e+03 logL: -2.52e+03 KL: 1.83e+01 MMD: 2.62e-02\n",
      "config 22, alpha = 0.0, lambda = 1443.1, dropout = 0.00; 2 hidden layers with 73, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.04e+03 logL: -3.98e+03 KL: 1.04e+02 MMD: 1.36e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.32e+03 logL: -3.95e+03 KL: 1.26e+01 MMD: 2.49e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.58e+03 logL: -3.39e+03 KL: 1.64e+01 MMD: 1.19e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.43e+03 logL: -3.29e+03 KL: 1.77e+01 MMD: 8.60e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.35e+03 logL: -3.20e+03 KL: 1.82e+01 MMD: 8.49e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.32e+03 logL: -3.19e+03 KL: 1.83e+01 MMD: 7.77e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 2.76e+03 logL: -2.65e+03 KL: 1.87e+01 MMD: 6.05e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 2.60e+03 logL: -2.49e+03 KL: 1.78e+01 MMD: 6.21e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 2.56e+03 logL: -2.48e+03 KL: 1.72e+01 MMD: 4.63e-02\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 2.45e+03 logL: -2.38e+03 KL: 1.68e+01 MMD: 3.30e-02\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.46e+03 logL: -2.37e+03 KL: 1.69e+01 MMD: 4.63e-02\n",
      "Epoch 00119: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 120 VALIDATION Loss: 2.46e+03 logL: -2.37e+03 KL: 1.69e+01 MMD: 5.17e-02\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 130 VALIDATION Loss: 2.45e+03 logL: -2.37e+03 KL: 1.69e+01 MMD: 4.32e-02\n",
      "Stopping\n",
      "====> Epoch: 132 VALIDATION Loss: 2.44e+03 logL: -2.37e+03 KL: 1.69e+01 MMD: 3.44e-02\n",
      "config 22, alpha = 0.0, lambda = 4139.7, dropout = 0.00; 2 hidden layers with 20, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.45e+03 logL: -5.56e+03 KL: 3.13e+01 MMD: 2.07e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.38e+03 logL: -4.00e+03 KL: 2.56e+01 MMD: 8.52e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.00e+03 logL: -3.64e+03 KL: 2.60e+01 MMD: 8.04e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.13e+03 logL: -2.89e+03 KL: 2.62e+01 MMD: 5.04e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 2.80e+03 logL: -2.59e+03 KL: 2.46e+01 MMD: 4.66e-02\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.73e+03 logL: -2.54e+03 KL: 2.41e+01 MMD: 4.17e-02\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.72e+03 logL: -2.53e+03 KL: 2.44e+01 MMD: 4.04e-02\n",
      "Stopping\n",
      "====> Epoch: 72 VALIDATION Loss: 2.72e+03 logL: -2.53e+03 KL: 2.44e+01 MMD: 4.00e-02\n",
      "config 22, alpha = 0.0, lambda = 23.0, dropout = 0.00; 2 hidden layers with 20, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.82e+03 logL: -3.66e+03 KL: 9.86e+01 MMD: 2.65e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.66e+03 logL: -2.55e+03 KL: 6.79e+01 MMD: 2.12e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.64e+03 logL: -1.52e+03 KL: 6.94e+01 MMD: 2.00e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.19e+03 logL: -1.08e+03 KL: 6.88e+01 MMD: 2.07e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.04e+03 logL: -9.46e+02 KL: 5.56e+01 MMD: 1.82e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.03e+03 logL: -9.44e+02 KL: 4.97e+01 MMD: 1.62e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.02e+03 logL: -9.45e+02 KL: 4.64e+01 MMD: 1.21e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.01e+03 logL: -9.43e+02 KL: 4.36e+01 MMD: 1.02e+00\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.00e+03 logL: -9.37e+02 KL: 4.32e+01 MMD: 1.02e+00\n",
      "Stopping\n",
      "====> Epoch: 93 VALIDATION Loss: 1.00e+03 logL: -9.37e+02 KL: 4.33e+01 MMD: 9.51e-01\n",
      "config 23, alpha = 0.0, lambda = 73057.4, dropout = 0.00; 2 hidden layers with 31, 28 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.77e+03 logL: -4.36e+03 KL: 8.51e+00 MMD: 1.92e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.38e+03 logL: -3.82e+03 KL: 1.10e+01 MMD: 7.51e-03\n",
      "====> Epoch: 30 VALIDATION Loss: 6.03e+03 logL: -4.22e+03 KL: 1.13e+01 MMD: 2.46e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 4.23e+03 logL: -3.58e+03 KL: 1.31e+01 MMD: 8.70e-03\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.13e+03 logL: -3.49e+03 KL: 1.37e+01 MMD: 8.47e-03\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 54 VALIDATION Loss: 3.85e+03 logL: -3.48e+03 KL: 1.38e+01 MMD: 4.87e-03\n",
      "config 23, alpha = 0.0, lambda = 58.0, dropout = 0.00; 2 hidden layers with 38, 26 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.99e+03 logL: -1.85e+03 KL: 7.84e+01 MMD: 9.64e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.97e+03 logL: -1.86e+03 KL: 4.23e+01 MMD: 1.11e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.88e+03 logL: -1.80e+03 KL: 3.16e+01 MMD: 9.48e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.84e+03 logL: -1.76e+03 KL: 2.68e+01 MMD: 7.98e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.78e+03 logL: -1.72e+03 KL: 2.37e+01 MMD: 5.88e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.68e+03 logL: -1.64e+03 KL: 2.16e+01 MMD: 3.68e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.86e+03 logL: -1.82e+03 KL: 2.14e+01 MMD: 4.71e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.64e+03 logL: -1.61e+03 KL: 1.98e+01 MMD: 2.65e-01\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.61e+03 logL: -1.58e+03 KL: 1.96e+01 MMD: 2.49e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.58e+03 logL: -1.55e+03 KL: 1.94e+01 MMD: 1.88e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.57e+03 logL: -1.54e+03 KL: 1.94e+01 MMD: 1.27e-01\n",
      "Epoch 00116: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.58e+03 logL: -1.55e+03 KL: 1.93e+01 MMD: 1.83e-01\n",
      "Stopping\n",
      "====> Epoch: 120 VALIDATION Loss: 1.58e+03 logL: -1.55e+03 KL: 1.93e+01 MMD: 1.83e-01\n",
      "config 23, alpha = 0.0, lambda = 3.6, dropout = 0.00; 2 hidden layers with 27, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.68e+03 logL: -2.57e+03 KL: 1.05e+02 MMD: 1.42e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+03 logL: -1.31e+03 KL: 7.73e+01 MMD: 1.28e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.35e+03 logL: -1.29e+03 KL: 5.66e+01 MMD: 1.36e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.02e+03 logL: -9.60e+02 KL: 5.47e+01 MMD: 1.43e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.01e+03 logL: -9.65e+02 KL: 4.64e+01 MMD: 1.22e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.85e+02 logL: -9.40e+02 KL: 4.23e+01 MMD: 1.25e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.92e+02 logL: -9.50e+02 KL: 3.92e+01 MMD: 1.20e+00\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.72e+02 logL: -9.30e+02 KL: 3.90e+01 MMD: 1.19e+00\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 9.71e+02 logL: -9.29e+02 KL: 3.91e+01 MMD: 1.27e+00\n",
      "config 23, alpha = 0.0, lambda = 9960.5, dropout = 0.00; 2 hidden layers with 85, 45 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.58e+03 logL: -2.80e+03 KL: 1.64e+01 MMD: 7.62e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.01e+03 logL: -2.61e+03 KL: 1.77e+01 MMD: 3.84e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.88e+03 logL: -2.09e+03 KL: 2.09e+01 MMD: 7.77e-02\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.15e+03 logL: -1.84e+03 KL: 2.29e+01 MMD: 2.87e-02\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.21e+03 logL: -1.83e+03 KL: 2.34e+01 MMD: 3.61e-02\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 57 VALIDATION Loss: 2.21e+03 logL: -1.82e+03 KL: 2.34e+01 MMD: 3.67e-02\n",
      "config 23, alpha = 0.0, lambda = 370.2, dropout = 0.00; 2 hidden layers with 20, 20 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 4.74e+03 logL: -3.84e+03 KL: 1.41e+02 MMD: 2.04e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.46e+03 logL: -2.58e+03 KL: 9.72e+01 MMD: 2.12e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.81e+03 logL: -2.59e+03 KL: 4.40e+01 MMD: 4.52e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.06e+03 logL: -1.87e+03 KL: 4.57e+01 MMD: 3.76e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.55e+03 logL: -1.37e+03 KL: 4.46e+01 MMD: 3.65e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.44e+03 logL: -1.32e+03 KL: 4.17e+01 MMD: 2.19e-01\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.42e+03 logL: -1.31e+03 KL: 4.04e+01 MMD: 2.11e-01\n",
      "Stopping\n",
      "====> Epoch: 72 VALIDATION Loss: 1.42e+03 logL: -1.31e+03 KL: 4.03e+01 MMD: 1.99e-01\n",
      "config 24, alpha = 0.0, lambda = 9.2, dropout = 0.00; 2 hidden layers with 9, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.44e+03 logL: -5.33e+03 KL: 9.72e+01 MMD: 7.64e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.70e+03 logL: -3.64e+03 KL: 5.65e+01 MMD: 7.57e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.67e+03 logL: -3.63e+03 KL: 3.21e+01 MMD: 8.42e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.66e+03 logL: -3.63e+03 KL: 2.38e+01 MMD: 7.45e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.64e+03 logL: -3.62e+03 KL: 1.96e+01 MMD: 6.71e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.64e+03 logL: -3.61e+03 KL: 1.69e+01 MMD: 6.93e-01\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.63e+03 logL: -3.61e+03 KL: 1.63e+01 MMD: 6.77e-01\n",
      "Stopping\n",
      "====> Epoch: 73 VALIDATION Loss: 3.63e+03 logL: -3.61e+03 KL: 1.62e+01 MMD: 7.28e-01\n",
      "config 24, alpha = 0.0, lambda = 795.5, dropout = 0.00; 2 hidden layers with 167, 32 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.78e+03 logL: -1.84e+03 KL: 7.81e+01 MMD: 1.09e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.17e+03 logL: -1.85e+03 KL: 2.77e+01 MMD: 3.69e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.84e+03 logL: -1.77e+03 KL: 2.50e+01 MMD: 5.54e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.74e+03 logL: -1.67e+03 KL: 2.49e+01 MMD: 6.56e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.64e+03 logL: -1.59e+03 KL: 2.43e+01 MMD: 4.26e-02\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.59e+03 logL: -1.53e+03 KL: 2.35e+01 MMD: 4.09e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 1.57e+03 logL: -1.52e+03 KL: 2.31e+01 MMD: 3.70e-02\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.56e+03 logL: -1.51e+03 KL: 2.27e+01 MMD: 3.48e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 1.55e+03 logL: -1.51e+03 KL: 2.27e+01 MMD: 2.64e-02\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 94 VALIDATION Loss: 1.55e+03 logL: -1.51e+03 KL: 2.27e+01 MMD: 2.93e-02\n",
      "config 24, alpha = 0.0, lambda = 138.2, dropout = 0.00; 2 hidden layers with 64, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.09e+03 logL: -3.75e+03 KL: 1.33e+02 MMD: 1.51e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.80e+03 logL: -2.54e+03 KL: 8.02e+01 MMD: 1.33e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.78e+03 logL: -2.53e+03 KL: 5.34e+01 MMD: 1.47e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.70e+03 logL: -2.50e+03 KL: 3.60e+01 MMD: 1.20e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.49e+03 logL: -2.35e+03 KL: 3.40e+01 MMD: 8.10e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.90e+03 logL: -1.80e+03 KL: 2.81e+01 MMD: 5.13e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.37e+03 logL: -1.30e+03 KL: 3.16e+01 MMD: 3.32e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.33e+03 logL: -1.27e+03 KL: 3.06e+01 MMD: 2.17e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.33e+03 logL: -1.28e+03 KL: 2.99e+01 MMD: 1.62e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 2.87e+01 MMD: 1.46e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.31e+03 logL: -1.27e+03 KL: 2.80e+01 MMD: 1.15e-01\n",
      "Epoch 00113: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.30e+03 logL: -1.25e+03 KL: 2.79e+01 MMD: 1.53e-01\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.29e+03 logL: -1.24e+03 KL: 2.76e+01 MMD: 1.37e-01\n",
      "Epoch 00135: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 139 VALIDATION Loss: 1.29e+03 logL: -1.24e+03 KL: 2.77e+01 MMD: 1.15e-01\n",
      "config 24, alpha = 0.0, lambda = 18.9, dropout = 0.00; 2 hidden layers with 85, 56 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.17e+03 logL: -1.04e+03 KL: 9.99e+01 MMD: 1.64e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.93e+02 logL: -6.93e+02 KL: 7.11e+01 MMD: 1.60e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.91e+02 logL: -6.11e+02 KL: 5.40e+01 MMD: 1.42e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.78e+02 logL: -6.07e+02 KL: 4.66e+01 MMD: 1.32e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.64e+02 logL: -6.01e+02 KL: 4.24e+01 MMD: 1.14e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.61e+02 logL: -6.04e+02 KL: 4.09e+01 MMD: 8.99e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 6.50e+02 logL: -5.99e+02 KL: 3.85e+01 MMD: 6.56e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 6.45e+02 logL: -5.96e+02 KL: 3.67e+01 MMD: 6.86e-01\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 6.33e+02 logL: -5.85e+02 KL: 3.71e+01 MMD: 5.89e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 6.32e+02 logL: -5.86e+02 KL: 3.69e+01 MMD: 4.81e-01\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 6.28e+02 logL: -5.84e+02 KL: 3.63e+01 MMD: 4.43e-01\n",
      "Epoch 00115: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 119 VALIDATION Loss: 6.27e+02 logL: -5.83e+02 KL: 3.64e+01 MMD: 4.30e-01\n",
      "config 24, alpha = 0.0, lambda = 77644.1, dropout = 0.00; 2 hidden layers with 41, 30 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.68e+03 logL: -6.18e+03 KL: 3.27e+00 MMD: 4.51e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 7.99e+03 logL: -4.89e+03 KL: 4.95e+00 MMD: 3.98e-02\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 7.97e+03 logL: -4.65e+03 KL: 5.23e+00 MMD: 4.27e-02\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 7.72e+03 logL: -4.65e+03 KL: 5.57e+00 MMD: 3.95e-02\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 49 VALIDATION Loss: 8.32e+03 logL: -4.65e+03 KL: 5.59e+00 MMD: 4.72e-02\n",
      "config 25, alpha = 0.0, lambda = 12765.7, dropout = 0.00; 2 hidden layers with 8, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.11e+04 logL: -6.75e+03 KL: 1.35e+02 MMD: 3.33e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.77e+03 logL: -4.56e+03 KL: 2.26e+01 MMD: 1.45e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.41e+03 logL: -4.08e+03 KL: 1.93e+01 MMD: 2.45e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.85e+03 logL: -3.69e+03 KL: 1.95e+01 MMD: 1.16e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.87e+03 logL: -3.61e+03 KL: 1.83e+01 MMD: 1.90e-02\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.61e+03 logL: -3.51e+03 KL: 1.88e+01 MMD: 6.34e-03\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.61e+03 logL: -3.50e+03 KL: 1.87e+01 MMD: 7.03e-03\n",
      "Stopping\n",
      "====> Epoch: 70 VALIDATION Loss: 3.61e+03 logL: -3.50e+03 KL: 1.87e+01 MMD: 7.03e-03\n",
      "config 25, alpha = 0.0, lambda = 34.9, dropout = 0.00; 2 hidden layers with 7, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.21e+03 logL: -3.96e+03 KL: 2.05e+02 MMD: 1.17e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.77e+03 logL: -3.65e+03 KL: 7.52e+01 MMD: 1.25e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.72e+03 logL: -3.63e+03 KL: 4.86e+01 MMD: 1.32e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.70e+03 logL: -3.61e+03 KL: 3.92e+01 MMD: 1.26e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.61e+03 logL: -2.53e+03 KL: 4.73e+01 MMD: 1.14e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.59e+03 logL: -2.51e+03 KL: 3.61e+01 MMD: 1.10e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.58e+03 logL: -2.51e+03 KL: 3.07e+01 MMD: 1.00e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.57e+03 logL: -2.51e+03 KL: 2.67e+01 MMD: 8.90e-01\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 2.56e+03 logL: -2.50e+03 KL: 2.52e+01 MMD: 8.24e-01\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 97 VALIDATION Loss: 2.56e+03 logL: -2.50e+03 KL: 2.50e+01 MMD: 9.61e-01\n",
      "config 25, alpha = 0.0, lambda = 15122.5, dropout = 0.00; 2 hidden layers with 94, 30 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 4.26e+03 logL: -3.17e+03 KL: 1.49e+01 MMD: 7.14e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.30e+03 logL: -2.68e+03 KL: 1.62e+01 MMD: 4.01e-02\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 2.99e+03 logL: -2.55e+03 KL: 1.77e+01 MMD: 2.73e-02\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.97e+03 logL: -2.53e+03 KL: 1.85e+01 MMD: 2.79e-02\n",
      "Stopping\n",
      "====> Epoch: 42 VALIDATION Loss: 2.94e+03 logL: -2.54e+03 KL: 1.84e+01 MMD: 2.56e-02\n",
      "config 25, alpha = 0.0, lambda = 70.8, dropout = 0.00; 2 hidden layers with 12, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.08e+03 logL: -3.79e+03 KL: 1.58e+02 MMD: 1.78e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.82e+03 logL: -2.59e+03 KL: 9.38e+01 MMD: 1.94e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.09e+03 logL: -1.88e+03 KL: 8.18e+01 MMD: 1.82e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.52e+03 logL: -1.33e+03 KL: 6.95e+01 MMD: 1.68e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.45e+03 logL: -1.29e+03 KL: 5.25e+01 MMD: 1.56e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.41e+03 logL: -1.29e+03 KL: 4.26e+01 MMD: 1.13e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.15e+03 logL: -1.05e+03 KL: 4.07e+01 MMD: 8.45e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.04e+03 logL: -9.50e+02 KL: 3.79e+01 MMD: 7.15e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.02e+03 logL: -9.49e+02 KL: 3.61e+01 MMD: 5.12e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.00e+03 logL: -9.40e+02 KL: 3.54e+01 MMD: 3.87e-01\n",
      "Epoch 00106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 9.95e+02 logL: -9.34e+02 KL: 3.44e+01 MMD: 3.76e-01\n",
      "Epoch 00118: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 9.92e+02 logL: -9.33e+02 KL: 3.42e+01 MMD: 3.52e-01\n",
      "Stopping\n",
      "====> Epoch: 122 VALIDATION Loss: 1.00e+03 logL: -9.34e+02 KL: 3.42e+01 MMD: 4.60e-01\n",
      "config 25, alpha = 0.0, lambda = 2675.9, dropout = 0.00; 2 hidden layers with 36, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.58e+03 logL: -3.99e+03 KL: 4.54e+01 MMD: 2.01e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.07e+03 logL: -3.64e+03 KL: 4.33e+01 MMD: 1.43e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.87e+03 logL: -2.57e+03 KL: 3.70e+01 MMD: 9.78e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.76e+03 logL: -1.43e+03 KL: 3.27e+01 MMD: 1.08e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.48e+03 logL: -1.31e+03 KL: 3.02e+01 MMD: 5.30e-02\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+03 logL: -1.18e+03 KL: 3.12e+01 MMD: 5.33e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 1.23e+03 logL: -1.07e+03 KL: 3.05e+01 MMD: 5.15e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 1.14e+03 logL: -1.00e+03 KL: 2.98e+01 MMD: 3.95e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 1.23e+03 logL: -9.80e+02 KL: 2.96e+01 MMD: 8.13e-02\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 94 VALIDATION Loss: 1.16e+03 logL: -9.75e+02 KL: 2.96e+01 MMD: 5.72e-02\n",
      "config 26, alpha = 0.0, lambda = 73.2, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.19e+03 logL: -4.98e+03 KL: 1.61e+02 MMD: 6.90e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.72e+03 logL: -3.59e+03 KL: 7.88e+01 MMD: 6.71e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.47e+03 logL: -3.37e+03 KL: 4.57e+01 MMD: 6.91e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.46e+03 logL: -3.37e+03 KL: 3.47e+01 MMD: 7.60e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.33e+03 logL: -3.24e+03 KL: 2.85e+01 MMD: 7.56e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.26e+03 logL: -3.18e+03 KL: 2.33e+01 MMD: 7.84e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.18e+03 logL: -3.11e+03 KL: 2.04e+01 MMD: 6.74e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.16e+03 logL: -3.09e+03 KL: 1.83e+01 MMD: 6.22e-01\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.22e+03 logL: -3.16e+03 KL: 1.70e+01 MMD: 7.02e-01\n",
      "Stopping\n",
      "====> Epoch: 92 VALIDATION Loss: 3.20e+03 logL: -3.14e+03 KL: 1.71e+01 MMD: 6.17e-01\n",
      "config 26, alpha = 0.0, lambda = 9670.4, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.48e+03 logL: -6.34e+03 KL: 2.50e+00 MMD: 1.18e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.65e+03 logL: -5.43e+03 KL: 3.48e+00 MMD: 2.23e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 5.24e+03 logL: -5.08e+03 KL: 4.60e+00 MMD: 1.62e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 4.79e+03 logL: -4.62e+03 KL: 5.30e+00 MMD: 1.66e-02\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.77e+03 logL: -4.58e+03 KL: 5.75e+00 MMD: 1.95e-02\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.67e+03 logL: -4.58e+03 KL: 5.80e+00 MMD: 8.84e-03\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 4.84e+03 logL: -4.58e+03 KL: 5.83e+00 MMD: 2.71e-02\n",
      "Stopping\n",
      "====> Epoch: 70 VALIDATION Loss: 4.84e+03 logL: -4.58e+03 KL: 5.83e+00 MMD: 2.71e-02\n",
      "config 26, alpha = 0.0, lambda = 31880.8, dropout = 0.00; 2 hidden layers with 8, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.45e+03 logL: -6.05e+03 KL: 4.08e+00 MMD: 4.36e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 6.07e+03 logL: -4.85e+03 KL: 5.82e+00 MMD: 3.80e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 5.40e+03 logL: -4.60e+03 KL: 7.18e+00 MMD: 2.51e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 5.32e+03 logL: -4.57e+03 KL: 7.77e+00 MMD: 2.34e-02\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 5.69e+03 logL: -4.51e+03 KL: 8.46e+00 MMD: 3.66e-02\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 5.54e+03 logL: -4.51e+03 KL: 8.53e+00 MMD: 3.19e-02\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 66 VALIDATION Loss: 5.40e+03 logL: -4.51e+03 KL: 8.54e+00 MMD: 2.76e-02\n",
      "config 26, alpha = 0.0, lambda = 2237.8, dropout = 0.00; 2 hidden layers with 32, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.48e+03 logL: -3.51e+03 KL: 1.74e+02 MMD: 1.25e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.97e+03 logL: -2.42e+03 KL: 4.44e+01 MMD: 2.25e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.35e+03 logL: -2.01e+03 KL: 3.92e+01 MMD: 1.35e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.76e+03 logL: -1.48e+03 KL: 3.93e+01 MMD: 1.09e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.57e+03 logL: -1.34e+03 KL: 3.65e+01 MMD: 8.85e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.29e+03 logL: -1.03e+03 KL: 3.46e+01 MMD: 1.03e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.19e+03 logL: -1.02e+03 KL: 3.39e+01 MMD: 6.09e-02\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.12e+03 logL: -9.47e+02 KL: 3.37e+01 MMD: 6.38e-02\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 1.10e+03 logL: -9.48e+02 KL: 3.40e+01 MMD: 5.27e-02\n",
      "config 26, alpha = 0.0, lambda = 24989.1, dropout = 0.00; 2 hidden layers with 40, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.95e+03 logL: -5.96e+03 KL: 1.61e+01 MMD: 1.19e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 6.31e+03 logL: -4.74e+03 KL: 1.77e+01 MMD: 6.22e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 5.14e+03 logL: -3.84e+03 KL: 1.88e+01 MMD: 5.13e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 4.46e+03 logL: -3.14e+03 KL: 1.88e+01 MMD: 5.20e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.83e+03 logL: -2.85e+03 KL: 1.85e+01 MMD: 3.88e-02\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.65e+03 logL: -2.57e+03 KL: 1.82e+01 MMD: 4.25e-02\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.96e+03 logL: -2.52e+03 KL: 1.88e+01 MMD: 5.68e-02\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.64e+03 logL: -2.52e+03 KL: 1.88e+01 MMD: 4.41e-02\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 84 VALIDATION Loss: 3.66e+03 logL: -2.53e+03 KL: 1.88e+01 MMD: 4.48e-02\n",
      "config 27, alpha = 0.0, lambda = 6889.5, dropout = 0.00; 2 hidden layers with 10, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.43e+03 logL: -6.01e+03 KL: 5.03e+01 MMD: 4.89e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.84e+03 logL: -4.65e+03 KL: 4.88e+00 MMD: 2.75e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.69e+03 logL: -4.50e+03 KL: 6.10e+00 MMD: 2.62e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 4.51e+03 logL: -4.41e+03 KL: 6.76e+00 MMD: 1.32e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 4.17e+03 logL: -4.04e+03 KL: 1.04e+01 MMD: 1.77e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.87e+03 logL: -3.63e+03 KL: 1.29e+01 MMD: 3.35e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 3.73e+03 logL: -3.49e+03 KL: 1.21e+01 MMD: 3.30e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 3.49e+03 logL: -3.44e+03 KL: 1.11e+01 MMD: 6.13e-03\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.50e+03 logL: -3.39e+03 KL: 1.20e+01 MMD: 1.47e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 3.45e+03 logL: -3.39e+03 KL: 1.21e+01 MMD: 7.23e-03\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 110 VALIDATION Loss: 3.48e+03 logL: -3.38e+03 KL: 1.21e+01 MMD: 1.23e-02\n",
      "Stopping\n",
      "====> Epoch: 113 VALIDATION Loss: 3.48e+03 logL: -3.38e+03 KL: 1.21e+01 MMD: 1.31e-02\n",
      "config 27, alpha = 0.0, lambda = 1.7, dropout = 0.00; 2 hidden layers with 9, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.49e+03 logL: -5.36e+03 KL: 1.32e+02 MMD: 1.44e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.70e+03 logL: -3.64e+03 KL: 6.50e+01 MMD: 1.28e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.49e+03 logL: -3.44e+03 KL: 4.66e+01 MMD: 1.21e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.57e+03 logL: -2.52e+03 KL: 4.40e+01 MMD: 1.26e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.56e+03 logL: -2.52e+03 KL: 3.58e+01 MMD: 1.22e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.54e+03 logL: -2.51e+03 KL: 3.16e+01 MMD: 1.22e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.54e+03 logL: -2.51e+03 KL: 2.89e+01 MMD: 1.13e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.54e+03 logL: -2.51e+03 KL: 2.69e+01 MMD: 1.12e+00\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 2.52e+03 logL: -2.49e+03 KL: 2.65e+01 MMD: 1.14e+00\n",
      "Stopping\n",
      "====> Epoch: 94 VALIDATION Loss: 2.52e+03 logL: -2.49e+03 KL: 2.63e+01 MMD: 1.15e+00\n",
      "config 27, alpha = 0.0, lambda = 29144.6, dropout = 0.00; 2 hidden layers with 21, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.11e+03 logL: -5.18e+03 KL: 8.70e+00 MMD: 3.15e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.73e+03 logL: -3.64e+03 KL: 1.17e+01 MMD: 3.71e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.21e+03 logL: -3.43e+03 KL: 1.13e+01 MMD: 2.62e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 4.51e+03 logL: -3.43e+03 KL: 1.21e+01 MMD: 3.69e-02\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.36e+03 logL: -3.29e+03 KL: 1.33e+01 MMD: 3.65e-02\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 57 VALIDATION Loss: 4.26e+03 logL: -3.27e+03 KL: 1.36e+01 MMD: 3.34e-02\n",
      "config 27, alpha = 0.0, lambda = 37263.0, dropout = 0.00; 2 hidden layers with 15, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.20e+03 logL: -5.74e+03 KL: 4.14e+00 MMD: 3.92e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 7.01e+03 logL: -4.76e+03 KL: 5.32e+00 MMD: 6.01e-02\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.62e+03 logL: -4.54e+03 KL: 5.79e+00 MMD: 2.89e-02\n",
      "Stopping\n",
      "====> Epoch: 32 VALIDATION Loss: 6.00e+03 logL: -4.53e+03 KL: 5.90e+00 MMD: 3.94e-02\n",
      "config 27, alpha = 0.0, lambda = 451.0, dropout = 0.00; 2 hidden layers with 195, 74 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.51e+03 logL: -6.51e+02 KL: 8.58e+01 MMD: 1.71e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.47e+02 logL: -5.21e+02 KL: 5.05e+01 MMD: 1.67e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.70e+02 logL: -4.79e+02 KL: 4.95e+01 MMD: 9.28e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 5.72e+02 logL: -4.73e+02 KL: 4.81e+01 MMD: 1.13e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 5.27e+02 logL: -4.46e+02 KL: 4.96e+01 MMD: 6.93e-02\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.87e+02 logL: -4.09e+02 KL: 4.52e+01 MMD: 7.25e-02\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 4.89e+02 logL: -4.04e+02 KL: 4.50e+01 MMD: 8.90e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 4.79e+02 logL: -4.03e+02 KL: 4.50e+01 MMD: 6.90e-02\n",
      "Stopping\n",
      "====> Epoch: 87 VALIDATION Loss: 4.78e+02 logL: -4.03e+02 KL: 4.50e+01 MMD: 6.72e-02\n",
      "config 28, alpha = 0.0, lambda = 30681.3, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.24e+04 logL: -1.17e+04 KL: 4.92e+01 MMD: 2.09e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 9.78e+03 logL: -8.58e+03 KL: 7.12e+01 MMD: 3.68e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 5.74e+03 logL: -5.00e+03 KL: 4.18e+01 MMD: 2.28e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 4.82e+03 logL: -4.23e+03 KL: 2.37e+01 MMD: 1.85e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 4.40e+03 logL: -3.98e+03 KL: 1.76e+01 MMD: 1.31e-02\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 59 VALIDATION Loss: 4.51e+03 logL: -4.00e+03 KL: 1.97e+01 MMD: 1.60e-02\n",
      "config 28, alpha = 0.0, lambda = 1584.1, dropout = 0.00; 2 hidden layers with 11, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.44e+03 logL: -6.52e+03 KL: 3.10e+02 MMD: 1.02e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.87e+03 logL: -5.80e+03 KL: 2.60e+02 MMD: 1.14e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.32e+03 logL: -5.36e+03 KL: 1.88e+02 MMD: 1.12e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.49e+03 logL: -4.75e+03 KL: 1.29e+02 MMD: 1.02e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.48e+03 logL: -3.79e+03 KL: 5.31e+01 MMD: 4.05e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.16e+03 logL: -2.96e+03 KL: 2.29e+01 MMD: 1.08e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 2.10e+03 logL: -2.02e+03 KL: 2.14e+01 MMD: 3.46e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 1.98e+03 logL: -1.90e+03 KL: 2.11e+01 MMD: 3.94e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 1.93e+03 logL: -1.87e+03 KL: 2.11e+01 MMD: 2.59e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 2.05e+03 logL: -1.96e+03 KL: 2.02e+01 MMD: 4.00e-02\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.88e+03 logL: -1.82e+03 KL: 2.02e+01 MMD: 2.73e-02\n",
      "Epoch 00118: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 2.02e+01 MMD: 2.42e-02\n",
      "Stopping\n",
      "====> Epoch: 122 VALIDATION Loss: 1.86e+03 logL: -1.81e+03 KL: 2.02e+01 MMD: 1.77e-02\n",
      "config 28, alpha = 0.0, lambda = 1122.8, dropout = 0.00; 2 hidden layers with 109, 64 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.67e+03 logL: -2.03e+03 KL: 1.90e+02 MMD: 1.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.84e+03 logL: -1.25e+03 KL: 1.04e+02 MMD: 1.33e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.97e+03 logL: -1.16e+03 KL: 4.12e+01 MMD: 6.81e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.28e+03 logL: -1.11e+03 KL: 3.66e+01 MMD: 1.25e-01\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.11e+03 logL: -9.93e+02 KL: 3.65e+01 MMD: 7.42e-02\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.09e+03 logL: -9.83e+02 KL: 3.65e+01 MMD: 6.22e-02\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.06e+03 logL: -9.81e+02 KL: 3.65e+01 MMD: 4.11e-02\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.08e+03 logL: -9.81e+02 KL: 3.65e+01 MMD: 5.48e-02\n",
      "Stopping\n",
      "====> Epoch: 80 VALIDATION Loss: 1.08e+03 logL: -9.81e+02 KL: 3.65e+01 MMD: 5.48e-02\n",
      "config 28, alpha = 0.0, lambda = 41408.8, dropout = 0.00; 2 hidden layers with 16, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.13e+04 logL: -8.66e+03 KL: 3.22e+00 MMD: 6.37e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 8.30e+03 logL: -6.34e+03 KL: 2.87e+00 MMD: 4.72e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 6.81e+03 logL: -5.05e+03 KL: 2.85e+00 MMD: 4.25e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 6.50e+03 logL: -4.70e+03 KL: 3.28e+00 MMD: 4.35e-02\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 46 VALIDATION Loss: 6.24e+03 logL: -4.63e+03 KL: 3.36e+00 MMD: 3.88e-02\n",
      "config 28, alpha = 0.0, lambda = 30662.4, dropout = 0.00; 2 hidden layers with 45, 40 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.30e+03 logL: -4.67e+03 KL: 4.28e+00 MMD: 5.32e-02\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 5.77e+03 logL: -4.46e+03 KL: 5.62e+00 MMD: 4.24e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 30 VALIDATION Loss: 5.62e+03 logL: -4.41e+03 KL: 6.43e+00 MMD: 3.95e-02\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 5.75e+03 logL: -4.37e+03 KL: 6.63e+00 MMD: 4.50e-02\n",
      "config 29, alpha = 0.0, lambda = 529.3, dropout = 0.00; 2 hidden layers with 116, 67 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.65e+03 logL: -3.23e+03 KL: 4.98e+01 MMD: 7.18e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.46e+03 logL: -3.14e+03 KL: 2.61e+01 MMD: 5.64e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.18e+03 logL: -3.13e+03 KL: 1.30e+01 MMD: 6.59e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.15e+03 logL: -3.11e+03 KL: 1.30e+01 MMD: 4.88e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.13e+03 logL: -3.08e+03 KL: 1.20e+01 MMD: 6.38e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.18e+03 logL: -3.14e+03 KL: 1.14e+01 MMD: 5.07e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 3.19e+03 logL: -3.15e+03 KL: 1.07e+01 MMD: 4.50e-02\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.00e+03 logL: -2.98e+03 KL: 1.02e+01 MMD: 1.90e-02\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 3.01e+03 logL: -2.98e+03 KL: 1.04e+01 MMD: 3.44e-02\n",
      "config 29, alpha = 0.0, lambda = 989.7, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.39e+03 logL: -5.17e+03 KL: 1.65e+02 MMD: 1.07e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.91e+03 logL: -3.71e+03 KL: 1.12e+02 MMD: 1.09e+00\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 4.87e+03 logL: -3.67e+03 KL: 9.94e+01 MMD: 1.12e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.86e+03 logL: -3.66e+03 KL: 9.43e+01 MMD: 1.11e+00\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.80e+03 logL: -3.66e+03 KL: 8.79e+01 MMD: 1.06e+00\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.76e+03 logL: -3.65e+03 KL: 8.85e+01 MMD: 1.03e+00\n",
      "Stopping\n",
      "====> Epoch: 61 VALIDATION Loss: 4.85e+03 logL: -3.65e+03 KL: 8.85e+01 MMD: 1.12e+00\n",
      "config 29, alpha = 0.0, lambda = 11.8, dropout = 0.00; 2 hidden layers with 146, 36 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.04e+03 logL: -9.50e+02 KL: 7.42e+01 MMD: 1.24e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.01e+03 logL: -9.50e+02 KL: 4.96e+01 MMD: 1.25e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.82e+02 logL: -9.30e+02 KL: 3.93e+01 MMD: 1.16e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.70e+02 logL: -9.23e+02 KL: 3.50e+01 MMD: 1.07e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.73e+02 logL: -9.32e+02 KL: 3.22e+01 MMD: 8.48e-01\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.45e+02 logL: -9.04e+02 KL: 3.16e+01 MMD: 8.96e-01\n",
      "Stopping\n",
      "====> Epoch: 64 VALIDATION Loss: 9.43e+02 logL: -9.03e+02 KL: 3.15e+01 MMD: 7.93e-01\n",
      "config 29, alpha = 0.0, lambda = 169.8, dropout = 0.00; 2 hidden layers with 19, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.16e+03 logL: -3.67e+03 KL: 1.48e+02 MMD: 1.98e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.90e+03 logL: -2.53e+03 KL: 8.82e+01 MMD: 1.70e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.16e+03 logL: -1.84e+03 KL: 5.62e+01 MMD: 1.56e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.99e+03 logL: -1.84e+03 KL: 3.88e+01 MMD: 6.11e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.93e+03 logL: -1.83e+03 KL: 3.83e+01 MMD: 3.86e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.91e+03 logL: -1.83e+03 KL: 3.46e+01 MMD: 2.62e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.90e+03 logL: -1.82e+03 KL: 3.45e+01 MMD: 2.53e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.90e+03 logL: -1.83e+03 KL: 3.23e+01 MMD: 2.31e-01\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.87e+03 logL: -1.80e+03 KL: 3.16e+01 MMD: 2.17e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.86e+03 logL: -1.80e+03 KL: 3.15e+01 MMD: 2.07e-01\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 109 VALIDATION Loss: 1.87e+03 logL: -1.80e+03 KL: 3.14e+01 MMD: 2.15e-01\n",
      "config 29, alpha = 0.0, lambda = 4.0, dropout = 0.00; 2 hidden layers with 33, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.64e+03 logL: -2.54e+03 KL: 9.57e+01 MMD: 2.25e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.03e+03 logL: -9.36e+02 KL: 8.86e+01 MMD: 1.85e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.35e+02 logL: -7.60e+02 KL: 6.88e+01 MMD: 1.82e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.83e+02 logL: -6.13e+02 KL: 6.37e+01 MMD: 2.00e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.71e+02 logL: -6.09e+02 KL: 5.59e+01 MMD: 2.05e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.60e+02 logL: -6.03e+02 KL: 5.18e+01 MMD: 1.71e+00\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 6.50e+02 logL: -5.93e+02 KL: 5.14e+01 MMD: 1.79e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.49e+02 logL: -5.93e+02 KL: 5.06e+01 MMD: 1.70e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.48e+02 logL: -5.93e+02 KL: 4.94e+01 MMD: 1.84e+00\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00099: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 100 VALIDATION Loss: 6.46e+02 logL: -5.91e+02 KL: 4.95e+01 MMD: 1.67e+00\n",
      "Stopping\n",
      "====> Epoch: 101 VALIDATION Loss: 6.47e+02 logL: -5.91e+02 KL: 4.95e+01 MMD: 1.84e+00\n",
      "config 30, alpha = 0.0, lambda = 1459.9, dropout = 0.00; 2 hidden layers with 5, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.75e+03 logL: -5.48e+03 KL: 1.59e+02 MMD: 7.65e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 6.53e+03 logL: -5.31e+03 KL: 1.26e+02 MMD: 7.55e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.97e+03 logL: -4.93e+03 KL: 3.70e+01 MMD: 6.87e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 4.58e+03 logL: -4.53e+03 KL: 4.39e+00 MMD: 3.22e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.91e+03 logL: -3.86e+03 KL: 7.44e+00 MMD: 3.01e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.78e+03 logL: -3.72e+03 KL: 1.01e+01 MMD: 3.76e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 3.73e+03 logL: -3.69e+03 KL: 1.02e+01 MMD: 2.04e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 3.70e+03 logL: -3.66e+03 KL: 1.06e+01 MMD: 1.96e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 3.68e+03 logL: -3.65e+03 KL: 1.04e+01 MMD: 1.48e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 3.68e+03 logL: -3.64e+03 KL: 1.05e+01 MMD: 2.00e-02\n",
      "====> Epoch: 110 VALIDATION Loss: 3.66e+03 logL: -3.63e+03 KL: 1.03e+01 MMD: 1.16e-02\n",
      "Stopping\n",
      "====> Epoch: 111 VALIDATION Loss: 3.68e+03 logL: -3.64e+03 KL: 1.05e+01 MMD: 2.07e-02\n",
      "config 30, alpha = 0.0, lambda = 4.2, dropout = 0.00; 2 hidden layers with 89, 43 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.92e+03 logL: -1.84e+03 KL: 7.49e+01 MMD: 1.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.86e+03 logL: -1.81e+03 KL: 4.97e+01 MMD: 1.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.84e+03 logL: -1.79e+03 KL: 4.21e+01 MMD: 9.89e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.72e+03 logL: -1.68e+03 KL: 3.67e+01 MMD: 1.01e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.63e+03 logL: -1.59e+03 KL: 3.24e+01 MMD: 1.00e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.62e+03 logL: -1.59e+03 KL: 2.87e+01 MMD: 9.52e-01\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.58e+03 logL: -1.55e+03 KL: 2.81e+01 MMD: 7.96e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.56e+03 logL: -1.53e+03 KL: 2.74e+01 MMD: 9.50e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.55e+03 logL: -1.52e+03 KL: 2.69e+01 MMD: 9.46e-01\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 97 VALIDATION Loss: 1.55e+03 logL: -1.52e+03 KL: 2.70e+01 MMD: 9.39e-01\n",
      "config 30, alpha = 0.0, lambda = 2321.9, dropout = 0.00; 2 hidden layers with 19, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.93e+03 logL: -4.63e+03 KL: 1.83e+02 MMD: 1.34e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.46e+03 logL: -4.05e+03 KL: 3.89e+01 MMD: 1.58e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.86e+03 logL: -2.66e+03 KL: 2.63e+01 MMD: 7.11e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.31e+03 logL: -2.09e+03 KL: 2.67e+01 MMD: 8.25e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.98e+03 logL: -1.86e+03 KL: 2.62e+01 MMD: 3.76e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 2.05e+03 logL: -1.89e+03 KL: 2.56e+01 MMD: 5.90e-02\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.93e+03 logL: -1.82e+03 KL: 2.54e+01 MMD: 3.61e-02\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 1.95e+03 logL: -1.81e+03 KL: 2.52e+01 MMD: 4.78e-02\n",
      "config 30, alpha = 0.0, lambda = 756.2, dropout = 0.00; 2 hidden layers with 49, 49 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 2.75e+03 logL: -1.37e+03 KL: 1.31e+02 MMD: 1.66e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.46e+03 logL: -1.10e+03 KL: 4.19e+01 MMD: 4.19e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.03e+03 logL: -9.04e+02 KL: 4.16e+01 MMD: 1.14e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 7.88e+02 logL: -6.94e+02 KL: 4.05e+01 MMD: 7.10e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 7.27e+02 logL: -6.40e+02 KL: 3.92e+01 MMD: 6.32e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 7.40e+02 logL: -6.47e+02 KL: 3.87e+01 MMD: 7.20e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 7.40e+02 logL: -6.51e+02 KL: 3.67e+01 MMD: 6.90e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 7.08e+02 logL: -6.22e+02 KL: 3.53e+01 MMD: 6.68e-02\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 6.68e+02 logL: -6.01e+02 KL: 3.52e+01 MMD: 4.25e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 6.73e+02 logL: -6.02e+02 KL: 3.46e+01 MMD: 4.83e-02\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 105 VALIDATION Loss: 6.69e+02 logL: -5.96e+02 KL: 3.48e+01 MMD: 5.04e-02\n",
      "config 30, alpha = 0.0, lambda = 275.5, dropout = 0.00; 2 hidden layers with 12, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.25e+03 logL: -5.51e+03 KL: 1.87e+02 MMD: 2.00e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.81e+03 logL: -4.11e+03 KL: 1.29e+02 MMD: 2.09e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.30e+03 logL: -3.69e+03 KL: 7.06e+01 MMD: 1.98e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.90e+03 logL: -2.70e+03 KL: 4.36e+01 MMD: 5.68e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.68e+03 logL: -2.57e+03 KL: 3.88e+01 MMD: 2.71e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 2.63e+03 logL: -2.54e+03 KL: 3.32e+01 MMD: 2.04e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 2.60e+03 logL: -2.52e+03 KL: 2.93e+01 MMD: 1.58e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 2.58e+03 logL: -2.51e+03 KL: 2.72e+01 MMD: 1.46e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 2.32e+03 logL: -2.24e+03 KL: 2.87e+01 MMD: 1.91e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.95e+03 logL: -1.89e+03 KL: 2.66e+01 MMD: 1.20e-01\n",
      "Epoch 00107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.99e+03 logL: -1.92e+03 KL: 2.46e+01 MMD: 1.67e-01\n",
      "Stopping\n",
      "====> Epoch: 111 VALIDATION Loss: 2.00e+03 logL: -1.92e+03 KL: 2.46e+01 MMD: 1.98e-01\n",
      "config 31, alpha = 0.0, lambda = 92.2, dropout = 0.00; 2 hidden layers with 6, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.55e+03 logL: -5.36e+03 KL: 1.09e+02 MMD: 8.65e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.85e+03 logL: -3.71e+03 KL: 6.83e+01 MMD: 8.06e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.75e+03 logL: -3.64e+03 KL: 3.69e+01 MMD: 8.69e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.72e+03 logL: -3.62e+03 KL: 2.39e+01 MMD: 7.99e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.69e+03 logL: -3.61e+03 KL: 1.77e+01 MMD: 5.93e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.66e+03 logL: -3.61e+03 KL: 1.35e+01 MMD: 4.15e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.64e+03 logL: -3.61e+03 KL: 1.16e+01 MMD: 2.74e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.63e+03 logL: -3.60e+03 KL: 1.04e+01 MMD: 1.73e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.62e+03 logL: -3.60e+03 KL: 1.03e+01 MMD: 1.06e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.57e+03 logL: -3.55e+03 KL: 1.15e+01 MMD: 1.15e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 3.45e+03 logL: -3.42e+03 KL: 1.57e+01 MMD: 1.56e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 3.35e+03 logL: -3.32e+03 KL: 1.52e+01 MMD: 1.54e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 3.25e+03 logL: -3.22e+03 KL: 1.38e+01 MMD: 1.22e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 3.18e+03 logL: -3.15e+03 KL: 1.41e+01 MMD: 1.36e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 3.16e+03 logL: -3.12e+03 KL: 1.62e+01 MMD: 2.33e-01\n",
      "Epoch 00157: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 160 VALIDATION Loss: 3.11e+03 logL: -3.09e+03 KL: 1.27e+01 MMD: 1.07e-01\n",
      "Stopping\n",
      "====> Epoch: 161 VALIDATION Loss: 3.11e+03 logL: -3.09e+03 KL: 1.27e+01 MMD: 1.02e-01\n",
      "config 31, alpha = 0.0, lambda = 65.0, dropout = 0.00; 2 hidden layers with 18, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.41e+03 logL: -4.23e+03 KL: 1.01e+02 MMD: 1.26e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.84e+03 logL: -3.69e+03 KL: 7.08e+01 MMD: 1.29e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.78e+03 logL: -3.65e+03 KL: 4.88e+01 MMD: 1.35e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.77e+03 logL: -3.64e+03 KL: 4.31e+01 MMD: 1.27e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.74e+03 logL: -3.63e+03 KL: 3.35e+01 MMD: 1.16e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.71e+03 logL: -3.62e+03 KL: 2.63e+01 MMD: 1.02e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.03e+03 logL: -2.90e+03 KL: 4.59e+01 MMD: 1.33e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.81e+03 logL: -2.69e+03 KL: 4.82e+01 MMD: 1.14e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.74e+03 logL: -2.61e+03 KL: 4.88e+01 MMD: 1.15e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.68e+03 logL: -2.56e+03 KL: 4.77e+01 MMD: 1.18e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.64e+03 logL: -2.53e+03 KL: 4.45e+01 MMD: 1.05e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.61e+03 logL: -2.51e+03 KL: 3.93e+01 MMD: 9.83e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 2.02e+03 logL: -1.92e+03 KL: 3.92e+01 MMD: 9.15e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 1.90e+03 logL: -1.83e+03 KL: 2.99e+01 MMD: 6.32e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 1.88e+03 logL: -1.82e+03 KL: 2.51e+01 MMD: 5.41e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 1.86e+03 logL: -1.83e+03 KL: 2.34e+01 MMD: 2.34e-01\n",
      "====> Epoch: 170 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 2.19e+01 MMD: 1.93e-01\n",
      "====> Epoch: 180 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 2.09e+01 MMD: 1.20e-01\n",
      "====> Epoch: 190 VALIDATION Loss: 1.82e+03 logL: -1.80e+03 KL: 1.99e+01 MMD: 4.79e-02\n",
      "Epoch 00196: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 200 VALIDATION Loss: 1.82e+03 logL: -1.80e+03 KL: 1.95e+01 MMD: 4.37e-02\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 1.82e+03 logL: -1.80e+03 KL: 1.95e+01 MMD: 4.37e-02\n",
      "config 31, alpha = 0.0, lambda = 2.4, dropout = 0.00; 2 hidden layers with 22, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.66e+03 logL: -2.54e+03 KL: 1.15e+02 MMD: 1.38e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.56e+03 logL: -1.48e+03 KL: 7.47e+01 MMD: 1.35e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.34e+03 logL: -1.28e+03 KL: 5.56e+01 MMD: 1.41e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 4.72e+01 MMD: 1.30e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.86e+02 logL: -9.37e+02 KL: 4.79e+01 MMD: 1.35e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.81e+02 logL: -9.37e+02 KL: 4.24e+01 MMD: 1.27e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.78e+02 logL: -9.37e+02 KL: 3.94e+01 MMD: 1.14e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.75e+02 logL: -9.36e+02 KL: 3.73e+01 MMD: 1.23e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.74e+02 logL: -9.37e+02 KL: 3.53e+01 MMD: 1.22e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.64e+02 logL: -9.30e+02 KL: 3.31e+01 MMD: 1.10e+00\n",
      "Epoch 00106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 9.59e+02 logL: -9.25e+02 KL: 3.23e+01 MMD: 1.17e+00\n",
      "Epoch 00113: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 117 VALIDATION Loss: 9.59e+02 logL: -9.25e+02 KL: 3.22e+01 MMD: 1.18e+00\n",
      "config 31, alpha = 0.0, lambda = 1.4, dropout = 0.00; 2 hidden layers with 10, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.79e+03 logL: -3.66e+03 KL: 1.27e+02 MMD: 1.80e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.60e+03 logL: -2.52e+03 KL: 7.77e+01 MMD: 1.85e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.97e+03 logL: -1.91e+03 KL: 6.56e+01 MMD: 1.90e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.39e+03 logL: -1.31e+03 KL: 6.96e+01 MMD: 1.66e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.34e+03 logL: -1.29e+03 KL: 5.70e+01 MMD: 1.73e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.34e+03 logL: -1.29e+03 KL: 5.05e+01 MMD: 1.75e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 4.68e+01 MMD: 1.84e+00\n",
      "Epoch 00071: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.31e+03 logL: -1.27e+03 KL: 4.57e+01 MMD: 1.80e+00\n",
      "Stopping\n",
      "====> Epoch: 82 VALIDATION Loss: 1.31e+03 logL: -1.27e+03 KL: 4.57e+01 MMD: 1.81e+00\n",
      "config 31, alpha = 0.0, lambda = 4938.7, dropout = 0.00; 2 hidden layers with 32, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.56e+03 logL: -3.98e+03 KL: 2.65e+01 MMD: 1.12e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 2.82e+03 logL: -2.41e+03 KL: 2.77e+01 MMD: 7.87e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 30 VALIDATION Loss: 2.37e+03 logL: -1.96e+03 KL: 2.69e+01 MMD: 7.74e-02\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.13e+03 logL: -1.85e+03 KL: 2.84e+01 MMD: 5.14e-02\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.16e+03 logL: -1.84e+03 KL: 2.84e+01 MMD: 5.89e-02\n",
      "Stopping\n",
      "====> Epoch: 51 VALIDATION Loss: 2.20e+03 logL: -1.84e+03 KL: 2.84e+01 MMD: 6.76e-02\n",
      "config 32, alpha = 0.0, lambda = 3621.6, dropout = 0.00; 2 hidden layers with 109, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.33e+03 logL: -4.63e+03 KL: 1.10e+02 MMD: 7.15e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 7.08e+03 logL: -4.60e+03 KL: 5.48e+01 MMD: 6.68e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 4.62e+03 logL: -4.55e+03 KL: 7.45e+00 MMD: 1.63e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.81e+03 logL: -3.69e+03 KL: 1.04e+01 MMD: 3.01e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.56e+03 logL: -3.51e+03 KL: 1.28e+01 MMD: 1.26e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 5.18e+03 logL: -4.54e+03 KL: 1.97e+01 MMD: 1.73e-01\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 66 VALIDATION Loss: 4.38e+03 logL: -4.07e+03 KL: 1.99e+01 MMD: 7.91e-02\n",
      "config 32, alpha = 0.0, lambda = 148.4, dropout = 0.00; 2 hidden layers with 10, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.86e+03 logL: -5.50e+03 KL: 2.01e+02 MMD: 1.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.58e+03 logL: -5.32e+03 KL: 1.04e+02 MMD: 1.11e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.34e+03 logL: -5.11e+03 KL: 6.16e+01 MMD: 1.12e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.82e+03 logL: -4.61e+03 KL: 4.33e+01 MMD: 1.17e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.00e+03 logL: -3.76e+03 KL: 7.36e+01 MMD: 1.13e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.88e+03 logL: -3.65e+03 KL: 6.63e+01 MMD: 1.14e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.83e+03 logL: -3.62e+03 KL: 5.04e+01 MMD: 1.05e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.79e+03 logL: -3.61e+03 KL: 3.08e+01 MMD: 1.00e+00\n",
      "====> Epoch: 90 VALIDATION Loss: inf logL: -inf KL: inf MMD: 7.89e-01\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 97 VALIDATION Loss: inf logL: -inf KL: inf MMD: 7.42e-01\n",
      "config 32, alpha = 0.0, lambda = 10.4, dropout = 0.00; 2 hidden layers with 159, 40 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.25e+03 logL: -3.98e+03 KL: 2.56e+02 MMD: 1.35e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.79e+03 logL: -2.66e+03 KL: 1.16e+02 MMD: 1.42e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.60e+03 logL: -1.50e+03 KL: 9.65e+01 MMD: 1.25e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.15e+03 logL: -1.06e+03 KL: 8.18e+01 MMD: 1.28e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.09e+03 logL: -1.01e+03 KL: 6.77e+01 MMD: 1.41e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.06e+03 logL: -9.90e+02 KL: 5.77e+01 MMD: 1.28e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.03e+03 logL: -9.70e+02 KL: 5.07e+01 MMD: 1.38e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.01e+03 logL: -9.56e+02 KL: 4.58e+01 MMD: 1.23e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.00e+03 logL: -9.49e+02 KL: 4.17e+01 MMD: 1.07e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.87e+02 logL: -9.40e+02 KL: 3.77e+01 MMD: 9.86e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 9.79e+02 logL: -9.36e+02 KL: 3.52e+01 MMD: 8.83e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 9.79e+02 logL: -9.38e+02 KL: 3.37e+01 MMD: 8.70e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 9.65e+02 logL: -9.27e+02 KL: 3.12e+01 MMD: 7.21e-01\n",
      "Epoch 00139: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 9.43e+02 logL: -9.06e+02 KL: 3.05e+01 MMD: 6.64e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 9.44e+02 logL: -9.07e+02 KL: 3.00e+01 MMD: 6.96e-01\n",
      "Epoch 00151: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 160 VALIDATION Loss: 9.41e+02 logL: -9.05e+02 KL: 2.97e+01 MMD: 6.71e-01\n",
      "Epoch 00165: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 169 VALIDATION Loss: 9.39e+02 logL: -9.04e+02 KL: 2.96e+01 MMD: 5.27e-01\n",
      "config 32, alpha = 0.0, lambda = 15.3, dropout = 0.00; 2 hidden layers with 58, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.40e+03 logL: -1.30e+03 KL: 7.89e+01 MMD: 1.65e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.43e+02 logL: -7.55e+02 KL: 6.53e+01 MMD: 1.61e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.19e+02 logL: -7.45e+02 KL: 5.17e+01 MMD: 1.57e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.66e+02 logL: -7.01e+02 KL: 4.65e+01 MMD: 1.28e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.80e+02 logL: -6.22e+02 KL: 4.17e+01 MMD: 1.14e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.70e+02 logL: -6.18e+02 KL: 3.80e+01 MMD: 9.67e-01\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 6.39e+02 logL: -5.89e+02 KL: 3.71e+01 MMD: 9.26e-01\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 6.38e+02 logL: -5.88e+02 KL: 3.70e+01 MMD: 9.25e-01\n",
      "config 32, alpha = 0.0, lambda = 62567.9, dropout = 0.00; 2 hidden layers with 26, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.11e+04 logL: -7.80e+03 KL: 2.66e+00 MMD: 5.25e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 8.00e+03 logL: -5.06e+03 KL: 3.54e+00 MMD: 4.69e-02\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 8.02e+03 logL: -4.66e+03 KL: 4.74e+00 MMD: 5.36e-02\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 7.81e+03 logL: -4.64e+03 KL: 4.85e+00 MMD: 5.06e-02\n",
      "Stopping\n",
      "====> Epoch: 41 VALIDATION Loss: 7.63e+03 logL: -4.63e+03 KL: 4.86e+00 MMD: 4.79e-02\n",
      "config 33, alpha = 0.0, lambda = 97.0, dropout = 0.00; 2 hidden layers with 5, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.60e+03 logL: -5.38e+03 KL: 1.40e+02 MMD: 8.09e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.19e+03 logL: -3.99e+03 KL: 1.28e+02 MMD: 6.78e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.77e+03 logL: -3.64e+03 KL: 5.53e+01 MMD: 7.52e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.74e+03 logL: -3.63e+03 KL: 3.46e+01 MMD: 7.57e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.70e+03 logL: -3.62e+03 KL: 2.38e+01 MMD: 5.63e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.68e+03 logL: -3.62e+03 KL: 1.71e+01 MMD: 4.88e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.65e+03 logL: -3.61e+03 KL: 1.33e+01 MMD: 3.20e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.63e+03 logL: -3.61e+03 KL: 1.13e+01 MMD: 1.37e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.63e+03 logL: -3.61e+03 KL: 1.05e+01 MMD: 8.44e-02\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 3.62e+03 logL: -3.60e+03 KL: 1.03e+01 MMD: 5.62e-02\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 102 VALIDATION Loss: 3.62e+03 logL: -3.60e+03 KL: 1.03e+01 MMD: 6.41e-02\n",
      "config 33, alpha = 0.0, lambda = 20.7, dropout = 0.00; 2 hidden layers with 28, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.70e+03 logL: -2.57e+03 KL: 1.09e+02 MMD: 9.41e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 2.63e+03 logL: -2.55e+03 KL: 5.61e+01 MMD: 1.10e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.95e+03 logL: -1.87e+03 KL: 6.30e+01 MMD: 1.05e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.89e+03 logL: -1.82e+03 KL: 4.22e+01 MMD: 1.14e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.87e+03 logL: -1.82e+03 KL: 3.57e+01 MMD: 1.01e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.87e+03 logL: -1.82e+03 KL: 3.16e+01 MMD: 9.70e-01\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 3.03e+01 MMD: 9.82e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.95e+01 MMD: 9.48e-01\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.94e+01 MMD: 9.93e-01\n",
      "config 33, alpha = 0.0, lambda = 70150.3, dropout = 0.00; 2 hidden layers with 17, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.90e+03 logL: -6.76e+03 KL: 2.27e+00 MMD: 1.63e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 7.13e+03 logL: -5.08e+03 KL: 3.89e+00 MMD: 2.93e-02\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 6.40e+03 logL: -4.77e+03 KL: 4.23e+00 MMD: 2.31e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 7.06e+03 logL: -4.73e+03 KL: 4.48e+00 MMD: 3.31e-02\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 44 VALIDATION Loss: 6.00e+03 logL: -4.72e+03 KL: 4.55e+00 MMD: 1.81e-02\n",
      "config 33, alpha = 0.0, lambda = 4570.2, dropout = 0.00; 2 hidden layers with 15, 15 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 8.82e+03 logL: -6.45e+03 KL: 5.08e+01 MMD: 5.08e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.74e+03 logL: -5.25e+03 KL: 2.03e+01 MMD: 1.05e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 4.26e+03 logL: -3.88e+03 KL: 2.01e+01 MMD: 7.77e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.88e+03 logL: -3.56e+03 KL: 1.85e+01 MMD: 6.49e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 2.82e+03 logL: -2.59e+03 KL: 1.86e+01 MMD: 4.43e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 2.83e+03 logL: -2.57e+03 KL: 1.80e+01 MMD: 5.30e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 2.74e+03 logL: -2.52e+03 KL: 1.75e+01 MMD: 4.42e-02\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 2.70e+03 logL: -2.51e+03 KL: 1.72e+01 MMD: 3.78e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 2.67e+03 logL: -2.51e+03 KL: 1.73e+01 MMD: 3.06e-02\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 95 VALIDATION Loss: 2.67e+03 logL: -2.51e+03 KL: 1.73e+01 MMD: 3.27e-02\n",
      "config 33, alpha = 0.0, lambda = 89.6, dropout = 0.00; 2 hidden layers with 81, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.84e+03 logL: -2.51e+03 KL: 1.60e+02 MMD: 1.89e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.50e+03 logL: -1.24e+03 KL: 9.73e+01 MMD: 1.90e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.20e+03 logL: -9.70e+02 KL: 7.00e+01 MMD: 1.76e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.13e+03 logL: -9.63e+02 KL: 4.98e+01 MMD: 1.31e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.06e+03 logL: -9.50e+02 KL: 4.59e+01 MMD: 6.91e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.03e+03 logL: -9.45e+02 KL: 4.44e+01 MMD: 4.24e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.41e+02 logL: -7.70e+02 KL: 4.42e+01 MMD: 3.01e-01\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 8.11e+02 logL: -7.38e+02 KL: 4.22e+01 MMD: 3.51e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 8.02e+02 logL: -7.37e+02 KL: 4.19e+01 MMD: 2.60e-01\n",
      "Epoch 00098: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 7.99e+02 logL: -7.34e+02 KL: 4.13e+01 MMD: 2.60e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 7.99e+02 logL: -7.35e+02 KL: 4.13e+01 MMD: 2.58e-01\n",
      "Epoch 00110: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 114 VALIDATION Loss: 7.98e+02 logL: -7.34e+02 KL: 4.12e+01 MMD: 2.55e-01\n",
      "config 34, alpha = 0.0, lambda = 36.3, dropout = 0.00; 2 hidden layers with 5, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.56e+03 logL: -5.38e+03 KL: 1.49e+02 MMD: 8.17e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.07e+03 logL: -3.91e+03 KL: 1.36e+02 MMD: 7.02e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.71e+03 logL: -3.64e+03 KL: 4.57e+01 MMD: 7.79e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.68e+03 logL: -3.63e+03 KL: 2.96e+01 MMD: 7.28e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.67e+03 logL: -3.62e+03 KL: 2.22e+01 MMD: 7.83e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.66e+03 logL: -3.62e+03 KL: 1.79e+01 MMD: 6.98e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.65e+03 logL: -3.62e+03 KL: 1.51e+01 MMD: 6.44e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.65e+03 logL: -3.61e+03 KL: 1.30e+01 MMD: 5.48e-01\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.64e+03 logL: -3.61e+03 KL: 1.20e+01 MMD: 4.75e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.64e+03 logL: -3.61e+03 KL: 1.19e+01 MMD: 4.52e-01\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 100 VALIDATION Loss: 3.64e+03 logL: -3.61e+03 KL: 1.19e+01 MMD: 4.52e-01\n",
      "config 34, alpha = 0.0, lambda = 620.7, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.15e+03 logL: -5.32e+03 KL: 1.99e+02 MMD: 1.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.61e+03 logL: -3.75e+03 KL: 1.30e+02 MMD: 1.18e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.94e+03 logL: -3.19e+03 KL: 1.26e+02 MMD: 1.01e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.24e+03 logL: -2.54e+03 KL: 7.24e+01 MMD: 1.02e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.53e+03 logL: -1.85e+03 KL: 6.29e+01 MMD: 9.82e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 2.39e+03 logL: -1.86e+03 KL: 3.29e+01 MMD: 7.99e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 2.03e+03 logL: -1.89e+03 KL: 2.30e+01 MMD: 1.98e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 2.00e+03 logL: -1.89e+03 KL: 2.28e+01 MMD: 1.45e-01\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.91e+03 logL: -1.83e+03 KL: 2.22e+01 MMD: 7.72e-02\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.90e+03 logL: -1.83e+03 KL: 2.22e+01 MMD: 7.21e-02\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.90e+03 logL: -1.83e+03 KL: 2.22e+01 MMD: 7.73e-02\n",
      "Epoch 00111: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 115 VALIDATION Loss: 1.90e+03 logL: -1.83e+03 KL: 2.22e+01 MMD: 6.86e-02\n",
      "config 34, alpha = 0.0, lambda = 3.3, dropout = 0.00; 2 hidden layers with 100, 80 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.08e+03 logL: -9.81e+02 KL: 9.79e+01 MMD: 1.21e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.01e+03 logL: -9.51e+02 KL: 5.81e+01 MMD: 1.37e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.93e+02 logL: -9.43e+02 KL: 4.71e+01 MMD: 1.22e+00\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 9.65e+02 logL: -9.20e+02 KL: 4.27e+01 MMD: 1.18e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.64e+02 logL: -9.20e+02 KL: 4.13e+01 MMD: 1.33e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.62e+02 logL: -9.20e+02 KL: 3.98e+01 MMD: 1.13e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.58e+02 logL: -9.17e+02 KL: 3.88e+01 MMD: 1.10e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.58e+02 logL: -9.18e+02 KL: 3.80e+01 MMD: 1.10e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.53e+02 logL: -9.13e+02 KL: 3.75e+01 MMD: 1.06e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.55e+02 logL: -9.16e+02 KL: 3.69e+01 MMD: 1.13e+00\n",
      "Stopping\n",
      "====> Epoch: 100 VALIDATION Loss: 9.55e+02 logL: -9.16e+02 KL: 3.69e+01 MMD: 1.13e+00\n",
      "config 34, alpha = 0.0, lambda = 1.5, dropout = 0.00; 2 hidden layers with 15, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.51e+03 logL: -5.38e+03 KL: 1.28e+02 MMD: 2.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.74e+03 logL: -2.64e+03 KL: 9.32e+01 MMD: 2.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.00e+03 logL: -1.92e+03 KL: 8.04e+01 MMD: 1.74e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.91e+03 logL: -1.85e+03 KL: 5.76e+01 MMD: 1.87e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.36e+03 logL: -1.30e+03 KL: 5.97e+01 MMD: 1.70e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.33e+03 logL: -1.28e+03 KL: 5.08e+01 MMD: 1.63e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.33e+03 logL: -1.28e+03 KL: 4.63e+01 MMD: 1.81e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.00e+03 logL: -9.55e+02 KL: 4.79e+01 MMD: 1.61e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.81e+02 logL: -9.36e+02 KL: 4.49e+01 MMD: 1.72e+00\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 9.74e+02 logL: -9.30e+02 KL: 4.39e+01 MMD: 1.61e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 9.74e+02 logL: -9.30e+02 KL: 4.35e+01 MMD: 1.55e+00\n",
      "Stopping\n",
      "====> Epoch: 110 VALIDATION Loss: 9.74e+02 logL: -9.30e+02 KL: 4.35e+01 MMD: 1.55e+00\n",
      "config 34, alpha = 0.0, lambda = 10.0, dropout = 0.00; 2 hidden layers with 29, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.72e+03 logL: -2.57e+03 KL: 1.22e+02 MMD: 2.42e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.97e+03 logL: -1.87e+03 KL: 7.58e+01 MMD: 2.22e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.46e+03 logL: -1.34e+03 KL: 9.48e+01 MMD: 2.18e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.38e+03 logL: -1.30e+03 KL: 5.86e+01 MMD: 2.40e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.35e+03 logL: -1.28e+03 KL: 5.13e+01 MMD: 2.18e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.34e+03 logL: -1.27e+03 KL: 4.62e+01 MMD: 1.96e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.34e+03 logL: -1.28e+03 KL: 4.21e+01 MMD: 1.62e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 3.94e+01 MMD: 1.35e+00\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.32e+03 logL: -1.26e+03 KL: 3.87e+01 MMD: 1.36e+00\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 98 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 3.86e+01 MMD: 1.39e+00\n",
      "config 35, alpha = 0.0, lambda = 1.0, dropout = 0.00; 2 hidden layers with 63, 4 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 3.75e+03 logL: -3.65e+03 KL: 1.04e+02 MMD: 7.73e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.64e+03 logL: -3.58e+03 KL: 5.71e+01 MMD: 7.03e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.45e+03 logL: -3.40e+03 KL: 4.38e+01 MMD: 7.11e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.40e+03 logL: -3.36e+03 KL: 3.96e+01 MMD: 6.71e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.22e+03 logL: -3.19e+03 KL: 3.13e+01 MMD: 6.52e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.18e+03 logL: -3.15e+03 KL: 2.82e+01 MMD: 6.80e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.20e+03 logL: -3.17e+03 KL: 2.65e+01 MMD: 7.65e-01\n",
      "Epoch 00071: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.12e+03 logL: -3.09e+03 KL: 2.46e+01 MMD: 7.30e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.11e+03 logL: -3.09e+03 KL: 2.42e+01 MMD: 6.52e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.11e+03 logL: -3.08e+03 KL: 2.37e+01 MMD: 6.90e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 3.10e+03 logL: -3.07e+03 KL: 2.32e+01 MMD: 7.17e-01\n",
      "Stopping\n",
      "====> Epoch: 119 VALIDATION Loss: 3.10e+03 logL: -3.07e+03 KL: 2.25e+01 MMD: 6.91e-01\n",
      "config 35, alpha = 0.0, lambda = 219.0, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.04e+03 logL: -6.57e+03 KL: 2.08e+02 MMD: 1.19e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.95e+03 logL: -5.51e+03 KL: 1.89e+02 MMD: 1.15e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.69e+03 logL: -5.27e+03 KL: 1.40e+02 MMD: 1.24e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.53e+03 logL: -5.16e+03 KL: 9.85e+01 MMD: 1.25e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.38e+03 logL: -5.06e+03 KL: 7.17e+01 MMD: 1.15e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.25e+03 logL: -4.96e+03 KL: 4.87e+01 MMD: 1.14e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.99e+03 logL: -4.76e+03 KL: 2.79e+01 MMD: 9.33e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.93e+03 logL: -3.76e+03 KL: 2.03e+01 MMD: 6.83e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.71e+03 logL: -3.65e+03 KL: 1.47e+01 MMD: 2.19e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.62e+03 logL: -3.58e+03 KL: 1.55e+01 MMD: 1.16e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 2.62e+03 logL: -2.56e+03 KL: 1.90e+01 MMD: 1.81e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 2.58e+03 logL: -2.53e+03 KL: 1.90e+01 MMD: 1.41e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 2.57e+03 logL: -2.52e+03 KL: 1.84e+01 MMD: 1.15e-01\n",
      "Epoch 00138: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 2.56e+03 logL: -2.52e+03 KL: 1.70e+01 MMD: 1.07e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 2.55e+03 logL: -2.51e+03 KL: 1.71e+01 MMD: 9.80e-02\n",
      "====> Epoch: 160 VALIDATION Loss: 2.55e+03 logL: -2.51e+03 KL: 1.66e+01 MMD: 1.17e-01\n",
      "Stopping\n",
      "====> Epoch: 163 VALIDATION Loss: 2.55e+03 logL: -2.51e+03 KL: 1.63e+01 MMD: 8.70e-02\n",
      "config 35, alpha = 0.0, lambda = 463.4, dropout = 0.00; 2 hidden layers with 39, 33 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.06e+03 logL: -1.35e+03 KL: 1.10e+02 MMD: 1.30e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.61e+03 logL: -1.04e+03 KL: 4.59e+01 MMD: 1.13e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.08e+03 logL: -9.98e+02 KL: 3.52e+01 MMD: 1.11e-01\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.01e+03 logL: -9.49e+02 KL: 3.42e+01 MMD: 5.53e-02\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.01e+03 logL: -9.46e+02 KL: 3.40e+01 MMD: 5.62e-02\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 57 VALIDATION Loss: 1.01e+03 logL: -9.46e+02 KL: 3.40e+01 MMD: 5.79e-02\n",
      "config 35, alpha = 0.0, lambda = 6.3, dropout = 0.00; 2 hidden layers with 26, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.94e+03 logL: -1.83e+03 KL: 9.86e+01 MMD: 1.60e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.04e+03 logL: -9.55e+02 KL: 7.70e+01 MMD: 1.47e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.94e+02 logL: -9.28e+02 KL: 5.73e+01 MMD: 1.58e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.09e+02 logL: -7.49e+02 KL: 5.17e+01 MMD: 1.58e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.05e+02 logL: -7.50e+02 KL: 4.59e+01 MMD: 1.58e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.99e+02 logL: -6.45e+02 KL: 4.63e+01 MMD: 1.47e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.53e+02 logL: -6.03e+02 KL: 4.32e+01 MMD: 1.40e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.44e+02 logL: -5.97e+02 KL: 4.05e+01 MMD: 1.38e+00\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 6.38e+02 logL: -5.93e+02 KL: 3.92e+01 MMD: 1.16e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.37e+02 logL: -5.91e+02 KL: 3.88e+01 MMD: 1.32e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.36e+02 logL: -5.90e+02 KL: 3.85e+01 MMD: 1.30e+00\n",
      "Epoch 00117: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 6.34e+02 logL: -5.90e+02 KL: 3.81e+01 MMD: 1.27e+00\n",
      "Epoch 00124: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 128 VALIDATION Loss: 6.34e+02 logL: -5.90e+02 KL: 3.81e+01 MMD: 1.23e+00\n",
      "config 35, alpha = 0.0, lambda = 60.4, dropout = 0.00; 2 hidden layers with 122, 111 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.03e+03 logL: -8.15e+02 KL: 1.01e+02 MMD: 1.99e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.66e+02 logL: -5.93e+02 KL: 7.21e+01 MMD: 1.69e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.91e+02 logL: -4.51e+02 KL: 5.76e+01 MMD: 1.39e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.59e+02 logL: -4.58e+02 KL: 5.01e+01 MMD: 8.66e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 5.45e+02 logL: -4.71e+02 KL: 4.62e+01 MMD: 4.73e-01\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.82e+02 logL: -4.14e+02 KL: 4.59e+01 MMD: 3.69e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 4.74e+02 logL: -4.13e+02 KL: 4.57e+01 MMD: 2.53e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 4.71e+02 logL: -4.12e+02 KL: 4.58e+01 MMD: 2.30e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 4.64e+02 logL: -4.09e+02 KL: 4.49e+01 MMD: 1.76e-01\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 4.62e+02 logL: -4.06e+02 KL: 4.47e+01 MMD: 1.99e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 4.61e+02 logL: -4.05e+02 KL: 4.49e+01 MMD: 1.80e-01\n",
      "Epoch 00110: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 120 VALIDATION Loss: 4.63e+02 logL: -4.05e+02 KL: 4.48e+01 MMD: 2.10e-01\n",
      "Epoch 00122: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 126 VALIDATION Loss: 4.62e+02 logL: -4.05e+02 KL: 4.48e+01 MMD: 2.14e-01\n",
      "config 36, alpha = 0.0, lambda = 386.8, dropout = 0.00; 2 hidden layers with 11, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.18e+03 logL: -3.76e+03 KL: 1.63e+02 MMD: 6.77e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.03e+03 logL: -3.67e+03 KL: 7.22e+01 MMD: 7.48e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.97e+03 logL: -3.64e+03 KL: 5.25e+01 MMD: 7.19e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.96e+03 logL: -3.62e+03 KL: 4.06e+01 MMD: 7.73e-01\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.94e+03 logL: -3.62e+03 KL: 3.88e+01 MMD: 7.17e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.96e+03 logL: -3.62e+03 KL: 3.58e+01 MMD: 7.88e-01\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.92e+03 logL: -3.62e+03 KL: 3.41e+01 MMD: 7.05e-01\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 3.94e+03 logL: -3.62e+03 KL: 3.40e+01 MMD: 7.53e-01\n",
      "config 36, alpha = 0.0, lambda = 16.2, dropout = 0.00; 2 hidden layers with 131, 98 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.88e+03 logL: -1.81e+03 KL: 5.32e+01 MMD: 1.11e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.82e+03 logL: -1.76e+03 KL: 4.31e+01 MMD: 1.03e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.64e+03 logL: -1.59e+03 KL: 3.44e+01 MMD: 1.04e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.61e+03 logL: -1.57e+03 KL: 3.20e+01 MMD: 9.38e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.58e+03 logL: -1.54e+03 KL: 3.01e+01 MMD: 9.20e-01\n",
      "Epoch 00058: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.54e+03 logL: -1.50e+03 KL: 2.67e+01 MMD: 8.74e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.53e+03 logL: -1.49e+03 KL: 2.63e+01 MMD: 8.96e-01\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 77 VALIDATION Loss: 1.55e+03 logL: -1.51e+03 KL: 2.63e+01 MMD: 8.19e-01\n",
      "config 36, alpha = 0.0, lambda = 91256.6, dropout = 0.00; 2 hidden layers with 135, 30 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 8.20e+03 logL: -5.28e+03 KL: 3.53e+00 MMD: 3.20e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 8.35e+03 logL: -4.81e+03 KL: 4.40e+00 MMD: 3.87e-02\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 26 VALIDATION Loss: 7.21e+03 logL: -4.67e+03 KL: 5.17e+00 MMD: 2.78e-02\n",
      "config 36, alpha = 0.0, lambda = 32.8, dropout = 0.00; 2 hidden layers with 15, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.17e+03 logL: -3.89e+03 KL: 2.29e+02 MMD: 1.70e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.70e+03 logL: -2.56e+03 KL: 8.43e+01 MMD: 1.83e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.03e+03 logL: -1.90e+03 KL: 7.23e+01 MMD: 1.72e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.95e+03 logL: -1.84e+03 KL: 5.29e+01 MMD: 1.81e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.91e+03 logL: -1.82e+03 KL: 4.35e+01 MMD: 1.39e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.40e+03 logL: -1.31e+03 KL: 4.59e+01 MMD: 1.43e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+03 logL: -1.27e+03 KL: 4.03e+01 MMD: 1.16e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.34e+03 logL: -1.27e+03 KL: 3.73e+01 MMD: 9.76e-01\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.33e+03 logL: -1.26e+03 KL: 3.67e+01 MMD: 8.09e-01\n",
      "Epoch 00098: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.33e+03 logL: -1.26e+03 KL: 3.64e+01 MMD: 7.97e-01\n",
      "Stopping\n",
      "====> Epoch: 102 VALIDATION Loss: 1.33e+03 logL: -1.26e+03 KL: 3.64e+01 MMD: 8.74e-01\n",
      "config 36, alpha = 0.0, lambda = 107.9, dropout = 0.00; 2 hidden layers with 14, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.28e+03 logL: -1.93e+03 KL: 1.41e+02 MMD: 2.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.13e+03 logL: -1.85e+03 KL: 7.11e+01 MMD: 1.98e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.52e+03 logL: -1.32e+03 KL: 5.25e+01 MMD: 1.46e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.43e+03 logL: -1.30e+03 KL: 4.47e+01 MMD: 7.55e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.37e+03 logL: -1.29e+03 KL: 4.23e+01 MMD: 4.01e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.36e+03 logL: -1.29e+03 KL: 4.06e+01 MMD: 3.11e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+03 logL: -1.28e+03 KL: 3.97e+01 MMD: 3.02e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.35e+03 logL: -1.29e+03 KL: 3.79e+01 MMD: 2.66e-01\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.34e+03 logL: -1.27e+03 KL: 3.77e+01 MMD: 3.22e-01\n",
      "Stopping\n",
      "====> Epoch: 92 VALIDATION Loss: 1.34e+03 logL: -1.27e+03 KL: 3.76e+01 MMD: 2.84e-01\n",
      "config 37, alpha = 0.0, lambda = 75044.0, dropout = 0.00; 2 hidden layers with 6, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.89e+03 logL: -6.40e+03 KL: 1.79e+00 MMD: 6.41e-03\n",
      "====> Epoch: 20 VALIDATION Loss: 6.83e+03 logL: -5.10e+03 KL: 2.93e+00 MMD: 2.30e-02\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.15e+03 logL: -4.63e+03 KL: 3.49e+00 MMD: 6.91e-03\n",
      "====> Epoch: 40 VALIDATION Loss: 4.96e+03 logL: -4.60e+03 KL: 3.63e+00 MMD: 4.71e-03\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 5.10e+03 logL: -4.59e+03 KL: 3.76e+00 MMD: 6.78e-03\n",
      "Stopping\n",
      "====> Epoch: 53 VALIDATION Loss: 5.10e+03 logL: -4.59e+03 KL: 3.77e+00 MMD: 6.77e-03\n",
      "config 37, alpha = 0.0, lambda = 6187.9, dropout = 0.00; 2 hidden layers with 54, 28 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.14e+03 logL: -3.69e+03 KL: 2.09e+01 MMD: 6.90e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 2.90e+03 logL: -2.70e+03 KL: 1.59e+01 MMD: 3.09e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.70e+03 logL: -2.54e+03 KL: 1.62e+01 MMD: 2.28e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.07e+03 logL: -1.89e+03 KL: 2.10e+01 MMD: 2.61e-02\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.95e+03 logL: -1.83e+03 KL: 2.20e+01 MMD: 1.53e-02\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.94e+03 logL: -1.82e+03 KL: 2.22e+01 MMD: 1.50e-02\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.00e+03 logL: -1.82e+03 KL: 2.23e+01 MMD: 2.56e-02\n",
      "Stopping\n",
      "====> Epoch: 72 VALIDATION Loss: 1.96e+03 logL: -1.82e+03 KL: 2.23e+01 MMD: 1.87e-02\n",
      "config 37, alpha = 0.0, lambda = 156.0, dropout = 0.00; 2 hidden layers with 61, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.75e+03 logL: -4.42e+03 KL: 7.80e+01 MMD: 1.64e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.92e+03 logL: -3.63e+03 KL: 6.05e+01 MMD: 1.44e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.97e+03 logL: -3.68e+03 KL: 7.15e+01 MMD: 1.39e+00\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 3.87e+03 logL: -3.59e+03 KL: 5.48e+01 MMD: 1.46e+00\n",
      "config 37, alpha = 0.0, lambda = 5332.4, dropout = 0.00; 2 hidden layers with 64, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.40e+03 logL: -4.62e+03 KL: 1.56e+01 MMD: 1.43e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.17e+03 logL: -3.74e+03 KL: 2.07e+01 MMD: 7.64e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.95e+03 logL: -3.58e+03 KL: 2.52e+01 MMD: 6.49e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.92e+03 logL: -2.58e+03 KL: 2.91e+01 MMD: 5.80e-02\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.82e+03 logL: -2.52e+03 KL: 2.51e+01 MMD: 5.32e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 2.77e+03 logL: -2.51e+03 KL: 2.39e+01 MMD: 4.33e-02\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.91e+03 logL: -2.51e+03 KL: 2.34e+01 MMD: 7.03e-02\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 77 VALIDATION Loss: 2.85e+03 logL: -2.51e+03 KL: 2.35e+01 MMD: 5.91e-02\n",
      "config 37, alpha = 0.0, lambda = 28805.9, dropout = 0.00; 2 hidden layers with 45, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.40e+03 logL: -5.03e+03 KL: 3.44e+00 MMD: 4.75e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 6.64e+03 logL: -4.59e+03 KL: 5.04e+00 MMD: 7.10e-02\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.88e+03 logL: -4.55e+03 KL: 5.08e+00 MMD: 4.60e-02\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 39 VALIDATION Loss: 5.78e+03 logL: -4.54e+03 KL: 5.30e+00 MMD: 4.27e-02\n",
      "config 38, alpha = 0.0, lambda = 16.4, dropout = 0.00; 2 hidden layers with 5, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.06e+04 logL: -1.04e+04 KL: 1.86e+02 MMD: 7.43e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 7.96e+03 logL: -7.74e+03 KL: 2.04e+02 MMD: 6.84e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 6.33e+03 logL: -6.11e+03 KL: 2.08e+02 MMD: 7.72e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 5.90e+03 logL: -5.69e+03 KL: 2.06e+02 MMD: 7.77e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 5.68e+03 logL: -5.48e+03 KL: 1.90e+02 MMD: 7.52e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 5.47e+03 logL: -5.29e+03 KL: 1.62e+02 MMD: 7.05e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 5.25e+03 logL: -5.12e+03 KL: 1.20e+02 MMD: 7.10e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 4.98e+03 logL: -4.89e+03 KL: 8.00e+01 MMD: 7.59e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 4.18e+03 logL: -4.08e+03 KL: 9.42e+01 MMD: 7.43e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.67e+03 logL: -3.60e+03 KL: 6.68e+01 MMD: 6.79e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 3.53e+03 logL: -3.47e+03 KL: 4.93e+01 MMD: 6.95e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 3.50e+03 logL: -3.45e+03 KL: 3.65e+01 MMD: 6.94e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 3.49e+03 logL: -3.45e+03 KL: 2.94e+01 MMD: 6.67e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 3.47e+03 logL: -3.43e+03 KL: 2.49e+01 MMD: 6.80e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 3.46e+03 logL: -3.42e+03 KL: 2.20e+01 MMD: 6.37e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 3.44e+03 logL: -3.41e+03 KL: 1.98e+01 MMD: 6.51e-01\n",
      "====> Epoch: 170 VALIDATION Loss: 3.42e+03 logL: -3.40e+03 KL: 1.81e+01 MMD: 5.60e-01\n",
      "====> Epoch: 180 VALIDATION Loss: 3.40e+03 logL: -3.37e+03 KL: 1.74e+01 MMD: 5.29e-01\n",
      "====> Epoch: 190 VALIDATION Loss: 3.39e+03 logL: -3.36e+03 KL: 1.74e+01 MMD: 5.21e-01\n",
      "====> Epoch: 200 VALIDATION Loss: 3.39e+03 logL: -3.36e+03 KL: 1.67e+01 MMD: 4.67e-01\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 3.39e+03 logL: -3.36e+03 KL: 1.67e+01 MMD: 4.67e-01\n",
      "config 38, alpha = 0.0, lambda = 3.9, dropout = 0.00; 2 hidden layers with 135, 132 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 5.04e+01 MMD: 1.04e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.74e+03 logL: -1.70e+03 KL: 4.00e+01 MMD: 9.27e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.64e+03 logL: -1.60e+03 KL: 3.37e+01 MMD: 1.06e+00\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.59e+03 logL: -1.56e+03 KL: 3.23e+01 MMD: 9.96e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.56e+03 logL: -1.52e+03 KL: 3.10e+01 MMD: 9.66e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.55e+03 logL: -1.52e+03 KL: 3.02e+01 MMD: 9.68e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.56e+03 logL: -1.53e+03 KL: 2.94e+01 MMD: 9.41e-01\n",
      "Epoch 00071: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 75 VALIDATION Loss: 1.55e+03 logL: -1.52e+03 KL: 2.96e+01 MMD: 9.68e-01\n",
      "config 38, alpha = 0.0, lambda = 1.5, dropout = 0.00; 2 hidden layers with 103, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.71e+03 logL: -3.62e+03 KL: 8.40e+01 MMD: 1.77e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.90e+03 logL: -1.83e+03 KL: 7.10e+01 MMD: 1.54e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.36e+03 logL: -1.30e+03 KL: 6.27e+01 MMD: 1.49e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 4.78e+01 MMD: 1.51e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 4.16e+01 MMD: 1.32e+00\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.29e+03 logL: -1.25e+03 KL: 3.90e+01 MMD: 1.31e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.29e+03 logL: -1.25e+03 KL: 3.82e+01 MMD: 1.52e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.28e+03 logL: -1.24e+03 KL: 3.77e+01 MMD: 1.40e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.17e+03 logL: -1.13e+03 KL: 4.24e+01 MMD: 1.39e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.94e+02 logL: -9.49e+02 KL: 4.39e+01 MMD: 1.45e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 9.74e+02 logL: -9.33e+02 KL: 4.03e+01 MMD: 1.34e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 9.68e+02 logL: -9.29e+02 KL: 3.82e+01 MMD: 1.39e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 9.68e+02 logL: -9.30e+02 KL: 3.69e+01 MMD: 1.38e+00\n",
      "Epoch 00135: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 137 VALIDATION Loss: 9.65e+02 logL: -9.28e+02 KL: 3.65e+01 MMD: 1.31e+00\n",
      "config 38, alpha = 0.0, lambda = 3.6, dropout = 0.00; 2 hidden layers with 114, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.07e+03 logL: -1.93e+03 KL: 1.36e+02 MMD: 1.72e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+03 logL: -1.30e+03 KL: 8.00e+01 MMD: 1.72e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.43e+03 logL: -1.37e+03 KL: 6.00e+01 MMD: 1.76e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.35e+03 logL: -1.29e+03 KL: 5.26e+01 MMD: 1.72e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.30e+03 logL: -1.25e+03 KL: 4.84e+01 MMD: 1.66e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.02e+03 logL: -9.63e+02 KL: 4.95e+01 MMD: 1.63e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.83e+02 logL: -9.34e+02 KL: 4.52e+01 MMD: 1.47e+00\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.60e+02 logL: -9.13e+02 KL: 4.40e+01 MMD: 1.43e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.60e+02 logL: -9.13e+02 KL: 4.28e+01 MMD: 1.55e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.57e+02 logL: -9.12e+02 KL: 4.17e+01 MMD: 1.52e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 9.57e+02 logL: -9.12e+02 KL: 4.10e+01 MMD: 1.59e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 9.55e+02 logL: -9.11e+02 KL: 3.99e+01 MMD: 1.46e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 9.54e+02 logL: -9.12e+02 KL: 3.91e+01 MMD: 1.45e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 9.52e+02 logL: -9.10e+02 KL: 3.84e+01 MMD: 1.46e+00\n",
      "Stopping\n",
      "====> Epoch: 142 VALIDATION Loss: 9.53e+02 logL: -9.11e+02 KL: 3.80e+01 MMD: 1.46e+00\n",
      "config 38, alpha = 0.0, lambda = 9.7, dropout = 0.00; 2 hidden layers with 25, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.79e+03 logL: -4.66e+03 KL: 1.13e+02 MMD: 2.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.00e+03 logL: -1.88e+03 KL: 1.03e+02 MMD: 1.98e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.41e+03 logL: -1.31e+03 KL: 7.81e+01 MMD: 2.12e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+03 logL: -1.28e+03 KL: 6.04e+01 MMD: 2.02e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.35e+03 logL: -1.28e+03 KL: 5.26e+01 MMD: 2.04e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.34e+03 logL: -1.27e+03 KL: 4.79e+01 MMD: 1.77e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.02e+03 logL: -9.50e+02 KL: 5.06e+01 MMD: 1.84e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.96e+02 logL: -9.36e+02 KL: 4.70e+01 MMD: 1.52e+00\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 9.92e+02 logL: -9.32e+02 KL: 4.55e+01 MMD: 1.65e+00\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 9.90e+02 logL: -9.31e+02 KL: 4.52e+01 MMD: 1.55e+00\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 104 VALIDATION Loss: 9.90e+02 logL: -9.31e+02 KL: 4.51e+01 MMD: 1.54e+00\n",
      "config 39, alpha = 0.0, lambda = 11.8, dropout = 0.00; 2 hidden layers with 12, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.81e+03 logL: -3.65e+03 KL: 1.52e+02 MMD: 7.84e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.65e+03 logL: -3.58e+03 KL: 5.90e+01 MMD: 7.83e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.54e+03 logL: -3.49e+03 KL: 4.27e+01 MMD: 8.00e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.43e+03 logL: -3.39e+03 KL: 3.42e+01 MMD: 7.02e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.29e+03 logL: -3.25e+03 KL: 2.85e+01 MMD: 8.69e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.20e+03 logL: -3.17e+03 KL: 2.42e+01 MMD: 7.95e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.13e+03 logL: -3.10e+03 KL: 1.96e+01 MMD: 7.61e-01\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.12e+03 logL: -3.10e+03 KL: 1.80e+01 MMD: 8.27e-01\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 3.12e+03 logL: -3.09e+03 KL: 1.78e+01 MMD: 8.22e-01\n",
      "config 39, alpha = 0.0, lambda = 1.1, dropout = 0.00; 2 hidden layers with 12, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.88e+03 logL: -3.74e+03 KL: 1.42e+02 MMD: 1.02e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.53e+03 logL: -3.47e+03 KL: 6.07e+01 MMD: 1.20e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.59e+03 logL: -2.54e+03 KL: 4.99e+01 MMD: 1.15e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.88e+03 logL: -1.83e+03 KL: 4.83e+01 MMD: 1.04e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.87e+03 logL: -1.83e+03 KL: 3.81e+01 MMD: 1.17e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 3.29e+01 MMD: 1.09e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 3.03e+01 MMD: 9.99e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 2.76e+01 MMD: 1.06e+00\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 2.76e+01 MMD: 1.11e+00\n",
      "config 39, alpha = 0.0, lambda = 37475.9, dropout = 0.00; 2 hidden layers with 8, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.53e+03 logL: -7.56e+03 KL: 3.67e+00 MMD: 5.26e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 7.43e+03 logL: -5.52e+03 KL: 3.51e+00 MMD: 5.09e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 1.36e+04 logL: -1.07e+04 KL: 5.79e+00 MMD: 7.61e-02\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 34 VALIDATION Loss: 1.37e+04 logL: -1.04e+04 KL: 5.46e+00 MMD: 8.91e-02\n",
      "config 39, alpha = 0.0, lambda = 11961.1, dropout = 0.00; 2 hidden layers with 72, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.52e+03 logL: -4.47e+03 KL: 1.31e+01 MMD: 8.70e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.42e+03 logL: -2.91e+03 KL: 1.75e+01 MMD: 4.13e-02\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.12e+03 logL: -2.62e+03 KL: 1.87e+01 MMD: 4.02e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.08e+03 logL: -2.60e+03 KL: 1.95e+01 MMD: 3.82e-02\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 48 VALIDATION Loss: 3.16e+03 logL: -2.59e+03 KL: 1.94e+01 MMD: 4.64e-02\n",
      "config 39, alpha = 0.0, lambda = 1.1, dropout = 0.00; 2 hidden layers with 56, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.39e+03 logL: -1.30e+03 KL: 9.76e+01 MMD: 2.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.95e+02 logL: -9.20e+02 KL: 7.48e+01 MMD: 1.99e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.24e+02 logL: -7.59e+02 KL: 6.47e+01 MMD: 1.97e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.73e+02 logL: -6.14e+02 KL: 5.91e+01 MMD: 1.90e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 50 VALIDATION Loss: 7.01e+02 logL: -6.47e+02 KL: 5.38e+01 MMD: 1.78e+00\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 6.46e+02 logL: -5.93e+02 KL: 5.30e+01 MMD: 1.79e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.44e+02 logL: -5.92e+02 KL: 5.16e+01 MMD: 1.98e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.41e+02 logL: -5.90e+02 KL: 5.07e+01 MMD: 1.87e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.40e+02 logL: -5.91e+02 KL: 4.96e+01 MMD: 1.95e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.39e+02 logL: -5.90e+02 KL: 4.91e+01 MMD: 1.80e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.29e+02 logL: -5.81e+02 KL: 4.86e+01 MMD: 1.78e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.86e+02 logL: -5.36e+02 KL: 4.99e+01 MMD: 1.84e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 5.55e+02 logL: -5.04e+02 KL: 5.06e+01 MMD: 1.73e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 5.49e+02 logL: -4.99e+02 KL: 4.94e+01 MMD: 1.62e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 5.44e+02 logL: -4.95e+02 KL: 4.88e+01 MMD: 1.59e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 5.41e+02 logL: -4.93e+02 KL: 4.82e+01 MMD: 1.66e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 5.38e+02 logL: -4.91e+02 KL: 4.75e+01 MMD: 1.70e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 5.37e+02 logL: -4.89e+02 KL: 4.73e+01 MMD: 1.80e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 5.36e+02 logL: -4.89e+02 KL: 4.69e+01 MMD: 1.71e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 5.29e+02 logL: -4.82e+02 KL: 4.67e+01 MMD: 1.60e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 5.29e+02 logL: -4.82e+02 KL: 4.67e+01 MMD: 1.60e+00\n",
      "config 40, alpha = 0.0, lambda = 3.8, dropout = 0.00; 2 hidden layers with 9, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.57e+03 logL: -5.41e+03 KL: 1.64e+02 MMD: 9.15e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.32e+03 logL: -5.25e+03 KL: 6.74e+01 MMD: 9.53e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.87e+03 logL: -3.82e+03 KL: 5.26e+01 MMD: 8.33e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.68e+03 logL: -3.64e+03 KL: 3.94e+01 MMD: 8.72e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.66e+03 logL: -3.63e+03 KL: 3.04e+01 MMD: 8.13e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.64e+03 logL: -3.62e+03 KL: 2.58e+01 MMD: 8.99e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.64e+03 logL: -3.61e+03 KL: 2.26e+01 MMD: 8.70e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.63e+03 logL: -3.61e+03 KL: 2.05e+01 MMD: 8.55e-01\n",
      "Stopping\n",
      "====> Epoch: 82 VALIDATION Loss: 3.63e+03 logL: -3.61e+03 KL: 2.00e+01 MMD: 8.49e-01\n",
      "config 40, alpha = 0.0, lambda = 7813.6, dropout = 0.00; 2 hidden layers with 174, 33 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.09e+03 logL: -2.83e+03 KL: 1.53e+01 MMD: 3.16e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 2.64e+03 logL: -2.11e+03 KL: 2.08e+01 MMD: 6.50e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.22e+03 logL: -1.89e+03 KL: 2.10e+01 MMD: 3.98e-02\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.95e+03 logL: -1.81e+03 KL: 2.27e+01 MMD: 1.54e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 2.01e+03 logL: -1.81e+03 KL: 2.27e+01 MMD: 2.24e-02\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 54 VALIDATION Loss: 1.99e+03 logL: -1.80e+03 KL: 2.25e+01 MMD: 2.17e-02\n",
      "config 40, alpha = 0.0, lambda = 29659.4, dropout = 0.00; 2 hidden layers with 54, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.25e+03 logL: -5.39e+03 KL: 4.39e+00 MMD: 2.88e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.85e+03 logL: -4.59e+03 KL: 5.87e+00 MMD: 4.24e-02\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.53e+03 logL: -4.54e+03 KL: 6.91e+00 MMD: 3.33e-02\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 38 VALIDATION Loss: 5.58e+03 logL: -4.50e+03 KL: 7.42e+00 MMD: 3.62e-02\n",
      "config 40, alpha = 0.0, lambda = 705.0, dropout = 0.00; 2 hidden layers with 59, 59 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.03e+03 logL: -7.88e+02 KL: 9.78e+01 MMD: 1.63e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.91e+02 logL: -8.27e+02 KL: 4.23e+01 MMD: 1.73e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 8.73e+02 logL: -7.57e+02 KL: 3.95e+01 MMD: 1.09e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 8.10e+02 logL: -7.25e+02 KL: 4.05e+01 MMD: 6.33e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 7.48e+02 logL: -6.54e+02 KL: 3.93e+01 MMD: 7.76e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 7.64e+02 logL: -6.62e+02 KL: 3.70e+01 MMD: 9.23e-02\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 6.71e+02 logL: -5.98e+02 KL: 3.71e+01 MMD: 5.01e-02\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 6.87e+02 logL: -5.96e+02 KL: 3.66e+01 MMD: 7.75e-02\n",
      "config 40, alpha = 0.0, lambda = 45168.3, dropout = 0.00; 2 hidden layers with 77, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.81e+03 logL: -5.04e+03 KL: 3.57e+00 MMD: 3.89e-02\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 7.01e+03 logL: -4.66e+03 KL: 4.49e+00 MMD: 5.18e-02\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 6.80e+03 logL: -4.64e+03 KL: 4.78e+00 MMD: 4.78e-02\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 40 VALIDATION Loss: 6.98e+03 logL: -4.63e+03 KL: 4.80e+00 MMD: 5.17e-02\n",
      "Stopping\n",
      "====> Epoch: 43 VALIDATION Loss: 6.31e+03 logL: -4.64e+03 KL: 4.80e+00 MMD: 3.68e-02\n",
      "config 41, alpha = 0.0, lambda = 178.7, dropout = 0.00; 2 hidden layers with 78, 27 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.55e+03 logL: -3.36e+03 KL: 5.90e+01 MMD: 7.53e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.34e+03 logL: -3.16e+03 KL: 4.93e+01 MMD: 7.32e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.30e+03 logL: -3.14e+03 KL: 3.15e+01 MMD: 7.34e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.23e+03 logL: -3.10e+03 KL: 2.89e+01 MMD: 5.84e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.16e+03 logL: -3.07e+03 KL: 2.23e+01 MMD: 3.45e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.17e+03 logL: -3.12e+03 KL: 1.55e+01 MMD: 2.30e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.14e+03 logL: -3.10e+03 KL: 2.03e+01 MMD: 1.29e-01\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.05e+03 logL: -3.01e+03 KL: 1.84e+01 MMD: 1.30e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.02e+03 logL: -2.98e+03 KL: 1.71e+01 MMD: 1.20e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.03e+03 logL: -3.00e+03 KL: 1.52e+01 MMD: 1.21e-01\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 3.01e+03 logL: -2.97e+03 KL: 1.59e+01 MMD: 1.09e-01\n",
      "Stopping\n",
      "====> Epoch: 113 VALIDATION Loss: 3.01e+03 logL: -2.97e+03 KL: 1.57e+01 MMD: 1.07e-01\n",
      "config 41, alpha = 0.0, lambda = 6139.5, dropout = 0.00; 2 hidden layers with 56, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.73e+03 logL: -5.24e+03 KL: 4.03e+00 MMD: 8.05e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.71e+03 logL: -4.52e+03 KL: 5.78e+00 MMD: 3.04e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.89e+03 logL: -3.70e+03 KL: 1.13e+01 MMD: 2.92e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.85e+03 logL: -3.63e+03 KL: 1.25e+01 MMD: 3.39e-02\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.76e+03 logL: -3.62e+03 KL: 1.24e+01 MMD: 2.14e-02\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 56 VALIDATION Loss: 3.78e+03 logL: -3.61e+03 KL: 1.23e+01 MMD: 2.51e-02\n",
      "config 41, alpha = 0.0, lambda = 23367.7, dropout = 0.00; 2 hidden layers with 29, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.77e+03 logL: -7.10e+03 KL: 1.72e+01 MMD: 7.07e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.68e+03 logL: -4.57e+03 KL: 1.25e+01 MMD: 4.70e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 5.34e+03 logL: -3.53e+03 KL: 1.43e+01 MMD: 7.65e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.82e+03 logL: -3.01e+03 KL: 1.60e+01 MMD: 3.40e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.56e+03 logL: -2.80e+03 KL: 1.54e+01 MMD: 3.18e-02\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.21e+03 logL: -2.70e+03 KL: 1.56e+01 MMD: 2.12e-02\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.35e+03 logL: -2.68e+03 KL: 1.59e+01 MMD: 2.81e-02\n",
      "Stopping\n",
      "====> Epoch: 70 VALIDATION Loss: 3.35e+03 logL: -2.68e+03 KL: 1.59e+01 MMD: 2.81e-02\n",
      "config 41, alpha = 0.0, lambda = 7.4, dropout = 0.00; 2 hidden layers with 53, 11 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 2.03e+03 logL: -1.87e+03 KL: 1.42e+02 MMD: 1.64e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.40e+03 logL: -1.31e+03 KL: 8.09e+01 MMD: 1.69e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.36e+03 logL: -1.29e+03 KL: 5.97e+01 MMD: 1.72e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.01e+03 logL: -9.47e+02 KL: 5.71e+01 MMD: 1.51e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.96e+02 logL: -9.36e+02 KL: 5.03e+01 MMD: 1.53e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.06e+02 logL: -7.46e+02 KL: 5.01e+01 MMD: 1.62e+00\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 7.91e+02 logL: -7.36e+02 KL: 4.60e+01 MMD: 1.37e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 7.89e+02 logL: -7.36e+02 KL: 4.51e+01 MMD: 1.28e+00\n",
      "Stopping\n",
      "====> Epoch: 85 VALIDATION Loss: 7.90e+02 logL: -7.36e+02 KL: 4.47e+01 MMD: 1.47e+00\n",
      "config 41, alpha = 0.0, lambda = 142.7, dropout = 0.00; 2 hidden layers with 103, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.91e+03 logL: -1.49e+03 KL: 1.25e+02 MMD: 2.09e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.33e+03 logL: -9.55e+02 KL: 7.95e+01 MMD: 2.10e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.10e+03 logL: -8.52e+02 KL: 5.82e+01 MMD: 1.35e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.17e+02 logL: -7.83e+02 KL: 4.99e+01 MMD: 5.93e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 8.64e+02 logL: -7.67e+02 KL: 4.72e+01 MMD: 3.45e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 8.75e+02 logL: -7.88e+02 KL: 4.82e+01 MMD: 2.79e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 7.06e+02 logL: -6.26e+02 KL: 4.69e+01 MMD: 2.36e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 6.79e+02 logL: -6.08e+02 KL: 4.43e+01 MMD: 1.91e-01\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 6.65e+02 logL: -5.96e+02 KL: 4.42e+01 MMD: 1.74e-01\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 6.71e+02 logL: -5.95e+02 KL: 4.43e+01 MMD: 2.23e-01\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 104 VALIDATION Loss: 6.66e+02 logL: -5.94e+02 KL: 4.43e+01 MMD: 1.90e-01\n",
      "config 42, alpha = 0.0, lambda = 70070.1, dropout = 0.00; 2 hidden layers with 5, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.57e+04 logL: -1.51e+04 KL: 9.51e-04 MMD: 8.51e-03\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 1.58e+04 logL: -1.50e+04 KL: 7.30e-04 MMD: 1.08e-02\n",
      "Stopping\n",
      "====> Epoch: 23 VALIDATION Loss: 1.57e+04 logL: -1.50e+04 KL: 7.69e-04 MMD: 1.07e-02\n",
      "config 42, alpha = 0.0, lambda = 20.7, dropout = 0.00; 2 hidden layers with 18, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.69e+03 logL: -2.57e+03 KL: 1.03e+02 MMD: 1.12e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.62e+03 logL: -2.54e+03 KL: 5.35e+01 MMD: 1.17e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.92e+03 logL: -1.85e+03 KL: 5.40e+01 MMD: 1.01e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.88e+03 logL: -1.82e+03 KL: 4.01e+01 MMD: 1.15e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 3.37e+01 MMD: 1.08e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.86e+03 logL: -1.81e+03 KL: 2.95e+01 MMD: 9.31e-01\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.74e+01 MMD: 9.99e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.68e+01 MMD: 9.91e-01\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 83 VALIDATION Loss: 1.84e+03 logL: -1.80e+03 KL: 2.66e+01 MMD: 9.07e-01\n",
      "config 42, alpha = 0.0, lambda = 51.1, dropout = 0.00; 2 hidden layers with 49, 30 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.46e+03 logL: -1.31e+03 KL: 9.15e+01 MMD: 1.28e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.11e+03 logL: -9.89e+02 KL: 6.09e+01 MMD: 1.25e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.07e+03 logL: -9.65e+02 KL: 4.39e+01 MMD: 1.20e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.03e+03 logL: -9.39e+02 KL: 3.72e+01 MMD: 1.02e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.02e+03 logL: -9.56e+02 KL: 3.22e+01 MMD: 6.50e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.88e+02 logL: -9.40e+02 KL: 3.03e+01 MMD: 3.49e-01\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.72e+02 logL: -9.25e+02 KL: 3.01e+01 MMD: 3.28e-01\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 74 VALIDATION Loss: 9.72e+02 logL: -9.23e+02 KL: 3.02e+01 MMD: 3.61e-01\n",
      "config 42, alpha = 0.0, lambda = 49.2, dropout = 0.00; 2 hidden layers with 17, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.80e+03 logL: -5.40e+03 KL: 3.09e+02 MMD: 1.83e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.91e+03 logL: -3.68e+03 KL: 1.40e+02 MMD: 1.75e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.03e+03 logL: -2.85e+03 KL: 9.06e+01 MMD: 1.90e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.69e+03 logL: -2.55e+03 KL: 6.28e+01 MMD: 1.75e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.66e+03 logL: -2.54e+03 KL: 4.73e+01 MMD: 1.62e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.62e+03 logL: -2.53e+03 KL: 3.55e+01 MMD: 1.25e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.59e+03 logL: -2.53e+03 KL: 2.89e+01 MMD: 7.45e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 2.57e+03 logL: -2.52e+03 KL: 2.57e+01 MMD: 5.14e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 2.57e+03 logL: -2.52e+03 KL: 2.50e+01 MMD: 3.63e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.94e+03 logL: -1.89e+03 KL: 2.95e+01 MMD: 4.82e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.87e+03 logL: -1.82e+03 KL: 2.65e+01 MMD: 3.42e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 2.59e+01 MMD: 2.83e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 2.54e+01 MMD: 2.20e-01\n",
      "Stopping\n",
      "====> Epoch: 135 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 2.51e+01 MMD: 1.87e-01\n",
      "config 42, alpha = 0.0, lambda = 4.4, dropout = 0.00; 2 hidden layers with 77, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.96e+03 logL: -1.85e+03 KL: 1.08e+02 MMD: 2.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.26e+02 logL: -8.34e+02 KL: 8.55e+01 MMD: 2.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.49e+02 logL: -6.73e+02 KL: 6.92e+01 MMD: 1.80e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.90e+02 logL: -5.20e+02 KL: 6.37e+01 MMD: 1.78e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.70e+02 logL: -5.07e+02 KL: 5.69e+01 MMD: 1.68e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.64e+02 logL: -5.07e+02 KL: 5.25e+01 MMD: 1.58e+00\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.44e+02 logL: -4.89e+02 KL: 4.98e+01 MMD: 1.58e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.42e+02 logL: -4.88e+02 KL: 4.91e+01 MMD: 1.48e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.41e+02 logL: -4.87e+02 KL: 4.86e+01 MMD: 1.60e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.35e+02 logL: -4.83e+02 KL: 4.77e+01 MMD: 1.47e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.05e+02 logL: -4.52e+02 KL: 4.78e+01 MMD: 1.38e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.67e+02 logL: -4.14e+02 KL: 4.82e+01 MMD: 1.40e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 4.58e+02 logL: -4.05e+02 KL: 4.77e+01 MMD: 1.47e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 4.56e+02 logL: -4.04e+02 KL: 4.72e+01 MMD: 1.42e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 4.56e+02 logL: -4.04e+02 KL: 4.67e+01 MMD: 1.51e+00\n",
      "Epoch 00158: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 160 VALIDATION Loss: 4.52e+02 logL: -4.01e+02 KL: 4.61e+01 MMD: 1.51e+00\n",
      "Epoch 00167: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 170 VALIDATION Loss: 4.51e+02 logL: -4.01e+02 KL: 4.60e+01 MMD: 1.35e+00\n",
      "Stopping\n",
      "====> Epoch: 171 VALIDATION Loss: 4.52e+02 logL: -4.01e+02 KL: 4.60e+01 MMD: 1.40e+00\n",
      "config 43, alpha = 0.0, lambda = 4.2, dropout = 0.00; 2 hidden layers with 29, 21 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.61e+03 logL: -6.20e+03 KL: 2.41e+03 MMD: 6.94e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.99e+03 logL: -5.51e+03 KL: 4.75e+02 MMD: 6.73e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.58e+03 logL: -5.38e+03 KL: 1.95e+02 MMD: 7.46e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 5.29e+03 logL: -5.19e+03 KL: 9.73e+01 MMD: 7.16e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 4.05e+03 logL: -3.90e+03 KL: 1.45e+02 MMD: 6.62e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.84e+03 logL: -3.75e+03 KL: 8.64e+01 MMD: 7.18e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.76e+03 logL: -3.70e+03 KL: 5.72e+01 MMD: 7.00e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.70e+03 logL: -3.66e+03 KL: 4.20e+01 MMD: 8.00e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 90 VALIDATION Loss: 3.67e+03 logL: -3.63e+03 KL: 3.22e+01 MMD: 7.31e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.65e+03 logL: -3.62e+03 KL: 2.58e+01 MMD: 6.45e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 3.63e+03 logL: -3.60e+03 KL: 2.23e+01 MMD: 6.43e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 3.48e+03 logL: -3.45e+03 KL: 2.73e+01 MMD: 6.44e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 3.35e+03 logL: -3.32e+03 KL: 3.00e+01 MMD: 7.27e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 3.19e+03 logL: -3.17e+03 KL: 2.22e+01 MMD: 6.18e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 3.15e+03 logL: -3.13e+03 KL: 2.38e+01 MMD: 5.97e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 3.12e+03 logL: -3.09e+03 KL: 2.79e+01 MMD: 6.36e-01\n",
      "Epoch 00160: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 170 VALIDATION Loss: 3.10e+03 logL: -3.07e+03 KL: 2.44e+01 MMD: 5.59e-01\n",
      "====> Epoch: 180 VALIDATION Loss: 3.10e+03 logL: -3.08e+03 KL: 2.30e+01 MMD: 5.11e-01\n",
      "====> Epoch: 190 VALIDATION Loss: 3.09e+03 logL: -3.06e+03 KL: 2.22e+01 MMD: 5.87e-01\n",
      "Stopping\n",
      "====> Epoch: 197 VALIDATION Loss: 3.09e+03 logL: -3.07e+03 KL: 2.23e+01 MMD: 5.62e-01\n",
      "config 43, alpha = 0.0, lambda = 13055.8, dropout = 0.00; 2 hidden layers with 28, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.80e+03 logL: -5.12e+03 KL: 2.41e+01 MMD: 5.04e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.00e+03 logL: -4.47e+03 KL: 9.99e+00 MMD: 3.97e-02\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 9.46e+03 logL: -9.08e+03 KL: 1.62e+01 MMD: 2.80e-02\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 5.75e+03 logL: -5.36e+03 KL: 1.93e+01 MMD: 2.78e-02\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 45 VALIDATION Loss: 4.61e+03 logL: -4.24e+03 KL: 1.82e+01 MMD: 2.71e-02\n",
      "config 43, alpha = 0.0, lambda = 48297.1, dropout = 0.00; 2 hidden layers with 32, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.44e+03 logL: -5.40e+03 KL: 3.79e+00 MMD: 2.14e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.81e+03 logL: -4.65e+03 KL: 5.33e+00 MMD: 2.38e-02\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.69e+03 logL: -4.57e+03 KL: 5.81e+00 MMD: 2.32e-02\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 34 VALIDATION Loss: 5.83e+03 logL: -4.56e+03 KL: 5.84e+00 MMD: 2.62e-02\n",
      "config 43, alpha = 0.0, lambda = 117.2, dropout = 0.00; 2 hidden layers with 66, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.65e+03 logL: -1.32e+03 KL: 1.34e+02 MMD: 1.66e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+03 logL: -1.14e+03 KL: 7.08e+01 MMD: 1.50e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.13e+03 logL: -9.40e+02 KL: 4.93e+01 MMD: 1.17e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.06e+02 logL: -7.81e+02 KL: 4.38e+01 MMD: 6.97e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 8.82e+02 logL: -7.97e+02 KL: 4.07e+01 MMD: 3.82e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 7.41e+02 logL: -6.72e+02 KL: 4.14e+01 MMD: 2.40e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 6.80e+02 logL: -6.22e+02 KL: 3.82e+01 MMD: 1.68e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 6.82e+02 logL: -6.26e+02 KL: 3.80e+01 MMD: 1.61e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 6.51e+02 logL: -6.04e+02 KL: 3.70e+01 MMD: 8.11e-02\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 6.47e+02 logL: -5.94e+02 KL: 3.68e+01 MMD: 1.40e-01\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 6.42e+02 logL: -5.93e+02 KL: 3.67e+01 MMD: 1.11e-01\n",
      "Epoch 00114: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 116 VALIDATION Loss: 6.40e+02 logL: -5.93e+02 KL: 3.67e+01 MMD: 8.89e-02\n",
      "config 43, alpha = 0.0, lambda = 634.1, dropout = 0.00; 2 hidden layers with 200, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.01e+03 logL: -3.65e+03 KL: 1.05e+02 MMD: 1.98e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.26e+03 logL: -2.68e+03 KL: 3.61e+01 MMD: 8.65e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.07e+03 logL: -1.91e+03 KL: 3.37e+01 MMD: 2.00e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.49e+03 logL: -1.37e+03 KL: 3.23e+01 MMD: 1.34e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.53e+03 logL: -1.41e+03 KL: 3.19e+01 MMD: 1.43e-01\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.37e+03 logL: -1.27e+03 KL: 3.19e+01 MMD: 1.12e-01\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+03 logL: -1.26e+03 KL: 3.18e+01 MMD: 9.57e-02\n",
      "Epoch 00071: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 75 VALIDATION Loss: 1.37e+03 logL: -1.26e+03 KL: 3.18e+01 MMD: 1.17e-01\n",
      "config 44, alpha = 0.0, lambda = 3654.3, dropout = 0.00; 2 hidden layers with 9, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.28e+03 logL: -5.63e+03 KL: 1.12e+02 MMD: 6.96e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 7.63e+03 logL: -5.48e+03 KL: 4.65e+01 MMD: 5.76e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.20e+03 logL: -5.14e+03 KL: 7.55e+00 MMD: 1.66e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 4.63e+03 logL: -4.56e+03 KL: 8.21e+00 MMD: 1.78e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 4.56e+03 logL: -4.52e+03 KL: 6.19e+00 MMD: 1.04e-02\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.55e+03 logL: -4.51e+03 KL: 6.58e+00 MMD: 9.19e-03\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 4.55e+03 logL: -4.51e+03 KL: 6.58e+00 MMD: 7.30e-03\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 74 VALIDATION Loss: 4.55e+03 logL: -4.51e+03 KL: 6.58e+00 MMD: 8.02e-03\n",
      "config 44, alpha = 0.0, lambda = 1346.9, dropout = 0.00; 2 hidden layers with 8, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.94e+03 logL: -5.36e+03 KL: 1.28e+02 MMD: 1.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.28e+03 logL: -3.68e+03 KL: 1.32e+02 MMD: 1.09e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.21e+03 logL: -3.65e+03 KL: 8.99e+01 MMD: 1.10e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.16e+03 logL: -3.68e+03 KL: 2.04e+01 MMD: 3.36e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.82e+03 logL: -3.63e+03 KL: 2.44e+01 MMD: 1.22e-01\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.76e+03 logL: -3.63e+03 KL: 2.50e+01 MMD: 7.96e-02\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.81e+03 logL: -3.62e+03 KL: 2.52e+01 MMD: 1.24e-01\n",
      "Stopping\n",
      "====> Epoch: 70 VALIDATION Loss: 3.81e+03 logL: -3.62e+03 KL: 2.52e+01 MMD: 1.24e-01\n",
      "config 44, alpha = 0.0, lambda = 1.8, dropout = 0.00; 2 hidden layers with 137, 59 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.05e+03 logL: -9.74e+02 KL: 7.70e+01 MMD: 1.39e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.96e+02 logL: -9.42e+02 KL: 5.30e+01 MMD: 1.29e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.80e+02 logL: -9.35e+02 KL: 4.40e+01 MMD: 1.24e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.62e+02 logL: -9.20e+02 KL: 4.07e+01 MMD: 1.28e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.70e+02 logL: -9.31e+02 KL: 3.81e+01 MMD: 1.10e+00\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.39e+02 logL: -9.03e+02 KL: 3.49e+01 MMD: 1.06e+00\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.37e+02 logL: -9.02e+02 KL: 3.47e+01 MMD: 1.03e+00\n",
      "Stopping\n",
      "====> Epoch: 77 VALIDATION Loss: 9.37e+02 logL: -9.02e+02 KL: 3.45e+01 MMD: 9.92e-01\n",
      "config 44, alpha = 0.0, lambda = 891.2, dropout = 0.00; 2 hidden layers with 10, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.28e+03 logL: -6.52e+03 KL: 2.60e+02 MMD: 1.68e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.42e+03 logL: -5.93e+03 KL: 1.81e+02 MMD: 1.47e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.94e+03 logL: -5.55e+03 KL: 1.12e+02 MMD: 1.43e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.96e+03 logL: -4.90e+03 KL: 5.72e+01 MMD: 1.13e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.51e+03 logL: -4.28e+03 KL: 4.51e+01 MMD: 2.02e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 4.09e+03 logL: -3.94e+03 KL: 3.08e+01 MMD: 1.36e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.42e+03 logL: -3.27e+03 KL: 2.78e+01 MMD: 1.37e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 2.79e+03 logL: -2.69e+03 KL: 2.46e+01 MMD: 8.16e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 2.55e+03 logL: -2.45e+03 KL: 2.44e+01 MMD: 8.80e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 100 VALIDATION Loss: 2.00e+03 logL: -1.93e+03 KL: 2.38e+01 MMD: 5.24e-02\n",
      "====> Epoch: 110 VALIDATION Loss: 1.95e+03 logL: -1.86e+03 KL: 2.28e+01 MMD: 7.39e-02\n",
      "====> Epoch: 120 VALIDATION Loss: 1.93e+03 logL: -1.85e+03 KL: 2.20e+01 MMD: 6.55e-02\n",
      "====> Epoch: 130 VALIDATION Loss: 1.90e+03 logL: -1.83e+03 KL: 2.16e+01 MMD: 5.40e-02\n",
      "Stopping\n",
      "====> Epoch: 135 VALIDATION Loss: 1.91e+03 logL: -1.83e+03 KL: 2.13e+01 MMD: 6.45e-02\n",
      "config 44, alpha = 0.0, lambda = 1.1, dropout = 0.00; 2 hidden layers with 81, 59 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.32e+02 logL: -8.23e+02 KL: 1.09e+02 MMD: 1.94e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.99e+02 logL: -6.26e+02 KL: 7.28e+01 MMD: 1.94e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.42e+02 logL: -4.78e+02 KL: 6.41e+01 MMD: 1.74e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.03e+02 logL: -4.44e+02 KL: 5.93e+01 MMD: 1.78e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.78e+02 logL: -4.25e+02 KL: 5.36e+01 MMD: 1.66e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.65e+02 logL: -4.14e+02 KL: 5.05e+01 MMD: 1.50e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.60e+02 logL: -4.11e+02 KL: 4.88e+01 MMD: 1.56e+00\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 4.46e+02 logL: -3.99e+02 KL: 4.76e+01 MMD: 1.61e+00\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 4.44e+02 logL: -3.98e+02 KL: 4.63e+01 MMD: 1.69e+00\n",
      "Stopping\n",
      "====> Epoch: 99 VALIDATION Loss: 4.44e+02 logL: -3.98e+02 KL: 4.60e+01 MMD: 1.53e+00\n",
      "config 45, alpha = 0.0, lambda = 1.5, dropout = 0.00; 2 hidden layers with 103, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.79e+03 logL: -3.68e+03 KL: 1.13e+02 MMD: 7.15e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.68e+03 logL: -3.64e+03 KL: 4.53e+01 MMD: 8.53e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.64e+03 logL: -3.61e+03 KL: 3.16e+01 MMD: 7.41e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.26e+03 logL: -3.22e+03 KL: 3.47e+01 MMD: 8.24e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.14e+03 logL: -3.11e+03 KL: 2.91e+01 MMD: 7.90e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.13e+03 logL: -3.10e+03 KL: 2.43e+01 MMD: 7.02e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.09e+03 logL: -3.07e+03 KL: 2.28e+01 MMD: 7.70e-01\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.05e+03 logL: -3.03e+03 KL: 2.29e+01 MMD: 7.07e-01\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.05e+03 logL: -3.02e+03 KL: 2.25e+01 MMD: 7.33e-01\n",
      "Stopping\n",
      "====> Epoch: 92 VALIDATION Loss: 3.05e+03 logL: -3.02e+03 KL: 2.27e+01 MMD: 6.42e-01\n",
      "config 45, alpha = 0.0, lambda = 22708.7, dropout = 0.00; 2 hidden layers with 121, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.99e+03 logL: -4.86e+03 KL: 4.83e+00 MMD: 4.96e-02\n",
      "====> Epoch: 20 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 27 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "config 45, alpha = 0.0, lambda = 77093.7, dropout = 0.00; 2 hidden layers with 31, 27 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.50e+03 logL: -5.12e+03 KL: 4.00e+00 MMD: 3.08e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 6.48e+03 logL: -4.74e+03 KL: 4.16e+00 MMD: 2.25e-02\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 6.01e+03 logL: -4.64e+03 KL: 4.95e+00 MMD: 1.77e-02\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 8.25e+03 logL: -4.62e+03 KL: 5.35e+00 MMD: 4.71e-02\n",
      "Stopping\n",
      "====> Epoch: 40 VALIDATION Loss: 8.25e+03 logL: -4.62e+03 KL: 5.35e+00 MMD: 4.71e-02\n",
      "config 45, alpha = 0.0, lambda = 785.4, dropout = 0.00; 2 hidden layers with 68, 39 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.76e+03 logL: -1.33e+03 KL: 1.30e+02 MMD: 1.66e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.17e+03 logL: -1.32e+03 KL: 4.54e+01 MMD: 1.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.07e+03 logL: -9.11e+02 KL: 4.35e+01 MMD: 1.42e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 8.49e+02 logL: -7.48e+02 KL: 4.23e+01 MMD: 7.49e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 8.09e+02 logL: -7.12e+02 KL: 4.04e+01 MMD: 7.20e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 7.28e+02 logL: -6.32e+02 KL: 3.97e+01 MMD: 7.25e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 7.68e+02 logL: -6.67e+02 KL: 3.87e+01 MMD: 8.04e-02\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 6.84e+02 logL: -6.01e+02 KL: 3.83e+01 MMD: 5.69e-02\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 84 VALIDATION Loss: 6.84e+02 logL: -5.99e+02 KL: 3.82e+01 MMD: 5.97e-02\n",
      "config 45, alpha = 0.0, lambda = 49.9, dropout = 0.00; 2 hidden layers with 28, 24 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.10e+03 logL: -1.86e+03 KL: 1.31e+02 MMD: 2.13e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.27e+03 logL: -1.10e+03 KL: 8.34e+01 MMD: 1.93e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.39e+02 logL: -7.85e+02 KL: 6.69e+01 MMD: 1.77e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.99e+02 logL: -7.70e+02 KL: 5.46e+01 MMD: 1.53e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.62e+02 logL: -7.54e+02 KL: 4.86e+01 MMD: 1.22e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.01e+02 logL: -6.08e+02 KL: 4.66e+01 MMD: 9.34e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 5.96e+02 logL: -5.17e+02 KL: 4.40e+01 MMD: 7.12e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 5.50e+02 logL: -4.72e+02 KL: 4.38e+01 MMD: 7.02e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 4.99e+02 logL: -4.34e+02 KL: 4.22e+01 MMD: 4.66e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 4.80e+02 logL: -4.16e+02 KL: 4.11e+01 MMD: 4.64e-01\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 4.64e+02 logL: -4.09e+02 KL: 4.07e+01 MMD: 3.02e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 4.69e+02 logL: -4.08e+02 KL: 4.06e+01 MMD: 4.08e-01\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 124 VALIDATION Loss: 4.66e+02 logL: -4.07e+02 KL: 4.06e+01 MMD: 3.94e-01\n",
      "config 46, alpha = 0.0, lambda = 3036.7, dropout = 0.00; 2 hidden layers with 35, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.31e+03 logL: -4.96e+03 KL: 1.40e+02 MMD: 7.27e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 6.80e+03 logL: -4.61e+03 KL: 8.64e+01 MMD: 6.93e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 4.63e+03 logL: -4.57e+03 KL: 3.57e+00 MMD: 1.77e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.74e+03 logL: -3.70e+03 KL: 8.88e+00 MMD: 1.02e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.65e+03 logL: -3.62e+03 KL: 1.11e+01 MMD: 7.48e-03\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 59 VALIDATION Loss: 1.60e+04 logL: -1.47e+04 KL: 1.86e+01 MMD: 4.42e-01\n",
      "config 46, alpha = 0.0, lambda = 4385.8, dropout = 0.00; 2 hidden layers with 22, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.38e+03 logL: -4.61e+03 KL: 1.15e+02 MMD: 1.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.74e+03 logL: -4.33e+03 KL: 1.08e+01 MMD: 9.04e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.86e+03 logL: -3.76e+03 KL: 1.21e+01 MMD: 2.00e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.77e+03 logL: -3.67e+03 KL: 1.28e+01 MMD: 1.94e-02\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.78e+03 logL: -3.65e+03 KL: 1.31e+01 MMD: 2.63e-02\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.76e+03 logL: -3.64e+03 KL: 1.31e+01 MMD: 2.40e-02\n",
      "Stopping\n",
      "====> Epoch: 61 VALIDATION Loss: 3.78e+03 logL: -3.64e+03 KL: 1.31e+01 MMD: 3.01e-02\n",
      "config 46, alpha = 0.0, lambda = 200.3, dropout = 0.00; 2 hidden layers with 194, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.31e+03 logL: -9.60e+02 KL: 7.60e+01 MMD: 1.35e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.23e+03 logL: -9.66e+02 KL: 4.32e+01 MMD: 1.10e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.07e+03 logL: -9.71e+02 KL: 3.21e+01 MMD: 3.21e-01\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 9.76e+02 logL: -9.18e+02 KL: 3.20e+01 MMD: 1.28e-01\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 9.65e+02 logL: -9.16e+02 KL: 3.20e+01 MMD: 8.88e-02\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.67e+02 logL: -9.16e+02 KL: 3.20e+01 MMD: 9.57e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: 9.71e+02 logL: -9.15e+02 KL: 3.20e+01 MMD: 1.16e-01\n",
      "config 46, alpha = 0.0, lambda = 1095.3, dropout = 0.00; 2 hidden layers with 68, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.12e+03 logL: -5.32e+03 KL: 1.04e+02 MMD: 1.55e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.40e+03 logL: -4.07e+03 KL: 1.52e+01 MMD: 2.90e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.79e+03 logL: -3.64e+03 KL: 1.91e+01 MMD: 1.23e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.75e+03 logL: -3.61e+03 KL: 1.98e+01 MMD: 1.06e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.73e+03 logL: -3.60e+03 KL: 1.97e+01 MMD: 9.67e-02\n",
      "====> Epoch: 60 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "config 46, alpha = 0.0, lambda = 2.7, dropout = 0.00; 2 hidden layers with 35, 30 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.32e+03 logL: -1.22e+03 KL: 1.03e+02 MMD: 1.92e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.38e+02 logL: -7.62e+02 KL: 7.34e+01 MMD: 1.88e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.23e+02 logL: -7.59e+02 KL: 6.10e+01 MMD: 1.93e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.67e+02 logL: -6.08e+02 KL: 5.60e+01 MMD: 1.77e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.67e+02 logL: -6.13e+02 KL: 5.14e+01 MMD: 1.64e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.66e+02 logL: -5.13e+02 KL: 5.10e+01 MMD: 1.59e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.55e+02 logL: -5.04e+02 KL: 4.83e+01 MMD: 1.66e+00\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 5.34e+02 logL: -4.84e+02 KL: 4.66e+01 MMD: 1.68e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.32e+02 logL: -4.83e+02 KL: 4.62e+01 MMD: 1.68e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.31e+02 logL: -4.83e+02 KL: 4.53e+01 MMD: 1.61e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.31e+02 logL: -4.83e+02 KL: 4.50e+01 MMD: 1.58e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.28e+02 logL: -4.81e+02 KL: 4.45e+01 MMD: 1.40e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 5.27e+02 logL: -4.81e+02 KL: 4.38e+01 MMD: 1.35e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 5.23e+02 logL: -4.77e+02 KL: 4.36e+01 MMD: 1.60e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 5.00e+02 logL: -4.53e+02 KL: 4.42e+01 MMD: 1.44e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 4.61e+02 logL: -4.14e+02 KL: 4.50e+01 MMD: 1.47e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 4.52e+02 logL: -4.05e+02 KL: 4.46e+01 MMD: 1.40e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 4.49e+02 logL: -4.03e+02 KL: 4.40e+01 MMD: 1.37e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 4.46e+02 logL: -4.00e+02 KL: 4.38e+01 MMD: 1.29e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 4.45e+02 logL: -3.99e+02 KL: 4.34e+01 MMD: 1.43e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 4.45e+02 logL: -3.99e+02 KL: 4.34e+01 MMD: 1.43e+00\n",
      "config 47, alpha = 0.0, lambda = 2044.5, dropout = 0.00; 2 hidden layers with 21, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.26e+03 logL: -3.69e+03 KL: 1.52e+02 MMD: 6.93e-01\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 5.19e+03 logL: -3.65e+03 KL: 1.15e+02 MMD: 6.98e-01\n",
      "Stopping\n",
      "====> Epoch: 21 VALIDATION Loss: 5.27e+03 logL: -3.65e+03 KL: 1.15e+02 MMD: 7.37e-01\n",
      "config 47, alpha = 0.0, lambda = 17.1, dropout = 0.00; 2 hidden layers with 12, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.79e+03 logL: -3.66e+03 KL: 1.03e+02 MMD: 1.15e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.64e+03 logL: -2.55e+03 KL: 7.02e+01 MMD: 1.09e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.59e+03 logL: -2.53e+03 KL: 4.47e+01 MMD: 1.18e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.57e+03 logL: -2.52e+03 KL: 3.54e+01 MMD: 1.14e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.53e+03 logL: -2.48e+03 KL: 3.24e+01 MMD: 1.03e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.88e+03 logL: -1.83e+03 KL: 3.62e+01 MMD: 1.06e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.86e+03 logL: -1.81e+03 KL: 3.07e+01 MMD: 1.03e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.86e+03 logL: -1.81e+03 KL: 2.71e+01 MMD: 8.85e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.44e+01 MMD: 1.04e+00\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 1.86e+03 logL: -1.82e+03 KL: 2.45e+01 MMD: 8.95e-01\n",
      "config 47, alpha = 0.0, lambda = 2.4, dropout = 0.00; 2 hidden layers with 11, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.69e+03 logL: -5.53e+03 KL: 1.53e+02 MMD: 1.57e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.40e+03 logL: -5.29e+03 KL: 1.00e+02 MMD: 1.70e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.84e+03 logL: -3.74e+03 KL: 9.47e+01 MMD: 1.47e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.70e+03 logL: -3.65e+03 KL: 5.42e+01 MMD: 1.82e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.69e+03 logL: -3.65e+03 KL: 4.23e+01 MMD: 1.79e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.67e+03 logL: -3.63e+03 KL: 3.78e+01 MMD: 1.89e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.61e+03 logL: -2.55e+03 KL: 5.95e+01 MMD: 1.64e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.56e+03 logL: -2.51e+03 KL: 4.49e+01 MMD: 1.70e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.55e+03 logL: -2.51e+03 KL: 3.96e+01 MMD: 1.74e+00\n",
      "Epoch 00099: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 2.54e+03 logL: -2.50e+03 KL: 3.66e+01 MMD: 1.62e+00\n",
      "Epoch 00108: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.53e+03 logL: -2.50e+03 KL: 3.61e+01 MMD: 1.65e+00\n",
      "Stopping\n",
      "====> Epoch: 110 VALIDATION Loss: 2.53e+03 logL: -2.50e+03 KL: 3.61e+01 MMD: 1.65e+00\n",
      "config 47, alpha = 0.0, lambda = 5255.1, dropout = 0.00; 2 hidden layers with 46, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.71e+03 logL: -4.24e+03 KL: 2.34e+01 MMD: 8.54e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.65e+03 logL: -3.00e+03 KL: 2.39e+01 MMD: 1.20e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.53e+03 logL: -2.04e+03 KL: 2.29e+01 MMD: 8.75e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.32e+03 logL: -1.96e+03 KL: 2.27e+01 MMD: 6.43e-02\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.09e+03 logL: -1.85e+03 KL: 2.39e+01 MMD: 4.26e-02\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 54 VALIDATION Loss: 2.17e+03 logL: -1.84e+03 KL: 2.39e+01 MMD: 5.79e-02\n",
      "config 47, alpha = 0.0, lambda = 64303.4, dropout = 0.00; 2 hidden layers with 146, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.22e+04 logL: -8.65e+03 KL: 3.56e+00 MMD: 5.58e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 9.10e+03 logL: -5.13e+03 KL: 3.79e+00 MMD: 6.16e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 7.16e+03 logL: -4.74e+03 KL: 5.51e+00 MMD: 3.74e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 7.65e+03 logL: -4.73e+03 KL: 4.27e+00 MMD: 4.53e-02\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 46 VALIDATION Loss: 7.57e+03 logL: -4.58e+03 KL: 6.03e+00 MMD: 4.64e-02\n",
      "config 48, alpha = 0.0, lambda = 107.6, dropout = 0.00; 2 hidden layers with 189, 71 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.49e+03 logL: -3.35e+03 KL: 4.84e+01 MMD: 8.26e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.22e+03 logL: -3.10e+03 KL: 3.38e+01 MMD: 7.46e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.19e+03 logL: -3.09e+03 KL: 2.57e+01 MMD: 6.84e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.19e+03 logL: -3.11e+03 KL: 2.16e+01 MMD: 5.31e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.12e+03 logL: -3.06e+03 KL: 1.99e+01 MMD: 3.33e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 2.98e+03 logL: -2.94e+03 KL: 1.83e+01 MMD: 1.84e-01\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.94e+03 logL: -2.90e+03 KL: 1.69e+01 MMD: 2.20e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 2.92e+03 logL: -2.88e+03 KL: 1.62e+01 MMD: 2.06e-01\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 2.91e+03 logL: -2.88e+03 KL: 1.62e+01 MMD: 1.72e-01\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 96 VALIDATION Loss: 2.92e+03 logL: -2.88e+03 KL: 1.62e+01 MMD: 2.12e-01\n",
      "config 48, alpha = 0.0, lambda = 1057.8, dropout = 0.00; 2 hidden layers with 8, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.51e+03 logL: -5.44e+03 KL: 6.01e+01 MMD: 9.57e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 6.28e+03 logL: -5.30e+03 KL: 2.82e+01 MMD: 9.00e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.13e+03 logL: -5.01e+03 KL: 1.10e+01 MMD: 1.00e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 4.70e+03 logL: -4.61e+03 KL: 1.00e+01 MMD: 7.73e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.90e+03 logL: -3.82e+03 KL: 1.51e+01 MMD: 5.56e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.68e+03 logL: -3.64e+03 KL: 1.26e+01 MMD: 1.89e-02\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.66e+03 logL: -3.63e+03 KL: 1.16e+01 MMD: 1.86e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 3.66e+03 logL: -3.62e+03 KL: 1.15e+01 MMD: 2.01e-02\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 85 VALIDATION Loss: 3.65e+03 logL: -3.62e+03 KL: 1.15e+01 MMD: 1.86e-02\n",
      "config 48, alpha = 0.0, lambda = 22018.4, dropout = 0.00; 2 hidden layers with 167, 152 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.92e+03 logL: -1.98e+03 KL: 4.00e+01 MMD: 4.09e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 2.90e+03 logL: -1.80e+03 KL: 2.91e+01 MMD: 4.85e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.34e+03 logL: -1.58e+03 KL: 2.34e+01 MMD: 7.87e-02\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.90e+03 logL: -1.37e+03 KL: 2.55e+01 MMD: 2.29e-02\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.13e+03 logL: -1.32e+03 KL: 2.60e+01 MMD: 3.56e-02\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.04e+03 logL: -1.31e+03 KL: 2.63e+01 MMD: 3.19e-02\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.94e+03 logL: -1.31e+03 KL: 2.63e+01 MMD: 2.77e-02\n",
      "Stopping\n",
      "====> Epoch: 71 VALIDATION Loss: 2.21e+03 logL: -1.30e+03 KL: 2.63e+01 MMD: 4.01e-02\n",
      "config 48, alpha = 0.0, lambda = 3075.5, dropout = 0.00; 2 hidden layers with 12, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.55e+04 logL: -1.17e+04 KL: 2.17e+03 MMD: 5.14e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 6.42e+03 logL: -5.61e+03 KL: 1.32e+01 MMD: 2.61e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.71e+03 logL: -5.35e+03 KL: 1.36e+01 MMD: 1.12e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 5.46e+03 logL: -5.06e+03 KL: 1.72e+01 MMD: 1.24e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 5.08e+03 logL: -4.64e+03 KL: 2.81e+01 MMD: 1.33e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 4.70e+03 logL: -4.24e+03 KL: 2.99e+01 MMD: 1.42e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 4.26e+03 logL: -3.89e+03 KL: 3.36e+01 MMD: 1.10e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 4.02e+03 logL: -3.73e+03 KL: 2.91e+01 MMD: 8.42e-02\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 4.11e+03 logL: -3.73e+03 KL: 3.17e+01 MMD: 1.12e-01\n",
      "Stopping\n",
      "====> Epoch: 90 VALIDATION Loss: 4.11e+03 logL: -3.73e+03 KL: 3.17e+01 MMD: 1.12e-01\n",
      "config 48, alpha = 0.0, lambda = 11.3, dropout = 0.00; 2 hidden layers with 200, 51 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.08e+03 logL: -9.69e+02 KL: 8.70e+01 MMD: 2.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.25e+02 logL: -5.39e+02 KL: 6.69e+01 MMD: 1.76e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.81e+02 logL: -5.07e+02 KL: 5.60e+01 MMD: 1.72e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.71e+02 logL: -5.08e+02 KL: 5.02e+01 MMD: 1.22e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.74e+02 logL: -4.11e+02 KL: 4.93e+01 MMD: 1.27e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.66e+02 logL: -4.08e+02 KL: 4.63e+01 MMD: 1.12e+00\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 4.50e+02 logL: -3.95e+02 KL: 4.44e+01 MMD: 1.02e+00\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 4.47e+02 logL: -3.93e+02 KL: 4.42e+01 MMD: 9.04e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 4.46e+02 logL: -3.93e+02 KL: 4.42e+01 MMD: 8.65e-01\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 95 VALIDATION Loss: 4.46e+02 logL: -3.93e+02 KL: 4.41e+01 MMD: 8.80e-01\n",
      "config 49, alpha = 0.0, lambda = 3079.7, dropout = 0.00; 2 hidden layers with 13, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.90e+03 logL: -5.51e+03 KL: 1.16e+03 MMD: 7.24e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 7.67e+03 logL: -5.26e+03 KL: 3.57e+02 MMD: 6.64e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 6.86e+03 logL: -4.40e+03 KL: 2.21e+02 MMD: 7.27e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 6.10e+03 logL: -3.79e+03 KL: 1.30e+02 MMD: 7.09e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 5.88e+03 logL: -3.68e+03 KL: 8.47e+01 MMD: 6.87e-01\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 5.75e+03 logL: -3.68e+03 KL: 5.59e+01 MMD: 6.57e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 5.72e+03 logL: -3.74e+03 KL: 3.01e+01 MMD: 6.33e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 4.23e+03 logL: -3.85e+03 KL: 1.16e+01 MMD: 1.19e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.83e+03 logL: -3.75e+03 KL: 1.17e+01 MMD: 2.19e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 3.79e+03 logL: -3.73e+03 KL: 1.27e+01 MMD: 1.45e-02\n",
      "====> Epoch: 110 VALIDATION Loss: 3.71e+03 logL: -3.66e+03 KL: 1.32e+01 MMD: 1.27e-02\n",
      "====> Epoch: 120 VALIDATION Loss: 3.69e+03 logL: -3.64e+03 KL: 1.35e+01 MMD: 1.31e-02\n",
      "Epoch 00127: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 3.69e+03 logL: -3.63e+03 KL: 1.34e+01 MMD: 1.55e-02\n",
      "Epoch 00134: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 138 VALIDATION Loss: 3.69e+03 logL: -3.63e+03 KL: 1.35e+01 MMD: 1.59e-02\n",
      "config 49, alpha = 0.0, lambda = 1730.7, dropout = 0.00; 2 hidden layers with 34, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.75e+03 logL: -5.00e+03 KL: 8.96e+01 MMD: 9.62e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.96e+03 logL: -4.63e+03 KL: 1.58e+01 MMD: 1.79e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.88e+03 logL: -3.70e+03 KL: 2.23e+01 MMD: 9.30e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.85e+03 logL: -3.62e+03 KL: 1.92e+01 MMD: 1.23e-01\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.76e+03 logL: -3.59e+03 KL: 1.62e+01 MMD: 8.68e-02\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.78e+03 logL: -3.59e+03 KL: 1.57e+01 MMD: 1.02e-01\n",
      "Stopping\n",
      "====> Epoch: 61 VALIDATION Loss: 3.82e+03 logL: -3.59e+03 KL: 1.57e+01 MMD: 1.24e-01\n",
      "config 49, alpha = 0.0, lambda = 751.7, dropout = 0.00; 2 hidden layers with 54, 26 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.96e+03 logL: -1.81e+03 KL: 1.54e+02 MMD: 1.33e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.38e+03 logL: -1.30e+03 KL: 7.93e+01 MMD: 1.33e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.37e+03 logL: -1.13e+03 KL: 3.14e+01 MMD: 2.72e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.15e+03 logL: -1.08e+03 KL: 3.15e+01 MMD: 5.48e-02\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.02e+03 logL: -9.51e+02 KL: 3.08e+01 MMD: 5.04e-02\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.02e+03 logL: -9.48e+02 KL: 3.07e+01 MMD: 5.11e-02\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.02e+03 logL: -9.47e+02 KL: 3.07e+01 MMD: 5.40e-02\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 74 VALIDATION Loss: 1.02e+03 logL: -9.47e+02 KL: 3.07e+01 MMD: 6.04e-02\n",
      "config 49, alpha = 0.0, lambda = 15021.0, dropout = 0.00; 2 hidden layers with 48, 46 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.06e+03 logL: -2.44e+03 KL: 1.73e+01 MMD: 4.04e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 2.91e+03 logL: -2.11e+03 KL: 2.13e+01 MMD: 5.17e-02\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 2.34e+03 logL: -1.91e+03 KL: 2.20e+01 MMD: 2.70e-02\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.65e+03 logL: -1.90e+03 KL: 2.27e+01 MMD: 4.86e-02\n",
      "Stopping\n",
      "====> Epoch: 40 VALIDATION Loss: 2.65e+03 logL: -1.90e+03 KL: 2.27e+01 MMD: 4.86e-02\n",
      "config 49, alpha = 0.0, lambda = 61425.2, dropout = 0.00; 2 hidden layers with 24, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.64e+03 logL: -5.87e+03 KL: 4.68e+00 MMD: 6.13e-02\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 7.51e+03 logL: -4.71e+03 KL: 4.78e+00 MMD: 4.56e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 7.25e+03 logL: -4.57e+03 KL: 5.44e+00 MMD: 4.36e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 39 VALIDATION Loss: 7.46e+03 logL: -4.45e+03 KL: 5.70e+00 MMD: 4.89e-02\n",
      "config 50, alpha = 0.0, lambda = 12.4, dropout = 0.00; 2 hidden layers with 29, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.71e+03 logL: -3.62e+03 KL: 7.72e+01 MMD: 7.12e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.40e+03 logL: -3.35e+03 KL: 4.07e+01 MMD: 7.60e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.26e+03 logL: -3.22e+03 KL: 3.02e+01 MMD: 7.67e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.26e+03 logL: -3.23e+03 KL: 2.55e+01 MMD: 7.57e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.16e+03 logL: -3.13e+03 KL: 2.28e+01 MMD: 6.91e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.13e+03 logL: -3.10e+03 KL: 2.05e+01 MMD: 6.67e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.12e+03 logL: -3.10e+03 KL: 1.88e+01 MMD: 6.56e-01\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.09e+03 logL: -3.07e+03 KL: 1.87e+01 MMD: 6.02e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.08e+03 logL: -3.06e+03 KL: 1.84e+01 MMD: 6.62e-01\n",
      "Stopping\n",
      "====> Epoch: 99 VALIDATION Loss: 3.08e+03 logL: -3.06e+03 KL: 1.79e+01 MMD: 6.52e-01\n",
      "config 50, alpha = 0.0, lambda = 5.2, dropout = 0.00; 2 hidden layers with 76, 30 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.90e+03 logL: -1.84e+03 KL: 6.15e+01 MMD: 9.70e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.83e+03 logL: -1.78e+03 KL: 4.07e+01 MMD: 1.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.71e+03 logL: -1.67e+03 KL: 3.55e+01 MMD: 9.82e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.69e+03 logL: -1.66e+03 KL: 2.98e+01 MMD: 1.01e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.67e+03 logL: -1.64e+03 KL: 2.72e+01 MMD: 9.18e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.61e+03 logL: -1.57e+03 KL: 3.28e+01 MMD: 7.88e-01\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.55e+03 logL: -1.52e+03 KL: 2.92e+01 MMD: 8.15e-01\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 1.55e+03 logL: -1.52e+03 KL: 2.80e+01 MMD: 8.60e-01\n",
      "config 50, alpha = 0.0, lambda = 289.1, dropout = 0.00; 2 hidden layers with 26, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.67e+03 logL: -5.03e+03 KL: 2.22e+02 MMD: 1.45e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.65e+03 logL: -3.11e+03 KL: 1.25e+02 MMD: 1.44e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.04e+03 logL: -2.56e+03 KL: 8.34e+01 MMD: 1.39e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.33e+03 logL: -1.86e+03 KL: 7.46e+01 MMD: 1.37e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.81e+03 logL: -1.37e+03 KL: 5.74e+01 MMD: 1.34e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.50e+03 logL: -1.36e+03 KL: 3.15e+01 MMD: 3.81e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.33e+03 logL: -1.26e+03 KL: 3.18e+01 MMD: 1.10e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.04e+03 logL: -9.68e+02 KL: 3.11e+01 MMD: 1.29e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.00e+03 logL: -9.51e+02 KL: 3.07e+01 MMD: 6.89e-02\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 9.86e+02 logL: -9.36e+02 KL: 3.00e+01 MMD: 7.10e-02\n",
      "Stopping\n",
      "====> Epoch: 109 VALIDATION Loss: 9.84e+02 logL: -9.38e+02 KL: 3.00e+01 MMD: 5.53e-02\n",
      "config 50, alpha = 0.0, lambda = 94.9, dropout = 0.00; 2 hidden layers with 82, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.00e+03 logL: -3.73e+03 KL: 1.15e+02 MMD: 1.68e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.13e+03 logL: -1.89e+03 KL: 8.81e+01 MMD: 1.62e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.24e+03 logL: -1.01e+03 KL: 7.60e+01 MMD: 1.60e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.17e+03 logL: -9.68e+02 KL: 5.61e+01 MMD: 1.51e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.03e+03 logL: -8.86e+02 KL: 4.36e+01 MMD: 1.03e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.50e+02 logL: -7.55e+02 KL: 4.04e+01 MMD: 5.82e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 7.13e+02 logL: -6.47e+02 KL: 3.83e+01 MMD: 2.96e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 6.77e+02 logL: -6.13e+02 KL: 3.88e+01 MMD: 2.67e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 6.69e+02 logL: -6.11e+02 KL: 3.76e+01 MMD: 2.17e-01\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 6.51e+02 logL: -5.94e+02 KL: 3.71e+01 MMD: 2.13e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 6.44e+02 logL: -5.94e+02 KL: 3.70e+01 MMD: 1.41e-01\n",
      "Epoch 00115: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 119 VALIDATION Loss: 6.42e+02 logL: -5.91e+02 KL: 3.69e+01 MMD: 1.42e-01\n",
      "config 50, alpha = 0.0, lambda = 100.0, dropout = 0.00; 2 hidden layers with 194, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.19e+03 logL: -1.88e+03 KL: 9.52e+01 MMD: 2.11e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.30e+03 logL: -1.05e+03 KL: 7.36e+01 MMD: 1.77e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.08e+03 logL: -9.61e+02 KL: 4.99e+01 MMD: 7.16e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.04e+03 logL: -9.56e+02 KL: 4.84e+01 MMD: 3.86e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.02e+03 logL: -9.42e+02 KL: 4.72e+01 MMD: 3.32e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.86e+02 logL: -9.09e+02 KL: 4.63e+01 MMD: 3.12e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.29e+02 logL: -7.55e+02 KL: 4.49e+01 MMD: 2.98e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.18e+02 logL: -7.50e+02 KL: 4.32e+01 MMD: 2.55e-01\n",
      "Stopping\n",
      "====> Epoch: 84 VALIDATION Loss: 8.27e+02 logL: -7.63e+02 KL: 4.22e+01 MMD: 2.21e-01\n",
      "config 51, alpha = 0.0, lambda = 5.8, dropout = 0.00; 2 hidden layers with 67, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.64e+03 logL: -3.58e+03 KL: 5.35e+01 MMD: 8.23e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.23e+03 logL: -3.19e+03 KL: 3.29e+01 MMD: 7.96e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.15e+03 logL: -3.12e+03 KL: 2.50e+01 MMD: 7.12e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.09e+03 logL: -3.06e+03 KL: 2.12e+01 MMD: 7.77e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.19e+03 logL: -3.17e+03 KL: 1.90e+01 MMD: 7.01e-01\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.02e+03 logL: -3.00e+03 KL: 1.81e+01 MMD: 6.97e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.01e+03 logL: -2.99e+03 KL: 1.78e+01 MMD: 7.06e-01\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.00e+03 logL: -2.98e+03 KL: 1.75e+01 MMD: 6.91e-01\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 86 VALIDATION Loss: 3.00e+03 logL: -2.98e+03 KL: 1.74e+01 MMD: 7.29e-01\n",
      "config 51, alpha = 0.0, lambda = 17.5, dropout = 0.00; 2 hidden layers with 70, 55 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.94e+03 logL: -1.86e+03 KL: 6.49e+01 MMD: 9.85e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.85e+03 logL: -1.79e+03 KL: 4.19e+01 MMD: 9.73e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.75e+03 logL: -1.69e+03 KL: 3.69e+01 MMD: 1.14e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.66e+03 logL: -1.62e+03 KL: 3.21e+01 MMD: 9.05e-01\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.61e+03 logL: -1.56e+03 KL: 3.04e+01 MMD: 8.74e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.60e+03 logL: -1.55e+03 KL: 3.00e+01 MMD: 8.70e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.59e+03 logL: -1.55e+03 KL: 2.92e+01 MMD: 7.83e-01\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.59e+03 logL: -1.55e+03 KL: 2.85e+01 MMD: 7.67e-01\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 1.60e+03 logL: -1.55e+03 KL: 2.86e+01 MMD: 8.62e-01\n",
      "config 51, alpha = 0.0, lambda = 31.6, dropout = 0.00; 2 hidden layers with 163, 52 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.04e+03 logL: -9.37e+02 KL: 6.20e+01 MMD: 1.30e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.98e+02 logL: -9.25e+02 KL: 4.13e+01 MMD: 1.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.92e+02 logL: -9.33e+02 KL: 3.37e+01 MMD: 8.24e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.64e+02 logL: -9.19e+02 KL: 2.99e+01 MMD: 4.98e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.59e+02 logL: -9.19e+02 KL: 2.95e+01 MMD: 3.47e-01\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.31e+02 logL: -8.95e+02 KL: 2.83e+01 MMD: 2.41e-01\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.31e+02 logL: -8.94e+02 KL: 2.84e+01 MMD: 2.97e-01\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 9.30e+02 logL: -8.94e+02 KL: 2.83e+01 MMD: 2.52e-01\n",
      "config 51, alpha = 0.0, lambda = 109.2, dropout = 0.00; 2 hidden layers with 25, 21 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 2.23e+03 logL: -1.92e+03 KL: 1.32e+02 MMD: 1.63e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.59e+03 logL: -1.32e+03 KL: 1.03e+02 MMD: 1.58e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.20e+03 logL: -9.51e+02 KL: 7.80e+01 MMD: 1.62e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.18e+03 logL: -9.51e+02 KL: 5.75e+01 MMD: 1.56e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.03e+03 logL: -8.64e+02 KL: 4.62e+01 MMD: 1.13e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.79e+02 logL: -7.62e+02 KL: 4.10e+01 MMD: 6.99e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.41e+02 logL: -7.59e+02 KL: 3.99e+01 MMD: 3.91e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.20e+02 logL: -7.51e+02 KL: 3.97e+01 MMD: 2.72e-01\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 8.09e+02 logL: -7.44e+02 KL: 3.85e+01 MMD: 2.46e-01\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 8.07e+02 logL: -7.43e+02 KL: 3.87e+01 MMD: 2.35e-01\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 108 VALIDATION Loss: 8.12e+02 logL: -7.42e+02 KL: 3.87e+01 MMD: 2.84e-01\n",
      "config 51, alpha = 0.0, lambda = 23662.9, dropout = 0.00; 2 hidden layers with 50, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.70e+03 logL: -4.06e+03 KL: 8.27e+00 MMD: 6.89e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.47e+03 logL: -2.89e+03 KL: 1.42e+01 MMD: 6.63e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.01e+03 logL: -2.65e+03 KL: 1.65e+01 MMD: 5.65e-02\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 38 VALIDATION Loss: 3.76e+03 logL: -2.59e+03 KL: 1.72e+01 MMD: 4.89e-02\n",
      "config 52, alpha = 0.0, lambda = 19770.7, dropout = 0.00; 2 hidden layers with 13, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.01e+04 logL: -8.51e+03 KL: 2.64e+00 MMD: 8.19e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.27e+03 logL: -5.04e+03 KL: 2.91e+00 MMD: 1.17e-02\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 4.83e+03 logL: -4.65e+03 KL: 3.86e+00 MMD: 8.82e-03\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 4.95e+03 logL: -4.63e+03 KL: 3.94e+00 MMD: 1.61e-02\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 48 VALIDATION Loss: 4.75e+03 logL: -4.62e+03 KL: 3.96e+00 MMD: 6.36e-03\n",
      "config 52, alpha = 0.0, lambda = 431.2, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.71e+03 logL: -5.97e+03 KL: 2.60e+02 MMD: 1.12e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.01e+03 logL: -5.38e+03 KL: 1.20e+02 MMD: 1.18e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.75e+03 logL: -5.19e+03 KL: 7.91e+01 MMD: 1.12e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.29e+03 logL: -4.74e+03 KL: 5.11e+01 MMD: 1.15e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.29e+03 logL: -3.81e+03 KL: 5.05e+01 MMD: 9.94e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 4.11e+03 logL: -3.67e+03 KL: 3.16e+01 MMD: 9.50e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.89e+03 logL: -3.62e+03 KL: 1.35e+01 MMD: 5.81e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.64e+03 logL: -3.60e+03 KL: 9.42e+00 MMD: 7.54e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 3.59e+03 logL: -3.56e+03 KL: 9.77e+00 MMD: 4.71e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 3.55e+03 logL: -3.52e+03 KL: 1.00e+01 MMD: 6.59e-02\n",
      "====> Epoch: 110 VALIDATION Loss: 3.47e+03 logL: -3.45e+03 KL: 1.16e+01 MMD: 2.29e-02\n",
      "Epoch 00116: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 3.73e+05 logL: -3.72e+05 KL: 5.29e+02 MMD: 1.13e+00\n",
      "Stopping\n",
      "====> Epoch: 120 VALIDATION Loss: 3.73e+05 logL: -3.72e+05 KL: 5.29e+02 MMD: 1.13e+00\n",
      "config 52, alpha = 0.0, lambda = 3.5, dropout = 0.00; 2 hidden layers with 143, 101 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.04e+03 logL: -9.71e+02 KL: 6.15e+01 MMD: 1.28e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.78e+02 logL: -9.31e+02 KL: 4.33e+01 MMD: 1.36e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.64e+02 logL: -9.24e+02 KL: 3.69e+01 MMD: 1.07e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.53e+02 logL: -9.15e+02 KL: 3.45e+01 MMD: 1.08e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.59e+02 logL: -9.25e+02 KL: 3.16e+01 MMD: 9.58e-01\n",
      "Epoch 00058: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.32e+02 logL: -8.99e+02 KL: 3.05e+01 MMD: 9.13e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.31e+02 logL: -8.99e+02 KL: 3.02e+01 MMD: 8.14e-01\n",
      "Stopping\n",
      "====> Epoch: 72 VALIDATION Loss: 9.31e+02 logL: -8.99e+02 KL: 3.03e+01 MMD: 8.55e-01\n",
      "config 52, alpha = 0.0, lambda = 10447.6, dropout = 0.00; 2 hidden layers with 163, 44 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.43e+03 logL: -3.67e+03 KL: 2.04e+01 MMD: 7.10e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 2.19e+03 logL: -1.62e+03 KL: 2.36e+01 MMD: 5.20e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.20e+03 logL: -1.43e+03 KL: 2.43e+01 MMD: 7.21e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.00e+03 logL: -1.34e+03 KL: 2.49e+01 MMD: 6.02e-02\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.63e+03 logL: -1.10e+03 KL: 2.75e+01 MMD: 4.75e-02\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.71e+03 logL: -1.08e+03 KL: 2.75e+01 MMD: 5.80e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 1.60e+03 logL: -1.07e+03 KL: 2.76e+01 MMD: 4.84e-02\n",
      "Epoch 00071: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.57e+03 logL: -1.06e+03 KL: 2.78e+01 MMD: 4.60e-02\n",
      "Stopping\n",
      "====> Epoch: 83 VALIDATION Loss: 1.66e+03 logL: -1.06e+03 KL: 2.78e+01 MMD: 5.39e-02\n",
      "config 52, alpha = 0.0, lambda = 85732.0, dropout = 0.00; 2 hidden layers with 14, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.08e+04 logL: -6.82e+03 KL: 3.53e+00 MMD: 4.62e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 8.62e+03 logL: -4.79e+03 KL: 4.65e+00 MMD: 4.47e-02\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 8.80e+03 logL: -4.68e+03 KL: 5.22e+00 MMD: 4.81e-02\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 8.47e+03 logL: -4.66e+03 KL: 5.85e+00 MMD: 4.45e-02\n",
      "Stopping\n",
      "====> Epoch: 43 VALIDATION Loss: 8.14e+03 logL: -4.66e+03 KL: 5.85e+00 MMD: 4.05e-02\n",
      "config 53, alpha = 0.0, lambda = 69.1, dropout = 0.00; 2 hidden layers with 87, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.90e+03 logL: -3.73e+03 KL: 1.26e+02 MMD: 7.23e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.74e+03 logL: -3.64e+03 KL: 5.25e+01 MMD: 7.90e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.70e+03 logL: -3.61e+03 KL: 3.65e+01 MMD: 7.61e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.28e+03 logL: -3.18e+03 KL: 4.68e+01 MMD: 7.04e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.19e+03 logL: -3.11e+03 KL: 3.00e+01 MMD: 6.92e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.12e+03 logL: -3.05e+03 KL: 2.52e+01 MMD: 7.18e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.09e+03 logL: -3.01e+03 KL: 2.35e+01 MMD: 8.01e-01\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.03e+03 logL: -2.96e+03 KL: 1.93e+01 MMD: 6.86e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.03e+03 logL: -2.96e+03 KL: 1.92e+01 MMD: 6.42e-01\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 96 VALIDATION Loss: 3.00e+03 logL: -2.94e+03 KL: 1.92e+01 MMD: 6.03e-01\n",
      "config 53, alpha = 0.0, lambda = 73200.4, dropout = 0.00; 2 hidden layers with 8, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.24e+04 logL: -1.01e+04 KL: 1.14e+00 MMD: 3.09e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 7.42e+03 logL: -5.90e+03 KL: 2.89e+00 MMD: 2.07e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 6.50e+03 logL: -5.04e+03 KL: 3.81e+00 MMD: 1.99e-02\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 6.31e+03 logL: -5.00e+03 KL: 4.08e+00 MMD: 1.78e-02\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 46 VALIDATION Loss: 6.25e+03 logL: -4.99e+03 KL: 4.14e+00 MMD: 1.72e-02\n",
      "config 53, alpha = 0.0, lambda = 97.3, dropout = 0.00; 2 hidden layers with 33, 21 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.82e+03 logL: -3.56e+03 KL: 1.19e+02 MMD: 1.44e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.74e+03 logL: -2.55e+03 KL: 7.39e+01 MMD: 1.30e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 30 VALIDATION Loss: 2.63e+03 logL: -2.45e+03 KL: 4.82e+01 MMD: 1.44e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.03e+03 logL: -1.89e+03 KL: 3.86e+01 MMD: 1.03e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.36e+03 logL: -1.27e+03 KL: 3.51e+01 MMD: 6.23e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.01e+03 logL: -9.47e+02 KL: 3.35e+01 MMD: 2.79e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.87e+02 logL: -9.38e+02 KL: 3.15e+01 MMD: 1.82e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.92e+02 logL: -9.45e+02 KL: 3.06e+01 MMD: 1.67e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.77e+02 logL: -9.39e+02 KL: 2.96e+01 MMD: 8.64e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 9.65e+02 logL: -9.26e+02 KL: 2.89e+01 MMD: 9.96e-02\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 9.60e+02 logL: -9.22e+02 KL: 2.83e+01 MMD: 1.02e-01\n",
      "Epoch 00112: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 9.56e+02 logL: -9.21e+02 KL: 2.83e+01 MMD: 6.97e-02\n",
      "Epoch 00122: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 126 VALIDATION Loss: 9.56e+02 logL: -9.21e+02 KL: 2.83e+01 MMD: 6.62e-02\n",
      "config 53, alpha = 0.0, lambda = 94215.7, dropout = 0.00; 2 hidden layers with 31, 31 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.24e+04 logL: -8.35e+03 KL: 2.75e+00 MMD: 4.30e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 1.04e+04 logL: -5.38e+03 KL: 5.30e+00 MMD: 5.32e-02\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 8.07e+03 logL: -4.39e+03 KL: 9.13e+00 MMD: 3.90e-02\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 7.94e+03 logL: -4.30e+03 KL: 9.64e+00 MMD: 3.86e-02\n",
      "Stopping\n",
      "====> Epoch: 41 VALIDATION Loss: 8.24e+03 logL: -4.29e+03 KL: 9.64e+00 MMD: 4.19e-02\n",
      "config 53, alpha = 0.0, lambda = 6449.2, dropout = 0.00; 2 hidden layers with 21, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.96e+03 logL: -4.06e+03 KL: 4.33e+01 MMD: 1.33e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.32e+03 logL: -3.71e+03 KL: 3.25e+01 MMD: 8.99e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.86e+03 logL: -3.13e+03 KL: 2.80e+01 MMD: 1.08e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.00e+03 logL: -2.58e+03 KL: 2.48e+01 MMD: 6.13e-02\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.84e+03 logL: -2.54e+03 KL: 2.60e+01 MMD: 4.25e-02\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.87e+03 logL: -2.53e+03 KL: 2.57e+01 MMD: 4.82e-02\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.03e+03 logL: -2.53e+03 KL: 2.58e+01 MMD: 7.22e-02\n",
      "Stopping\n",
      "====> Epoch: 72 VALIDATION Loss: 2.88e+03 logL: -2.53e+03 KL: 2.58e+01 MMD: 5.01e-02\n",
      "config 54, alpha = 0.0, lambda = 95865.6, dropout = 0.00; 2 hidden layers with 8, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.06e+04 logL: -9.60e+03 KL: 6.58e-01 MMD: 1.07e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 6.59e+03 logL: -5.13e+03 KL: 3.12e+00 MMD: 1.51e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 5.46e+03 logL: -4.63e+03 KL: 3.89e+00 MMD: 8.59e-03\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 4.98e+03 logL: -4.62e+03 KL: 4.18e+00 MMD: 3.69e-03\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 47 VALIDATION Loss: 5.17e+03 logL: -4.64e+03 KL: 4.22e+00 MMD: 5.55e-03\n",
      "config 54, alpha = 0.0, lambda = 53.5, dropout = 0.00; 2 hidden layers with 51, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.91e+03 logL: -2.78e+03 KL: 7.56e+01 MMD: 1.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.10e+03 logL: -1.98e+03 KL: 5.89e+01 MMD: 1.10e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.95e+03 logL: -1.85e+03 KL: 4.09e+01 MMD: 1.08e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.89e+03 logL: -1.81e+03 KL: 3.34e+01 MMD: 9.50e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.86e+03 logL: -1.79e+03 KL: 3.07e+01 MMD: 7.72e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.85e+03 logL: -1.79e+03 KL: 2.72e+01 MMD: 6.41e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.77e+03 logL: -1.71e+03 KL: 2.60e+01 MMD: 5.65e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.73e+03 logL: -1.68e+03 KL: 2.51e+01 MMD: 4.94e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.72e+03 logL: -1.68e+03 KL: 2.32e+01 MMD: 3.75e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.69e+03 logL: -1.65e+03 KL: 2.29e+01 MMD: 3.23e-01\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.64e+03 logL: -1.61e+03 KL: 2.23e+01 MMD: 2.65e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.63e+03 logL: -1.60e+03 KL: 2.19e+01 MMD: 2.04e-01\n",
      "Epoch 00123: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.62e+03 logL: -1.59e+03 KL: 2.19e+01 MMD: 1.96e-01\n",
      "Epoch 00137: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 140 VALIDATION Loss: 1.62e+03 logL: -1.59e+03 KL: 2.19e+01 MMD: 2.41e-01\n",
      "Stopping\n",
      "====> Epoch: 141 VALIDATION Loss: 1.62e+03 logL: -1.59e+03 KL: 2.19e+01 MMD: 1.87e-01\n",
      "config 54, alpha = 0.0, lambda = 3424.4, dropout = 0.00; 2 hidden layers with 19, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.03e+03 logL: -6.20e+03 KL: 1.34e+02 MMD: 7.88e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.46e+03 logL: -3.98e+03 KL: 2.60e+01 MMD: 1.32e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.85e+03 logL: -3.61e+03 KL: 2.14e+01 MMD: 6.34e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.92e+03 logL: -2.67e+03 KL: 2.18e+01 MMD: 6.79e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 2.22e+03 logL: -1.99e+03 KL: 2.26e+01 MMD: 6.11e-02\n",
      "Epoch 00058: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.97e+03 logL: -1.84e+03 KL: 2.19e+01 MMD: 3.18e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 1.95e+03 logL: -1.84e+03 KL: 2.17e+01 MMD: 2.56e-02\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 1.96e+03 logL: -1.83e+03 KL: 2.19e+01 MMD: 3.10e-02\n",
      "config 54, alpha = 0.0, lambda = 4043.6, dropout = 0.00; 2 hidden layers with 18, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.12e+03 logL: -4.64e+03 KL: 2.55e+01 MMD: 1.11e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.02e+03 logL: -2.53e+03 KL: 2.65e+01 MMD: 1.15e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.18e+03 logL: -1.92e+03 KL: 2.46e+01 MMD: 5.63e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.18e+03 logL: -1.92e+03 KL: 2.33e+01 MMD: 5.92e-02\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.02e+03 logL: -1.83e+03 KL: 2.44e+01 MMD: 4.10e-02\n",
      "Stopping\n",
      "====> Epoch: 53 VALIDATION Loss: 2.03e+03 logL: -1.83e+03 KL: 2.43e+01 MMD: 4.41e-02\n",
      "config 54, alpha = 0.0, lambda = 31359.5, dropout = 0.00; 2 hidden layers with 13, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.65e+03 logL: -7.09e+03 KL: 3.91e+00 MMD: 4.96e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 6.50e+03 logL: -4.95e+03 KL: 5.43e+00 MMD: 4.92e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 6.72e+03 logL: -4.61e+03 KL: 6.45e+00 MMD: 6.73e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 5.56e+03 logL: -4.61e+03 KL: 5.92e+00 MMD: 3.03e-02\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 6.40e+03 logL: -4.53e+03 KL: 6.36e+00 MMD: 5.95e-02\n",
      "Stopping\n",
      "====> Epoch: 50 VALIDATION Loss: 6.40e+03 logL: -4.53e+03 KL: 6.36e+00 MMD: 5.95e-02\n",
      "config 55, alpha = 0.0, lambda = 43955.0, dropout = 0.00; 2 hidden layers with 106, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.74e+03 logL: -5.26e+03 KL: 3.59e+00 MMD: 1.07e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.98e+03 logL: -4.76e+03 KL: 4.67e+00 MMD: 4.90e-03\n",
      "====> Epoch: 30 VALIDATION Loss: 5.12e+03 logL: -4.65e+03 KL: 5.16e+00 MMD: 1.05e-02\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 5.25e+03 logL: -4.60e+03 KL: 5.24e+00 MMD: 1.47e-02\n",
      "Stopping\n",
      "====> Epoch: 41 VALIDATION Loss: 5.03e+03 logL: -4.60e+03 KL: 5.24e+00 MMD: 9.85e-03\n",
      "config 55, alpha = 0.0, lambda = 895.7, dropout = 0.00; 2 hidden layers with 15, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.61e+03 logL: -2.57e+03 KL: 1.30e+02 MMD: 1.02e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.57e+03 logL: -2.54e+03 KL: 7.79e+01 MMD: 1.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.90e+03 logL: -2.59e+03 KL: 2.67e+01 MMD: 3.21e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 2.52e+03 logL: -2.44e+03 KL: 2.69e+01 MMD: 5.70e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.93e+03 logL: -1.86e+03 KL: 2.47e+01 MMD: 5.04e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.91e+03 logL: -1.83e+03 KL: 2.34e+01 MMD: 6.48e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 1.93e+03 logL: -1.87e+03 KL: 2.15e+01 MMD: 4.40e-02\n",
      "Epoch 00071: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 2.15e+01 MMD: 4.04e-02\n",
      "Stopping\n",
      "====> Epoch: 82 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 2.15e+01 MMD: 4.18e-02\n",
      "config 55, alpha = 0.0, lambda = 79.6, dropout = 0.00; 2 hidden layers with 12, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.65e+03 logL: -5.42e+03 KL: 1.15e+02 MMD: 1.54e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.94e+03 logL: -3.73e+03 KL: 9.69e+01 MMD: 1.44e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.85e+03 logL: -2.66e+03 KL: 8.25e+01 MMD: 1.41e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.72e+03 logL: -2.55e+03 KL: 6.01e+01 MMD: 1.47e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.68e+03 logL: -2.52e+03 KL: 4.95e+01 MMD: 1.40e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.99e+03 logL: -1.83e+03 KL: 5.13e+01 MMD: 1.46e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.95e+03 logL: -1.83e+03 KL: 3.87e+01 MMD: 1.16e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.93e+03 logL: -1.83e+03 KL: 3.02e+01 MMD: 9.70e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.89e+03 logL: -1.82e+03 KL: 2.62e+01 MMD: 5.46e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.88e+03 logL: -1.82e+03 KL: 2.48e+01 MMD: 5.22e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 2.36e+01 MMD: 3.69e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.86e+03 logL: -1.82e+03 KL: 2.34e+01 MMD: 2.54e-01\n",
      "Epoch 00124: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.31e+01 MMD: 2.64e-01\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 136 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.31e+01 MMD: 2.62e-01\n",
      "config 55, alpha = 0.0, lambda = 10.7, dropout = 0.00; 2 hidden layers with 30, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.48e+03 logL: -3.35e+03 KL: 1.12e+02 MMD: 1.96e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.62e+03 logL: -2.53e+03 KL: 6.96e+01 MMD: 1.92e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.47e+03 logL: -1.38e+03 KL: 6.95e+01 MMD: 1.80e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.14e+03 logL: -1.07e+03 KL: 6.15e+01 MMD: 1.71e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.03e+03 logL: -9.59e+02 KL: 5.47e+01 MMD: 1.63e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.01e+03 logL: -9.43e+02 KL: 4.83e+01 MMD: 1.61e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.95e+02 logL: -9.37e+02 KL: 4.44e+01 MMD: 1.42e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.01e+03 logL: -9.58e+02 KL: 4.12e+01 MMD: 1.32e+00\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 9.80e+02 logL: -9.27e+02 KL: 4.07e+01 MMD: 1.31e+00\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 9.79e+02 logL: -9.26e+02 KL: 4.06e+01 MMD: 1.26e+00\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 103 VALIDATION Loss: 9.79e+02 logL: -9.27e+02 KL: 4.06e+01 MMD: 1.26e+00\n",
      "config 55, alpha = 0.0, lambda = 98.3, dropout = 0.00; 2 hidden layers with 106, 28 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.05e+03 logL: -2.69e+03 KL: 1.39e+02 MMD: 2.18e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.53e+03 logL: -1.25e+03 KL: 9.87e+01 MMD: 1.90e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.03e+03 logL: -7.85e+02 KL: 7.31e+01 MMD: 1.72e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.65e+02 logL: -5.83e+02 KL: 5.73e+01 MMD: 1.28e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.44e+02 logL: -5.23e+02 KL: 4.85e+01 MMD: 7.37e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 5.43e+02 logL: -4.60e+02 KL: 4.74e+01 MMD: 3.75e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 5.04e+02 logL: -4.35e+02 KL: 4.60e+01 MMD: 2.33e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 4.93e+02 logL: -4.26e+02 KL: 4.46e+01 MMD: 2.27e-01\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 4.74e+02 logL: -4.12e+02 KL: 4.35e+01 MMD: 1.91e-01\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 96 VALIDATION Loss: 4.70e+02 logL: -4.10e+02 KL: 4.34e+01 MMD: 1.72e-01\n",
      "config 56, alpha = 0.0, lambda = 2.7, dropout = 0.00; 2 hidden layers with 44, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.77e+03 logL: -3.66e+03 KL: 1.09e+02 MMD: 6.76e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.69e+03 logL: -3.63e+03 KL: 4.96e+01 MMD: 7.62e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.49e+03 logL: -3.45e+03 KL: 4.35e+01 MMD: 7.11e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.34e+03 logL: -3.30e+03 KL: 3.39e+01 MMD: 7.34e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.19e+03 logL: -3.16e+03 KL: 3.09e+01 MMD: 7.46e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.19e+03 logL: -3.16e+03 KL: 2.67e+01 MMD: 6.94e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.13e+03 logL: -3.11e+03 KL: 2.44e+01 MMD: 7.73e-01\n",
      "Epoch 00071: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.11e+03 logL: -3.08e+03 KL: 2.37e+01 MMD: 7.31e-01\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 87 VALIDATION Loss: 3.11e+03 logL: -3.08e+03 KL: 2.35e+01 MMD: 7.04e-01\n",
      "config 56, alpha = 0.0, lambda = 2544.6, dropout = 0.00; 2 hidden layers with 9, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.95e+03 logL: -5.17e+03 KL: 1.74e+02 MMD: 1.02e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.58e+03 logL: -2.65e+03 KL: 1.23e+02 MMD: 1.10e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.91e+03 logL: -2.88e+03 KL: 4.85e+01 MMD: 7.78e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.94e+03 logL: -2.69e+03 KL: 2.10e+01 MMD: 9.03e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 2.80e+03 logL: -2.63e+03 KL: 2.09e+01 MMD: 5.78e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 2.72e+03 logL: -2.59e+03 KL: 2.14e+01 MMD: 4.37e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 2.02e+03 logL: -1.93e+03 KL: 1.96e+01 MMD: 2.50e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 2.00e+03 logL: -1.90e+03 KL: 1.90e+01 MMD: 3.35e-02\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.92e+03 logL: -1.83e+03 KL: 1.87e+01 MMD: 2.73e-02\n",
      "Epoch 00095: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 99 VALIDATION Loss: 1.89e+03 logL: -1.83e+03 KL: 1.85e+01 MMD: 1.54e-02\n",
      "config 56, alpha = 0.0, lambda = 349.5, dropout = 0.00; 2 hidden layers with 14, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.42e+03 logL: -3.72e+03 KL: 1.78e+02 MMD: 1.49e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.16e+03 logL: -2.57e+03 KL: 1.15e+02 MMD: 1.36e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.14e+03 logL: -2.55e+03 KL: 7.97e+01 MMD: 1.48e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.87e+03 logL: -2.55e+03 KL: 3.55e+01 MMD: 8.14e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.70e+03 logL: -2.55e+03 KL: 3.45e+01 MMD: 3.47e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 2.68e+03 logL: -2.54e+03 KL: 3.44e+01 MMD: 2.86e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 2.64e+03 logL: -2.53e+03 KL: 3.04e+01 MMD: 2.02e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 2.60e+03 logL: -2.52e+03 KL: 2.85e+01 MMD: 1.59e-01\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 2.59e+03 logL: -2.50e+03 KL: 2.77e+01 MMD: 1.71e-01\n",
      "Epoch 00098: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 99 VALIDATION Loss: 2.58e+03 logL: -2.50e+03 KL: 2.73e+01 MMD: 1.36e-01\n",
      "config 56, alpha = 0.0, lambda = 221.0, dropout = 0.00; 2 hidden layers with 78, 55 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.47e+03 logL: -9.59e+02 KL: 1.12e+02 MMD: 1.83e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.06e+03 logL: -6.43e+02 KL: 6.85e+01 MMD: 1.58e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.38e+02 logL: -6.38e+02 KL: 4.41e+01 MMD: 7.06e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 7.44e+02 logL: -6.50e+02 KL: 4.05e+01 MMD: 2.42e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 6.87e+02 logL: -6.22e+02 KL: 3.98e+01 MMD: 1.14e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 6.81e+02 logL: -6.24e+02 KL: 3.82e+01 MMD: 8.92e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 6.60e+02 logL: -6.11e+02 KL: 3.65e+01 MMD: 5.65e-02\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 80 VALIDATION Loss: 6.45e+02 logL: -5.95e+02 KL: 3.53e+01 MMD: 6.85e-02\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 6.42e+02 logL: -5.90e+02 KL: 3.54e+01 MMD: 7.57e-02\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 6.39e+02 logL: -5.90e+02 KL: 3.52e+01 MMD: 5.86e-02\n",
      "config 56, alpha = 0.0, lambda = 4.3, dropout = 0.00; 2 hidden layers with 172, 112 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.57e+02 logL: -6.47e+02 KL: 1.03e+02 MMD: 2.14e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.27e+02 logL: -4.46e+02 KL: 7.49e+01 MMD: 1.98e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.97e+02 logL: -4.30e+02 KL: 6.13e+01 MMD: 1.75e+00\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 4.72e+02 logL: -4.08e+02 KL: 5.86e+01 MMD: 1.72e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.69e+02 logL: -4.09e+02 KL: 5.47e+01 MMD: 1.62e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.65e+02 logL: -4.06e+02 KL: 5.32e+01 MMD: 1.64e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.62e+02 logL: -4.05e+02 KL: 5.21e+01 MMD: 1.45e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.63e+02 logL: -4.07e+02 KL: 5.06e+01 MMD: 1.60e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.57e+02 logL: -4.03e+02 KL: 4.91e+01 MMD: 1.41e+00\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 4.52e+02 logL: -3.99e+02 KL: 4.90e+01 MMD: 1.36e+00\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 110 VALIDATION Loss: 4.53e+02 logL: -3.98e+02 KL: 4.90e+01 MMD: 1.57e+00\n",
      "Epoch 00110: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 114 VALIDATION Loss: 4.53e+02 logL: -3.99e+02 KL: 4.89e+01 MMD: 1.48e+00\n",
      "config 57, alpha = 0.0, lambda = 63793.7, dropout = 0.00; 2 hidden layers with 43, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.34e+03 logL: -4.83e+03 KL: 8.98e+00 MMD: 2.34e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.92e+03 logL: -3.93e+03 KL: 1.28e+01 MMD: 1.53e-02\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 4.10e+03 logL: -3.67e+03 KL: 1.30e+01 MMD: 6.44e-03\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 4.02e+03 logL: -3.64e+03 KL: 1.35e+01 MMD: 5.72e-03\n",
      "====> Epoch: 50 VALIDATION Loss: 4.51e+03 logL: -3.63e+03 KL: 1.36e+01 MMD: 1.36e-02\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 59 VALIDATION Loss: 4.25e+03 logL: -3.63e+03 KL: 1.36e+01 MMD: 9.51e-03\n",
      "config 57, alpha = 0.0, lambda = 359.3, dropout = 0.00; 2 hidden layers with 11, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.84e+03 logL: -3.30e+03 KL: 1.34e+02 MMD: 1.12e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.97e+03 logL: -2.54e+03 KL: 6.72e+01 MMD: 1.01e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.89e+03 logL: -2.53e+03 KL: 3.53e+01 MMD: 9.11e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.04e+03 logL: -1.88e+03 KL: 2.32e+01 MMD: 3.68e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.92e+03 logL: -1.87e+03 KL: 2.18e+01 MMD: 7.49e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.87e+03 logL: -1.83e+03 KL: 2.11e+01 MMD: 6.50e-02\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 2.09e+01 MMD: 5.93e-02\n",
      "Stopping\n",
      "====> Epoch: 73 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 2.10e+01 MMD: 6.39e-02\n",
      "config 57, alpha = 0.0, lambda = 44.1, dropout = 0.00; 2 hidden layers with 56, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.07e+03 logL: -1.91e+03 KL: 9.24e+01 MMD: 1.51e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.44e+03 logL: -1.32e+03 KL: 6.53e+01 MMD: 1.36e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.39e+03 logL: -1.29e+03 KL: 4.80e+01 MMD: 1.30e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.15e+03 logL: -1.06e+03 KL: 4.48e+01 MMD: 1.11e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.03e+03 logL: -9.53e+02 KL: 3.72e+01 MMD: 9.70e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.03e+03 logL: -9.66e+02 KL: 3.14e+01 MMD: 7.41e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.96e+02 logL: -9.44e+02 KL: 3.08e+01 MMD: 4.84e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.01e+03 logL: -9.67e+02 KL: 2.88e+01 MMD: 3.16e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.78e+02 logL: -9.39e+02 KL: 2.89e+01 MMD: 2.31e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 9.94e+02 logL: -9.59e+02 KL: 2.68e+01 MMD: 1.99e-01\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 9.56e+02 logL: -9.21e+02 KL: 2.76e+01 MMD: 1.77e-01\n",
      "Epoch 00115: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 9.54e+02 logL: -9.20e+02 KL: 2.76e+01 MMD: 1.36e-01\n",
      "Stopping\n",
      "====> Epoch: 127 VALIDATION Loss: 9.55e+02 logL: -9.20e+02 KL: 2.76e+01 MMD: 1.72e-01\n",
      "config 57, alpha = 0.0, lambda = 5506.1, dropout = 0.00; 2 hidden layers with 62, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.45e+03 logL: -3.89e+03 KL: 1.75e+01 MMD: 9.84e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.19e+03 logL: -2.67e+03 KL: 2.74e+01 MMD: 9.10e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.61e+03 logL: -2.04e+03 KL: 2.60e+01 MMD: 9.77e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.18e+03 logL: -1.85e+03 KL: 2.44e+01 MMD: 5.48e-02\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.15e+03 logL: -1.83e+03 KL: 2.51e+01 MMD: 5.41e-02\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 54 VALIDATION Loss: 2.11e+03 logL: -1.82e+03 KL: 2.48e+01 MMD: 4.93e-02\n",
      "config 57, alpha = 0.0, lambda = 1664.7, dropout = 0.00; 2 hidden layers with 12, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.18e+03 logL: -5.69e+03 KL: 7.24e+01 MMD: 1.45e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.40e+03 logL: -4.99e+03 KL: 4.80e+01 MMD: 2.18e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.19e+03 logL: -2.86e+03 KL: 4.22e+01 MMD: 1.77e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.88e+03 logL: -2.61e+03 KL: 3.70e+01 MMD: 1.37e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.82e+03 logL: -2.57e+03 KL: 3.48e+01 MMD: 1.31e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 2.76e+03 logL: -2.56e+03 KL: 3.14e+01 MMD: 1.06e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 2.72e+03 logL: -2.53e+03 KL: 2.95e+01 MMD: 9.78e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 2.08e+03 logL: -1.88e+03 KL: 2.87e+01 MMD: 1.04e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 2.03e+03 logL: -1.85e+03 KL: 2.62e+01 MMD: 9.30e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 1.55e+03 logL: -1.41e+03 KL: 2.71e+01 MMD: 6.37e-02\n",
      "====> Epoch: 110 VALIDATION Loss: 1.43e+03 logL: -1.31e+03 KL: 2.59e+01 MMD: 5.96e-02\n",
      "Epoch 00112: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00119: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.40e+03 logL: -1.28e+03 KL: 2.55e+01 MMD: 5.66e-02\n",
      "Epoch 00127: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.39e+03 logL: -1.28e+03 KL: 2.55e+01 MMD: 5.21e-02\n",
      "Stopping\n",
      "====> Epoch: 131 VALIDATION Loss: 1.40e+03 logL: -1.28e+03 KL: 2.55e+01 MMD: 6.06e-02\n",
      "config 58, alpha = 0.0, lambda = 13.4, dropout = 0.00; 2 hidden layers with 50, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.77e+03 logL: -3.65e+03 KL: 1.10e+02 MMD: 7.63e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.66e+03 logL: -3.61e+03 KL: 4.06e+01 MMD: 8.10e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.39e+03 logL: -3.34e+03 KL: 3.78e+01 MMD: 8.22e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.21e+03 logL: -3.17e+03 KL: 3.05e+01 MMD: 7.61e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.18e+03 logL: -3.15e+03 KL: 2.52e+01 MMD: 8.64e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.05e+03 logL: -3.02e+03 KL: 2.36e+01 MMD: 7.02e-01\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.98e+03 logL: -2.95e+03 KL: 2.39e+01 MMD: 7.35e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 2.95e+03 logL: -2.92e+03 KL: 2.29e+01 MMD: 7.71e-01\n",
      "Stopping\n",
      "====> Epoch: 86 VALIDATION Loss: 2.95e+03 logL: -2.92e+03 KL: 2.24e+01 MMD: 6.75e-01\n",
      "config 58, alpha = 0.0, lambda = 17.9, dropout = 0.00; 2 hidden layers with 23, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.97e+03 logL: -1.86e+03 KL: 9.30e+01 MMD: 1.04e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.89e+03 logL: -1.83e+03 KL: 4.78e+01 MMD: 1.03e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 3.58e+01 MMD: 1.03e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 1.84e+03 logL: -1.79e+03 KL: 3.11e+01 MMD: 9.89e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.84e+03 logL: -1.79e+03 KL: 2.77e+01 MMD: 8.76e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.84e+03 logL: -1.80e+03 KL: 2.68e+01 MMD: 7.66e-01\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.81e+03 logL: -1.77e+03 KL: 2.57e+01 MMD: 6.46e-01\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.81e+03 logL: -1.77e+03 KL: 2.57e+01 MMD: 7.87e-01\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 87 VALIDATION Loss: 1.80e+03 logL: -1.77e+03 KL: 2.57e+01 MMD: 7.20e-01\n",
      "config 58, alpha = 0.0, lambda = 20.9, dropout = 0.00; 2 hidden layers with 104, 27 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.45e+03 logL: -1.33e+03 KL: 9.46e+01 MMD: 1.43e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.08e+03 logL: -9.97e+02 KL: 5.76e+01 MMD: 1.26e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.01e+03 logL: -9.36e+02 KL: 4.51e+01 MMD: 1.22e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.97e+02 logL: -9.36e+02 KL: 3.95e+01 MMD: 1.08e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.78e+02 logL: -9.22e+02 KL: 3.54e+01 MMD: 1.04e+00\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.62e+02 logL: -9.13e+02 KL: 3.42e+01 MMD: 7.70e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.62e+02 logL: -9.14e+02 KL: 3.35e+01 MMD: 7.24e-01\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.58e+02 logL: -9.12e+02 KL: 3.29e+01 MMD: 6.59e-01\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 90 VALIDATION Loss: 9.58e+02 logL: -9.12e+02 KL: 3.29e+01 MMD: 6.79e-01\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 9.58e+02 logL: -9.11e+02 KL: 3.29e+01 MMD: 6.76e-01\n",
      "config 58, alpha = 0.0, lambda = 32.6, dropout = 0.00; 2 hidden layers with 13, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.90e+03 logL: -3.70e+03 KL: 1.38e+02 MMD: 2.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.75e+03 logL: -2.60e+03 KL: 8.35e+01 MMD: 1.84e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.66e+03 logL: -2.54e+03 KL: 5.65e+01 MMD: 2.08e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.96e+03 logL: -1.85e+03 KL: 5.77e+01 MMD: 1.76e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.40e+03 logL: -1.29e+03 KL: 5.57e+01 MMD: 1.69e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.38e+03 logL: -1.28e+03 KL: 4.62e+01 MMD: 1.62e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.36e+03 logL: -1.28e+03 KL: 4.11e+01 MMD: 1.16e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.35e+03 logL: -1.28e+03 KL: 3.78e+01 MMD: 1.10e+00\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.33e+03 logL: -1.27e+03 KL: 3.57e+01 MMD: 8.30e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.33e+03 logL: -1.27e+03 KL: 3.55e+01 MMD: 8.76e-01\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 102 VALIDATION Loss: 1.33e+03 logL: -1.27e+03 KL: 3.55e+01 MMD: 8.28e-01\n",
      "config 58, alpha = 0.0, lambda = 1.9, dropout = 0.00; 2 hidden layers with 29, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.93e+03 logL: -2.77e+03 KL: 1.54e+02 MMD: 2.15e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.64e+03 logL: -2.56e+03 KL: 8.09e+01 MMD: 2.20e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.90e+03 logL: -1.83e+03 KL: 6.68e+01 MMD: 2.14e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.37e+03 logL: -1.31e+03 KL: 6.07e+01 MMD: 2.01e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.03e+03 logL: -9.65e+02 KL: 6.04e+01 MMD: 1.87e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.97e+02 logL: -9.45e+02 KL: 5.09e+01 MMD: 1.76e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.88e+02 logL: -9.41e+02 KL: 4.55e+01 MMD: 1.65e+00\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.78e+02 logL: -9.31e+02 KL: 4.51e+01 MMD: 1.63e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.76e+02 logL: -9.31e+02 KL: 4.44e+01 MMD: 1.58e+00\n",
      "Stopping\n",
      "====> Epoch: 99 VALIDATION Loss: 9.76e+02 logL: -9.31e+02 KL: 4.36e+01 MMD: 1.60e+00\n",
      "config 59, alpha = 0.0, lambda = 1240.1, dropout = 0.00; 2 hidden layers with 43, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.31e+03 logL: -3.41e+03 KL: 9.06e+01 MMD: 6.51e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.16e+03 logL: -3.16e+03 KL: 6.52e+01 MMD: 7.50e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.89e+03 logL: -3.17e+03 KL: 2.73e+01 MMD: 5.58e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.26e+03 logL: -3.16e+03 KL: 1.90e+01 MMD: 6.54e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.20e+03 logL: -3.08e+03 KL: 1.62e+01 MMD: 8.03e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.12e+03 logL: -3.07e+03 KL: 1.43e+01 MMD: 3.07e-02\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.08e+03 logL: -3.02e+03 KL: 1.42e+01 MMD: 3.17e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 3.05e+03 logL: -3.01e+03 KL: 1.38e+01 MMD: 2.08e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 3.03e+03 logL: -3.00e+03 KL: 1.31e+01 MMD: 1.22e-02\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 3.03e+03 logL: -2.99e+03 KL: 1.28e+01 MMD: 1.75e-02\n",
      "Stopping\n",
      "====> Epoch: 100 VALIDATION Loss: 3.03e+03 logL: -2.99e+03 KL: 1.28e+01 MMD: 1.75e-02\n",
      "config 59, alpha = 0.0, lambda = 269.8, dropout = 0.00; 2 hidden layers with 11, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.00e+03 logL: -4.52e+03 KL: 2.04e+02 MMD: 1.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.97e+03 logL: -2.60e+03 KL: 8.93e+01 MMD: 1.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.21e+03 logL: -1.86e+03 KL: 6.93e+01 MMD: 1.02e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.16e+03 logL: -1.84e+03 KL: 4.66e+01 MMD: 1.04e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.04e+03 logL: -1.84e+03 KL: 2.88e+01 MMD: 6.61e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.98e+03 logL: -1.90e+03 KL: 2.40e+01 MMD: 1.95e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.90e+03 logL: -1.85e+03 KL: 2.30e+01 MMD: 9.94e-02\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 76 VALIDATION Loss: 1.89e+03 logL: -1.85e+03 KL: 2.32e+01 MMD: 7.76e-02\n",
      "config 59, alpha = 0.0, lambda = 198.8, dropout = 0.00; 2 hidden layers with 12, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.09e+03 logL: -3.69e+03 KL: 1.28e+02 MMD: 1.39e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.00e+03 logL: -3.64e+03 KL: 6.62e+01 MMD: 1.53e+00\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.96e+03 logL: -3.63e+03 KL: 5.14e+01 MMD: 1.45e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.95e+03 logL: -3.63e+03 KL: 4.61e+01 MMD: 1.38e+00\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 47 VALIDATION Loss: 3.91e+03 logL: -3.63e+03 KL: 4.36e+01 MMD: 1.23e+00\n",
      "config 59, alpha = 0.0, lambda = 1360.2, dropout = 0.00; 2 hidden layers with 19, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.03e+03 logL: -4.35e+03 KL: 1.10e+02 MMD: 1.16e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.47e+03 logL: -3.22e+03 KL: 3.33e+01 MMD: 1.61e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.22e+03 logL: -2.06e+03 KL: 3.45e+01 MMD: 8.75e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.14e+03 logL: -1.96e+03 KL: 3.27e+01 MMD: 1.08e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.89e+03 logL: -1.73e+03 KL: 3.21e+01 MMD: 9.84e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.41e+03 logL: -1.31e+03 KL: 3.00e+01 MMD: 5.40e-02\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.38e+03 logL: -1.28e+03 KL: 2.98e+01 MMD: 5.63e-02\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.38e+03 logL: -1.27e+03 KL: 2.96e+01 MMD: 5.42e-02\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 89 VALIDATION Loss: 1.36e+03 logL: -1.27e+03 KL: 2.97e+01 MMD: 4.52e-02\n",
      "config 59, alpha = 0.0, lambda = 896.6, dropout = 0.00; 2 hidden layers with 152, 80 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.60e+03 logL: -1.56e+03 KL: 1.82e+02 MMD: 2.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.35e+03 logL: -1.02e+03 KL: 5.61e+01 MMD: 3.06e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.19e+03 logL: -9.19e+02 KL: 5.57e+01 MMD: 2.44e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 6.97e+02 logL: -5.70e+02 KL: 5.52e+01 MMD: 8.00e-02\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 50 VALIDATION Loss: 5.83e+02 logL: -4.62e+02 KL: 5.24e+01 MMD: 7.60e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 5.66e+02 logL: -4.57e+02 KL: 5.20e+01 MMD: 6.32e-02\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.52e+02 logL: -4.48e+02 KL: 5.15e+01 MMD: 5.81e-02\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 80 VALIDATION Loss: 5.47e+02 logL: -4.46e+02 KL: 5.17e+01 MMD: 5.45e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 5.48e+02 logL: -4.45e+02 KL: 5.17e+01 MMD: 5.63e-02\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 100 VALIDATION Loss: 5.42e+02 logL: -4.45e+02 KL: 5.17e+01 MMD: 5.05e-02\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Stopping\n",
      "====> Epoch: 105 VALIDATION Loss: 5.60e+02 logL: -4.45e+02 KL: 5.17e+01 MMD: 7.05e-02\n",
      "config 60, alpha = 0.0, lambda = 11.8, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.45e+03 logL: -5.36e+03 KL: 8.06e+01 MMD: 1.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.29e+03 logL: -5.24e+03 KL: 3.66e+01 MMD: 1.09e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.89e+03 logL: -4.83e+03 KL: 5.33e+01 MMD: 7.85e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.74e+03 logL: -3.69e+03 KL: 4.51e+01 MMD: 7.49e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.69e+03 logL: -3.64e+03 KL: 3.33e+01 MMD: 8.20e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.67e+03 logL: -3.63e+03 KL: 2.67e+01 MMD: 8.31e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.66e+03 logL: -3.63e+03 KL: 2.26e+01 MMD: 8.69e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.65e+03 logL: -3.62e+03 KL: 1.99e+01 MMD: 9.58e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.65e+03 logL: -3.62e+03 KL: 1.79e+01 MMD: 8.43e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.64e+03 logL: -3.61e+03 KL: 1.66e+01 MMD: 8.59e-01\n",
      "Stopping\n",
      "====> Epoch: 105 VALIDATION Loss: 3.64e+03 logL: -3.62e+03 KL: 1.59e+01 MMD: 7.53e-01\n",
      "config 60, alpha = 0.0, lambda = 28805.8, dropout = 0.00; 2 hidden layers with 82, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.48e+03 logL: -3.87e+03 KL: 1.10e+01 MMD: 2.09e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.39e+03 logL: -2.62e+03 KL: 1.53e+01 MMD: 2.64e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.94e+03 logL: -2.46e+03 KL: 1.69e+01 MMD: 5.09e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.96e+03 logL: -2.28e+03 KL: 1.83e+01 MMD: 2.30e-02\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.73e+03 logL: -2.10e+03 KL: 2.18e+01 MMD: 2.13e-02\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.38e+03 logL: -2.08e+03 KL: 2.25e+01 MMD: 9.65e-03\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.36e+03 logL: -2.07e+03 KL: 2.27e+01 MMD: 9.39e-03\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 80 VALIDATION Loss: 2.69e+03 logL: -2.07e+03 KL: 2.27e+01 MMD: 2.07e-02\n",
      "Stopping\n",
      "====> Epoch: 80 VALIDATION Loss: 2.69e+03 logL: -2.07e+03 KL: 2.27e+01 MMD: 2.07e-02\n",
      "config 60, alpha = 0.0, lambda = 30623.4, dropout = 0.00; 2 hidden layers with 74, 57 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.72e+03 logL: -2.56e+03 KL: 1.63e+01 MMD: 3.74e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.44e+03 logL: -1.89e+03 KL: 2.16e+01 MMD: 5.00e-02\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 2.39e+03 logL: -1.49e+03 KL: 2.42e+01 MMD: 2.86e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.10e+03 logL: -1.46e+03 KL: 2.63e+01 MMD: 2.00e-02\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.14e+03 logL: -1.42e+03 KL: 2.69e+01 MMD: 2.27e-02\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 56 VALIDATION Loss: 2.13e+03 logL: -1.42e+03 KL: 2.69e+01 MMD: 2.26e-02\n",
      "config 60, alpha = 0.0, lambda = 4340.7, dropout = 0.00; 2 hidden layers with 104, 58 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.22e+03 logL: -2.58e+03 KL: 4.24e+01 MMD: 1.39e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.68e+03 logL: -1.24e+03 KL: 3.04e+01 MMD: 9.38e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 1.31e+03 logL: -9.34e+02 KL: 3.23e+01 MMD: 8.03e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.07e+03 logL: -7.60e+02 KL: 3.70e+01 MMD: 6.22e-02\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.00e+03 logL: -6.70e+02 KL: 3.97e+01 MMD: 6.80e-02\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.16e+02 logL: -6.56e+02 KL: 3.91e+01 MMD: 5.10e-02\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.15e+02 logL: -6.56e+02 KL: 3.91e+01 MMD: 5.07e-02\n",
      "Stopping\n",
      "====> Epoch: 73 VALIDATION Loss: 8.88e+02 logL: -6.55e+02 KL: 3.91e+01 MMD: 4.46e-02\n",
      "config 60, alpha = 0.0, lambda = 33.6, dropout = 0.00; 2 hidden layers with 61, 30 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.10e+03 logL: -9.50e+02 KL: 9.07e+01 MMD: 1.96e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.85e+02 logL: -6.59e+02 KL: 6.45e+01 MMD: 1.89e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.06e+02 logL: -5.01e+02 KL: 5.59e+01 MMD: 1.50e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.89e+02 logL: -5.00e+02 KL: 4.87e+01 MMD: 1.23e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.07e+02 logL: -4.29e+02 KL: 4.61e+01 MMD: 9.73e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 4.81e+02 logL: -4.16e+02 KL: 4.40e+01 MMD: 6.47e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 4.90e+02 logL: -4.30e+02 KL: 4.32e+01 MMD: 5.32e-01\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 4.59e+02 logL: -4.02e+02 KL: 4.24e+01 MMD: 4.44e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 4.56e+02 logL: -4.02e+02 KL: 4.23e+01 MMD: 3.62e-01\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 98 VALIDATION Loss: 4.56e+02 logL: -4.01e+02 KL: 4.24e+01 MMD: 4.00e-01\n",
      "config 61, alpha = 0.0, lambda = 1080.4, dropout = 0.00; 2 hidden layers with 166, 88 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.37e+03 logL: -3.44e+03 KL: 7.31e+01 MMD: 7.94e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.02e+03 logL: -3.12e+03 KL: 5.77e+01 MMD: 7.77e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.77e+03 logL: -3.23e+03 KL: 3.25e+01 MMD: 4.66e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.28e+03 logL: -3.14e+03 KL: 1.48e+01 MMD: 1.19e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.22e+03 logL: -3.15e+03 KL: 1.23e+01 MMD: 5.72e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.12e+03 logL: -3.04e+03 KL: 1.17e+01 MMD: 6.29e-02\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.00e+03 logL: -2.96e+03 KL: 1.22e+01 MMD: 2.70e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 2.97e+03 logL: -2.94e+03 KL: 1.20e+01 MMD: 1.95e-02\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 85 VALIDATION Loss: 2.96e+03 logL: -2.93e+03 KL: 1.20e+01 MMD: 1.98e-02\n",
      "config 61, alpha = 0.0, lambda = 51.0, dropout = 0.00; 2 hidden layers with 8, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.59e+03 logL: -5.41e+03 KL: 1.14e+02 MMD: 1.37e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.21e+03 logL: -5.06e+03 KL: 6.84e+01 MMD: 1.56e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.83e+03 logL: -3.70e+03 KL: 7.60e+01 MMD: 1.25e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.77e+03 logL: -3.65e+03 KL: 5.18e+01 MMD: 1.35e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.74e+03 logL: -3.63e+03 KL: 3.94e+01 MMD: 1.28e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.71e+03 logL: -3.62e+03 KL: 3.09e+01 MMD: 1.19e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.68e+03 logL: -3.61e+03 KL: 2.59e+01 MMD: 9.98e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.67e+03 logL: -3.60e+03 KL: 2.18e+01 MMD: 9.30e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.65e+03 logL: -3.60e+03 KL: 1.86e+01 MMD: 7.46e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.72e+03 logL: -3.67e+03 KL: 1.80e+01 MMD: 6.37e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 3.02e+03 logL: -2.96e+03 KL: 2.38e+01 MMD: 7.33e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 2.60e+03 logL: -2.54e+03 KL: 2.46e+01 MMD: 7.92e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 2.57e+03 logL: -2.52e+03 KL: 2.19e+01 MMD: 5.84e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 2.55e+03 logL: -2.51e+03 KL: 1.99e+01 MMD: 4.53e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 2.53e+03 logL: -2.50e+03 KL: 1.88e+01 MMD: 2.60e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 160 VALIDATION Loss: 2.53e+03 logL: -2.50e+03 KL: 1.80e+01 MMD: 2.61e-01\n",
      "Stopping\n",
      "====> Epoch: 160 VALIDATION Loss: 2.53e+03 logL: -2.50e+03 KL: 1.80e+01 MMD: 2.61e-01\n",
      "config 61, alpha = 0.0, lambda = 5.4, dropout = 0.00; 2 hidden layers with 120, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.09e+03 logL: -1.94e+03 KL: 1.44e+02 MMD: 1.30e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+03 logL: -1.31e+03 KL: 7.63e+01 MMD: 1.28e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.34e+03 logL: -1.28e+03 KL: 5.52e+01 MMD: 1.32e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.18e+03 logL: -1.13e+03 KL: 5.22e+01 MMD: 1.33e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.00e+03 logL: -9.50e+02 KL: 4.62e+01 MMD: 1.15e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.77e+02 logL: -9.30e+02 KL: 4.20e+01 MMD: 1.18e+00\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.60e+02 logL: -9.15e+02 KL: 4.01e+01 MMD: 1.10e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.59e+02 logL: -9.15e+02 KL: 3.90e+01 MMD: 1.12e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.57e+02 logL: -9.14e+02 KL: 3.80e+01 MMD: 1.16e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.59e+02 logL: -9.16e+02 KL: 3.73e+01 MMD: 1.22e+00\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 101 VALIDATION Loss: 9.56e+02 logL: -9.14e+02 KL: 3.73e+01 MMD: 1.03e+00\n",
      "config 61, alpha = 0.0, lambda = 141.7, dropout = 0.00; 2 hidden layers with 174, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.97e+03 logL: -2.58e+03 KL: 1.25e+02 MMD: 1.93e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.89e+03 logL: -2.53e+03 KL: 8.36e+01 MMD: 1.94e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.75e+03 logL: -2.50e+03 KL: 5.51e+01 MMD: 1.38e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.97e+03 logL: -1.85e+03 KL: 4.39e+01 MMD: 5.82e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.40e+03 logL: -1.30e+03 KL: 4.39e+01 MMD: 3.80e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.36e+03 logL: -1.28e+03 KL: 4.08e+01 MMD: 2.73e-01\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.33e+03 logL: -1.26e+03 KL: 4.05e+01 MMD: 2.44e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.34e+03 logL: -1.25e+03 KL: 4.01e+01 MMD: 3.06e-01\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 88 VALIDATION Loss: 1.33e+03 logL: -1.25e+03 KL: 3.99e+01 MMD: 2.67e-01\n",
      "config 61, alpha = 0.0, lambda = 2.7, dropout = 0.00; 2 hidden layers with 81, 56 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.74e+02 logL: -7.78e+02 KL: 9.29e+01 MMD: 1.80e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.86e+02 logL: -5.11e+02 KL: 7.20e+01 MMD: 1.71e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.44e+02 logL: -4.80e+02 KL: 6.12e+01 MMD: 1.89e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.78e+02 logL: -4.19e+02 KL: 5.62e+01 MMD: 1.73e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.77e+02 logL: -4.24e+02 KL: 5.04e+01 MMD: 1.67e+00\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.56e+02 logL: -4.02e+02 KL: 5.07e+01 MMD: 1.68e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.54e+02 logL: -4.02e+02 KL: 4.94e+01 MMD: 1.62e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.54e+02 logL: -4.03e+02 KL: 4.88e+01 MMD: 1.49e+00\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 4.50e+02 logL: -4.00e+02 KL: 4.77e+01 MMD: 1.60e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.50e+02 logL: -4.00e+02 KL: 4.75e+01 MMD: 1.48e+00\n",
      "Stopping\n",
      "====> Epoch: 105 VALIDATION Loss: 4.50e+02 logL: -4.00e+02 KL: 4.75e+01 MMD: 1.42e+00\n",
      "config 62, alpha = 0.0, lambda = 1262.2, dropout = 0.00; 2 hidden layers with 143, 38 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.14e+03 logL: -3.19e+03 KL: 6.89e+01 MMD: 6.97e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.35e+03 logL: -3.16e+03 KL: 2.38e+01 MMD: 1.32e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.17e+03 logL: -3.11e+03 KL: 1.31e+01 MMD: 3.77e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.12e+03 logL: -3.09e+03 KL: 1.30e+01 MMD: 1.94e-02\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.10e+03 logL: -3.06e+03 KL: 1.15e+01 MMD: 2.34e-02\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.07e+03 logL: -3.05e+03 KL: 1.14e+01 MMD: 1.20e-02\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: 3.07e+03 logL: -3.04e+03 KL: 1.14e+01 MMD: 1.43e-02\n",
      "config 62, alpha = 0.0, lambda = 43.3, dropout = 0.00; 2 hidden layers with 87, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.96e+03 logL: -1.84e+03 KL: 7.39e+01 MMD: 1.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.90e+03 logL: -1.80e+03 KL: 4.85e+01 MMD: 1.07e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.84e+03 logL: -1.76e+03 KL: 3.86e+01 MMD: 9.82e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.78e+03 logL: -1.71e+03 KL: 3.45e+01 MMD: 9.43e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.75e+03 logL: -1.68e+03 KL: 3.03e+01 MMD: 9.42e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.71e+03 logL: -1.65e+03 KL: 2.56e+01 MMD: 7.38e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.67e+03 logL: -1.62e+03 KL: 2.38e+01 MMD: 6.12e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.65e+03 logL: -1.61e+03 KL: 2.35e+01 MMD: 4.45e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.62e+03 logL: -1.58e+03 KL: 2.29e+01 MMD: 3.43e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.63e+03 logL: -1.60e+03 KL: 2.14e+01 MMD: 2.72e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.60e+03 logL: -1.57e+03 KL: 2.21e+01 MMD: 3.07e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.65e+03 logL: -1.62e+03 KL: 2.01e+01 MMD: 1.93e-01\n",
      "Epoch 00125: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.57e+03 logL: -1.54e+03 KL: 2.23e+01 MMD: 1.88e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 1.56e+03 logL: -1.53e+03 KL: 2.18e+01 MMD: 1.58e-01\n",
      "Epoch 00146: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 150 VALIDATION Loss: 1.56e+03 logL: -1.53e+03 KL: 2.15e+01 MMD: 1.71e-01\n",
      "Stopping\n",
      "====> Epoch: 150 VALIDATION Loss: 1.56e+03 logL: -1.53e+03 KL: 2.15e+01 MMD: 1.71e-01\n",
      "config 62, alpha = 0.0, lambda = 64116.2, dropout = 0.00; 2 hidden layers with 33, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.88e+03 logL: -5.78e+03 KL: 3.59e+00 MMD: 3.27e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 7.84e+03 logL: -4.96e+03 KL: 4.69e+00 MMD: 4.48e-02\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 6.52e+03 logL: -4.74e+03 KL: 5.18e+00 MMD: 2.78e-02\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 7.07e+03 logL: -4.72e+03 KL: 5.47e+00 MMD: 3.66e-02\n",
      "Stopping\n",
      "====> Epoch: 43 VALIDATION Loss: 6.31e+03 logL: -4.71e+03 KL: 5.48e+00 MMD: 2.48e-02\n",
      "config 62, alpha = 0.0, lambda = 79970.6, dropout = 0.00; 2 hidden layers with 82, 21 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.62e+03 logL: -5.69e+03 KL: 3.18e+00 MMD: 3.65e-02\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 7.84e+03 logL: -4.72e+03 KL: 4.52e+00 MMD: 3.89e-02\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 7.77e+03 logL: -4.67e+03 KL: 4.89e+00 MMD: 3.86e-02\n",
      "Stopping\n",
      "====> Epoch: 31 VALIDATION Loss: 7.38e+03 logL: -4.66e+03 KL: 4.89e+00 MMD: 3.39e-02\n",
      "config 62, alpha = 0.0, lambda = 61108.6, dropout = 0.00; 2 hidden layers with 105, 75 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.15e+03 logL: -5.08e+03 KL: 4.10e+00 MMD: 6.64e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 7.51e+03 logL: -4.66e+03 KL: 4.53e+00 MMD: 4.65e-02\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 28 VALIDATION Loss: 8.01e+03 logL: -4.58e+03 KL: 4.92e+00 MMD: 5.60e-02\n",
      "config 63, alpha = 0.0, lambda = 1216.1, dropout = 0.00; 2 hidden layers with 36, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.25e+03 logL: -5.09e+03 KL: 1.91e+02 MMD: 7.99e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.66e+03 logL: -3.67e+03 KL: 1.11e+02 MMD: 7.20e-01\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 29 VALIDATION Loss: 4.61e+03 logL: -3.64e+03 KL: 8.77e+01 MMD: 7.25e-01\n",
      "config 63, alpha = 0.0, lambda = 691.4, dropout = 0.00; 2 hidden layers with 116, 57 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.66e+03 logL: -1.84e+03 KL: 7.76e+01 MMD: 1.07e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.62e+03 logL: -2.07e+03 KL: 3.00e+01 MMD: 7.61e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 30 VALIDATION Loss: 1.86e+03 logL: -1.79e+03 KL: 2.44e+01 MMD: 6.87e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.70e+03 logL: -1.63e+03 KL: 2.54e+01 MMD: 6.55e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.68e+03 logL: -1.60e+03 KL: 2.54e+01 MMD: 7.74e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.70e+03 logL: -1.63e+03 KL: 2.31e+01 MMD: 6.11e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 1.67e+03 logL: -1.59e+03 KL: 2.30e+01 MMD: 7.87e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 1.58e+03 logL: -1.54e+03 KL: 2.07e+01 MMD: 2.97e-02\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.53e+03 logL: -1.49e+03 KL: 2.05e+01 MMD: 2.72e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 1.53e+03 logL: -1.49e+03 KL: 2.01e+01 MMD: 3.10e-02\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 106 VALIDATION Loss: 1.53e+03 logL: -1.49e+03 KL: 2.03e+01 MMD: 3.03e-02\n",
      "config 63, alpha = 0.0, lambda = 21.4, dropout = 0.00; 2 hidden layers with 157, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.76e+03 logL: -3.65e+03 KL: 7.00e+01 MMD: 2.00e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.64e+03 logL: -2.53e+03 KL: 7.25e+01 MMD: 1.76e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.93e+03 logL: -1.85e+03 KL: 5.61e+01 MMD: 1.48e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.37e+03 logL: -1.29e+03 KL: 5.23e+01 MMD: 1.36e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.37e+03 logL: -1.30e+03 KL: 4.47e+01 MMD: 1.25e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.32e+03 logL: -1.26e+03 KL: 4.02e+01 MMD: 1.03e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.14e+03 logL: -1.07e+03 KL: 4.08e+01 MMD: 1.13e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.92e+02 logL: -9.37e+02 KL: 3.58e+01 MMD: 9.41e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.85e+02 logL: -9.35e+02 KL: 3.32e+01 MMD: 8.31e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 9.65e+02 logL: -9.21e+02 KL: 3.18e+01 MMD: 5.77e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 9.72e+02 logL: -9.31e+02 KL: 3.00e+01 MMD: 5.54e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 9.62e+02 logL: -9.22e+02 KL: 2.93e+01 MMD: 5.30e-01\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 9.44e+02 logL: -9.07e+02 KL: 2.85e+01 MMD: 4.33e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 9.45e+02 logL: -9.06e+02 KL: 2.84e+01 MMD: 4.73e-01\n",
      "Epoch 00148: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 150 VALIDATION Loss: 9.40e+02 logL: -9.04e+02 KL: 2.82e+01 MMD: 3.57e-01\n",
      "Stopping\n",
      "====> Epoch: 152 VALIDATION Loss: 9.40e+02 logL: -9.05e+02 KL: 2.82e+01 MMD: 3.70e-01\n",
      "config 63, alpha = 0.0, lambda = 616.1, dropout = 0.00; 2 hidden layers with 40, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.51e+03 logL: -3.35e+03 KL: 1.18e+02 MMD: 1.69e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.34e+03 logL: -1.34e+03 KL: 6.47e+01 MMD: 1.51e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.44e+03 logL: -1.22e+03 KL: 4.00e+01 MMD: 2.84e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.14e+03 logL: -1.00e+03 KL: 3.86e+01 MMD: 1.62e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.50e+02 logL: -8.43e+02 KL: 3.72e+01 MMD: 1.13e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 8.92e+02 logL: -7.73e+02 KL: 3.63e+01 MMD: 1.35e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.11e+02 logL: -8.26e+02 KL: 3.42e+01 MMD: 8.35e-02\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 8.40e+02 logL: -7.52e+02 KL: 3.50e+01 MMD: 8.61e-02\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 87 VALIDATION Loss: 8.34e+02 logL: -7.46e+02 KL: 3.48e+01 MMD: 8.71e-02\n",
      "config 63, alpha = 0.0, lambda = 37489.1, dropout = 0.00; 2 hidden layers with 71, 28 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.71e+03 logL: -4.82e+03 KL: 4.07e+00 MMD: 5.03e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 6.73e+03 logL: -4.59e+03 KL: 5.41e+00 MMD: 5.72e-02\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 6.16e+03 logL: -4.52e+03 KL: 5.59e+00 MMD: 4.38e-02\n",
      "Stopping\n",
      "====> Epoch: 33 VALIDATION Loss: 6.43e+03 logL: -4.52e+03 KL: 5.77e+00 MMD: 5.06e-02\n",
      "config 64, alpha = 0.0, lambda = 19825.1, dropout = 0.00; 2 hidden layers with 37, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.21e+03 logL: -3.74e+03 KL: 9.62e+00 MMD: 2.34e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.60e+03 logL: -3.48e+03 KL: 1.12e+01 MMD: 5.57e-03\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.59e+03 logL: -3.39e+03 KL: 1.15e+01 MMD: 9.69e-03\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 3.54e+03 logL: -3.38e+03 KL: 1.18e+01 MMD: 7.47e-03\n",
      "config 64, alpha = 0.0, lambda = 4976.3, dropout = 0.00; 2 hidden layers with 32, 26 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.53e+03 logL: -3.85e+03 KL: 2.82e+02 MMD: 1.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.95e+03 logL: -3.61e+03 KL: 1.65e+02 MMD: 1.04e+00\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 9.29e+03 logL: -3.47e+03 KL: 1.69e+02 MMD: 1.14e+00\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 8.52e+03 logL: -3.42e+03 KL: 1.71e+02 MMD: 9.91e-01\n",
      "config 64, alpha = 0.0, lambda = 20.0, dropout = 0.00; 2 hidden layers with 64, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.77e+03 logL: -5.54e+03 KL: 2.07e+02 MMD: 1.47e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.00e+03 logL: -3.86e+03 KL: 1.13e+02 MMD: 1.38e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.81e+03 logL: -2.70e+03 KL: 8.80e+01 MMD: 1.37e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.32e+03 logL: -2.22e+03 KL: 7.52e+01 MMD: 1.47e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.51e+03 logL: -1.41e+03 KL: 7.43e+01 MMD: 1.28e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.16e+03 logL: -1.07e+03 KL: 6.71e+01 MMD: 1.26e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.05e+03 logL: -9.69e+02 KL: 5.93e+01 MMD: 1.30e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.04e+03 logL: -9.61e+02 KL: 5.16e+01 MMD: 1.25e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.01e+03 logL: -9.45e+02 KL: 4.52e+01 MMD: 1.15e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.00e+03 logL: -9.43e+02 KL: 4.08e+01 MMD: 9.98e-01\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 9.89e+02 logL: -9.27e+02 KL: 3.99e+01 MMD: 1.20e+00\n",
      "Epoch 00118: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 118 VALIDATION Loss: 9.87e+02 logL: -9.27e+02 KL: 3.91e+01 MMD: 1.09e+00\n",
      "config 64, alpha = 0.0, lambda = 4.0, dropout = 0.00; 2 hidden layers with 41, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.66e+03 logL: -2.56e+03 KL: 1.01e+02 MMD: 1.83e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.91e+03 logL: -1.83e+03 KL: 6.56e+01 MMD: 1.99e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.04e+03 logL: -9.65e+02 KL: 6.71e+01 MMD: 1.55e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.99e+02 logL: -9.41e+02 KL: 5.24e+01 MMD: 1.75e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.55e+02 logL: -9.02e+02 KL: 4.87e+01 MMD: 1.56e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.98e+02 logL: -7.47e+02 KL: 4.58e+01 MMD: 1.58e+00\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 7.84e+02 logL: -7.37e+02 KL: 4.24e+01 MMD: 1.36e+00\n",
      "Stopping\n",
      "====> Epoch: 79 VALIDATION Loss: 7.83e+02 logL: -7.37e+02 KL: 4.18e+01 MMD: 1.46e+00\n",
      "config 64, alpha = 0.0, lambda = 8.5, dropout = 0.00; 2 hidden layers with 66, 55 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.08e+03 logL: -9.65e+02 KL: 9.98e+01 MMD: 2.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.00e+02 logL: -6.14e+02 KL: 7.23e+01 MMD: 1.86e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.38e+02 logL: -4.62e+02 KL: 6.34e+01 MMD: 1.76e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.96e+02 logL: -4.24e+02 KL: 5.84e+01 MMD: 1.76e+00\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.74e+02 logL: -4.06e+02 KL: 5.54e+01 MMD: 1.60e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.72e+02 logL: -4.06e+02 KL: 5.37e+01 MMD: 1.63e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.68e+02 logL: -4.05e+02 KL: 5.18e+01 MMD: 1.52e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.66e+02 logL: -4.05e+02 KL: 5.08e+01 MMD: 1.41e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.64e+02 logL: -4.05e+02 KL: 4.95e+01 MMD: 1.36e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.63e+02 logL: -4.04e+02 KL: 4.91e+01 MMD: 1.34e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00108: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 4.61e+02 logL: -4.02e+02 KL: 4.83e+01 MMD: 1.38e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.59e+02 logL: -4.02e+02 KL: 4.81e+01 MMD: 1.10e+00\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 130 VALIDATION Loss: 4.61e+02 logL: -4.02e+02 KL: 4.80e+01 MMD: 1.46e+00\n",
      "Stopping\n",
      "====> Epoch: 130 VALIDATION Loss: 4.61e+02 logL: -4.02e+02 KL: 4.80e+01 MMD: 1.46e+00\n",
      "config 65, alpha = 0.0, lambda = 15510.1, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.04e+04 logL: -7.22e+03 KL: 3.60e+01 MMD: 2.00e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.49e+03 logL: -5.36e+03 KL: 5.04e+00 MMD: 8.65e-03\n",
      "====> Epoch: 30 VALIDATION Loss: 5.42e+03 logL: -5.20e+03 KL: 6.12e+00 MMD: 1.35e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 5.12e+03 logL: -4.99e+03 KL: 6.59e+00 MMD: 8.30e-03\n",
      "====> Epoch: 50 VALIDATION Loss: 4.62e+03 logL: -4.52e+03 KL: 6.92e+00 MMD: 6.27e-03\n",
      "====> Epoch: 60 VALIDATION Loss: 4.58e+03 logL: -4.34e+03 KL: 7.38e+00 MMD: 1.51e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 4.03e+03 logL: -3.80e+03 KL: 9.57e+00 MMD: 1.42e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 3.82e+03 logL: -3.70e+03 KL: 1.07e+01 MMD: 6.90e-03\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.83e+03 logL: -3.64e+03 KL: 1.13e+01 MMD: 1.12e-02\n",
      "Stopping\n",
      "====> Epoch: 90 VALIDATION Loss: 3.83e+03 logL: -3.64e+03 KL: 1.13e+01 MMD: 1.12e-02\n",
      "config 65, alpha = 0.0, lambda = 21769.2, dropout = 0.00; 2 hidden layers with 17, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.70e+03 logL: -4.81e+03 KL: 9.96e+00 MMD: 4.05e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.16e+03 logL: -3.53e+03 KL: 1.10e+01 MMD: 2.82e-02\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.85e+03 logL: -3.41e+03 KL: 1.20e+01 MMD: 1.96e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.71e+03 logL: -3.35e+03 KL: 1.27e+01 MMD: 1.58e-02\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.78e+03 logL: -3.33e+03 KL: 1.29e+01 MMD: 1.99e-02\n",
      "Stopping\n",
      "====> Epoch: 51 VALIDATION Loss: 3.71e+03 logL: -3.32e+03 KL: 1.29e+01 MMD: 1.73e-02\n",
      "config 65, alpha = 0.0, lambda = 30686.1, dropout = 0.00; 2 hidden layers with 86, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.64e+03 logL: -4.83e+03 KL: 4.27e+00 MMD: 2.63e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.31e+03 logL: -4.57e+03 KL: 5.13e+00 MMD: 2.40e-02\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.34e+03 logL: -4.49e+03 KL: 5.85e+00 MMD: 2.77e-02\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 5.41e+03 logL: -4.46e+03 KL: 6.03e+00 MMD: 3.08e-02\n",
      "Stopping\n",
      "====> Epoch: 42 VALIDATION Loss: 5.06e+03 logL: -4.47e+03 KL: 6.06e+00 MMD: 1.91e-02\n",
      "config 65, alpha = 0.0, lambda = 40.8, dropout = 0.00; 2 hidden layers with 12, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.92e+03 logL: -3.70e+03 KL: 1.38e+02 MMD: 2.11e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.80e+03 logL: -3.64e+03 KL: 6.90e+01 MMD: 2.16e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.74e+03 logL: -2.59e+03 KL: 7.38e+01 MMD: 1.80e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.02e+03 logL: -1.88e+03 KL: 7.18e+01 MMD: 1.78e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.94e+03 logL: -1.83e+03 KL: 5.03e+01 MMD: 1.54e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.91e+03 logL: -1.83e+03 KL: 3.98e+01 MMD: 1.10e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.89e+03 logL: -1.82e+03 KL: 3.50e+01 MMD: 8.98e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 3.27e+01 MMD: 5.27e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.87e+03 logL: -1.82e+03 KL: 3.18e+01 MMD: 3.64e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 3.09e+01 MMD: 3.35e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 3.05e+01 MMD: 3.16e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.84e+03 logL: -1.80e+03 KL: 3.00e+01 MMD: 2.44e-01\n",
      "Stopping\n",
      "====> Epoch: 125 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 3.01e+01 MMD: 2.63e-01\n",
      "config 65, alpha = 0.0, lambda = 142.3, dropout = 0.00; 2 hidden layers with 160, 27 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.08e+03 logL: -2.63e+03 KL: 1.67e+02 MMD: 2.00e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.25e+03 logL: -1.87e+03 KL: 9.33e+01 MMD: 2.05e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.31e+03 logL: -9.77e+02 KL: 7.60e+01 MMD: 1.80e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.76e+02 logL: -8.14e+02 KL: 5.09e+01 MMD: 7.88e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 8.90e+02 logL: -7.85e+02 KL: 4.91e+01 MMD: 3.94e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 6.52e+02 logL: -5.69e+02 KL: 4.97e+01 MMD: 2.38e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 5.90e+02 logL: -5.17e+02 KL: 4.80e+01 MMD: 1.79e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 5.88e+02 logL: -5.16e+02 KL: 4.77e+01 MMD: 1.73e-01\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 5.63e+02 logL: -4.89e+02 KL: 4.69e+01 MMD: 1.92e-01\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 5.59e+02 logL: -4.88e+02 KL: 4.68e+01 MMD: 1.65e-01\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 105 VALIDATION Loss: 5.50e+02 logL: -4.88e+02 KL: 4.68e+01 MMD: 1.13e-01\n",
      "config 66, alpha = 0.0, lambda = 1.8, dropout = 0.00; 2 hidden layers with 21, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.82e+03 logL: -3.66e+03 KL: 1.59e+02 MMD: 6.71e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.70e+03 logL: -3.64e+03 KL: 6.46e+01 MMD: 7.88e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.69e+03 logL: -3.65e+03 KL: 4.18e+01 MMD: 7.75e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.64e+03 logL: -3.61e+03 KL: 3.10e+01 MMD: 7.65e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.61e+03 logL: -3.58e+03 KL: 2.74e+01 MMD: 8.03e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.45e+03 logL: -3.42e+03 KL: 2.69e+01 MMD: 7.70e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.34e+03 logL: -3.31e+03 KL: 2.71e+01 MMD: 7.43e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.26e+03 logL: -3.24e+03 KL: 2.48e+01 MMD: 7.40e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.24e+03 logL: -3.22e+03 KL: 2.21e+01 MMD: 7.18e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.20e+03 logL: -3.18e+03 KL: 2.02e+01 MMD: 7.51e-01\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 3.15e+03 logL: -3.13e+03 KL: 1.99e+01 MMD: 7.22e-01\n",
      "Epoch 00110: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 114 VALIDATION Loss: 3.15e+03 logL: -3.13e+03 KL: 1.98e+01 MMD: 6.88e-01\n",
      "config 66, alpha = 0.0, lambda = 3244.8, dropout = 0.00; 2 hidden layers with 52, 41 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.00e+03 logL: -2.51e+03 KL: 1.68e+02 MMD: 1.02e+00\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 5.32e+03 logL: -1.85e+03 KL: 1.02e+02 MMD: 1.04e+00\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 4.97e+03 logL: -1.84e+03 KL: 9.61e+01 MMD: 9.34e-01\n",
      "Stopping\n",
      "====> Epoch: 32 VALIDATION Loss: 5.45e+03 logL: -1.84e+03 KL: 9.58e+01 MMD: 1.08e+00\n",
      "config 66, alpha = 0.0, lambda = 30.5, dropout = 0.00; 2 hidden layers with 39, 28 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.41e+03 logL: -1.29e+03 KL: 8.20e+01 MMD: 1.26e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.04e+03 logL: -9.50e+02 KL: 5.66e+01 MMD: 1.19e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.02e+03 logL: -9.41e+02 KL: 4.26e+01 MMD: 1.31e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.00e+03 logL: -9.37e+02 KL: 3.62e+01 MMD: 1.09e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.90e+02 logL: -9.32e+02 KL: 3.16e+01 MMD: 8.94e-01\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.69e+02 logL: -9.22e+02 KL: 2.99e+01 MMD: 5.83e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.72e+02 logL: -9.21e+02 KL: 2.98e+01 MMD: 7.03e-01\n",
      "Stopping\n",
      "====> Epoch: 70 VALIDATION Loss: 9.72e+02 logL: -9.21e+02 KL: 2.98e+01 MMD: 7.03e-01\n",
      "config 66, alpha = 0.0, lambda = 241.3, dropout = 0.00; 2 hidden layers with 13, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.07e+03 logL: -5.52e+03 KL: 1.40e+02 MMD: 1.70e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.42e+03 logL: -3.87e+03 KL: 1.14e+02 MMD: 1.82e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 30 VALIDATION Loss: 3.06e+03 logL: -2.55e+03 KL: 9.65e+01 MMD: 1.70e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.95e+03 logL: -1.48e+03 KL: 8.01e+01 MMD: 1.62e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.74e+03 logL: -1.36e+03 KL: 5.10e+01 MMD: 1.37e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.51e+03 logL: -1.35e+03 KL: 3.68e+01 MMD: 5.13e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.25e+03 logL: -1.13e+03 KL: 3.66e+01 MMD: 3.44e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.07e+03 logL: -9.72e+02 KL: 3.62e+01 MMD: 2.59e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.03e+03 logL: -9.48e+02 KL: 3.52e+01 MMD: 1.99e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 8.76e+02 logL: -8.01e+02 KL: 3.60e+01 MMD: 1.63e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 8.38e+02 logL: -7.65e+02 KL: 3.56e+01 MMD: 1.58e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 8.22e+02 logL: -7.55e+02 KL: 3.50e+01 MMD: 1.31e-01\n",
      "Epoch 00129: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 8.09e+02 logL: -7.46e+02 KL: 3.44e+01 MMD: 1.16e-01\n",
      "Epoch 00139: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 140 VALIDATION Loss: 8.05e+02 logL: -7.44e+02 KL: 3.43e+01 MMD: 1.13e-01\n",
      "Epoch 00149: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 150 VALIDATION Loss: 8.01e+02 logL: -7.43e+02 KL: 3.43e+01 MMD: 9.51e-02\n",
      "Stopping\n",
      "====> Epoch: 153 VALIDATION Loss: 8.08e+02 logL: -7.43e+02 KL: 3.43e+01 MMD: 1.27e-01\n",
      "config 66, alpha = 0.0, lambda = 79.9, dropout = 0.00; 2 hidden layers with 141, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.61e+03 logL: -1.34e+03 KL: 1.10e+02 MMD: 1.98e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.17e+03 logL: -9.63e+02 KL: 6.66e+01 MMD: 1.76e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.27e+02 logL: -7.65e+02 KL: 5.65e+01 MMD: 1.34e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.73e+02 logL: -7.69e+02 KL: 4.97e+01 MMD: 6.87e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 8.36e+02 logL: -7.55e+02 KL: 4.85e+01 MMD: 4.18e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 8.21e+02 logL: -7.45e+02 KL: 4.71e+01 MMD: 3.79e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 6.67e+02 logL: -6.02e+02 KL: 4.62e+01 MMD: 2.47e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 6.72e+02 logL: -6.07e+02 KL: 4.43e+01 MMD: 2.64e-01\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 6.46e+02 logL: -5.89e+02 KL: 4.33e+01 MMD: 1.82e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 6.47e+02 logL: -5.90e+02 KL: 4.30e+01 MMD: 1.83e-01\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 6.46e+02 logL: -5.87e+02 KL: 4.31e+01 MMD: 2.00e-01\n",
      "Epoch 00111: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 115 VALIDATION Loss: 6.45e+02 logL: -5.88e+02 KL: 4.31e+01 MMD: 1.82e-01\n",
      "config 67, alpha = 0.0, lambda = 769.0, dropout = 0.00; 2 hidden layers with 60, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.37e+03 logL: -3.68e+03 KL: 1.22e+02 MMD: 7.43e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.33e+03 logL: -3.63e+03 KL: 6.79e+01 MMD: 8.23e-01\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 4.25e+03 logL: -3.61e+03 KL: 6.11e+01 MMD: 7.47e-01\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 4.23e+03 logL: -3.61e+03 KL: 6.03e+01 MMD: 7.19e-01\n",
      "config 67, alpha = 0.0, lambda = 8.1, dropout = 0.00; 2 hidden layers with 193, 112 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.83e+03 logL: -1.77e+03 KL: 4.78e+01 MMD: 1.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.65e+03 logL: -1.61e+03 KL: 3.57e+01 MMD: 9.61e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.60e+03 logL: -1.56e+03 KL: 2.77e+01 MMD: 9.08e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.56e+03 logL: -1.53e+03 KL: 2.62e+01 MMD: 7.54e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.70e+03 logL: -1.66e+03 KL: 2.48e+01 MMD: 1.02e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.52e+03 logL: -1.49e+03 KL: 2.37e+01 MMD: 5.85e-01\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.48e+03 logL: -1.45e+03 KL: 2.29e+01 MMD: 5.91e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.48e+03 logL: -1.45e+03 KL: 2.31e+01 MMD: 5.75e-01\n",
      "Stopping\n",
      "====> Epoch: 88 VALIDATION Loss: 1.47e+03 logL: -1.45e+03 KL: 2.26e+01 MMD: 6.69e-01\n",
      "config 67, alpha = 0.0, lambda = 23599.0, dropout = 0.00; 2 hidden layers with 19, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.38e+03 logL: -5.59e+03 KL: 7.33e+00 MMD: 3.30e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.30e+03 logL: -3.68e+03 KL: 1.19e+01 MMD: 2.57e-02\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 4.14e+03 logL: -3.53e+03 KL: 1.33e+01 MMD: 2.54e-02\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 4.03e+03 logL: -3.52e+03 KL: 1.37e+01 MMD: 2.09e-02\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 47 VALIDATION Loss: 4.85e+03 logL: -3.52e+03 KL: 1.39e+01 MMD: 5.59e-02\n",
      "config 67, alpha = 0.0, lambda = 35.5, dropout = 0.00; 2 hidden layers with 19, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.91e+03 logL: -3.71e+03 KL: 1.28e+02 MMD: 1.85e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.19e+03 logL: -2.03e+03 KL: 9.42e+01 MMD: 1.72e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.44e+03 logL: -1.31e+03 KL: 7.66e+01 MMD: 1.66e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.10e+03 logL: -9.82e+02 KL: 6.32e+01 MMD: 1.55e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.05e+03 logL: -9.52e+02 KL: 5.29e+01 MMD: 1.44e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.03e+03 logL: -9.41e+02 KL: 4.60e+01 MMD: 1.21e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.01e+03 logL: -9.30e+02 KL: 4.27e+01 MMD: 1.20e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 8.39e+02 logL: -7.57e+02 KL: 4.05e+01 MMD: 1.20e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 8.27e+02 logL: -7.53e+02 KL: 3.86e+01 MMD: 1.02e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 8.10e+02 logL: -7.50e+02 KL: 3.64e+01 MMD: 6.85e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 8.13e+02 logL: -7.50e+02 KL: 3.51e+01 MMD: 7.92e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 7.99e+02 logL: -7.49e+02 KL: 3.46e+01 MMD: 4.62e-01\n",
      "Epoch 00123: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 7.94e+02 logL: -7.40e+02 KL: 3.38e+01 MMD: 5.82e-01\n",
      "Epoch 00130: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 134 VALIDATION Loss: 7.90e+02 logL: -7.39e+02 KL: 3.38e+01 MMD: 5.03e-01\n",
      "config 67, alpha = 0.0, lambda = 426.6, dropout = 0.00; 2 hidden layers with 176, 84 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.79e+03 logL: -8.63e+02 KL: 9.59e+01 MMD: 1.96e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.17e+03 logL: -1.01e+03 KL: 4.74e+01 MMD: 2.79e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 6.39e+02 logL: -5.33e+02 KL: 4.77e+01 MMD: 1.38e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 5.35e+02 logL: -4.61e+02 KL: 4.74e+01 MMD: 6.32e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 5.71e+02 logL: -4.87e+02 KL: 4.70e+01 MMD: 8.59e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 5.20e+02 logL: -4.44e+02 KL: 4.45e+01 MMD: 7.30e-02\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 4.86e+02 logL: -4.10e+02 KL: 4.44e+01 MMD: 7.34e-02\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 4.77e+02 logL: -4.05e+02 KL: 4.41e+01 MMD: 6.54e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 4.72e+02 logL: -4.05e+02 KL: 4.40e+01 MMD: 5.50e-02\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 95 VALIDATION Loss: 4.77e+02 logL: -4.04e+02 KL: 4.40e+01 MMD: 6.84e-02\n",
      "config 68, alpha = 0.0, lambda = 747.6, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.23e+03 logL: -5.43e+03 KL: 1.98e+02 MMD: 8.13e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 6.07e+03 logL: -5.32e+03 KL: 1.16e+02 MMD: 8.53e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.79e+03 logL: -5.18e+03 KL: 6.53e+01 MMD: 7.26e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 5.14e+03 logL: -4.61e+03 KL: 2.09e+01 MMD: 6.83e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 4.57e+03 logL: -4.53e+03 KL: 5.26e+00 MMD: 4.58e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 4.53e+03 logL: -4.52e+03 KL: 5.98e+00 MMD: 7.98e-03\n",
      "====> Epoch: 70 VALIDATION Loss: 4.51e+03 logL: -4.48e+03 KL: 6.84e+00 MMD: 4.00e-02\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 80 VALIDATION Loss: 4.48e+03 logL: -4.42e+03 KL: 8.89e+00 MMD: 7.21e-02\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 4.46e+03 logL: -4.41e+03 KL: 8.88e+00 MMD: 5.00e-02\n",
      "config 68, alpha = 0.0, lambda = 117.6, dropout = 0.00; 2 hidden layers with 8, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.49e+03 logL: -5.16e+03 KL: 2.01e+02 MMD: 1.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.75e+03 logL: -3.54e+03 KL: 7.47e+01 MMD: 1.14e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.74e+03 logL: -2.55e+03 KL: 6.21e+01 MMD: 1.09e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.70e+03 logL: -2.53e+03 KL: 4.38e+01 MMD: 1.08e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.67e+03 logL: -2.53e+03 KL: 3.17e+01 MMD: 9.45e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 2.64e+03 logL: -2.52e+03 KL: 2.28e+01 MMD: 7.96e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 2.59e+03 logL: -2.52e+03 KL: 1.80e+01 MMD: 4.35e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 2.56e+03 logL: -2.52e+03 KL: 1.63e+01 MMD: 2.31e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 2.56e+03 logL: -2.52e+03 KL: 1.56e+01 MMD: 1.60e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 2.54e+03 logL: -2.51e+03 KL: 1.53e+01 MMD: 1.36e-01\n",
      "Stopping\n",
      "====> Epoch: 105 VALIDATION Loss: 2.54e+03 logL: -2.51e+03 KL: 1.50e+01 MMD: 1.11e-01\n",
      "config 68, alpha = 0.0, lambda = 83.3, dropout = 0.00; 2 hidden layers with 9, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.40e+03 logL: -5.14e+03 KL: 1.35e+02 MMD: 1.52e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.96e+03 logL: -2.74e+03 KL: 1.02e+02 MMD: 1.42e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.72e+03 logL: -2.54e+03 KL: 5.88e+01 MMD: 1.53e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.67e+03 logL: -2.53e+03 KL: 4.07e+01 MMD: 1.24e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.62e+03 logL: -2.51e+03 KL: 3.13e+01 MMD: 1.01e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.95e+03 logL: -1.84e+03 KL: 3.10e+01 MMD: 9.47e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.90e+03 logL: -1.83e+03 KL: 2.77e+01 MMD: 6.09e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.88e+03 logL: -1.81e+03 KL: 2.63e+01 MMD: 4.72e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 2.50e+01 MMD: 3.94e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.72e+03 logL: -1.66e+03 KL: 2.98e+01 MMD: 3.65e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.37e+03 logL: -1.31e+03 KL: 2.91e+01 MMD: 3.87e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.34e+03 logL: -1.29e+03 KL: 2.63e+01 MMD: 2.55e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 1.32e+03 logL: -1.28e+03 KL: 2.57e+01 MMD: 2.06e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 1.31e+03 logL: -1.27e+03 KL: 2.55e+01 MMD: 2.03e-01\n",
      "Epoch 00142: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 1.31e+03 logL: -1.27e+03 KL: 2.55e+01 MMD: 1.50e-01\n",
      "Epoch 00154: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 156 VALIDATION Loss: 1.31e+03 logL: -1.27e+03 KL: 2.55e+01 MMD: 1.98e-01\n",
      "config 68, alpha = 0.0, lambda = 17.8, dropout = 0.00; 2 hidden layers with 24, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.86e+03 logL: -3.71e+03 KL: 1.13e+02 MMD: 1.91e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.68e+03 logL: -2.57e+03 KL: 8.54e+01 MMD: 1.80e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.68e+03 logL: -1.58e+03 KL: 7.40e+01 MMD: 1.61e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.37e+03 logL: -1.28e+03 KL: 5.98e+01 MMD: 1.63e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.34e+03 logL: -1.26e+03 KL: 5.26e+01 MMD: 1.69e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.02e+03 logL: -9.40e+02 KL: 5.17e+01 MMD: 1.64e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.01e+03 logL: -9.34e+02 KL: 4.76e+01 MMD: 1.56e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 8.20e+02 logL: -7.47e+02 KL: 4.70e+01 MMD: 1.55e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 8.17e+02 logL: -7.52e+02 KL: 4.23e+01 MMD: 1.37e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 8.05e+02 logL: -7.48e+02 KL: 3.91e+01 MMD: 1.08e+00\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 7.90e+02 logL: -7.36e+02 KL: 3.81e+01 MMD: 9.61e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 7.88e+02 logL: -7.36e+02 KL: 3.77e+01 MMD: 8.69e-01\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 128 VALIDATION Loss: 7.89e+02 logL: -7.36e+02 KL: 3.74e+01 MMD: 9.77e-01\n",
      "config 68, alpha = 0.0, lambda = 98317.7, dropout = 0.00; 2 hidden layers with 47, 28 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -8.99e+03 KL: 2.75e+00 MMD: 5.25e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 1.20e+04 logL: -6.17e+03 KL: 3.58e+00 MMD: 5.96e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 8.91e+03 logL: -4.79e+03 KL: 4.33e+00 MMD: 4.18e-02\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "Stopping\n",
      "====> Epoch: 43 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "config 69, alpha = 0.0, lambda = 68.8, dropout = 0.00; 2 hidden layers with 18, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.57e+03 logL: -5.37e+03 KL: 1.47e+02 MMD: 7.77e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.74e+03 logL: -4.63e+03 KL: 5.45e+01 MMD: 8.18e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.80e+03 logL: -3.67e+03 KL: 6.94e+01 MMD: 7.79e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.71e+03 logL: -3.62e+03 KL: 4.39e+01 MMD: 7.70e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.65e+03 logL: -3.57e+03 KL: 3.35e+01 MMD: 7.20e-01\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "Stopping\n",
      "====> Epoch: 63 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "config 69, alpha = 0.0, lambda = 498.3, dropout = 0.00; 2 hidden layers with 69, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.62e+03 logL: -2.01e+03 KL: 1.38e+02 MMD: 9.57e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 2.43e+03 logL: -1.84e+03 KL: 6.13e+01 MMD: 1.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.06e+03 logL: -1.92e+03 KL: 2.68e+01 MMD: 2.18e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.92e+03 logL: -1.85e+03 KL: 2.58e+01 MMD: 9.11e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.88e+03 logL: -1.82e+03 KL: 2.47e+01 MMD: 7.32e-02\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.43e+01 MMD: 6.11e-02\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.41e+01 MMD: 5.77e-02\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.41e+01 MMD: 5.46e-02\n",
      "config 69, alpha = 0.0, lambda = 542.9, dropout = 0.00; 2 hidden layers with 27, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.48e+03 logL: -3.60e+03 KL: 1.50e+02 MMD: 1.34e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.80e+03 logL: -1.95e+03 KL: 1.01e+02 MMD: 1.38e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.11e+03 logL: -1.35e+03 KL: 8.52e+01 MMD: 1.24e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.84e+03 logL: -1.04e+03 KL: 5.75e+01 MMD: 1.38e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.18e+03 logL: -1.04e+03 KL: 3.38e+01 MMD: 1.83e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.11e+03 logL: -1.02e+03 KL: 3.30e+01 MMD: 1.16e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.07e+03 logL: -1.00e+03 KL: 3.22e+01 MMD: 7.19e-02\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.01e+03 logL: -9.47e+02 KL: 3.15e+01 MMD: 5.64e-02\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.02e+03 logL: -9.45e+02 KL: 3.16e+01 MMD: 7.40e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 1.01e+03 logL: -9.45e+02 KL: 3.15e+01 MMD: 5.78e-02\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 109 VALIDATION Loss: 1.01e+03 logL: -9.45e+02 KL: 3.15e+01 MMD: 6.43e-02\n",
      "config 69, alpha = 0.0, lambda = 6806.0, dropout = 0.00; 2 hidden layers with 142, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.34e+03 logL: -2.79e+03 KL: 2.05e+01 MMD: 7.70e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.12e+03 logL: -2.64e+03 KL: 2.26e+01 MMD: 6.69e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.85e+03 logL: -2.53e+03 KL: 2.13e+01 MMD: 4.33e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.79e+03 logL: -2.51e+03 KL: 2.16e+01 MMD: 3.78e-02\n",
      "====> Epoch: 50 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 55 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "config 69, alpha = 0.0, lambda = 19.4, dropout = 0.00; 2 hidden layers with 41, 26 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 1.17e+03 logL: -1.02e+03 KL: 1.13e+02 MMD: 1.92e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.62e+02 logL: -7.59e+02 KL: 7.08e+01 MMD: 1.76e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.58e+02 logL: -6.69e+02 KL: 6.03e+01 MMD: 1.58e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.08e+02 logL: -6.28e+02 KL: 5.36e+01 MMD: 1.44e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.11e+02 logL: -6.39e+02 KL: 4.81e+01 MMD: 1.30e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.68e+02 logL: -6.03e+02 KL: 4.50e+01 MMD: 1.12e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.61e+02 logL: -4.97e+02 KL: 4.48e+01 MMD: 1.06e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.46e+02 logL: -4.88e+02 KL: 4.31e+01 MMD: 8.28e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 4.69e+02 logL: -4.14e+02 KL: 4.18e+01 MMD: 7.51e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 4.64e+02 logL: -4.10e+02 KL: 4.09e+01 MMD: 7.17e-01\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 4.54e+02 logL: -4.01e+02 KL: 4.05e+01 MMD: 6.52e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 4.53e+02 logL: -4.01e+02 KL: 4.03e+01 MMD: 6.26e-01\n",
      "Epoch 00122: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 4.52e+02 logL: -4.00e+02 KL: 4.04e+01 MMD: 6.33e-01\n",
      "Epoch 00131: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 135 VALIDATION Loss: 4.53e+02 logL: -4.00e+02 KL: 4.03e+01 MMD: 6.94e-01\n",
      "config 70, alpha = 0.0, lambda = 701.5, dropout = 0.00; 2 hidden layers with 59, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.26e+03 logL: -4.67e+03 KL: 6.30e+01 MMD: 7.55e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.26e+03 logL: -3.67e+03 KL: 6.94e+01 MMD: 7.45e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 4.16e+03 logL: -3.56e+03 KL: 6.17e+01 MMD: 7.73e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 8.09e+03 logL: -7.48e+03 KL: 1.47e+02 MMD: 6.56e-01\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 48 VALIDATION Loss: 4.88e+03 logL: -4.31e+03 KL: 8.27e+01 MMD: 6.96e-01\n",
      "config 70, alpha = 0.0, lambda = 2028.6, dropout = 0.00; 2 hidden layers with 118, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.38e+03 logL: -4.12e+03 KL: 2.26e+02 MMD: 1.00e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.89e+03 logL: -2.79e+03 KL: 1.24e+02 MMD: 9.78e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.86e+03 logL: -1.89e+03 KL: 7.69e+01 MMD: 9.33e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.11e+03 logL: -1.93e+03 KL: 2.19e+01 MMD: 7.67e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 2.09e+03 logL: -1.90e+03 KL: 2.20e+01 MMD: 8.69e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 2.06e+03 logL: -1.89e+03 KL: 2.16e+01 MMD: 6.97e-02\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 67 VALIDATION Loss: 2.03e+03 logL: -1.90e+03 KL: 2.04e+01 MMD: 5.31e-02\n",
      "config 70, alpha = 0.0, lambda = 29.5, dropout = 0.00; 2 hidden layers with 85, 27 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.08e+03 logL: -1.92e+03 KL: 1.22e+02 MMD: 1.35e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.07e+03 logL: -9.53e+02 KL: 7.53e+01 MMD: 1.34e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.05e+03 logL: -9.63e+02 KL: 5.55e+01 MMD: 1.24e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.03e+03 logL: -9.43e+02 KL: 4.71e+01 MMD: 1.30e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.02e+03 logL: -9.44e+02 KL: 4.24e+01 MMD: 1.16e+00\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.90e+02 logL: -9.19e+02 KL: 4.05e+01 MMD: 1.07e+00\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.88e+02 logL: -9.17e+02 KL: 3.94e+01 MMD: 1.10e+00\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.85e+02 logL: -9.17e+02 KL: 3.92e+01 MMD: 1.01e+00\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 9.87e+02 logL: -9.17e+02 KL: 3.92e+01 MMD: 1.10e+00\n",
      "config 70, alpha = 0.0, lambda = 73455.9, dropout = 0.00; 2 hidden layers with 53, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.07e+04 logL: -7.32e+03 KL: 2.43e+00 MMD: 4.54e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 7.47e+03 logL: -4.82e+03 KL: 3.86e+00 MMD: 3.59e-02\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 7.44e+03 logL: -4.70e+03 KL: 4.43e+00 MMD: 3.72e-02\n",
      "Stopping\n",
      "====> Epoch: 33 VALIDATION Loss: 7.10e+03 logL: -4.71e+03 KL: 4.49e+00 MMD: 3.25e-02\n",
      "config 70, alpha = 0.0, lambda = 7.7, dropout = 0.00; 2 hidden layers with 15, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.27e+03 logL: -2.12e+03 KL: 1.32e+02 MMD: 2.18e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.38e+03 logL: -1.29e+03 KL: 8.34e+01 MMD: 2.12e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.04e+03 logL: -9.54e+02 KL: 6.97e+01 MMD: 1.92e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.01e+03 logL: -9.42e+02 KL: 5.79e+01 MMD: 1.92e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.00e+03 logL: -9.38e+02 KL: 5.05e+01 MMD: 1.77e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.00e+03 logL: -9.41e+02 KL: 4.83e+01 MMD: 1.77e+00\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.80e+02 logL: -9.23e+02 KL: 4.58e+01 MMD: 1.67e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.22e+02 logL: -8.64e+02 KL: 4.64e+01 MMD: 1.72e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 8.10e+02 logL: -7.51e+02 KL: 4.88e+01 MMD: 1.60e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 8.04e+02 logL: -7.45e+02 KL: 4.77e+01 MMD: 1.77e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 7.98e+02 logL: -7.41e+02 KL: 4.70e+01 MMD: 1.59e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 7.97e+02 logL: -7.40e+02 KL: 4.59e+01 MMD: 1.60e+00\n",
      "Epoch 00127: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 7.95e+02 logL: -7.39e+02 KL: 4.55e+01 MMD: 1.61e+00\n",
      "Stopping\n",
      "====> Epoch: 138 VALIDATION Loss: 7.96e+02 logL: -7.39e+02 KL: 4.54e+01 MMD: 1.75e+00\n",
      "config 71, alpha = 0.0, lambda = 36622.1, dropout = 0.00; 2 hidden layers with 28, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.24e+03 logL: -4.91e+03 KL: 4.66e+00 MMD: 8.92e-03\n",
      "====> Epoch: 20 VALIDATION Loss: 4.96e+03 logL: -4.43e+03 KL: 5.58e+00 MMD: 1.43e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 5.05e+03 logL: -4.72e+03 KL: 7.02e+00 MMD: 8.80e-03\n",
      "====> Epoch: 40 VALIDATION Loss: 4.58e+03 logL: -4.19e+03 KL: 1.03e+01 MMD: 1.04e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 4.20e+03 logL: -3.64e+03 KL: 9.67e+00 MMD: 1.51e-02\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.04e+03 logL: -3.57e+03 KL: 1.17e+01 MMD: 1.26e-02\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.79e+03 logL: -3.54e+03 KL: 1.19e+01 MMD: 6.55e-03\n",
      "Stopping\n",
      "====> Epoch: 71 VALIDATION Loss: 3.76e+03 logL: -3.54e+03 KL: 1.19e+01 MMD: 5.67e-03\n",
      "config 71, alpha = 0.0, lambda = 33124.2, dropout = 0.00; 2 hidden layers with 23, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.85e+03 logL: -6.59e+03 KL: 7.86e+00 MMD: 3.79e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.93e+03 logL: -3.90e+03 KL: 1.04e+01 MMD: 3.08e-02\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 4.12e+03 logL: -3.54e+03 KL: 1.09e+01 MMD: 1.71e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 4.00e+03 logL: -3.53e+03 KL: 1.13e+01 MMD: 1.40e-02\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.88e+03 logL: -3.51e+03 KL: 1.15e+01 MMD: 1.11e-02\n",
      "Stopping\n",
      "====> Epoch: 53 VALIDATION Loss: 3.88e+03 logL: -3.51e+03 KL: 1.15e+01 MMD: 1.07e-02\n",
      "config 71, alpha = 0.0, lambda = 62.5, dropout = 0.00; 2 hidden layers with 129, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.05e+03 logL: -2.80e+03 KL: 1.55e+02 MMD: 1.48e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.09e+03 logL: -1.91e+03 KL: 9.34e+01 MMD: 1.38e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.49e+03 logL: -1.33e+03 KL: 8.51e+01 MMD: 1.35e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.43e+03 logL: -1.29e+03 KL: 6.04e+01 MMD: 1.34e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.43e+03 logL: -1.28e+03 KL: 5.00e+01 MMD: 1.57e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.40e+03 logL: -1.28e+03 KL: 4.26e+01 MMD: 1.25e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.42e+03 logL: -1.33e+03 KL: 3.49e+01 MMD: 8.59e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.11e+03 logL: -1.01e+03 KL: 4.00e+01 MMD: 9.24e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.98e+02 logL: -9.29e+02 KL: 3.28e+01 MMD: 5.91e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 100 VALIDATION Loss: 1.00e+03 logL: -9.48e+02 KL: 3.00e+01 MMD: 3.58e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 9.71e+02 logL: -9.24e+02 KL: 2.93e+01 MMD: 2.75e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 9.66e+02 logL: -9.25e+02 KL: 2.85e+01 MMD: 2.08e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 9.62e+02 logL: -9.22e+02 KL: 2.75e+01 MMD: 2.07e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 9.74e+02 logL: -9.40e+02 KL: 2.69e+01 MMD: 1.19e-01\n",
      "Epoch 00143: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 9.37e+02 logL: -9.04e+02 KL: 2.68e+01 MMD: 1.04e-01\n",
      "Epoch 00153: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 160 VALIDATION Loss: 9.35e+02 logL: -9.03e+02 KL: 2.68e+01 MMD: 9.47e-02\n",
      "Epoch 00161: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 165 VALIDATION Loss: 9.40e+02 logL: -9.03e+02 KL: 2.68e+01 MMD: 1.62e-01\n",
      "config 71, alpha = 0.0, lambda = 93680.6, dropout = 0.00; 2 hidden layers with 46, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.25e+04 logL: -7.86e+03 KL: 2.92e+00 MMD: 4.97e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 8.75e+03 logL: -4.88e+03 KL: 6.43e+00 MMD: 4.13e-02\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 7.66e+03 logL: -4.03e+03 KL: 7.95e+00 MMD: 3.87e-02\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 6.58e+03 logL: -3.94e+03 KL: 8.72e+00 MMD: 2.81e-02\n",
      "Stopping\n",
      "====> Epoch: 41 VALIDATION Loss: 7.64e+03 logL: -3.95e+03 KL: 8.69e+00 MMD: 3.93e-02\n",
      "config 71, alpha = 0.0, lambda = 345.9, dropout = 0.00; 2 hidden layers with 172, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.43e+03 logL: -2.59e+03 KL: 1.28e+02 MMD: 2.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.39e+03 logL: -1.96e+03 KL: 5.15e+01 MMD: 1.10e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.32e+03 logL: -1.15e+03 KL: 5.01e+01 MMD: 3.37e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.15e+03 logL: -1.01e+03 KL: 4.67e+01 MMD: 2.80e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.07e+03 logL: -9.52e+02 KL: 4.56e+01 MMD: 2.06e-01\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.04e+03 logL: -9.32e+02 KL: 4.53e+01 MMD: 1.79e-01\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.03e+03 logL: -9.26e+02 KL: 4.45e+01 MMD: 1.67e-01\n",
      "Stopping\n",
      "====> Epoch: 73 VALIDATION Loss: 1.04e+03 logL: -9.27e+02 KL: 4.46e+01 MMD: 2.11e-01\n",
      "config 72, alpha = 0.0, lambda = 337.3, dropout = 0.00; 2 hidden layers with 17, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.99e+03 logL: -3.65e+03 KL: 1.04e+02 MMD: 7.06e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.94e+03 logL: -3.63e+03 KL: 5.57e+01 MMD: 7.50e-01\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.94e+03 logL: -3.62e+03 KL: 4.64e+01 MMD: 8.07e-01\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 3.93e+03 logL: -3.62e+03 KL: 4.55e+01 MMD: 7.78e-01\n",
      "config 72, alpha = 0.0, lambda = 1.4, dropout = 0.00; 2 hidden layers with 72, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.72e+03 logL: -3.63e+03 KL: 8.94e+01 MMD: 1.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.50e+03 logL: -2.43e+03 KL: 7.12e+01 MMD: 1.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.91e+03 logL: -1.85e+03 KL: 5.57e+01 MMD: 1.14e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.89e+03 logL: -1.84e+03 KL: 4.50e+01 MMD: 1.04e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.83e+03 logL: -1.79e+03 KL: 3.78e+01 MMD: 1.03e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.72e+03 logL: -1.68e+03 KL: 3.85e+01 MMD: 1.06e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.61e+03 logL: -1.57e+03 KL: 3.71e+01 MMD: 1.06e+00\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.57e+03 logL: -1.54e+03 KL: 3.42e+01 MMD: 1.03e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.56e+03 logL: -1.53e+03 KL: 3.31e+01 MMD: 1.15e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.55e+03 logL: -1.52e+03 KL: 3.13e+01 MMD: 1.02e+00\n",
      "Stopping\n",
      "====> Epoch: 106 VALIDATION Loss: 1.55e+03 logL: -1.52e+03 KL: 3.03e+01 MMD: 1.03e+00\n",
      "config 72, alpha = 0.0, lambda = 48218.6, dropout = 0.00; 2 hidden layers with 71, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.91e+03 logL: -5.06e+03 KL: 5.14e+00 MMD: 3.82e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.81e+03 logL: -4.65e+03 KL: 6.17e+00 MMD: 2.41e-02\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.58e+03 logL: -4.54e+03 KL: 6.76e+00 MMD: 2.14e-02\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 5.71e+03 logL: -4.53e+03 KL: 6.90e+00 MMD: 2.43e-02\n",
      "config 72, alpha = 0.0, lambda = 9.0, dropout = 0.00; 2 hidden layers with 79, 49 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.75e+03 logL: -2.62e+03 KL: 1.09e+02 MMD: 1.75e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.48e+03 logL: -1.38e+03 KL: 8.50e+01 MMD: 1.71e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.08e+03 logL: -9.94e+02 KL: 7.10e+01 MMD: 1.69e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.73e+02 logL: -6.97e+02 KL: 6.43e+01 MMD: 1.50e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.99e+02 logL: -6.29e+02 KL: 5.74e+01 MMD: 1.47e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.79e+02 logL: -6.16e+02 KL: 5.21e+01 MMD: 1.41e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.78e+02 logL: -6.20e+02 KL: 4.70e+01 MMD: 1.36e+00\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 6.57e+02 logL: -5.99e+02 KL: 4.66e+01 MMD: 1.38e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.55e+02 logL: -5.98e+02 KL: 4.58e+01 MMD: 1.41e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.50e+02 logL: -5.96e+02 KL: 4.47e+01 MMD: 1.24e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.47e+02 logL: -5.94e+02 KL: 4.37e+01 MMD: 1.11e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 6.46e+02 logL: -5.93e+02 KL: 4.32e+01 MMD: 1.17e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 6.43e+02 logL: -5.91e+02 KL: 4.20e+01 MMD: 1.21e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 6.41e+02 logL: -5.91e+02 KL: 4.13e+01 MMD: 1.13e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 6.37e+02 logL: -5.88e+02 KL: 4.09e+01 MMD: 1.03e+00\n",
      "Epoch 00158: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 160 VALIDATION Loss: 6.35e+02 logL: -5.86e+02 KL: 4.00e+01 MMD: 1.11e+00\n",
      "Epoch 00165: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 169 VALIDATION Loss: 6.34e+02 logL: -5.86e+02 KL: 3.99e+01 MMD: 1.03e+00\n",
      "config 72, alpha = 0.0, lambda = 5.5, dropout = 0.00; 2 hidden layers with 90, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.01e+03 logL: -3.80e+03 KL: 1.98e+02 MMD: 1.91e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.76e+03 logL: -3.65e+03 KL: 1.00e+02 MMD: 2.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.64e+03 logL: -2.55e+03 KL: 7.51e+01 MMD: 2.02e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.93e+03 logL: -1.86e+03 KL: 6.35e+01 MMD: 2.04e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.39e+03 logL: -1.32e+03 KL: 6.36e+01 MMD: 1.99e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+03 logL: -1.29e+03 KL: 5.39e+01 MMD: 1.94e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.02e+03 logL: -9.53e+02 KL: 5.71e+01 MMD: 1.73e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.97e+02 logL: -9.39e+02 KL: 5.14e+01 MMD: 1.60e+00\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 9.79e+02 logL: -9.23e+02 KL: 4.85e+01 MMD: 1.60e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.76e+02 logL: -9.21e+02 KL: 4.76e+01 MMD: 1.54e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 9.67e+02 logL: -9.13e+02 KL: 4.69e+01 MMD: 1.49e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 8.68e+02 logL: -8.14e+02 KL: 4.74e+01 MMD: 1.40e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 8.04e+02 logL: -7.50e+02 KL: 4.77e+01 MMD: 1.48e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 7.95e+02 logL: -7.41e+02 KL: 4.70e+01 MMD: 1.49e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 7.92e+02 logL: -7.39e+02 KL: 4.61e+01 MMD: 1.51e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 7.90e+02 logL: -7.38e+02 KL: 4.53e+01 MMD: 1.50e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 7.89e+02 logL: -7.37e+02 KL: 4.48e+01 MMD: 1.52e+00\n",
      "Stopping\n",
      "====> Epoch: 175 VALIDATION Loss: 7.90e+02 logL: -7.38e+02 KL: 4.46e+01 MMD: 1.49e+00\n",
      "config 73, alpha = 0.0, lambda = 548.3, dropout = 0.00; 2 hidden layers with 58, 35 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.80e+03 logL: -3.34e+03 KL: 6.61e+01 MMD: 7.14e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 20 VALIDATION Loss: 3.58e+03 logL: -3.16e+03 KL: 4.25e+01 MMD: 6.82e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.31e+03 logL: -3.14e+03 KL: 1.82e+01 MMD: 2.79e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.14e+03 logL: -3.07e+03 KL: 1.65e+01 MMD: 9.83e-02\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.11e+03 logL: -3.05e+03 KL: 1.49e+01 MMD: 6.91e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.08e+03 logL: -3.04e+03 KL: 1.51e+01 MMD: 3.67e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 3.06e+03 logL: -3.03e+03 KL: 1.44e+01 MMD: 3.23e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 3.06e+03 logL: -3.03e+03 KL: 1.36e+01 MMD: 3.61e-02\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.05e+03 logL: -3.02e+03 KL: 1.46e+01 MMD: 2.39e-02\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 3.05e+03 logL: -3.02e+03 KL: 1.46e+01 MMD: 3.32e-02\n",
      "config 73, alpha = 0.0, lambda = 17254.8, dropout = 0.00; 2 hidden layers with 162, 143 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.77e+03 logL: -2.28e+03 KL: 1.84e+01 MMD: 2.74e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 2.64e+03 logL: -2.20e+03 KL: 1.79e+01 MMD: 2.44e-02\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 2.23e+03 logL: -1.87e+03 KL: 2.15e+01 MMD: 1.95e-02\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 36 VALIDATION Loss: 2.14e+03 logL: -1.85e+03 KL: 2.18e+01 MMD: 1.53e-02\n",
      "config 73, alpha = 0.0, lambda = 78.3, dropout = 0.00; 2 hidden layers with 53, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.20e+03 logL: -3.90e+03 KL: 1.76e+02 MMD: 1.59e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.43e+03 logL: -3.19e+03 KL: 1.35e+02 MMD: 1.44e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.25e+03 logL: -2.02e+03 KL: 1.26e+02 MMD: 1.33e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.02e+03 logL: -1.83e+03 KL: 6.37e+01 MMD: 1.54e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.96e+03 logL: -1.81e+03 KL: 4.93e+01 MMD: 1.30e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.92e+03 logL: -1.80e+03 KL: 3.97e+01 MMD: 1.10e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.93e+03 logL: -1.82e+03 KL: 3.46e+01 MMD: 8.95e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.40e+03 logL: -1.30e+03 KL: 3.29e+01 MMD: 7.94e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.34e+03 logL: -1.27e+03 KL: 3.04e+01 MMD: 5.23e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.30e+03 logL: -1.24e+03 KL: 2.93e+01 MMD: 4.35e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 9.82e+02 logL: -9.32e+02 KL: 2.93e+01 MMD: 2.65e-01\n",
      "Epoch 00116: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 9.77e+02 logL: -9.26e+02 KL: 2.87e+01 MMD: 2.90e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 9.71e+02 logL: -9.26e+02 KL: 2.85e+01 MMD: 2.12e-01\n",
      "Epoch 00133: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 137 VALIDATION Loss: 9.71e+02 logL: -9.25e+02 KL: 2.83e+01 MMD: 2.40e-01\n",
      "config 73, alpha = 0.0, lambda = 77969.6, dropout = 0.00; 2 hidden layers with 19, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.51e+04 logL: -1.04e+04 KL: 1.89e+00 MMD: 6.04e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 9.14e+03 logL: -6.74e+03 KL: 2.32e+00 MMD: 3.08e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 8.05e+03 logL: -4.90e+03 KL: 3.86e+00 MMD: 4.04e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 7.49e+03 logL: -4.77e+03 KL: 4.49e+00 MMD: 3.48e-02\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 7.88e+03 logL: -4.72e+03 KL: 4.71e+00 MMD: 4.05e-02\n",
      "Stopping\n",
      "====> Epoch: 51 VALIDATION Loss: 8.90e+03 logL: -4.70e+03 KL: 4.71e+00 MMD: 5.37e-02\n",
      "config 73, alpha = 0.0, lambda = 24.1, dropout = 0.00; 2 hidden layers with 13, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.83e+03 logL: -3.66e+03 KL: 1.19e+02 MMD: 2.58e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.02e+03 logL: -1.87e+03 KL: 9.49e+01 MMD: 2.18e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.95e+03 logL: -1.84e+03 KL: 6.43e+01 MMD: 2.04e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.79e+03 logL: -1.69e+03 KL: 6.51e+01 MMD: 1.83e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.38e+03 logL: -1.29e+03 KL: 5.48e+01 MMD: 1.84e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.37e+03 logL: -1.29e+03 KL: 4.87e+01 MMD: 1.56e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.36e+03 logL: -1.28e+03 KL: 4.41e+01 MMD: 1.44e+00\n",
      "Stopping\n",
      "====> Epoch: 71 VALIDATION Loss: 1.36e+03 logL: -1.29e+03 KL: 4.34e+01 MMD: 1.29e+00\n",
      "config 74, alpha = 0.0, lambda = 16.0, dropout = 0.00; 2 hidden layers with 81, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.13e+03 logL: -5.03e+03 KL: 9.50e+01 MMD: 9.00e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.62e+03 logL: -4.55e+03 KL: 5.47e+01 MMD: 9.67e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 4.06e+03 logL: -3.90e+03 KL: 1.45e+02 MMD: 7.78e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.54e+03 logL: -3.49e+03 KL: 3.79e+01 MMD: 8.37e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.29e+03 logL: -3.25e+03 KL: 3.59e+01 MMD: 7.05e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.21e+03 logL: -3.17e+03 KL: 3.03e+01 MMD: 7.42e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.16e+03 logL: -3.12e+03 KL: 2.52e+01 MMD: 7.77e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.29e+03 logL: -3.24e+03 KL: 3.30e+01 MMD: 8.09e-01\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 84 VALIDATION Loss: 3.28e+03 logL: -3.23e+03 KL: 3.31e+01 MMD: 7.63e-01\n",
      "config 74, alpha = 0.0, lambda = 464.9, dropout = 0.00; 2 hidden layers with 37, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.42e+03 logL: -3.76e+03 KL: 1.20e+02 MMD: 1.17e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.14e+03 logL: -2.56e+03 KL: 8.73e+01 MMD: 1.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.11e+03 logL: -2.54e+03 KL: 6.47e+01 MMD: 1.10e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.98e+03 logL: -2.50e+03 KL: 3.41e+01 MMD: 9.48e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.94e+03 logL: -1.85e+03 KL: 2.40e+01 MMD: 1.34e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.89e+03 logL: -1.82e+03 KL: 2.46e+01 MMD: 8.26e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 1.90e+03 logL: -1.85e+03 KL: 2.37e+01 MMD: 6.54e-02\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.84e+03 logL: -1.80e+03 KL: 2.37e+01 MMD: 3.41e-02\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 89 VALIDATION Loss: 1.84e+03 logL: -1.80e+03 KL: 2.36e+01 MMD: 4.89e-02\n",
      "config 74, alpha = 0.0, lambda = 77.3, dropout = 0.00; 2 hidden layers with 74, 43 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.61e+03 logL: -1.39e+03 KL: 1.18e+02 MMD: 1.35e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.14e+03 logL: -9.66e+02 KL: 6.85e+01 MMD: 1.34e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.10e+03 logL: -9.50e+02 KL: 4.96e+01 MMD: 1.35e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.05e+03 logL: -9.43e+02 KL: 3.75e+01 MMD: 8.94e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.00e+03 logL: -9.40e+02 KL: 3.30e+01 MMD: 3.88e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.92e+02 logL: -9.40e+02 KL: 3.19e+01 MMD: 2.65e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.08e+03 logL: -1.04e+03 KL: 3.02e+01 MMD: 1.71e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.75e+02 logL: -9.34e+02 KL: 3.10e+01 MMD: 1.40e-01\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 9.57e+02 logL: -9.18e+02 KL: 2.97e+01 MMD: 1.21e-01\n",
      "Epoch 00099: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 9.52e+02 logL: -9.17e+02 KL: 2.94e+01 MMD: 7.91e-02\n",
      "Stopping\n",
      "====> Epoch: 103 VALIDATION Loss: 9.54e+02 logL: -9.16e+02 KL: 2.92e+01 MMD: 1.16e-01\n",
      "config 74, alpha = 0.0, lambda = 89.5, dropout = 0.00; 2 hidden layers with 18, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.87e+03 logL: -3.56e+03 KL: 1.40e+02 MMD: 1.82e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.95e+03 logL: -1.70e+03 KL: 1.02e+02 MMD: 1.60e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.50e+03 logL: -1.29e+03 KL: 6.89e+01 MMD: 1.62e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.29e+03 logL: -1.11e+03 KL: 5.45e+01 MMD: 1.46e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.09e+03 logL: -9.54e+02 KL: 4.37e+01 MMD: 1.01e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.26e+02 logL: -8.23e+02 KL: 4.12e+01 MMD: 6.93e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.35e+02 logL: -7.57e+02 KL: 3.93e+01 MMD: 4.36e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.22e+02 logL: -7.56e+02 KL: 3.79e+01 MMD: 3.11e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 90 VALIDATION Loss: 8.24e+02 logL: -7.65e+02 KL: 3.66e+01 MMD: 2.48e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 8.63e+02 logL: -8.10e+02 KL: 3.58e+01 MMD: 1.95e-01\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 105 VALIDATION Loss: 8.23e+02 logL: -7.68e+02 KL: 3.57e+01 MMD: 2.15e-01\n",
      "config 74, alpha = 0.0, lambda = 1.2, dropout = 0.00; 2 hidden layers with 181, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.07e+03 logL: -9.70e+02 KL: 9.82e+01 MMD: 2.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.34e+02 logL: -7.63e+02 KL: 7.04e+01 MMD: 1.90e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.80e+02 logL: -6.15e+02 KL: 6.42e+01 MMD: 1.87e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.62e+02 logL: -6.02e+02 KL: 5.94e+01 MMD: 1.83e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.36e+02 logL: -5.79e+02 KL: 5.65e+01 MMD: 1.81e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.94e+02 logL: -5.39e+02 KL: 5.43e+01 MMD: 1.65e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.46e+02 logL: -4.94e+02 KL: 5.16e+01 MMD: 1.48e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.37e+02 logL: -4.88e+02 KL: 4.94e+01 MMD: 1.41e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.35e+02 logL: -4.89e+02 KL: 4.60e+01 MMD: 1.46e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.62e+02 logL: -4.17e+02 KL: 4.51e+01 MMD: 1.33e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 4.45e+02 logL: -4.00e+02 KL: 4.44e+01 MMD: 1.41e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.52e+02 logL: -4.08e+02 KL: 4.35e+01 MMD: 1.23e+00\n",
      "Epoch 00124: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 4.35e+02 logL: -3.91e+02 KL: 4.30e+01 MMD: 1.38e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 4.34e+02 logL: -3.91e+02 KL: 4.26e+01 MMD: 1.44e+00\n",
      "Stopping\n",
      "====> Epoch: 144 VALIDATION Loss: 4.34e+02 logL: -3.91e+02 KL: 4.23e+01 MMD: 1.36e+00\n",
      "config 75, alpha = 0.0, lambda = 259.4, dropout = 0.00; 2 hidden layers with 24, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.09e+03 logL: -3.71e+03 KL: 1.83e+02 MMD: 7.53e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.90e+03 logL: -3.63e+03 KL: 6.79e+01 MMD: 7.50e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.86e+03 logL: -3.62e+03 KL: 4.25e+01 MMD: 7.65e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.78e+03 logL: -3.61e+03 KL: 2.16e+01 MMD: 5.41e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.59e+03 logL: -3.55e+03 KL: 1.59e+01 MMD: 1.11e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.43e+03 logL: -3.38e+03 KL: 1.54e+01 MMD: 1.39e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.26e+03 logL: -3.22e+03 KL: 1.34e+01 MMD: 9.79e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 3.23e+03 logL: -3.19e+03 KL: 1.25e+01 MMD: 1.15e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.33e+03 logL: -3.29e+03 KL: 1.21e+01 MMD: 1.23e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.19e+03 logL: -3.16e+03 KL: 1.12e+01 MMD: 7.46e-02\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 3.15e+03 logL: -3.13e+03 KL: 1.10e+01 MMD: 6.04e-02\n",
      "Epoch 00112: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 3.15e+03 logL: -3.12e+03 KL: 1.10e+01 MMD: 9.16e-02\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 124 VALIDATION Loss: 3.15e+03 logL: -3.12e+03 KL: 1.10e+01 MMD: 6.34e-02\n",
      "config 75, alpha = 0.0, lambda = 903.1, dropout = 0.00; 2 hidden layers with 35, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.89e+03 logL: -1.89e+03 KL: 1.19e+02 MMD: 9.74e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 2.68e+03 logL: -1.84e+03 KL: 4.26e+01 MMD: 8.74e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.01e+03 logL: -1.91e+03 KL: 2.39e+01 MMD: 7.98e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.90e+03 logL: -1.82e+03 KL: 2.33e+01 MMD: 5.54e-02\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.85e+03 logL: -1.79e+03 KL: 2.25e+01 MMD: 3.87e-02\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.84e+03 logL: -1.79e+03 KL: 2.24e+01 MMD: 3.17e-02\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.87e+03 logL: -1.79e+03 KL: 2.24e+01 MMD: 6.45e-02\n",
      "Stopping\n",
      "====> Epoch: 70 VALIDATION Loss: 1.87e+03 logL: -1.79e+03 KL: 2.24e+01 MMD: 6.45e-02\n",
      "config 75, alpha = 0.0, lambda = 14.3, dropout = 0.00; 2 hidden layers with 136, 53 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.93e+03 logL: -5.45e+03 KL: 4.59e+02 MMD: 1.54e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.19e+03 logL: -4.97e+03 KL: 2.02e+02 MMD: 1.39e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.12e+03 logL: -2.93e+03 KL: 1.73e+02 MMD: 1.27e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.98e+03 logL: -1.86e+03 KL: 1.08e+02 MMD: 1.36e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.44e+03 logL: -1.34e+03 KL: 8.96e+01 MMD: 1.31e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.26e+03 logL: -1.17e+03 KL: 7.26e+01 MMD: 1.32e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.06e+03 logL: -9.79e+02 KL: 6.04e+01 MMD: 1.24e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.03e+03 logL: -9.59e+02 KL: 5.05e+01 MMD: 1.25e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.01e+03 logL: -9.52e+02 KL: 4.36e+01 MMD: 1.16e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.96e+02 logL: -9.42e+02 KL: 3.98e+01 MMD: 1.09e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 9.86e+02 logL: -9.34e+02 KL: 3.67e+01 MMD: 1.12e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 9.75e+02 logL: -9.29e+02 KL: 3.40e+01 MMD: 9.12e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 9.90e+02 logL: -9.46e+02 KL: 3.24e+01 MMD: 9.15e-01\n",
      "Epoch 00134: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 9.50e+02 logL: -9.10e+02 KL: 3.14e+01 MMD: 6.90e-01\n",
      "Epoch 00146: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 150 VALIDATION Loss: 9.49e+02 logL: -9.08e+02 KL: 3.10e+01 MMD: 7.40e-01\n",
      "Epoch 00154: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 158 VALIDATION Loss: 9.49e+02 logL: -9.08e+02 KL: 3.10e+01 MMD: 7.64e-01\n",
      "config 75, alpha = 0.0, lambda = 8.7, dropout = 0.00; 2 hidden layers with 134, 37 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.05e+03 logL: -9.49e+02 KL: 8.54e+01 MMD: 1.58e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.96e+02 logL: -6.22e+02 KL: 6.15e+01 MMD: 1.72e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.68e+02 logL: -6.06e+02 KL: 5.13e+01 MMD: 1.40e+00\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 6.47e+02 logL: -5.89e+02 KL: 4.68e+01 MMD: 1.45e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.45e+02 logL: -5.89e+02 KL: 4.60e+01 MMD: 1.25e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.42e+02 logL: -5.88e+02 KL: 4.47e+01 MMD: 1.25e+00\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 6.40e+02 logL: -5.86e+02 KL: 4.36e+01 MMD: 1.32e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.39e+02 logL: -5.86e+02 KL: 4.35e+01 MMD: 1.32e+00\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 85 VALIDATION Loss: 6.38e+02 logL: -5.85e+02 KL: 4.35e+01 MMD: 1.24e+00\n",
      "config 75, alpha = 0.0, lambda = 3.8, dropout = 0.00; 2 hidden layers with 28, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.44e+03 logL: -1.32e+03 KL: 1.14e+02 MMD: 1.93e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.03e+03 logL: -9.55e+02 KL: 7.27e+01 MMD: 2.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.30e+02 logL: -7.63e+02 KL: 6.23e+01 MMD: 1.94e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.10e+02 logL: -7.51e+02 KL: 5.36e+01 MMD: 1.92e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.88e+02 logL: -7.33e+02 KL: 5.09e+01 MMD: 1.79e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.73e+02 logL: -5.16e+02 KL: 5.14e+01 MMD: 1.83e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.52e+02 logL: -4.99e+02 KL: 4.84e+01 MMD: 1.52e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.47e+02 logL: -4.96e+02 KL: 4.62e+01 MMD: 1.69e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.38e+02 logL: -4.89e+02 KL: 4.47e+01 MMD: 1.52e+00\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 5.33e+02 logL: -4.86e+02 KL: 4.32e+01 MMD: 1.30e+00\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 5.32e+02 logL: -4.85e+02 KL: 4.31e+01 MMD: 1.44e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.32e+02 logL: -4.84e+02 KL: 4.31e+01 MMD: 1.55e+00\n",
      "Stopping\n",
      "====> Epoch: 120 VALIDATION Loss: 5.32e+02 logL: -4.84e+02 KL: 4.31e+01 MMD: 1.55e+00\n",
      "config 76, alpha = 0.0, lambda = 1390.9, dropout = 0.00; 2 hidden layers with 19, 10 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 5.61e+03 logL: -4.38e+03 KL: 2.08e+02 MMD: 7.30e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.72e+03 logL: -3.63e+03 KL: 9.29e+01 MMD: 7.16e-01\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 26 VALIDATION Loss: 4.76e+03 logL: -3.63e+03 KL: 8.32e+01 MMD: 7.58e-01\n",
      "config 76, alpha = 0.0, lambda = 2857.6, dropout = 0.00; 2 hidden layers with 7, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.71e+03 logL: -3.67e+03 KL: 1.99e+02 MMD: 9.94e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.65e+03 logL: -2.56e+03 KL: 1.27e+02 MMD: 1.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.05e+03 logL: -1.91e+03 KL: 8.55e+01 MMD: 1.07e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.27e+03 logL: -2.07e+03 KL: 2.20e+01 MMD: 6.07e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 2.14e+03 logL: -1.93e+03 KL: 2.20e+01 MMD: 6.26e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 2.05e+03 logL: -1.91e+03 KL: 2.13e+01 MMD: 4.12e-02\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.99e+03 logL: -1.86e+03 KL: 2.13e+01 MMD: 3.86e-02\n",
      "Stopping\n",
      "====> Epoch: 71 VALIDATION Loss: 1.96e+03 logL: -1.86e+03 KL: 2.13e+01 MMD: 2.74e-02\n",
      "config 76, alpha = 0.0, lambda = 167.8, dropout = 0.00; 2 hidden layers with 9, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.76e+03 logL: -5.40e+03 KL: 1.06e+02 MMD: 1.48e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.96e+03 logL: -3.64e+03 KL: 6.97e+01 MMD: 1.45e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.88e+03 logL: -2.59e+03 KL: 6.19e+01 MMD: 1.39e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.13e+03 logL: -1.86e+03 KL: 5.19e+01 MMD: 1.32e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.00e+03 logL: -1.85e+03 KL: 3.19e+01 MMD: 7.16e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.93e+03 logL: -1.84e+03 KL: 3.00e+01 MMD: 3.54e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.89e+03 logL: -1.83e+03 KL: 2.88e+01 MMD: 2.01e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.87e+03 logL: -1.82e+03 KL: 2.82e+01 MMD: 1.66e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.68e+01 MMD: 1.39e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.35e+03 logL: -1.30e+03 KL: 2.61e+01 MMD: 1.70e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.35e+03 logL: -1.30e+03 KL: 2.56e+01 MMD: 1.36e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.33e+03 logL: -1.28e+03 KL: 2.53e+01 MMD: 1.34e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 1.32e+03 logL: -1.28e+03 KL: 2.46e+01 MMD: 9.80e-02\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 1.31e+03 logL: -1.27e+03 KL: 2.47e+01 MMD: 1.12e-01\n",
      "Epoch 00142: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 146 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 2.47e+01 MMD: 1.47e-01\n",
      "config 76, alpha = 0.0, lambda = 15.2, dropout = 0.00; 2 hidden layers with 10, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.04e+03 logL: -3.87e+03 KL: 1.38e+02 MMD: 2.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.76e+03 logL: -2.65e+03 KL: 8.73e+01 MMD: 1.92e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.59e+03 logL: -2.50e+03 KL: 5.67e+01 MMD: 2.18e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.91e+03 logL: -1.83e+03 KL: 5.30e+01 MMD: 1.77e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.89e+03 logL: -1.82e+03 KL: 4.42e+01 MMD: 1.77e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.88e+03 logL: -1.82e+03 KL: 3.78e+01 MMD: 1.62e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.87e+03 logL: -1.82e+03 KL: 3.38e+01 MMD: 1.33e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.86e+03 logL: -1.81e+03 KL: 3.18e+01 MMD: 1.11e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.37e+03 logL: -1.31e+03 KL: 3.85e+01 MMD: 1.35e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.33e+03 logL: -1.28e+03 KL: 3.52e+01 MMD: 1.29e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 3.33e+01 MMD: 1.17e+00\n",
      "Epoch 00114: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 3.27e+01 MMD: 9.69e-01\n",
      "Epoch 00121: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 125 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 3.27e+01 MMD: 1.12e+00\n",
      "config 76, alpha = 0.0, lambda = 42.5, dropout = 0.00; 2 hidden layers with 198, 125 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.92e+02 logL: -6.11e+02 KL: 9.82e+01 MMD: 2.00e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.85e+02 logL: -4.47e+02 KL: 6.92e+01 MMD: 1.64e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.48e+02 logL: -4.32e+02 KL: 5.54e+01 MMD: 1.46e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.04e+02 logL: -4.24e+02 KL: 4.68e+01 MMD: 8.13e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 4.86e+02 logL: -4.22e+02 KL: 4.35e+01 MMD: 4.98e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 5.07e+02 logL: -4.48e+02 KL: 4.25e+01 MMD: 3.99e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 5.37e+02 logL: -4.48e+02 KL: 5.22e+01 MMD: 9.01e-01\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 77 VALIDATION Loss: 4.80e+02 logL: -4.11e+02 KL: 4.61e+01 MMD: 5.50e-01\n",
      "config 77, alpha = 0.0, lambda = 27138.5, dropout = 0.00; 2 hidden layers with 13, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.37e+04 logL: -1.24e+04 KL: 3.46e-01 MMD: 4.68e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 6.29e+03 logL: -5.76e+03 KL: 1.96e+00 MMD: 1.92e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 5.07e+03 logL: -4.86e+03 KL: 3.61e+00 MMD: 7.63e-03\n",
      "====> Epoch: 40 VALIDATION Loss: 5.04e+03 logL: -4.67e+03 KL: 4.47e+00 MMD: 1.33e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 4.83e+03 logL: -4.61e+03 KL: 4.97e+00 MMD: 8.07e-03\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.72e+03 logL: -4.59e+03 KL: 5.10e+00 MMD: 4.43e-03\n",
      "Stopping\n",
      "====> Epoch: 63 VALIDATION Loss: 5.01e+03 logL: -4.60e+03 KL: 5.16e+00 MMD: 1.50e-02\n",
      "config 77, alpha = 0.0, lambda = 77.2, dropout = 0.00; 2 hidden layers with 122, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.03e+03 logL: -1.87e+03 KL: 7.64e+01 MMD: 1.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.95e+03 logL: -1.81e+03 KL: 4.69e+01 MMD: 1.15e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.89e+03 logL: -1.78e+03 KL: 3.76e+01 MMD: 9.15e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.82e+03 logL: -1.72e+03 KL: 3.20e+01 MMD: 8.92e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.77e+03 logL: -1.70e+03 KL: 2.70e+01 MMD: 5.75e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.69e+03 logL: -1.64e+03 KL: 2.37e+01 MMD: 3.41e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.66e+03 logL: -1.62e+03 KL: 2.29e+01 MMD: 2.88e-01\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 2.55e+01 MMD: 4.40e-01\n",
      "Stopping\n",
      "====> Epoch: 83 VALIDATION Loss: 1.86e+03 logL: -1.80e+03 KL: 2.54e+01 MMD: 4.28e-01\n",
      "config 77, alpha = 0.0, lambda = 4631.6, dropout = 0.00; 2 hidden layers with 100, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.40e+03 logL: -3.97e+03 KL: 1.96e+01 MMD: 8.82e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 2.85e+03 logL: -2.60e+03 KL: 1.85e+01 MMD: 4.99e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.23e+03 logL: -1.92e+03 KL: 2.19e+01 MMD: 6.19e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.96e+03 logL: -1.57e+03 KL: 2.59e+01 MMD: 7.77e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.67e+03 logL: -1.36e+03 KL: 2.54e+01 MMD: 6.11e-02\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.47e+03 logL: -1.29e+03 KL: 2.68e+01 MMD: 3.28e-02\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.49e+03 logL: -1.28e+03 KL: 2.69e+01 MMD: 3.93e-02\n",
      "Stopping\n",
      "====> Epoch: 73 VALIDATION Loss: 1.43e+03 logL: -1.28e+03 KL: 2.69e+01 MMD: 2.81e-02\n",
      "config 77, alpha = 0.0, lambda = 7.6, dropout = 0.00; 2 hidden layers with 31, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.01e+03 logL: -1.87e+03 KL: 1.20e+02 MMD: 1.72e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.06e+03 logL: -9.71e+02 KL: 7.92e+01 MMD: 1.65e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.02e+03 logL: -9.46e+02 KL: 6.05e+01 MMD: 1.68e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.38e+02 logL: -7.70e+02 KL: 5.76e+01 MMD: 1.59e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.08e+02 logL: -7.47e+02 KL: 5.15e+01 MMD: 1.55e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.77e+02 logL: -6.17e+02 KL: 4.95e+01 MMD: 1.55e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.58e+02 logL: -6.01e+02 KL: 4.71e+01 MMD: 1.43e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.55e+02 logL: -6.02e+02 KL: 4.33e+01 MMD: 1.43e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 90 VALIDATION Loss: 6.51e+02 logL: -6.01e+02 KL: 4.05e+01 MMD: 1.39e+00\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 6.50e+02 logL: -6.00e+02 KL: 4.07e+01 MMD: 1.40e+00\n",
      "config 77, alpha = 0.0, lambda = 15.4, dropout = 0.00; 2 hidden layers with 19, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.94e+03 logL: -3.77e+03 KL: 1.38e+02 MMD: 2.17e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.67e+03 logL: -2.55e+03 KL: 8.78e+01 MMD: 2.15e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.97e+03 logL: -1.87e+03 KL: 7.18e+01 MMD: 2.29e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.92e+03 logL: -1.83e+03 KL: 5.71e+01 MMD: 2.21e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.38e+03 logL: -1.29e+03 KL: 6.24e+01 MMD: 1.97e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.36e+03 logL: -1.28e+03 KL: 5.18e+01 MMD: 1.93e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+03 logL: -1.28e+03 KL: 4.49e+01 MMD: 1.53e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.34e+03 logL: -1.28e+03 KL: 4.04e+01 MMD: 1.35e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.33e+03 logL: -1.28e+03 KL: 3.75e+01 MMD: 1.22e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 3.58e+01 MMD: 1.05e+00\n",
      "Epoch 00106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 3.45e+01 MMD: 9.49e-01\n",
      "Stopping\n",
      "====> Epoch: 118 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 3.42e+01 MMD: 1.03e+00\n",
      "config 78, alpha = 0.0, lambda = 576.5, dropout = 0.00; 2 hidden layers with 72, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.13e+03 logL: -3.63e+03 KL: 8.56e+01 MMD: 7.06e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.88e+03 logL: -3.44e+03 KL: 6.10e+01 MMD: 6.46e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.61e+03 logL: -3.17e+03 KL: 4.55e+01 MMD: 6.93e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.47e+03 logL: -3.13e+03 KL: 2.94e+01 MMD: 5.43e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.31e+03 logL: -3.22e+03 KL: 2.22e+01 MMD: 1.27e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.18e+03 logL: -3.09e+03 KL: 2.26e+01 MMD: 1.07e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.12e+03 logL: -3.06e+03 KL: 1.95e+01 MMD: 7.56e-02\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.06e+03 logL: -3.01e+03 KL: 1.75e+01 MMD: 5.84e-02\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.06e+03 logL: -3.01e+03 KL: 1.68e+01 MMD: 6.63e-02\n",
      "Stopping\n",
      "====> Epoch: 90 VALIDATION Loss: 3.06e+03 logL: -3.01e+03 KL: 1.68e+01 MMD: 6.63e-02\n",
      "config 78, alpha = 0.0, lambda = 46227.0, dropout = 0.00; 2 hidden layers with 19, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.62e+03 logL: -7.31e+03 KL: 3.69e+00 MMD: 2.83e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 6.11e+03 logL: -4.96e+03 KL: 5.79e+00 MMD: 2.46e-02\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.97e+03 logL: -4.59e+03 KL: 5.88e+00 MMD: 2.97e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 5.26e+03 logL: -4.56e+03 KL: 6.36e+00 MMD: 1.50e-02\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 47 VALIDATION Loss: 5.06e+03 logL: -4.56e+03 KL: 6.46e+00 MMD: 1.07e-02\n",
      "config 78, alpha = 0.0, lambda = 5124.4, dropout = 0.00; 2 hidden layers with 96, 79 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.89e+03 logL: -1.45e+03 KL: 3.40e+01 MMD: 8.07e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 1.42e+03 logL: -1.16e+03 KL: 3.19e+01 MMD: 4.42e-02\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.16e+03 logL: -9.76e+02 KL: 3.18e+01 MMD: 2.89e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.14e+03 logL: -9.64e+02 KL: 3.07e+01 MMD: 2.86e-02\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 45 VALIDATION Loss: 1.11e+03 logL: -9.57e+02 KL: 3.09e+01 MMD: 2.45e-02\n",
      "config 78, alpha = 0.0, lambda = 16.4, dropout = 0.00; 2 hidden layers with 78, 46 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.07e+03 logL: -9.62e+02 KL: 8.68e+01 MMD: 1.67e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.07e+02 logL: -6.18e+02 KL: 6.33e+01 MMD: 1.63e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.83e+02 logL: -6.06e+02 KL: 5.27e+01 MMD: 1.55e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.73e+02 logL: -6.05e+02 KL: 4.76e+01 MMD: 1.34e+00\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 6.53e+02 logL: -5.91e+02 KL: 4.51e+01 MMD: 1.11e+00\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 6.52e+02 logL: -5.90e+02 KL: 4.41e+01 MMD: 1.13e+00\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: 6.53e+02 logL: -5.90e+02 KL: 4.40e+01 MMD: 1.24e+00\n",
      "config 78, alpha = 0.0, lambda = 3832.5, dropout = 0.00; 2 hidden layers with 41, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.04e+03 logL: -4.34e+03 KL: 3.79e+01 MMD: 1.73e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 2.60e+03 logL: -2.21e+03 KL: 3.13e+01 MMD: 9.54e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.35e+03 logL: -1.88e+03 KL: 3.10e+01 MMD: 1.14e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.97e+03 logL: -1.61e+03 KL: 3.06e+01 MMD: 8.64e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.51e+03 logL: -1.14e+03 KL: 3.12e+01 MMD: 9.06e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.34e+03 logL: -1.04e+03 KL: 3.09e+01 MMD: 7.21e-02\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.17e+03 logL: -9.65e+02 KL: 3.03e+01 MMD: 4.54e-02\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.23e+03 logL: -9.55e+02 KL: 3.04e+01 MMD: 6.48e-02\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 87 VALIDATION Loss: 1.23e+03 logL: -9.54e+02 KL: 3.04e+01 MMD: 6.50e-02\n",
      "config 79, alpha = 0.0, lambda = 72074.6, dropout = 0.00; 2 hidden layers with 24, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.99e+03 logL: -5.96e+03 KL: 5.09e+00 MMD: 1.42e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.99e+03 logL: -4.95e+03 KL: 4.90e+00 MMD: 1.43e-02\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.60e+03 logL: -4.88e+03 KL: 5.29e+00 MMD: 9.91e-03\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 5.16e+03 logL: -4.85e+03 KL: 5.50e+00 MMD: 4.16e-03\n",
      "====> Epoch: 50 VALIDATION Loss: 5.50e+03 logL: -4.82e+03 KL: 5.54e+00 MMD: 9.36e-03\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 56 VALIDATION Loss: 5.40e+03 logL: -4.84e+03 KL: 5.52e+00 MMD: 7.82e-03\n",
      "config 79, alpha = 0.0, lambda = 127.7, dropout = 0.00; 2 hidden layers with 27, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.79e+03 logL: -2.54e+03 KL: 1.13e+02 MMD: 1.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.04e+03 logL: -1.85e+03 KL: 7.03e+01 MMD: 9.67e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.01e+03 logL: -1.83e+03 KL: 4.76e+01 MMD: 1.07e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.97e+03 logL: -1.82e+03 KL: 3.48e+01 MMD: 8.97e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.91e+03 logL: -1.82e+03 KL: 2.55e+01 MMD: 5.46e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.25e+01 MMD: 2.39e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.84e+03 logL: -1.80e+03 KL: 2.09e+01 MMD: 1.55e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.83e+03 logL: -1.79e+03 KL: 1.99e+01 MMD: 1.24e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.81e+03 logL: -1.78e+03 KL: 1.96e+01 MMD: 9.14e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 1.82e+03 logL: -1.79e+03 KL: 1.93e+01 MMD: 7.88e-02\n",
      "====> Epoch: 110 VALIDATION Loss: 1.78e+03 logL: -1.75e+03 KL: 1.91e+01 MMD: 1.02e-01\n",
      "Epoch 00114: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.77e+03 logL: -1.74e+03 KL: 1.91e+01 MMD: 1.04e-01\n",
      "Stopping\n",
      "====> Epoch: 125 VALIDATION Loss: 1.76e+03 logL: -1.74e+03 KL: 1.92e+01 MMD: 5.70e-02\n",
      "config 79, alpha = 0.0, lambda = 730.0, dropout = 0.00; 2 hidden layers with 10, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.64e+03 logL: -5.43e+03 KL: 1.89e+02 MMD: 1.41e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.12e+03 logL: -3.00e+03 KL: 1.35e+02 MMD: 1.35e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.67e+03 logL: -2.57e+03 KL: 8.93e+01 MMD: 1.39e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.90e+03 logL: -2.68e+03 KL: 2.71e+01 MMD: 2.66e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.18e+03 logL: -2.07e+03 KL: 2.70e+01 MMD: 1.11e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 60 VALIDATION Loss: 2.07e+03 logL: -1.98e+03 KL: 2.50e+01 MMD: 9.54e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 2.00e+03 logL: -1.94e+03 KL: 2.17e+01 MMD: 5.49e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 1.95e+03 logL: -1.89e+03 KL: 2.05e+01 MMD: 5.11e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 1.89e+03 logL: -1.84e+03 KL: 1.94e+01 MMD: 3.49e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 1.88e+03 logL: -1.83e+03 KL: 1.86e+01 MMD: 4.15e-02\n",
      "====> Epoch: 110 VALIDATION Loss: 1.86e+03 logL: -1.82e+03 KL: 1.80e+01 MMD: 3.42e-02\n",
      "Epoch 00119: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.85e+03 logL: -1.81e+03 KL: 1.77e+01 MMD: 3.73e-02\n",
      "Epoch 00127: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.86e+03 logL: -1.80e+03 KL: 1.76e+01 MMD: 4.89e-02\n",
      "Stopping\n",
      "====> Epoch: 131 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 1.76e+01 MMD: 3.79e-02\n",
      "config 79, alpha = 0.0, lambda = 2732.6, dropout = 0.00; 2 hidden layers with 54, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.69e+03 logL: -4.13e+03 KL: 2.23e+01 MMD: 1.96e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.10e+03 logL: -3.68e+03 KL: 2.70e+01 MMD: 1.42e-01\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.98e+03 logL: -3.64e+03 KL: 2.66e+01 MMD: 1.16e-01\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 3.97e+03 logL: -3.64e+03 KL: 2.65e+01 MMD: 1.14e-01\n",
      "Stopping\n",
      "====> Epoch: 43 VALIDATION Loss: 3.95e+03 logL: -3.63e+03 KL: 2.65e+01 MMD: 1.07e-01\n",
      "config 79, alpha = 0.0, lambda = 211.2, dropout = 0.00; 2 hidden layers with 16, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.07e+03 logL: -2.54e+03 KL: 1.20e+02 MMD: 1.96e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.80e+03 logL: -1.35e+03 KL: 8.16e+01 MMD: 1.73e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.19e+03 logL: -9.86e+02 KL: 4.95e+01 MMD: 7.21e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.10e+03 logL: -9.73e+02 KL: 4.80e+01 MMD: 3.85e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 8.77e+02 logL: -7.79e+02 KL: 4.76e+01 MMD: 2.38e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 8.61e+02 logL: -7.60e+02 KL: 4.71e+01 MMD: 2.57e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.41e+02 logL: -7.54e+02 KL: 4.60e+01 MMD: 1.95e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 7.13e+02 logL: -6.30e+02 KL: 4.39e+01 MMD: 1.85e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 6.81e+02 logL: -6.07e+02 KL: 4.36e+01 MMD: 1.42e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 6.69e+02 logL: -6.04e+02 KL: 4.34e+01 MMD: 1.00e-01\n",
      "Epoch 00106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 6.67e+02 logL: -5.95e+02 KL: 4.25e+01 MMD: 1.41e-01\n",
      "Epoch 00115: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 119 VALIDATION Loss: 6.62e+02 logL: -5.93e+02 KL: 4.24e+01 MMD: 1.25e-01\n",
      "config 80, alpha = 0.0, lambda = 551.3, dropout = 0.00; 2 hidden layers with 48, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.11e+03 logL: -3.63e+03 KL: 8.55e+01 MMD: 7.13e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.09e+03 logL: -3.61e+03 KL: 5.40e+01 MMD: 7.68e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.95e+03 logL: -3.47e+03 KL: 5.01e+01 MMD: 7.72e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.71e+03 logL: -3.26e+03 KL: 4.12e+01 MMD: 7.41e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.63e+03 logL: -3.20e+03 KL: 3.19e+01 MMD: 7.29e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.39e+03 logL: -3.17e+03 KL: 2.18e+01 MMD: 3.49e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.27e+03 logL: -3.17e+03 KL: 1.80e+01 MMD: 1.61e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.30e+03 logL: -3.21e+03 KL: 1.93e+01 MMD: 1.13e-01\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.19e+03 logL: -3.09e+03 KL: 1.62e+01 MMD: 1.58e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.16e+03 logL: -3.07e+03 KL: 1.55e+01 MMD: 1.28e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 3.15e+03 logL: -3.07e+03 KL: 1.47e+01 MMD: 1.18e-01\n",
      "Epoch 00111: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 115 VALIDATION Loss: 3.16e+03 logL: -3.06e+03 KL: 1.45e+01 MMD: 1.42e-01\n",
      "config 80, alpha = 0.0, lambda = 49.8, dropout = 0.00; 2 hidden layers with 118, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.81e+03 logL: -2.63e+03 KL: 1.29e+02 MMD: 1.10e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.62e+03 logL: -2.50e+03 KL: 5.99e+01 MMD: 1.14e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.59e+03 logL: -2.49e+03 KL: 4.56e+01 MMD: 1.10e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.52e+03 logL: -2.43e+03 KL: 3.89e+01 MMD: 1.09e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.91e+03 logL: -1.81e+03 KL: 4.32e+01 MMD: 1.14e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.89e+03 logL: -1.80e+03 KL: 3.66e+01 MMD: 1.07e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.86e+03 logL: -1.78e+03 KL: 3.21e+01 MMD: 9.66e-01\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.84e+03 logL: -1.76e+03 KL: 3.12e+01 MMD: 8.86e-01\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.84e+03 logL: -1.76e+03 KL: 3.07e+01 MMD: 1.01e+00\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 98 VALIDATION Loss: 1.84e+03 logL: -1.76e+03 KL: 3.07e+01 MMD: 9.96e-01\n",
      "config 80, alpha = 0.0, lambda = 2.8, dropout = 0.00; 2 hidden layers with 40, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.63e+03 logL: -2.54e+03 KL: 8.27e+01 MMD: 1.48e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.90e+03 logL: -1.83e+03 KL: 6.30e+01 MMD: 1.36e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 4.84e+01 MMD: 1.56e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.32e+03 logL: -1.27e+03 KL: 4.56e+01 MMD: 1.36e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.30e+03 logL: -1.26e+03 KL: 4.08e+01 MMD: 1.24e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.30e+03 logL: -1.26e+03 KL: 3.75e+01 MMD: 1.12e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.31e+03 logL: -1.27e+03 KL: 3.49e+01 MMD: 1.12e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.28e+03 logL: -1.24e+03 KL: 3.38e+01 MMD: 1.16e+00\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.27e+03 logL: -1.24e+03 KL: 3.29e+01 MMD: 1.01e+00\n",
      "Stopping\n",
      "====> Epoch: 97 VALIDATION Loss: 1.27e+03 logL: -1.24e+03 KL: 3.26e+01 MMD: 1.07e+00\n",
      "config 80, alpha = 0.0, lambda = 2222.0, dropout = 0.00; 2 hidden layers with 10, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.56e+03 logL: -5.82e+03 KL: 1.02e+02 MMD: 1.19e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.10e+03 logL: -4.92e+03 KL: 1.84e+01 MMD: 7.47e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.54e+03 logL: -3.31e+03 KL: 2.03e+01 MMD: 9.26e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.78e+03 logL: -2.60e+03 KL: 1.70e+01 MMD: 7.05e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 2.67e+03 logL: -2.55e+03 KL: 1.68e+01 MMD: 4.78e-02\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.63e+03 logL: -2.54e+03 KL: 1.62e+01 MMD: 3.57e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 2.64e+03 logL: -2.53e+03 KL: 1.65e+01 MMD: 4.23e-02\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 74 VALIDATION Loss: 2.65e+03 logL: -2.53e+03 KL: 1.64e+01 MMD: 4.55e-02\n",
      "config 80, alpha = 0.0, lambda = 80.2, dropout = 0.00; 2 hidden layers with 21, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.78e+03 logL: -2.48e+03 KL: 1.35e+02 MMD: 2.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.54e+03 logL: -1.29e+03 KL: 9.49e+01 MMD: 2.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.20e+03 logL: -9.64e+02 KL: 7.43e+01 MMD: 1.98e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.24e+03 logL: -1.06e+03 KL: 5.56e+01 MMD: 1.61e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.08e+03 logL: -9.55e+02 KL: 4.68e+01 MMD: 9.76e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.06e+03 logL: -9.67e+02 KL: 4.45e+01 MMD: 6.62e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.01e+02 logL: -8.09e+02 KL: 4.47e+01 MMD: 5.92e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.30e+02 logL: -7.58e+02 KL: 4.25e+01 MMD: 3.77e-01\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 8.16e+02 logL: -7.46e+02 KL: 4.22e+01 MMD: 3.50e-01\n",
      "Epoch 00095: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 99 VALIDATION Loss: 8.16e+02 logL: -7.45e+02 KL: 4.16e+01 MMD: 3.69e-01\n",
      "config 81, alpha = 0.0, lambda = 247.2, dropout = 0.00; 2 hidden layers with 13, 4 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 5.87e+03 logL: -5.59e+03 KL: 8.47e+01 MMD: 8.08e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.59e+03 logL: -5.32e+03 KL: 6.36e+01 MMD: 8.24e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.35e+03 logL: -5.11e+03 KL: 4.56e+01 MMD: 7.80e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 4.04e+03 logL: -3.78e+03 KL: 7.84e+01 MMD: 7.40e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.87e+03 logL: -3.63e+03 KL: 4.72e+01 MMD: 7.71e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.85e+03 logL: -3.62e+03 KL: 3.59e+01 MMD: 8.04e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.85e+03 logL: -3.64e+03 KL: 2.84e+01 MMD: 7.27e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.77e+03 logL: -3.61e+03 KL: 1.77e+01 MMD: 5.79e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.68e+03 logL: -3.62e+03 KL: 1.18e+01 MMD: 2.02e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.06e+04 logL: -1.04e+04 KL: 2.36e+01 MMD: 6.53e-01\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 104 VALIDATION Loss: 1.04e+04 logL: -1.03e+04 KL: 2.36e+01 MMD: 5.72e-01\n",
      "config 81, alpha = 0.0, lambda = 21.7, dropout = 0.00; 2 hidden layers with 97, 74 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.88e+03 logL: -1.81e+03 KL: 4.97e+01 MMD: 1.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.77e+03 logL: -1.72e+03 KL: 3.57e+01 MMD: 1.08e+00\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.64e+03 logL: -1.59e+03 KL: 2.93e+01 MMD: 8.38e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.61e+03 logL: -1.57e+03 KL: 2.85e+01 MMD: 9.07e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.62e+03 logL: -1.57e+03 KL: 2.73e+01 MMD: 8.93e-01\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 54 VALIDATION Loss: 1.60e+03 logL: -1.56e+03 KL: 2.70e+01 MMD: 8.69e-01\n",
      "config 81, alpha = 0.0, lambda = 1153.6, dropout = 0.00; 2 hidden layers with 120, 45 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.64e+03 logL: -1.09e+03 KL: 9.12e+01 MMD: 1.26e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.26e+03 logL: -1.14e+03 KL: 3.36e+01 MMD: 7.85e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 1.10e+03 logL: -9.86e+02 KL: 3.23e+01 MMD: 7.11e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.08e+03 logL: -9.68e+02 KL: 3.14e+01 MMD: 6.78e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.03e+03 logL: -9.62e+02 KL: 3.07e+01 MMD: 3.63e-02\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.02e+03 logL: -9.21e+02 KL: 2.96e+01 MMD: 5.88e-02\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.96e+02 logL: -9.18e+02 KL: 2.97e+01 MMD: 4.23e-02\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.82e+02 logL: -9.17e+02 KL: 2.96e+01 MMD: 3.09e-02\n",
      "Stopping\n",
      "====> Epoch: 83 VALIDATION Loss: 9.89e+02 logL: -9.17e+02 KL: 2.96e+01 MMD: 3.68e-02\n",
      "config 81, alpha = 0.0, lambda = 13.6, dropout = 0.00; 2 hidden layers with 155, 38 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.10e+03 logL: -9.81e+02 KL: 9.79e+01 MMD: 1.54e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.96e+02 logL: -6.10e+02 KL: 6.69e+01 MMD: 1.52e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.88e+02 logL: -6.12e+02 KL: 5.49e+01 MMD: 1.64e+00\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 6.60e+02 logL: -5.90e+02 KL: 5.21e+01 MMD: 1.40e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.57e+02 logL: -5.90e+02 KL: 5.00e+01 MMD: 1.36e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.53e+02 logL: -5.89e+02 KL: 4.75e+01 MMD: 1.36e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.51e+02 logL: -5.88e+02 KL: 4.57e+01 MMD: 1.40e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.49e+02 logL: -5.89e+02 KL: 4.48e+01 MMD: 1.25e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.46e+02 logL: -5.89e+02 KL: 4.31e+01 MMD: 1.09e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.43e+02 logL: -5.87e+02 KL: 4.30e+01 MMD: 1.04e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.43e+02 logL: -5.88e+02 KL: 4.21e+01 MMD: 1.04e+00\n",
      "Epoch 00114: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 6.37e+02 logL: -5.84e+02 KL: 4.12e+01 MMD: 9.66e-01\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 130 VALIDATION Loss: 6.38e+02 logL: -5.84e+02 KL: 4.11e+01 MMD: 1.05e+00\n",
      "Stopping\n",
      "====> Epoch: 130 VALIDATION Loss: 6.38e+02 logL: -5.84e+02 KL: 4.11e+01 MMD: 1.05e+00\n",
      "config 81, alpha = 0.0, lambda = 257.7, dropout = 0.00; 2 hidden layers with 172, 124 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.72e+03 logL: -1.10e+03 KL: 1.46e+02 MMD: 1.85e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.27e+03 logL: -6.94e+02 KL: 1.03e+02 MMD: 1.85e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.34e+02 logL: -6.09e+02 KL: 5.65e+01 MMD: 1.05e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.14e+02 logL: -5.08e+02 KL: 5.01e+01 MMD: 2.18e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 5.68e+02 logL: -4.78e+02 KL: 4.88e+01 MMD: 1.61e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 5.34e+02 logL: -4.48e+02 KL: 4.77e+01 MMD: 1.51e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 5.14e+02 logL: -4.46e+02 KL: 4.73e+01 MMD: 8.04e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 1.26e+03 logL: -6.47e+02 KL: 1.11e+02 MMD: 1.95e+00\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 86 VALIDATION Loss: 5.80e+02 logL: -4.61e+02 KL: 5.15e+01 MMD: 2.62e-01\n",
      "config 82, alpha = 0.0, lambda = 229.7, dropout = 0.00; 2 hidden layers with 13, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.66e+03 logL: -5.35e+03 KL: 1.02e+02 MMD: 9.01e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.88e+03 logL: -3.64e+03 KL: 5.99e+01 MMD: 7.69e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.84e+03 logL: -3.62e+03 KL: 4.00e+01 MMD: 7.78e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.81e+03 logL: -3.61e+03 KL: 2.77e+01 MMD: 7.48e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.71e+03 logL: -3.60e+03 KL: 1.51e+01 MMD: 4.01e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.69e+03 logL: -3.62e+03 KL: 1.10e+01 MMD: 2.43e-01\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: 3.56e+03 logL: -3.50e+03 KL: 1.24e+01 MMD: 2.13e-01\n",
      "config 82, alpha = 0.0, lambda = 1330.4, dropout = 0.00; 2 hidden layers with 50, 27 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.37e+03 logL: -1.85e+03 KL: 1.03e+02 MMD: 1.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.08e+03 logL: -1.90e+03 KL: 4.20e+01 MMD: 8.57e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.04e+03 logL: -1.91e+03 KL: 2.28e+01 MMD: 8.31e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.92e+03 logL: -1.85e+03 KL: 2.16e+01 MMD: 3.67e-02\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.90e+03 logL: -1.80e+03 KL: 2.15e+01 MMD: 6.21e-02\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.88e+03 logL: -1.80e+03 KL: 2.15e+01 MMD: 4.34e-02\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.88e+03 logL: -1.80e+03 KL: 2.15e+01 MMD: 4.34e-02\n",
      "Stopping\n",
      "====> Epoch: 71 VALIDATION Loss: 1.88e+03 logL: -1.80e+03 KL: 2.15e+01 MMD: 4.57e-02\n",
      "config 82, alpha = 0.0, lambda = 1408.5, dropout = 0.00; 2 hidden layers with 55, 21 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.99e+03 logL: -2.98e+03 KL: 1.51e+02 MMD: 1.32e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.19e+03 logL: -1.29e+03 KL: 8.92e+01 MMD: 1.28e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.34e+03 logL: -1.15e+03 KL: 3.25e+01 MMD: 1.12e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.20e+03 logL: -1.05e+03 KL: 3.17e+01 MMD: 8.36e-02\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.04e+03 logL: -9.69e+02 KL: 3.21e+01 MMD: 2.89e-02\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.04e+03 logL: -9.56e+02 KL: 3.22e+01 MMD: 3.49e-02\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: 1.04e+03 logL: -9.55e+02 KL: 3.22e+01 MMD: 4.02e-02\n",
      "config 82, alpha = 0.0, lambda = 17.8, dropout = 0.00; 2 hidden layers with 12, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.76e+03 logL: -5.48e+03 KL: 2.44e+02 MMD: 1.87e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.90e+03 logL: -3.76e+03 KL: 1.14e+02 MMD: 1.86e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.74e+03 logL: -2.63e+03 KL: 8.67e+01 MMD: 1.65e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 2.17e+03 logL: -2.07e+03 KL: 7.60e+01 MMD: 1.66e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.09e+03 logL: -2.01e+03 KL: 5.77e+01 MMD: 1.59e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.40e+03 logL: -1.31e+03 KL: 6.33e+01 MMD: 1.61e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.36e+03 logL: -1.29e+03 KL: 5.00e+01 MMD: 1.41e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.35e+03 logL: -1.28e+03 KL: 4.26e+01 MMD: 1.54e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.35e+03 logL: -1.29e+03 KL: 3.91e+01 MMD: 1.52e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.33e+03 logL: -1.27e+03 KL: 3.63e+01 MMD: 1.30e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.33e+03 logL: -1.27e+03 KL: 3.48e+01 MMD: 1.16e+00\n",
      "Epoch 00118: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 3.32e+01 MMD: 9.82e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 3.30e+01 MMD: 1.01e+00\n",
      "Epoch 00134: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 135 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 3.28e+01 MMD: 1.04e+00\n",
      "config 82, alpha = 0.0, lambda = 50051.7, dropout = 0.00; 2 hidden layers with 26, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.90e+03 logL: -5.43e+03 KL: 4.42e+00 MMD: 6.94e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 7.50e+03 logL: -4.83e+03 KL: 5.31e+00 MMD: 5.33e-02\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 29 VALIDATION Loss: 6.84e+03 logL: -4.60e+03 KL: 5.69e+00 MMD: 4.47e-02\n",
      "config 83, alpha = 0.0, lambda = 17625.6, dropout = 0.00; 2 hidden layers with 13, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.38e+03 logL: -5.70e+03 KL: 8.22e+00 MMD: 3.81e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.25e+03 logL: -4.84e+03 KL: 1.02e+01 MMD: 2.27e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.37e+03 logL: -4.08e+03 KL: 1.03e+01 MMD: 1.59e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 4.14e+03 logL: -3.76e+03 KL: 1.06e+01 MMD: 2.13e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.92e+03 logL: -3.72e+03 KL: 1.10e+01 MMD: 1.05e-02\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.82e+03 logL: -3.67e+03 KL: 1.11e+01 MMD: 7.64e-03\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 65 VALIDATION Loss: 3.86e+03 logL: -3.66e+03 KL: 1.11e+01 MMD: 1.04e-02\n",
      "config 83, alpha = 0.0, lambda = 11.8, dropout = 0.00; 2 hidden layers with 21, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.85e+03 logL: -3.69e+03 KL: 1.44e+02 MMD: 1.12e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.01e+03 logL: -2.89e+03 KL: 1.08e+02 MMD: 1.11e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.63e+03 logL: -2.55e+03 KL: 6.55e+01 MMD: 1.23e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.61e+03 logL: -2.54e+03 KL: 4.99e+01 MMD: 1.23e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.59e+03 logL: -2.53e+03 KL: 4.17e+01 MMD: 1.23e+00\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.56e+03 logL: -2.51e+03 KL: 3.70e+01 MMD: 1.19e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.56e+03 logL: -2.51e+03 KL: 3.61e+01 MMD: 1.36e+00\n",
      "Stopping\n",
      "====> Epoch: 70 VALIDATION Loss: 2.56e+03 logL: -2.51e+03 KL: 3.61e+01 MMD: 1.36e+00\n",
      "config 83, alpha = 0.0, lambda = 6634.5, dropout = 0.00; 2 hidden layers with 19, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.25e+04 logL: -6.64e+03 KL: 8.42e+01 MMD: 8.71e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.74e+03 logL: -5.21e+03 KL: 2.26e+01 MMD: 7.71e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 5.11e+03 logL: -4.45e+03 KL: 2.14e+01 MMD: 9.66e-02\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 4.74e+03 logL: -4.29e+03 KL: 1.22e+01 MMD: 6.61e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 4.71e+03 logL: -4.26e+03 KL: 1.14e+01 MMD: 6.67e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 4.59e+03 logL: -4.20e+03 KL: 1.12e+01 MMD: 5.64e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 4.41e+03 logL: -4.13e+03 KL: 1.02e+01 MMD: 4.04e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 4.28e+03 logL: -4.02e+03 KL: 1.05e+01 MMD: 3.79e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 4.14e+03 logL: -3.90e+03 KL: 1.13e+01 MMD: 3.39e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 4.13e+03 logL: -3.80e+03 KL: 1.15e+01 MMD: 4.79e-02\n",
      "====> Epoch: 110 VALIDATION Loss: 4.15e+03 logL: -3.76e+03 KL: 1.17e+01 MMD: 5.78e-02\n",
      "====> Epoch: 120 VALIDATION Loss: 3.94e+03 logL: -3.68e+03 KL: 1.19e+01 MMD: 3.68e-02\n",
      "Epoch 00123: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 3.95e+03 logL: -3.66e+03 KL: 1.20e+01 MMD: 4.07e-02\n",
      "Epoch 00134: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 138 VALIDATION Loss: 3.87e+03 logL: -3.66e+03 KL: 1.21e+01 MMD: 2.97e-02\n",
      "config 83, alpha = 0.0, lambda = 10043.0, dropout = 0.00; 2 hidden layers with 20, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.27e+03 logL: -5.53e+03 KL: 2.97e+01 MMD: 7.02e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.04e+03 logL: -3.22e+03 KL: 3.01e+01 MMD: 7.86e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.95e+03 logL: -1.94e+03 KL: 3.22e+01 MMD: 9.78e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.28e+03 logL: -1.51e+03 KL: 3.08e+01 MMD: 7.30e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.90e+03 logL: -1.28e+03 KL: 2.93e+01 MMD: 5.95e-02\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.56e+03 logL: -1.09e+03 KL: 3.08e+01 MMD: 4.36e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 1.71e+03 logL: -1.06e+03 KL: 3.13e+01 MMD: 6.07e-02\n",
      "Epoch 00071: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.53e+03 logL: -1.05e+03 KL: 3.11e+01 MMD: 4.48e-02\n",
      "Stopping\n",
      "====> Epoch: 82 VALIDATION Loss: 1.46e+03 logL: -1.05e+03 KL: 3.11e+01 MMD: 3.81e-02\n",
      "config 83, alpha = 0.0, lambda = 1902.6, dropout = 0.00; 2 hidden layers with 46, 40 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.97e+03 logL: -4.20e+03 KL: 2.15e+02 MMD: 1.87e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.14e+03 logL: -2.55e+03 KL: 4.95e+01 MMD: 2.86e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.98e+03 logL: -1.67e+03 KL: 4.94e+01 MMD: 1.35e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+03 logL: -1.08e+03 KL: 4.84e+01 MMD: 1.19e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.11e+03 logL: -8.76e+02 KL: 4.64e+01 MMD: 9.66e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.01e+03 logL: -7.03e+02 KL: 4.65e+01 MMD: 1.39e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.21e+02 logL: -6.71e+02 KL: 4.46e+01 MMD: 1.08e-01\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 7.12e+02 logL: -5.58e+02 KL: 4.41e+01 MMD: 5.79e-02\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 7.10e+02 logL: -5.50e+02 KL: 4.41e+01 MMD: 6.11e-02\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 96 VALIDATION Loss: 7.14e+02 logL: -5.49e+02 KL: 4.42e+01 MMD: 6.37e-02\n",
      "config 84, alpha = 0.0, lambda = 165.6, dropout = 0.00; 2 hidden layers with 9, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.64e+03 logL: -5.40e+03 KL: 1.20e+02 MMD: 7.36e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.41e+03 logL: -5.22e+03 KL: 5.74e+01 MMD: 7.98e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 4.82e+03 logL: -4.68e+03 KL: 3.16e+01 MMD: 7.14e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 4.71e+03 logL: -4.58e+03 KL: 2.23e+01 MMD: 6.33e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 4.64e+03 logL: -4.53e+03 KL: 1.53e+01 MMD: 5.57e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.81e+03 logL: -3.67e+03 KL: 2.59e+01 MMD: 7.03e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.75e+03 logL: -3.65e+03 KL: 1.58e+01 MMD: 5.42e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.66e+03 logL: -3.63e+03 KL: 1.22e+01 MMD: 1.28e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.63e+03 logL: -3.61e+03 KL: 1.17e+01 MMD: 3.20e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 3.59e+03 logL: -3.58e+03 KL: 1.16e+01 MMD: 2.09e-02\n",
      "====> Epoch: 110 VALIDATION Loss: 3.48e+03 logL: -3.46e+03 KL: 1.19e+01 MMD: 3.15e-02\n",
      "====> Epoch: 120 VALIDATION Loss: 3.40e+03 logL: -3.38e+03 KL: 1.27e+01 MMD: 2.33e-02\n",
      "====> Epoch: 130 VALIDATION Loss: 3.34e+03 logL: -3.30e+03 KL: 2.20e+01 MMD: 7.52e-02\n",
      "====> Epoch: 140 VALIDATION Loss: 3.25e+03 logL: -3.22e+03 KL: 1.84e+01 MMD: 7.21e-02\n",
      "====> Epoch: 150 VALIDATION Loss: 3.19e+03 logL: -3.17e+03 KL: 1.54e+01 MMD: 7.80e-02\n",
      "====> Epoch: 160 VALIDATION Loss: 3.17e+03 logL: -3.15e+03 KL: 1.32e+01 MMD: 4.77e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00169: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 170 VALIDATION Loss: 3.14e+03 logL: -3.12e+03 KL: 1.30e+01 MMD: 5.25e-02\n",
      "Stopping\n",
      "====> Epoch: 173 VALIDATION Loss: 3.13e+03 logL: -3.11e+03 KL: 1.29e+01 MMD: 4.19e-02\n",
      "config 84, alpha = 0.0, lambda = 29.7, dropout = 0.00; 2 hidden layers with 105, 38 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.99e+03 logL: -1.88e+03 KL: 8.00e+01 MMD: 1.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.93e+03 logL: -1.85e+03 KL: 4.76e+01 MMD: 1.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.86e+03 logL: -1.79e+03 KL: 3.80e+01 MMD: 9.65e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.82e+03 logL: -1.76e+03 KL: 3.42e+01 MMD: 9.78e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.68e+03 logL: -1.62e+03 KL: 3.17e+01 MMD: 8.44e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.64e+03 logL: -1.59e+03 KL: 2.91e+01 MMD: 6.62e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.67e+03 logL: -1.62e+03 KL: 2.61e+01 MMD: 6.78e-01\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.56e+03 logL: -1.52e+03 KL: 2.46e+01 MMD: 5.09e-01\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.55e+03 logL: -1.51e+03 KL: 2.42e+01 MMD: 5.18e-01\n",
      "Stopping\n",
      "====> Epoch: 92 VALIDATION Loss: 1.55e+03 logL: -1.51e+03 KL: 2.42e+01 MMD: 6.07e-01\n",
      "config 84, alpha = 0.0, lambda = 22.8, dropout = 0.00; 2 hidden layers with 149, 75 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.49e+03 logL: -1.36e+03 KL: 1.05e+02 MMD: 1.36e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.09e+03 logL: -9.95e+02 KL: 6.73e+01 MMD: 1.21e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.03e+03 logL: -9.50e+02 KL: 4.99e+01 MMD: 1.36e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.02e+03 logL: -9.49e+02 KL: 4.24e+01 MMD: 1.19e+00\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 9.85e+02 logL: -9.19e+02 KL: 4.11e+01 MMD: 1.14e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.83e+02 logL: -9.19e+02 KL: 3.98e+01 MMD: 1.12e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.80e+02 logL: -9.20e+02 KL: 3.81e+01 MMD: 1.00e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.73e+02 logL: -9.17e+02 KL: 3.69e+01 MMD: 8.51e-01\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 9.69e+02 logL: -9.13e+02 KL: 3.67e+01 MMD: 8.87e-01\n",
      "Stopping\n",
      "====> Epoch: 98 VALIDATION Loss: 9.70e+02 logL: -9.14e+02 KL: 3.66e+01 MMD: 9.23e-01\n",
      "config 84, alpha = 0.0, lambda = 5472.0, dropout = 0.00; 2 hidden layers with 20, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.74e+03 logL: -4.14e+03 KL: 2.46e+01 MMD: 1.06e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.11e+03 logL: -3.61e+03 KL: 1.91e+01 MMD: 8.67e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.92e+03 logL: -2.65e+03 KL: 1.64e+01 MMD: 4.56e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.76e+03 logL: -2.55e+03 KL: 1.61e+01 MMD: 3.56e-02\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 48 VALIDATION Loss: 2.79e+03 logL: -2.53e+03 KL: 1.67e+01 MMD: 4.42e-02\n",
      "config 84, alpha = 0.0, lambda = 1.5, dropout = 0.00; 2 hidden layers with 171, 92 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.16e+03 logL: -1.03e+03 KL: 1.26e+02 MMD: 2.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.06e+02 logL: -7.20e+02 KL: 8.54e+01 MMD: 1.75e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.55e+02 logL: -4.79e+02 KL: 7.46e+01 MMD: 1.73e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.98e+02 logL: -4.32e+02 KL: 6.50e+01 MMD: 1.61e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.80e+02 logL: -4.20e+02 KL: 5.90e+01 MMD: 1.68e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.93e+02 logL: -4.35e+02 KL: 5.67e+01 MMD: 1.53e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.78e+02 logL: -4.25e+02 KL: 5.18e+01 MMD: 1.42e+00\n",
      "Epoch 00071: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 4.50e+02 logL: -3.98e+02 KL: 5.07e+01 MMD: 1.45e+00\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 4.48e+02 logL: -3.97e+02 KL: 4.96e+01 MMD: 1.46e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.48e+02 logL: -3.97e+02 KL: 4.94e+01 MMD: 1.34e+00\n",
      "Epoch 00107: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 107 VALIDATION Loss: 4.47e+02 logL: -3.97e+02 KL: 4.93e+01 MMD: 1.29e+00\n",
      "config 85, alpha = 0.0, lambda = 31058.3, dropout = 0.00; 2 hidden layers with 82, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.28e+03 logL: -5.63e+03 KL: 4.69e+00 MMD: 2.08e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.18e+03 logL: -4.76e+03 KL: 5.43e+00 MMD: 1.34e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.82e+03 logL: -4.69e+03 KL: 5.05e+00 MMD: 3.81e-03\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 4.88e+03 logL: -4.59e+03 KL: 6.33e+00 MMD: 9.21e-03\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.66e+03 logL: -4.58e+03 KL: 6.30e+00 MMD: 2.34e-03\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.73e+03 logL: -4.58e+03 KL: 6.32e+00 MMD: 4.55e-03\n",
      "Stopping\n",
      "====> Epoch: 60 VALIDATION Loss: 4.73e+03 logL: -4.58e+03 KL: 6.32e+00 MMD: 4.55e-03\n",
      "config 85, alpha = 0.0, lambda = 15.8, dropout = 0.00; 2 hidden layers with 49, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.95e+03 logL: -1.85e+03 KL: 8.55e+01 MMD: 9.34e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 4.94e+01 MMD: 1.07e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.80e+03 logL: -1.75e+03 KL: 4.03e+01 MMD: 1.02e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.76e+03 logL: -1.71e+03 KL: 3.66e+01 MMD: 1.00e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.83e+03 logL: -1.78e+03 KL: 3.64e+01 MMD: 1.02e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.68e+03 logL: -1.64e+03 KL: 2.95e+01 MMD: 9.17e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.64e+03 logL: -1.60e+03 KL: 2.82e+01 MMD: 7.55e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.65e+03 logL: -1.61e+03 KL: 2.61e+01 MMD: 8.66e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.61e+03 logL: -1.58e+03 KL: 2.59e+01 MMD: 7.44e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.63e+03 logL: -1.59e+03 KL: 2.75e+01 MMD: 6.52e-01\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.59e+03 logL: -1.55e+03 KL: 2.51e+01 MMD: 6.92e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.58e+03 logL: -1.55e+03 KL: 2.49e+01 MMD: 6.13e-01\n",
      "Epoch 00121: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.58e+03 logL: -1.54e+03 KL: 2.46e+01 MMD: 5.81e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 1.58e+03 logL: -1.54e+03 KL: 2.47e+01 MMD: 6.12e-01\n",
      "Stopping\n",
      "====> Epoch: 140 VALIDATION Loss: 1.58e+03 logL: -1.54e+03 KL: 2.47e+01 MMD: 6.12e-01\n",
      "config 85, alpha = 0.0, lambda = 144.2, dropout = 0.00; 2 hidden layers with 122, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.11e+03 logL: -3.75e+03 KL: 1.50e+02 MMD: 1.45e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.94e+03 logL: -2.63e+03 KL: 1.28e+02 MMD: 1.32e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.15e+03 logL: -1.87e+03 KL: 9.28e+01 MMD: 1.33e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.10e+03 logL: -1.84e+03 KL: 6.25e+01 MMD: 1.39e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.60e+03 logL: -1.38e+03 KL: 5.24e+01 MMD: 1.11e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.50e+03 logL: -1.37e+03 KL: 3.66e+01 MMD: 7.07e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.39e+03 logL: -1.31e+03 KL: 3.36e+01 MMD: 3.77e-01\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.33e+03 logL: -1.26e+03 KL: 3.31e+01 MMD: 1.98e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.32e+03 logL: -1.26e+03 KL: 3.28e+01 MMD: 1.99e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.31e+03 logL: -1.26e+03 KL: 3.25e+01 MMD: 1.49e-01\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 107 VALIDATION Loss: 1.31e+03 logL: -1.25e+03 KL: 3.25e+01 MMD: 1.42e-01\n",
      "config 85, alpha = 0.0, lambda = 58.3, dropout = 0.00; 2 hidden layers with 14, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.35e+03 logL: -4.01e+03 KL: 2.30e+02 MMD: 1.84e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.76e+03 logL: -2.56e+03 KL: 1.02e+02 MMD: 1.79e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.55e+03 logL: -1.36e+03 KL: 9.20e+01 MMD: 1.71e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.45e+03 logL: -1.29e+03 KL: 6.35e+01 MMD: 1.74e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 50 VALIDATION Loss: 1.42e+03 logL: -1.28e+03 KL: 5.15e+01 MMD: 1.55e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.40e+03 logL: -1.28e+03 KL: 4.42e+01 MMD: 1.30e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.06e+03 logL: -9.55e+02 KL: 4.40e+01 MMD: 1.14e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.03e+03 logL: -9.42e+02 KL: 3.98e+01 MMD: 9.29e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.02e+03 logL: -9.40e+02 KL: 3.81e+01 MMD: 7.24e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.01e+03 logL: -9.43e+02 KL: 3.64e+01 MMD: 5.80e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.03e+03 logL: -9.67e+02 KL: 3.60e+01 MMD: 4.82e-01\n",
      "Epoch 00115: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 9.94e+02 logL: -9.33e+02 KL: 3.58e+01 MMD: 4.46e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 9.88e+02 logL: -9.32e+02 KL: 3.55e+01 MMD: 3.53e-01\n",
      "Epoch 00139: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 140 VALIDATION Loss: 9.87e+02 logL: -9.31e+02 KL: 3.55e+01 MMD: 3.57e-01\n",
      "Stopping\n",
      "====> Epoch: 143 VALIDATION Loss: 9.86e+02 logL: -9.31e+02 KL: 3.54e+01 MMD: 3.42e-01\n",
      "config 85, alpha = 0.0, lambda = 348.9, dropout = 0.00; 2 hidden layers with 144, 88 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.47e+03 logL: -6.74e+02 KL: 8.11e+01 MMD: 2.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.24e+02 logL: -5.03e+02 KL: 4.85e+01 MMD: 2.08e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.59e+02 logL: -4.68e+02 KL: 4.92e+01 MMD: 1.20e-01\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 5.07e+02 logL: -4.28e+02 KL: 4.95e+01 MMD: 8.52e-02\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 5.00e+02 logL: -4.24e+02 KL: 4.91e+01 MMD: 7.74e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 4.94e+02 logL: -4.22e+02 KL: 4.93e+01 MMD: 6.53e-02\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 64 VALIDATION Loss: 4.94e+02 logL: -4.22e+02 KL: 4.92e+01 MMD: 6.50e-02\n",
      "config 86, alpha = 0.0, lambda = 10.0, dropout = 0.00; 2 hidden layers with 38, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.76e+03 logL: -3.66e+03 KL: 9.46e+01 MMD: 7.62e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.67e+03 logL: -3.62e+03 KL: 4.36e+01 MMD: 7.64e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.55e+03 logL: -3.50e+03 KL: 3.67e+01 MMD: 8.24e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.31e+03 logL: -3.27e+03 KL: 3.26e+01 MMD: 7.41e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.23e+03 logL: -3.20e+03 KL: 2.76e+01 MMD: 6.76e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.19e+03 logL: -3.16e+03 KL: 2.32e+01 MMD: 7.26e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.14e+03 logL: -3.11e+03 KL: 2.14e+01 MMD: 7.00e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.14e+03 logL: -3.11e+03 KL: 2.06e+01 MMD: 7.80e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.14e+03 logL: -3.11e+03 KL: 2.02e+01 MMD: 7.15e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.10e+03 logL: -3.07e+03 KL: 2.09e+01 MMD: 7.80e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 3.07e+03 logL: -3.04e+03 KL: 1.97e+01 MMD: 6.73e-01\n",
      "Epoch 00110: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 3.03e+03 logL: -3.00e+03 KL: 2.00e+01 MMD: 7.40e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 3.02e+03 logL: -2.99e+03 KL: 1.92e+01 MMD: 6.63e-01\n",
      "Epoch 00133: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 136 VALIDATION Loss: 3.02e+03 logL: -2.99e+03 KL: 1.92e+01 MMD: 7.29e-01\n",
      "config 86, alpha = 0.0, lambda = 193.1, dropout = 0.00; 2 hidden layers with 66, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.21e+03 logL: -1.93e+03 KL: 8.34e+01 MMD: 1.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.13e+03 logL: -1.89e+03 KL: 5.20e+01 MMD: 9.51e-01\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 2.07e+03 logL: -1.81e+03 KL: 4.96e+01 MMD: 1.09e+00\n",
      "Stopping\n",
      "====> Epoch: 32 VALIDATION Loss: 2.06e+03 logL: -1.81e+03 KL: 4.95e+01 MMD: 1.05e+00\n",
      "config 86, alpha = 0.0, lambda = 18.7, dropout = 0.00; 2 hidden layers with 10, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.60e+03 logL: -5.43e+03 KL: 1.39e+02 MMD: 1.53e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.80e+03 logL: -2.67e+03 KL: 1.07e+02 MMD: 1.41e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.64e+03 logL: -2.56e+03 KL: 6.12e+01 MMD: 1.35e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.60e+03 logL: -2.54e+03 KL: 4.30e+01 MMD: 1.41e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.56e+03 logL: -2.50e+03 KL: 3.53e+01 MMD: 1.29e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.90e+03 logL: -1.84e+03 KL: 4.28e+01 MMD: 1.31e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.88e+03 logL: -1.82e+03 KL: 3.71e+01 MMD: 1.17e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.87e+03 logL: -1.82e+03 KL: 3.28e+01 MMD: 1.06e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.86e+03 logL: -1.81e+03 KL: 3.02e+01 MMD: 9.64e-01\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.89e+01 MMD: 7.53e-01\n",
      "Epoch 00107: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.88e+01 MMD: 9.05e-01\n",
      "Stopping\n",
      "====> Epoch: 110 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.88e+01 MMD: 9.05e-01\n",
      "config 86, alpha = 0.0, lambda = 561.5, dropout = 0.00; 2 hidden layers with 14, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.27e+03 logL: -4.15e+03 KL: 1.63e+02 MMD: 1.72e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.64e+03 logL: -2.59e+03 KL: 1.16e+02 MMD: 1.67e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.97e+03 logL: -2.63e+03 KL: 3.61e+01 MMD: 5.54e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.12e+03 logL: -1.93e+03 KL: 3.75e+01 MMD: 2.68e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.00e+03 logL: -1.85e+03 KL: 3.65e+01 MMD: 2.00e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.95e+03 logL: -1.82e+03 KL: 3.58e+01 MMD: 1.68e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.47e+03 logL: -1.35e+03 KL: 3.39e+01 MMD: 1.58e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.44e+03 logL: -1.30e+03 KL: 3.27e+01 MMD: 1.84e-01\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.39e+03 logL: -1.28e+03 KL: 3.13e+01 MMD: 1.53e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.39e+03 logL: -1.28e+03 KL: 3.11e+01 MMD: 1.37e-01\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 104 VALIDATION Loss: 1.40e+03 logL: -1.27e+03 KL: 3.12e+01 MMD: 1.69e-01\n",
      "config 86, alpha = 0.0, lambda = 21377.8, dropout = 0.00; 2 hidden layers with 53, 44 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.27e+03 logL: -4.46e+03 KL: 7.02e+00 MMD: 8.42e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.05e+03 logL: -3.59e+03 KL: 8.48e+00 MMD: 6.77e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.53e+03 logL: -2.41e+03 KL: 1.33e+01 MMD: 5.21e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.83e+03 logL: -2.36e+03 KL: 1.64e+01 MMD: 6.78e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.02e+03 logL: -1.98e+03 KL: 1.89e+01 MMD: 4.79e-02\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 54 VALIDATION Loss: 2.91e+03 logL: -1.88e+03 KL: 1.97e+01 MMD: 4.72e-02\n",
      "config 87, alpha = 0.0, lambda = 1.6, dropout = 0.00; 2 hidden layers with 5, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.65e+03 logL: -5.37e+03 KL: 2.72e+02 MMD: 8.73e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.29e+03 logL: -5.19e+03 KL: 9.15e+01 MMD: 8.78e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 4.80e+03 logL: -4.76e+03 KL: 4.05e+01 MMD: 8.49e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 4.51e+03 logL: -4.48e+03 KL: 2.33e+01 MMD: 8.68e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.94e+03 logL: -3.91e+03 KL: 2.40e+01 MMD: 7.94e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.67e+03 logL: -3.65e+03 KL: 2.13e+01 MMD: 6.96e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.64e+03 logL: -3.62e+03 KL: 1.93e+01 MMD: 6.89e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.60e+03 logL: -3.58e+03 KL: 1.82e+01 MMD: 6.85e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.51e+03 logL: -3.49e+03 KL: 1.87e+01 MMD: 6.21e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.42e+03 logL: -3.40e+03 KL: 2.06e+01 MMD: 7.19e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 3.40e+03 logL: -3.37e+03 KL: 2.06e+01 MMD: 6.79e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 3.37e+03 logL: -3.35e+03 KL: 1.96e+01 MMD: 7.10e-01\n",
      "Epoch 00123: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 3.33e+03 logL: -3.31e+03 KL: 1.99e+01 MMD: 6.91e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00134: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 135 VALIDATION Loss: 3.34e+03 logL: -3.32e+03 KL: 1.96e+01 MMD: 6.67e-01\n",
      "config 87, alpha = 0.0, lambda = 61084.1, dropout = 0.00; 2 hidden layers with 41, 36 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.00e+03 logL: -4.78e+03 KL: 4.53e+00 MMD: 3.64e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 7.69e+03 logL: -5.03e+03 KL: 5.39e+00 MMD: 4.34e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 6.09e+03 logL: -4.49e+03 KL: 6.37e+00 MMD: 2.61e-02\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 34 VALIDATION Loss: 5.34e+03 logL: -4.39e+03 KL: 6.21e+00 MMD: 1.56e-02\n",
      "config 87, alpha = 0.0, lambda = 858.5, dropout = 0.00; 2 hidden layers with 36, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.25e+03 logL: -3.87e+03 KL: 1.61e+02 MMD: 1.42e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.18e+03 logL: -3.73e+03 KL: 2.69e+01 MMD: 4.90e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.85e+03 logL: -2.66e+03 KL: 3.67e+01 MMD: 1.78e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.65e+03 logL: -2.53e+03 KL: 2.48e+01 MMD: 1.13e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.94e+03 logL: -1.87e+03 KL: 2.43e+01 MMD: 5.68e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.90e+03 logL: -1.84e+03 KL: 2.19e+01 MMD: 4.87e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 2.10e+01 MMD: 4.55e-02\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 2.07e+01 MMD: 3.26e-02\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 87 VALIDATION Loss: 1.86e+03 logL: -1.80e+03 KL: 2.06e+01 MMD: 5.66e-02\n",
      "config 87, alpha = 0.0, lambda = 2853.1, dropout = 0.00; 2 hidden layers with 47, 31 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.55e+03 logL: -2.08e+03 KL: 3.73e+01 MMD: 1.53e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.58e+03 logL: -1.29e+03 KL: 3.03e+01 MMD: 9.10e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 1.23e+03 logL: -9.97e+02 KL: 3.05e+01 MMD: 7.13e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.21e+03 logL: -1.01e+03 KL: 3.11e+01 MMD: 5.97e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.26e+03 logL: -1.01e+03 KL: 3.10e+01 MMD: 7.61e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.11e+03 logL: -9.75e+02 KL: 2.99e+01 MMD: 3.79e-02\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.08e+03 logL: -9.35e+02 KL: 3.02e+01 MMD: 4.07e-02\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 1.06e+03 logL: -9.30e+02 KL: 2.99e+01 MMD: 3.64e-02\n",
      "config 87, alpha = 0.0, lambda = 825.4, dropout = 0.00; 2 hidden layers with 29, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.02e+03 logL: -5.41e+03 KL: 1.46e+02 MMD: 1.77e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.47e+03 logL: -3.93e+03 KL: 6.38e+01 MMD: 5.82e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.93e+03 logL: -2.74e+03 KL: 3.75e+01 MMD: 1.83e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.13e+03 logL: -1.95e+03 KL: 3.37e+01 MMD: 1.82e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.03e+03 logL: -1.88e+03 KL: 3.25e+01 MMD: 1.49e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.42e+03 logL: -1.31e+03 KL: 3.06e+01 MMD: 1.01e-01\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.39e+03 logL: -1.28e+03 KL: 2.97e+01 MMD: 1.04e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.39e+03 logL: -1.28e+03 KL: 2.92e+01 MMD: 1.01e-01\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 87 VALIDATION Loss: 1.38e+03 logL: -1.27e+03 KL: 2.92e+01 MMD: 9.23e-02\n",
      "config 88, alpha = 0.0, lambda = 81140.7, dropout = 0.00; 2 hidden layers with 28, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.27e+04 logL: -1.18e+04 KL: 7.32e-01 MMD: 1.06e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 1.22e+04 logL: -1.14e+04 KL: 9.52e-01 MMD: 1.01e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 1.20e+04 logL: -1.10e+04 KL: 9.74e-01 MMD: 1.30e-02\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.19e+04 logL: -1.08e+04 KL: 9.62e-01 MMD: 1.41e-02\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 47 VALIDATION Loss: 1.26e+04 logL: -1.08e+04 KL: 9.08e-01 MMD: 2.18e-02\n",
      "config 88, alpha = 0.0, lambda = 19.2, dropout = 0.00; 2 hidden layers with 33, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.92e+03 logL: -3.76e+03 KL: 1.39e+02 MMD: 1.16e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.69e+03 logL: -2.58e+03 KL: 8.90e+01 MMD: 1.01e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.92e+03 logL: -1.85e+03 KL: 5.44e+01 MMD: 1.18e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.88e+03 logL: -1.82e+03 KL: 4.12e+01 MMD: 1.07e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.85e+03 logL: -1.80e+03 KL: 3.48e+01 MMD: 1.12e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.86e+03 logL: -1.81e+03 KL: 3.10e+01 MMD: 1.01e+00\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.83e+03 logL: -1.78e+03 KL: 2.97e+01 MMD: 1.04e+00\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 1.83e+03 logL: -1.78e+03 KL: 2.94e+01 MMD: 1.03e+00\n",
      "config 88, alpha = 0.0, lambda = 28.3, dropout = 0.00; 2 hidden layers with 8, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.34e+03 logL: -5.18e+03 KL: 1.09e+02 MMD: 1.54e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.66e+03 logL: -2.55e+03 KL: 7.39e+01 MMD: 1.55e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.28e+03 logL: -2.17e+03 KL: 6.34e+01 MMD: 1.48e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.04e+03 logL: -1.95e+03 KL: 5.00e+01 MMD: 1.47e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.42e+03 logL: -1.33e+03 KL: 5.56e+01 MMD: 1.39e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.36e+03 logL: -1.27e+03 KL: 4.50e+01 MMD: 1.44e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+03 logL: -1.27e+03 KL: 3.91e+01 MMD: 1.31e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.34e+03 logL: -1.27e+03 KL: 3.53e+01 MMD: 1.18e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.33e+03 logL: -1.27e+03 KL: 3.23e+01 MMD: 1.09e+00\n",
      "Stopping\n",
      "====> Epoch: 95 VALIDATION Loss: 1.33e+03 logL: -1.27e+03 KL: 3.07e+01 MMD: 1.02e+00\n",
      "config 88, alpha = 0.0, lambda = 217.1, dropout = 0.00; 2 hidden layers with 67, 48 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.63e+03 logL: -1.17e+03 KL: 1.17e+02 MMD: 1.60e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.21e+03 logL: -8.00e+02 KL: 6.88e+01 MMD: 1.57e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.56e+02 logL: -6.63e+02 KL: 4.07e+01 MMD: 7.05e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 7.33e+02 logL: -6.38e+02 KL: 3.94e+01 MMD: 2.59e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 6.90e+02 logL: -6.24e+02 KL: 3.93e+01 MMD: 1.26e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 6.70e+02 logL: -6.16e+02 KL: 3.67e+01 MMD: 8.01e-02\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 6.55e+02 logL: -5.97e+02 KL: 3.71e+01 MMD: 9.68e-02\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 6.49e+02 logL: -5.95e+02 KL: 3.70e+01 MMD: 8.04e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 6.50e+02 logL: -5.95e+02 KL: 3.71e+01 MMD: 8.33e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 6.49e+02 logL: -5.95e+02 KL: 3.70e+01 MMD: 7.98e-02\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 105 VALIDATION Loss: 6.43e+02 logL: -5.95e+02 KL: 3.69e+01 MMD: 4.91e-02\n",
      "config 88, alpha = 0.0, lambda = 441.0, dropout = 0.00; 2 hidden layers with 177, 37 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.83e+03 logL: -9.37e+02 KL: 8.12e+01 MMD: 1.84e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.86e+02 logL: -7.23e+02 KL: 4.99e+01 MMD: 2.57e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.52e+02 logL: -4.52e+02 KL: 5.07e+01 MMD: 1.11e-01\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 5.05e+02 logL: -4.23e+02 KL: 5.06e+01 MMD: 7.35e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 5.00e+02 logL: -4.21e+02 KL: 4.97e+01 MMD: 6.75e-02\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.95e+02 logL: -4.15e+02 KL: 4.97e+01 MMD: 6.76e-02\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 69 VALIDATION Loss: 4.89e+02 logL: -4.14e+02 KL: 4.98e+01 MMD: 5.70e-02\n",
      "config 89, alpha = 0.0, lambda = 1.7, dropout = 0.00; 2 hidden layers with 27, 8 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 5.45e+03 logL: -5.33e+03 KL: 1.15e+02 MMD: 8.23e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.74e+03 logL: -3.66e+03 KL: 7.40e+01 MMD: 8.21e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.67e+03 logL: -3.63e+03 KL: 4.07e+01 MMD: 8.46e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.63e+03 logL: -3.60e+03 KL: 2.84e+01 MMD: 8.46e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.38e+03 logL: -3.35e+03 KL: 2.92e+01 MMD: 7.71e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.13e+03 logL: -3.11e+03 KL: 2.31e+01 MMD: 6.74e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.17e+03 logL: -3.15e+03 KL: 2.11e+01 MMD: 6.84e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.04e+03 logL: -3.02e+03 KL: 1.86e+01 MMD: 5.91e-01\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.00e+03 logL: -2.98e+03 KL: 1.79e+01 MMD: 7.33e-01\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 3.00e+03 logL: -2.98e+03 KL: 1.77e+01 MMD: 6.46e-01\n",
      "Stopping\n",
      "====> Epoch: 101 VALIDATION Loss: 3.00e+03 logL: -2.98e+03 KL: 1.77e+01 MMD: 6.79e-01\n",
      "config 89, alpha = 0.0, lambda = 1.4, dropout = 0.00; 2 hidden layers with 18, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.03e+03 logL: -3.84e+03 KL: 1.83e+02 MMD: 1.13e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.61e+03 logL: -2.54e+03 KL: 6.75e+01 MMD: 1.17e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.57e+03 logL: -2.53e+03 KL: 4.42e+01 MMD: 1.14e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.89e+03 logL: -1.84e+03 KL: 4.72e+01 MMD: 1.12e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.86e+03 logL: -1.82e+03 KL: 3.73e+01 MMD: 1.08e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 3.25e+01 MMD: 9.87e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 3.00e+01 MMD: 1.02e+00\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.83e+03 logL: -1.80e+03 KL: 2.83e+01 MMD: 9.71e-01\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 88 VALIDATION Loss: 1.83e+03 logL: -1.80e+03 KL: 2.82e+01 MMD: 1.05e+00\n",
      "config 89, alpha = 0.0, lambda = 1.7, dropout = 0.00; 2 hidden layers with 100, 93 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.11e+03 logL: -1.03e+03 KL: 8.33e+01 MMD: 1.42e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.00e+03 logL: -9.45e+02 KL: 5.45e+01 MMD: 1.43e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.00e+03 logL: -9.60e+02 KL: 4.37e+01 MMD: 1.22e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.66e+02 logL: -9.26e+02 KL: 3.98e+01 MMD: 1.19e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.94e+02 logL: -9.57e+02 KL: 3.64e+01 MMD: 1.08e+00\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.41e+02 logL: -9.05e+02 KL: 3.52e+01 MMD: 1.18e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.41e+02 logL: -9.05e+02 KL: 3.44e+01 MMD: 1.10e+00\n",
      "Stopping\n",
      "====> Epoch: 75 VALIDATION Loss: 9.41e+02 logL: -9.06e+02 KL: 3.42e+01 MMD: 1.14e+00\n",
      "config 89, alpha = 0.0, lambda = 33.5, dropout = 0.00; 2 hidden layers with 19, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.05e+03 logL: -1.88e+03 KL: 1.23e+02 MMD: 1.65e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.48e+03 logL: -1.35e+03 KL: 7.83e+01 MMD: 1.68e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.38e+03 logL: -1.28e+03 KL: 5.11e+01 MMD: 1.51e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.03e+03 logL: -9.41e+02 KL: 4.66e+01 MMD: 1.40e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.01e+03 logL: -9.31e+02 KL: 4.05e+01 MMD: 1.13e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.47e+02 logL: -7.76e+02 KL: 3.83e+01 MMD: 1.01e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.04e+02 logL: -7.47e+02 KL: 3.54e+01 MMD: 6.93e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 7.98e+02 logL: -7.45e+02 KL: 3.45e+01 MMD: 5.60e-01\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 7.90e+02 logL: -7.37e+02 KL: 3.43e+01 MMD: 5.90e-01\n",
      "Epoch 00095: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 99 VALIDATION Loss: 7.91e+02 logL: -7.36e+02 KL: 3.44e+01 MMD: 6.07e-01\n",
      "config 89, alpha = 0.0, lambda = 1.1, dropout = 0.00; 2 hidden layers with 28, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.26e+03 logL: -2.14e+03 KL: 1.21e+02 MMD: 2.02e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.09e+03 logL: -1.00e+03 KL: 8.45e+01 MMD: 1.89e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.48e+02 logL: -8.79e+02 KL: 6.95e+01 MMD: 2.13e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.02e+02 logL: -7.40e+02 KL: 6.15e+01 MMD: 1.81e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.84e+02 logL: -6.26e+02 KL: 5.84e+01 MMD: 1.77e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.54e+02 logL: -6.01e+02 KL: 5.35e+01 MMD: 1.81e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.50e+02 logL: -4.97e+02 KL: 5.25e+01 MMD: 1.76e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.41e+02 logL: -4.91e+02 KL: 4.96e+01 MMD: 1.65e+00\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 5.32e+02 logL: -4.84e+02 KL: 4.77e+01 MMD: 1.66e+00\n",
      "Stopping\n",
      "====> Epoch: 99 VALIDATION Loss: 5.31e+02 logL: -4.84e+02 KL: 4.74e+01 MMD: 1.77e+00\n",
      "config 90, alpha = 0.0, lambda = 8.1, dropout = 0.00; 2 hidden layers with 10, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.75e+03 logL: -3.64e+03 KL: 1.08e+02 MMD: 7.53e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.65e+03 logL: -3.60e+03 KL: 4.89e+01 MMD: 8.05e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.54e+03 logL: -3.49e+03 KL: 4.24e+01 MMD: 7.49e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.33e+03 logL: -3.28e+03 KL: 4.23e+01 MMD: 6.93e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.25e+03 logL: -3.21e+03 KL: 3.90e+01 MMD: 7.71e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.19e+03 logL: -3.15e+03 KL: 3.41e+01 MMD: 7.98e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.84e+03 logL: -3.81e+03 KL: 2.69e+01 MMD: 8.57e-01\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.12e+03 logL: -3.09e+03 KL: 2.92e+01 MMD: 7.63e-01\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.11e+03 logL: -3.08e+03 KL: 2.85e+01 MMD: 6.95e-01\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 97 VALIDATION Loss: 3.11e+03 logL: -3.08e+03 KL: 2.86e+01 MMD: 7.50e-01\n",
      "config 90, alpha = 0.0, lambda = 2.6, dropout = 0.00; 2 hidden layers with 31, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.70e+03 logL: -3.63e+03 KL: 6.29e+01 MMD: 1.38e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.58e+03 logL: -2.53e+03 KL: 4.86e+01 MMD: 1.13e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.54e+03 logL: -2.51e+03 KL: 3.56e+01 MMD: 1.19e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.91e+03 logL: -1.86e+03 KL: 5.71e+01 MMD: 1.04e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.87e+03 logL: -1.83e+03 KL: 3.43e+01 MMD: 1.02e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 2.91e+01 MMD: 1.01e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.83e+03 logL: -1.80e+03 KL: 2.64e+01 MMD: 8.99e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.83e+03 logL: -1.80e+03 KL: 2.49e+01 MMD: 9.38e-01\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.82e+03 logL: -1.79e+03 KL: 2.43e+01 MMD: 9.46e-01\n",
      "Stopping\n",
      "====> Epoch: 93 VALIDATION Loss: 1.81e+03 logL: -1.79e+03 KL: 2.44e+01 MMD: 9.48e-01\n",
      "config 90, alpha = 0.0, lambda = 5.3, dropout = 0.00; 2 hidden layers with 121, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.91e+03 logL: -1.82e+03 KL: 8.29e+01 MMD: 1.52e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.34e+03 logL: -1.28e+03 KL: 5.64e+01 MMD: 1.38e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.03e+03 logL: -9.75e+02 KL: 5.18e+01 MMD: 1.29e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.77e+02 logL: -9.28e+02 KL: 4.43e+01 MMD: 1.21e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.68e+02 logL: -9.22e+02 KL: 4.03e+01 MMD: 1.31e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.60e+02 logL: -9.18e+02 KL: 3.76e+01 MMD: 1.05e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.59e+02 logL: -9.20e+02 KL: 3.50e+01 MMD: 1.02e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.61e+02 logL: -9.23e+02 KL: 3.31e+01 MMD: 9.50e-01\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 9.42e+02 logL: -9.06e+02 KL: 3.24e+01 MMD: 1.05e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.41e+02 logL: -9.05e+02 KL: 3.19e+01 MMD: 9.77e-01\n",
      "Stopping\n",
      "====> Epoch: 101 VALIDATION Loss: 9.41e+02 logL: -9.05e+02 KL: 3.20e+01 MMD: 9.05e-01\n",
      "config 90, alpha = 0.0, lambda = 2363.4, dropout = 0.00; 2 hidden layers with 23, 20 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 8.36e+03 logL: -5.45e+03 KL: 1.25e+02 MMD: 1.18e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.24e+03 logL: -3.89e+03 KL: 2.95e+01 MMD: 1.36e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.05e+03 logL: -2.80e+03 KL: 2.79e+01 MMD: 9.57e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 2.31e+03 logL: -2.14e+03 KL: 2.62e+01 MMD: 6.07e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 2.06e+03 logL: -1.91e+03 KL: 2.52e+01 MMD: 5.39e-02\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.99e+03 logL: -1.85e+03 KL: 2.45e+01 MMD: 5.16e-02\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 69 VALIDATION Loss: 1.99e+03 logL: -1.84e+03 KL: 2.45e+01 MMD: 5.24e-02\n",
      "config 90, alpha = 0.0, lambda = 23040.9, dropout = 0.00; 2 hidden layers with 190, 103 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.07e+03 logL: -3.70e+03 KL: 4.12e+01 MMD: 1.88e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.34e+03 logL: -2.02e+03 KL: 3.48e+01 MMD: 9.91e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.70e+03 logL: -1.49e+03 KL: 3.06e+01 MMD: 9.47e-02\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.32e+03 logL: -1.17e+03 KL: 2.81e+01 MMD: 4.86e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 2.82e+03 logL: -1.11e+03 KL: 3.13e+01 MMD: 7.28e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 2.54e+03 logL: -1.12e+03 KL: 3.20e+01 MMD: 6.06e-02\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.91e+03 logL: -9.97e+02 KL: 3.31e+01 MMD: 3.80e-02\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 80 VALIDATION Loss: 2.12e+03 logL: -9.92e+02 KL: 3.34e+01 MMD: 4.76e-02\n",
      "Stopping\n",
      "====> Epoch: 80 VALIDATION Loss: 2.12e+03 logL: -9.92e+02 KL: 3.34e+01 MMD: 4.76e-02\n",
      "config 91, alpha = 0.0, lambda = 2.1, dropout = 0.00; 2 hidden layers with 68, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.68e+03 logL: -3.61e+03 KL: 6.70e+01 MMD: 7.24e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.30e+03 logL: -3.25e+03 KL: 5.05e+01 MMD: 7.01e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.16e+03 logL: -3.12e+03 KL: 3.28e+01 MMD: 7.33e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.12e+03 logL: -3.09e+03 KL: 2.48e+01 MMD: 7.66e-01\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.09e+03 logL: -3.06e+03 KL: 2.22e+01 MMD: 8.03e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.06e+03 logL: -3.04e+03 KL: 2.20e+01 MMD: 7.84e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.05e+03 logL: -3.03e+03 KL: 2.15e+01 MMD: 8.21e-01\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 74 VALIDATION Loss: 3.05e+03 logL: -3.02e+03 KL: 2.15e+01 MMD: 7.74e-01\n",
      "config 91, alpha = 0.0, lambda = 14.2, dropout = 0.00; 2 hidden layers with 126, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.94e+03 logL: -1.86e+03 KL: 6.87e+01 MMD: 1.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.87e+03 logL: -1.81e+03 KL: 4.29e+01 MMD: 9.54e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.84e+03 logL: -1.79e+03 KL: 3.64e+01 MMD: 1.02e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.79e+03 logL: -1.74e+03 KL: 3.36e+01 MMD: 8.61e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.78e+03 logL: -1.74e+03 KL: 3.34e+01 MMD: 8.24e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.66e+03 logL: -1.62e+03 KL: 3.24e+01 MMD: 8.51e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.63e+03 logL: -1.59e+03 KL: 3.28e+01 MMD: 7.81e-01\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.61e+03 logL: -1.56e+03 KL: 3.23e+01 MMD: 7.55e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.59e+03 logL: -1.55e+03 KL: 3.13e+01 MMD: 7.64e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.59e+03 logL: -1.54e+03 KL: 3.03e+01 MMD: 8.67e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.58e+03 logL: -1.54e+03 KL: 2.96e+01 MMD: 7.86e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.57e+03 logL: -1.53e+03 KL: 2.89e+01 MMD: 7.31e-01\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.56e+03 logL: -1.52e+03 KL: 2.88e+01 MMD: 7.29e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 1.56e+03 logL: -1.52e+03 KL: 2.87e+01 MMD: 6.46e-01\n",
      "Stopping\n",
      "====> Epoch: 142 VALIDATION Loss: 1.56e+03 logL: -1.52e+03 KL: 2.89e+01 MMD: 6.32e-01\n",
      "config 91, alpha = 0.0, lambda = 375.8, dropout = 0.00; 2 hidden layers with 57, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.58e+03 logL: -1.94e+03 KL: 1.42e+02 MMD: 1.34e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.46e+03 logL: -1.83e+03 KL: 7.59e+01 MMD: 1.49e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.73e+03 logL: -1.35e+03 KL: 3.92e+01 MMD: 9.01e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.39e+03 logL: -1.30e+03 KL: 3.48e+01 MMD: 1.53e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.39e+03 logL: -1.32e+03 KL: 3.46e+01 MMD: 1.10e-01\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+03 logL: -1.27e+03 KL: 3.43e+01 MMD: 1.20e-01\n",
      "Stopping\n",
      "====> Epoch: 69 VALIDATION Loss: 1.34e+03 logL: -1.27e+03 KL: 3.40e+01 MMD: 9.92e-02\n",
      "config 91, alpha = 0.0, lambda = 344.7, dropout = 0.00; 2 hidden layers with 75, 40 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.93e+03 logL: -1.31e+03 KL: 9.76e+01 MMD: 1.54e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.27e+03 logL: -7.76e+02 KL: 5.55e+01 MMD: 1.27e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.24e+02 logL: -7.14e+02 KL: 4.22e+01 MMD: 1.97e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 7.35e+02 logL: -6.54e+02 KL: 4.04e+01 MMD: 1.16e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 7.07e+02 logL: -6.41e+02 KL: 3.92e+01 MMD: 7.56e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 6.96e+02 logL: -6.24e+02 KL: 3.93e+01 MMD: 9.53e-02\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 6.62e+02 logL: -5.95e+02 KL: 3.84e+01 MMD: 8.30e-02\n",
      "Epoch 00071: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 6.55e+02 logL: -5.94e+02 KL: 3.85e+01 MMD: 6.58e-02\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 90 VALIDATION Loss: 6.52e+02 logL: -5.93e+02 KL: 3.84e+01 MMD: 5.77e-02\n",
      "Stopping\n",
      "====> Epoch: 92 VALIDATION Loss: 6.49e+02 logL: -5.94e+02 KL: 3.84e+01 MMD: 5.03e-02\n",
      "config 91, alpha = 0.0, lambda = 21214.5, dropout = 0.00; 2 hidden layers with 185, 86 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.79e+03 logL: -4.33e+03 KL: 4.30e+01 MMD: 1.14e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.62e+03 logL: -2.75e+03 KL: 1.81e+01 MMD: 8.69e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.56e+03 logL: -1.82e+03 KL: 2.14e+01 MMD: 8.10e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.07e+03 logL: -1.62e+03 KL: 2.38e+01 MMD: 6.68e-02\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.35e+03 logL: -1.42e+03 KL: 2.67e+01 MMD: 4.29e-02\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.61e+03 logL: -1.41e+03 KL: 2.70e+01 MMD: 5.53e-02\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.52e+03 logL: -1.41e+03 KL: 2.71e+01 MMD: 5.11e-02\n",
      "Stopping\n",
      "====> Epoch: 73 VALIDATION Loss: 2.38e+03 logL: -1.41e+03 KL: 2.71e+01 MMD: 4.47e-02\n",
      "config 92, alpha = 0.0, lambda = 7.3, dropout = 0.00; 2 hidden layers with 76, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.67e+03 logL: -3.61e+03 KL: 5.46e+01 MMD: 7.56e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.30e+03 logL: -3.25e+03 KL: 3.99e+01 MMD: 7.66e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.19e+03 logL: -3.14e+03 KL: 3.76e+01 MMD: 7.53e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.16e+03 logL: -3.12e+03 KL: 3.14e+01 MMD: 7.60e-01\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.09e+03 logL: -3.06e+03 KL: 2.59e+01 MMD: 7.05e-01\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 55 VALIDATION Loss: 3.09e+03 logL: -3.06e+03 KL: 2.53e+01 MMD: 7.45e-01\n",
      "config 92, alpha = 0.0, lambda = 1.8, dropout = 0.00; 2 hidden layers with 37, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.44e+03 logL: -5.32e+03 KL: 1.15e+02 MMD: 1.40e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.76e+03 logL: -3.68e+03 KL: 7.65e+01 MMD: 1.13e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.66e+03 logL: -3.62e+03 KL: 4.24e+01 MMD: 1.32e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.63e+03 logL: -3.60e+03 KL: 3.22e+01 MMD: 1.21e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 50 VALIDATION Loss: 2.57e+03 logL: -2.53e+03 KL: 3.96e+01 MMD: 1.27e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.53e+03 logL: -2.50e+03 KL: 3.41e+01 MMD: 1.21e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.52e+03 logL: -2.49e+03 KL: 3.06e+01 MMD: 1.23e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.48e+03 logL: -2.46e+03 KL: 2.88e+01 MMD: 1.15e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.48e+03 logL: -2.45e+03 KL: 2.76e+01 MMD: 1.21e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.43e+03 logL: -2.41e+03 KL: 2.63e+01 MMD: 1.04e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.95e+03 logL: -1.91e+03 KL: 3.31e+01 MMD: 1.00e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.86e+03 logL: -1.83e+03 KL: 3.07e+01 MMD: 1.14e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.83e+03 logL: -1.80e+03 KL: 2.92e+01 MMD: 1.08e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.83e+03 logL: -1.80e+03 KL: 2.80e+01 MMD: 1.02e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.74e+03 logL: -1.71e+03 KL: 2.62e+01 MMD: 1.04e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.72e+03 logL: -1.70e+03 KL: 2.54e+01 MMD: 1.12e+00\n",
      "Epoch 00165: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 170 VALIDATION Loss: 1.68e+03 logL: -1.66e+03 KL: 2.51e+01 MMD: 9.95e-01\n",
      "====> Epoch: 180 VALIDATION Loss: 1.68e+03 logL: -1.65e+03 KL: 2.50e+01 MMD: 1.18e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 1.67e+03 logL: -1.64e+03 KL: 2.49e+01 MMD: 9.95e-01\n",
      "====> Epoch: 200 VALIDATION Loss: 1.66e+03 logL: -1.63e+03 KL: 2.47e+01 MMD: 1.11e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 1.66e+03 logL: -1.63e+03 KL: 2.47e+01 MMD: 1.11e+00\n",
      "config 92, alpha = 0.0, lambda = 3.4, dropout = 0.00; 2 hidden layers with 72, 51 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.05e+03 logL: -9.64e+02 KL: 7.93e+01 MMD: 1.19e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.92e+02 logL: -9.38e+02 KL: 5.14e+01 MMD: 1.33e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.74e+02 logL: -9.29e+02 KL: 4.18e+01 MMD: 1.24e+00\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 9.57e+02 logL: -9.16e+02 KL: 3.83e+01 MMD: 1.12e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.55e+02 logL: -9.15e+02 KL: 3.75e+01 MMD: 1.18e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.53e+02 logL: -9.14e+02 KL: 3.64e+01 MMD: 1.20e+00\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.51e+02 logL: -9.13e+02 KL: 3.59e+01 MMD: 1.17e+00\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 76 VALIDATION Loss: 9.51e+02 logL: -9.12e+02 KL: 3.59e+01 MMD: 9.85e-01\n",
      "config 92, alpha = 0.0, lambda = 25.1, dropout = 0.00; 2 hidden layers with 13, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.35e+03 logL: -4.12e+03 KL: 1.76e+02 MMD: 1.99e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.22e+03 logL: -3.07e+03 KL: 9.63e+01 MMD: 1.97e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.92e+03 logL: -2.80e+03 KL: 7.64e+01 MMD: 1.87e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.78e+03 logL: -2.67e+03 KL: 6.66e+01 MMD: 1.82e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.16e+03 logL: -2.05e+03 KL: 6.68e+01 MMD: 1.69e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.00e+03 logL: -1.90e+03 KL: 5.79e+01 MMD: 1.64e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.42e+03 logL: -1.32e+03 KL: 5.79e+01 MMD: 1.59e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.11e+03 logL: -1.02e+03 KL: 5.70e+01 MMD: 1.45e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.03e+03 logL: -9.48e+02 KL: 4.90e+01 MMD: 1.38e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.02e+03 logL: -9.44e+02 KL: 4.36e+01 MMD: 1.29e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 8.72e+02 logL: -7.92e+02 KL: 4.50e+01 MMD: 1.46e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 8.26e+02 logL: -7.60e+02 KL: 3.97e+01 MMD: 1.13e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 8.06e+02 logL: -7.46e+02 KL: 3.74e+01 MMD: 9.44e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 8.04e+02 logL: -7.46e+02 KL: 3.68e+01 MMD: 9.04e-01\n",
      "Epoch 00142: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 7.92e+02 logL: -7.37e+02 KL: 3.61e+01 MMD: 7.69e-01\n",
      "Epoch 00151: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 155 VALIDATION Loss: 7.92e+02 logL: -7.37e+02 KL: 3.59e+01 MMD: 7.66e-01\n",
      "config 92, alpha = 0.0, lambda = 25.2, dropout = 0.00; 2 hidden layers with 22, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.77e+03 logL: -2.59e+03 KL: 1.29e+02 MMD: 2.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.51e+03 logL: -1.37e+03 KL: 8.91e+01 MMD: 2.03e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.40e+03 logL: -1.28e+03 KL: 6.72e+01 MMD: 1.91e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.06e+03 logL: -9.54e+02 KL: 6.29e+01 MMD: 1.94e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.03e+03 logL: -9.39e+02 KL: 5.38e+01 MMD: 1.71e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.55e+02 logL: -7.64e+02 KL: 5.20e+01 MMD: 1.63e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.42e+02 logL: -7.59e+02 KL: 4.65e+01 MMD: 1.49e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 8.25e+02 logL: -7.50e+02 KL: 4.48e+01 MMD: 1.27e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 8.18e+02 logL: -7.50e+02 KL: 4.28e+01 MMD: 1.06e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 8.15e+02 logL: -7.51e+02 KL: 4.18e+01 MMD: 8.92e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 8.07e+02 logL: -7.46e+02 KL: 4.04e+01 MMD: 8.48e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 8.01e+02 logL: -7.44e+02 KL: 4.02e+01 MMD: 6.95e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 6.98e+02 logL: -6.42e+02 KL: 4.08e+01 MMD: 6.34e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 6.68e+02 logL: -6.10e+02 KL: 3.96e+01 MMD: 7.37e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 6.52e+02 logL: -5.98e+02 KL: 3.98e+01 MMD: 6.05e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 6.48e+02 logL: -5.96e+02 KL: 3.98e+01 MMD: 5.05e-01\n",
      "Epoch 00163: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 170 VALIDATION Loss: 6.43e+02 logL: -5.90e+02 KL: 3.93e+01 MMD: 5.63e-01\n",
      "Epoch 00175: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 179 VALIDATION Loss: 6.41e+02 logL: -5.89e+02 KL: 3.92e+01 MMD: 5.13e-01\n",
      "config 93, alpha = 0.0, lambda = 1.6, dropout = 0.00; 2 hidden layers with 5, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.63e+03 logL: -5.38e+03 KL: 2.53e+02 MMD: 8.88e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.26e+03 logL: -5.17e+03 KL: 9.26e+01 MMD: 9.10e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.00e+03 logL: -4.94e+03 KL: 5.60e+01 MMD: 9.29e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 4.36e+03 logL: -4.32e+03 KL: 3.63e+01 MMD: 9.51e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.69e+03 logL: -3.65e+03 KL: 3.47e+01 MMD: 8.40e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.70e+03 logL: -3.67e+03 KL: 2.66e+01 MMD: 8.67e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.64e+03 logL: -3.62e+03 KL: 2.23e+01 MMD: 8.34e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.64e+03 logL: -3.62e+03 KL: 1.90e+01 MMD: 8.93e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.63e+03 logL: -3.61e+03 KL: 1.73e+01 MMD: 8.94e-01\n",
      "Stopping\n",
      "====> Epoch: 98 VALIDATION Loss: 3.62e+03 logL: -3.61e+03 KL: 1.62e+01 MMD: 8.16e-01\n",
      "config 93, alpha = 0.0, lambda = 2.0, dropout = 0.00; 2 hidden layers with 7, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.49e+03 logL: -5.36e+03 KL: 1.19e+02 MMD: 1.47e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.19e+03 logL: -3.09e+03 KL: 9.75e+01 MMD: 1.15e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.60e+03 logL: -2.55e+03 KL: 5.28e+01 MMD: 1.09e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.58e+03 logL: -2.54e+03 KL: 3.91e+01 MMD: 1.21e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.55e+03 logL: -2.52e+03 KL: 3.26e+01 MMD: 1.26e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.55e+03 logL: -2.52e+03 KL: 2.89e+01 MMD: 1.29e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.52e+03 logL: -2.49e+03 KL: 2.87e+01 MMD: 1.20e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.85e+03 logL: -1.82e+03 KL: 3.77e+01 MMD: 1.16e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 3.13e+01 MMD: 1.02e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 2.83e+01 MMD: 1.06e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.84e+03 logL: -1.81e+03 KL: 2.65e+01 MMD: 1.11e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.83e+03 logL: -1.80e+03 KL: 2.54e+01 MMD: 1.00e+00\n",
      "Stopping\n",
      "====> Epoch: 127 VALIDATION Loss: 1.83e+03 logL: -1.80e+03 KL: 2.49e+01 MMD: 1.03e+00\n",
      "config 93, alpha = 0.0, lambda = 6.2, dropout = 0.00; 2 hidden layers with 182, 114 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.09e+03 logL: -9.97e+02 KL: 8.87e+01 MMD: 1.26e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.02e+03 logL: -9.48e+02 KL: 6.68e+01 MMD: 1.33e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 30 VALIDATION Loss: 1.00e+03 logL: -9.41e+02 KL: 5.43e+01 MMD: 1.32e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.83e+02 logL: -9.30e+02 KL: 4.58e+01 MMD: 1.40e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.75e+02 logL: -9.28e+02 KL: 4.17e+01 MMD: 1.16e+00\n",
      "Epoch 00058: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.51e+02 logL: -9.07e+02 KL: 3.88e+01 MMD: 1.04e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.49e+02 logL: -9.06e+02 KL: 3.81e+01 MMD: 1.04e+00\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.46e+02 logL: -9.04e+02 KL: 3.70e+01 MMD: 1.05e+00\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 88 VALIDATION Loss: 9.45e+02 logL: -9.03e+02 KL: 3.69e+01 MMD: 1.03e+00\n",
      "config 93, alpha = 0.0, lambda = 10190.2, dropout = 0.00; 2 hidden layers with 78, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.48e+03 logL: -2.77e+03 KL: 1.71e+01 MMD: 6.77e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.19e+03 logL: -2.61e+03 KL: 1.80e+01 MMD: 5.52e-02\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.04e+03 logL: -2.53e+03 KL: 1.85e+01 MMD: 4.82e-02\n",
      "Stopping\n",
      "====> Epoch: 31 VALIDATION Loss: 3.00e+03 logL: -2.54e+03 KL: 1.87e+01 MMD: 4.43e-02\n",
      "config 93, alpha = 0.0, lambda = 1941.4, dropout = 0.00; 2 hidden layers with 35, 26 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.74e+03 logL: -3.21e+03 KL: 3.83e+01 MMD: 2.54e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.66e+03 logL: -1.43e+03 KL: 3.41e+01 MMD: 9.76e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 1.28e+03 logL: -1.06e+03 KL: 3.31e+01 MMD: 9.56e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.22e+03 logL: -1.02e+03 KL: 3.25e+01 MMD: 8.84e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.17e+03 logL: -1.04e+03 KL: 3.16e+01 MMD: 5.38e-02\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.11e+03 logL: -9.44e+02 KL: 3.05e+01 MMD: 7.04e-02\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.09e+03 logL: -9.42e+02 KL: 3.07e+01 MMD: 5.99e-02\n",
      "Stopping\n",
      "====> Epoch: 74 VALIDATION Loss: 1.08e+03 logL: -9.43e+02 KL: 3.08e+01 MMD: 5.45e-02\n",
      "config 94, alpha = 0.0, lambda = 18.9, dropout = 0.00; 2 hidden layers with 8, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.62e+03 logL: -5.45e+03 KL: 1.58e+02 MMD: 7.37e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.35e+03 logL: -5.26e+03 KL: 7.33e+01 MMD: 8.23e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.88e+03 logL: -3.75e+03 KL: 1.18e+02 MMD: 7.45e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.71e+03 logL: -3.64e+03 KL: 5.33e+01 MMD: 7.49e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.68e+03 logL: -3.63e+03 KL: 3.59e+01 MMD: 8.65e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.66e+03 logL: -3.62e+03 KL: 2.79e+01 MMD: 7.77e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.65e+03 logL: -3.61e+03 KL: 2.33e+01 MMD: 8.58e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.62e+03 logL: -3.59e+03 KL: 2.08e+01 MMD: 7.74e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.58e+03 logL: -3.54e+03 KL: 2.18e+01 MMD: 7.45e-01\n",
      "Epoch 00098: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 6.32e+03 logL: -6.27e+03 KL: 2.73e+01 MMD: 7.66e-01\n",
      "Stopping\n",
      "====> Epoch: 102 VALIDATION Loss: 6.02e+03 logL: -5.98e+03 KL: 2.71e+01 MMD: 7.69e-01\n",
      "config 94, alpha = 0.0, lambda = 62.6, dropout = 0.00; 2 hidden layers with 33, 31 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.99e+03 logL: -1.84e+03 KL: 7.82e+01 MMD: 1.04e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.93e+03 logL: -1.82e+03 KL: 4.61e+01 MMD: 1.01e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.86e+03 logL: -1.77e+03 KL: 3.68e+01 MMD: 1.01e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.81e+03 logL: -1.73e+03 KL: 3.24e+01 MMD: 8.52e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.68e+03 logL: -1.60e+03 KL: 2.84e+01 MMD: 7.35e-01\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.64e+03 logL: -1.58e+03 KL: 2.52e+01 MMD: 5.79e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.62e+03 logL: -1.56e+03 KL: 2.44e+01 MMD: 5.59e-01\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 74 VALIDATION Loss: 1.63e+03 logL: -1.57e+03 KL: 2.34e+01 MMD: 5.32e-01\n",
      "config 94, alpha = 0.0, lambda = 327.2, dropout = 0.00; 2 hidden layers with 22, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.31e+03 logL: -3.66e+03 KL: 1.32e+02 MMD: 1.60e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.06e+03 logL: -2.54e+03 KL: 8.96e+01 MMD: 1.31e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.91e+03 logL: -2.54e+03 KL: 4.01e+01 MMD: 1.02e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.65e+03 logL: -2.52e+03 KL: 3.12e+01 MMD: 2.97e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.97e+03 logL: -1.86e+03 KL: 3.02e+01 MMD: 2.47e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.96e+03 logL: -1.86e+03 KL: 3.01e+01 MMD: 2.16e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.90e+03 logL: -1.82e+03 KL: 2.86e+01 MMD: 1.42e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.84e+03 logL: -1.77e+03 KL: 2.75e+01 MMD: 1.34e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.35e+03 logL: -1.29e+03 KL: 2.69e+01 MMD: 1.00e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.34e+03 logL: -1.28e+03 KL: 2.60e+01 MMD: 9.77e-02\n",
      "Epoch 00107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.32e+03 logL: -1.26e+03 KL: 2.57e+01 MMD: 8.41e-02\n",
      "Epoch 00116: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.32e+03 logL: -1.26e+03 KL: 2.56e+01 MMD: 1.05e-01\n",
      "Epoch 00123: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 127 VALIDATION Loss: 1.32e+03 logL: -1.26e+03 KL: 2.56e+01 MMD: 8.69e-02\n",
      "config 94, alpha = 0.0, lambda = 123.0, dropout = 0.00; 2 hidden layers with 12, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.66e+03 logL: -6.12e+03 KL: 3.26e+02 MMD: 1.78e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.01e+03 logL: -5.61e+03 KL: 2.06e+02 MMD: 1.56e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.21e+03 logL: -3.83e+03 KL: 1.82e+02 MMD: 1.66e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.61e+03 logL: -3.27e+03 KL: 1.50e+02 MMD: 1.59e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.34e+03 logL: -3.02e+03 KL: 1.21e+02 MMD: 1.63e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.63e+03 logL: -2.32e+03 KL: 1.18e+02 MMD: 1.64e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.39e+03 logL: -2.10e+03 KL: 9.18e+01 MMD: 1.59e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.27e+03 logL: -1.99e+03 KL: 7.07e+01 MMD: 1.72e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.13e+03 logL: -1.90e+03 KL: 5.37e+01 MMD: 1.44e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.57e+03 logL: -1.36e+03 KL: 4.79e+01 MMD: 1.29e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.49e+03 logL: -1.33e+03 KL: 3.82e+01 MMD: 9.67e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.44e+03 logL: -1.34e+03 KL: 3.35e+01 MMD: 5.82e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 1.08e+03 logL: -9.89e+02 KL: 3.39e+01 MMD: 4.66e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 1.03e+03 logL: -9.60e+02 KL: 3.32e+01 MMD: 3.38e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 1.02e+03 logL: -9.41e+02 KL: 3.24e+01 MMD: 3.44e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 9.97e+02 logL: -9.41e+02 KL: 3.08e+01 MMD: 2.11e-01\n",
      "Epoch 00167: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 170 VALIDATION Loss: 9.89e+02 logL: -9.32e+02 KL: 3.07e+01 MMD: 2.13e-01\n",
      "Epoch 00174: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 180 VALIDATION Loss: 9.85e+02 logL: -9.31e+02 KL: 3.06e+01 MMD: 1.97e-01\n",
      "Epoch 00188: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 190 VALIDATION Loss: 9.83e+02 logL: -9.31e+02 KL: 3.06e+01 MMD: 1.78e-01\n",
      "Stopping\n",
      "====> Epoch: 192 VALIDATION Loss: 9.85e+02 logL: -9.31e+02 KL: 3.06e+01 MMD: 1.96e-01\n",
      "config 94, alpha = 0.0, lambda = 16002.3, dropout = 0.00; 2 hidden layers with 12, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.39e+03 logL: -7.28e+03 KL: 2.18e+01 MMD: 1.31e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 7.01e+03 logL: -5.52e+03 KL: 2.27e+01 MMD: 9.16e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 5.71e+03 logL: -4.73e+03 KL: 2.29e+01 MMD: 5.99e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 5.52e+03 logL: -4.07e+03 KL: 2.37e+01 MMD: 8.92e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 4.52e+03 logL: -3.08e+03 KL: 2.50e+01 MMD: 8.89e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.84e+03 logL: -2.77e+03 KL: 2.50e+01 MMD: 6.51e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 4.13e+03 logL: -2.72e+03 KL: 2.38e+01 MMD: 8.68e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 79 VALIDATION Loss: 3.70e+03 logL: -2.62e+03 KL: 2.27e+01 MMD: 6.66e-02\n",
      "config 95, alpha = 0.0, lambda = 1208.2, dropout = 0.00; 2 hidden layers with 47, 43 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.49e+03 logL: -3.58e+03 KL: 8.07e+01 MMD: 6.85e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.08e+03 logL: -3.17e+03 KL: 5.62e+01 MMD: 7.09e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.37e+03 logL: -3.18e+03 KL: 2.12e+01 MMD: 1.40e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.14e+03 logL: -3.05e+03 KL: 1.65e+01 MMD: 6.28e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.08e+03 logL: -3.04e+03 KL: 1.31e+01 MMD: 2.16e-02\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.12e+03 logL: -3.08e+03 KL: 1.25e+01 MMD: 2.29e-02\n",
      "Stopping\n",
      "====> Epoch: 60 VALIDATION Loss: 3.12e+03 logL: -3.08e+03 KL: 1.25e+01 MMD: 2.29e-02\n",
      "config 95, alpha = 0.0, lambda = 1320.6, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.40e+03 logL: -5.72e+03 KL: 6.06e+01 MMD: 1.23e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.83e+03 logL: -5.35e+03 KL: 8.85e+01 MMD: 1.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.50e+03 logL: -5.18e+03 KL: 5.55e+01 MMD: 9.63e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 4.26e+03 logL: -4.09e+03 KL: 1.25e+01 MMD: 1.21e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.77e+03 logL: -3.73e+03 KL: 1.32e+01 MMD: 2.37e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.71e+03 logL: -3.67e+03 KL: 1.23e+01 MMD: 2.46e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 3.70e+03 logL: -3.64e+03 KL: 1.08e+01 MMD: 3.14e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 3.68e+03 logL: -3.64e+03 KL: 1.03e+01 MMD: 2.44e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 3.65e+03 logL: -3.62e+03 KL: 9.63e+00 MMD: 1.35e-02\n",
      "Stopping\n",
      "====> Epoch: 95 VALIDATION Loss: 3.66e+03 logL: -3.62e+03 KL: 9.39e+00 MMD: 2.55e-02\n",
      "config 95, alpha = 0.0, lambda = 2274.0, dropout = 0.00; 2 hidden layers with 8, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.47e+03 logL: -5.49e+03 KL: 1.27e+02 MMD: 1.25e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.76e+03 logL: -4.64e+03 KL: 2.19e+01 MMD: 4.84e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 4.72e+03 logL: -4.35e+03 KL: 2.24e+01 MMD: 1.54e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 4.30e+03 logL: -4.03e+03 KL: 1.81e+01 MMD: 1.11e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.85e+03 logL: -3.60e+03 KL: 1.82e+01 MMD: 9.93e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.54e+03 logL: -3.35e+03 KL: 1.75e+01 MMD: 7.62e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 2.69e+03 logL: -2.57e+03 KL: 1.64e+01 MMD: 4.40e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 2.65e+03 logL: -2.53e+03 KL: 1.52e+01 MMD: 4.64e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 2.62e+03 logL: -2.55e+03 KL: 1.50e+01 MMD: 2.62e-02\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 2.62e+03 logL: -2.52e+03 KL: 1.45e+01 MMD: 3.86e-02\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.64e+03 logL: -2.51e+03 KL: 1.45e+01 MMD: 4.99e-02\n",
      "Stopping\n",
      "====> Epoch: 113 VALIDATION Loss: 2.62e+03 logL: -2.51e+03 KL: 1.45e+01 MMD: 3.92e-02\n",
      "config 95, alpha = 0.0, lambda = 18.1, dropout = 0.00; 2 hidden layers with 31, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.86e+03 logL: -3.72e+03 KL: 1.10e+02 MMD: 1.96e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.06e+03 logL: -1.94e+03 KL: 8.55e+01 MMD: 1.88e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.40e+03 logL: -1.30e+03 KL: 7.09e+01 MMD: 1.67e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.39e+03 logL: -1.30e+03 KL: 5.86e+01 MMD: 1.70e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.36e+03 logL: -1.28e+03 KL: 5.10e+01 MMD: 1.76e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+03 logL: -1.28e+03 KL: 4.67e+01 MMD: 1.53e+00\n",
      "Stopping\n",
      "====> Epoch: 62 VALIDATION Loss: 1.36e+03 logL: -1.29e+03 KL: 4.63e+01 MMD: 1.41e+00\n",
      "config 95, alpha = 0.0, lambda = 2.9, dropout = 0.00; 2 hidden layers with 110, 40 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.15e+03 logL: -1.02e+03 KL: 1.24e+02 MMD: 1.82e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.89e+02 logL: -6.95e+02 KL: 8.99e+01 MMD: 1.89e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.88e+02 logL: -5.11e+02 KL: 7.38e+01 MMD: 1.89e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.01e+02 logL: -4.33e+02 KL: 6.46e+01 MMD: 1.85e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.75e+02 logL: -4.13e+02 KL: 5.91e+01 MMD: 1.82e+00\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.60e+02 logL: -4.03e+02 KL: 5.43e+01 MMD: 1.73e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.57e+02 logL: -4.02e+02 KL: 5.30e+01 MMD: 1.44e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.57e+02 logL: -4.03e+02 KL: 5.11e+01 MMD: 1.68e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.55e+02 logL: -4.02e+02 KL: 4.98e+01 MMD: 1.53e+00\n",
      "Epoch 00099: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 4.50e+02 logL: -3.99e+02 KL: 4.86e+01 MMD: 1.32e+00\n",
      "Epoch 00106: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 110 VALIDATION Loss: 4.50e+02 logL: -3.99e+02 KL: 4.86e+01 MMD: 1.46e+00\n",
      "Stopping\n",
      "====> Epoch: 110 VALIDATION Loss: 4.50e+02 logL: -3.99e+02 KL: 4.86e+01 MMD: 1.46e+00\n",
      "config 96, alpha = 0.0, lambda = 43133.1, dropout = 0.00; 2 hidden layers with 193, 37 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.56e+03 logL: -5.00e+03 KL: 5.21e+00 MMD: 3.59e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.71e+03 logL: -4.45e+03 KL: 7.57e+00 MMD: 5.76e-03\n",
      "====> Epoch: 30 VALIDATION Loss: 4.52e+03 logL: -3.58e+03 KL: 1.22e+01 MMD: 2.14e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 4.08e+03 logL: -3.38e+03 KL: 1.26e+01 MMD: 1.61e-02\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.59e+03 logL: -3.33e+03 KL: 1.37e+01 MMD: 5.81e-03\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.08e+03 logL: -3.31e+03 KL: 1.41e+01 MMD: 1.75e-02\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.77e+03 logL: -3.31e+03 KL: 1.41e+01 MMD: 1.03e-02\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 76 VALIDATION Loss: 3.70e+03 logL: -3.31e+03 KL: 1.41e+01 MMD: 8.88e-03\n",
      "config 96, alpha = 0.0, lambda = 3151.1, dropout = 0.00; 2 hidden layers with 12, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.47e+03 logL: -5.76e+03 KL: 3.70e+02 MMD: 1.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.07e+03 logL: -5.49e+03 KL: 2.58e+02 MMD: 1.05e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.47e+03 logL: -5.06e+03 KL: 1.48e+02 MMD: 1.04e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.80e+03 logL: -4.23e+03 KL: 3.03e+01 MMD: 1.72e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.88e+03 logL: -3.72e+03 KL: 1.36e+01 MMD: 4.53e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.09e+03 logL: -2.86e+03 KL: 1.62e+01 MMD: 6.78e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 2.77e+03 logL: -2.65e+03 KL: 1.77e+01 MMD: 3.30e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 2.62e+03 logL: -2.53e+03 KL: 1.82e+01 MMD: 2.02e-02\n",
      "Stopping\n",
      "====> Epoch: 84 VALIDATION Loss: 2.67e+03 logL: -2.58e+03 KL: 1.77e+01 MMD: 2.26e-02\n",
      "config 96, alpha = 0.0, lambda = 17110.9, dropout = 0.00; 2 hidden layers with 14, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.60e+03 logL: -5.54e+03 KL: 6.03e+00 MMD: 6.18e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 4.95e+03 logL: -4.05e+03 KL: 1.03e+01 MMD: 5.19e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.13e+03 logL: -3.65e+03 KL: 1.21e+01 MMD: 2.73e-02\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 3.90e+03 logL: -3.41e+03 KL: 1.27e+01 MMD: 2.76e-02\n",
      "Stopping\n",
      "====> Epoch: 42 VALIDATION Loss: 3.89e+03 logL: -3.41e+03 KL: 1.25e+01 MMD: 2.75e-02\n",
      "config 96, alpha = 0.0, lambda = 29365.9, dropout = 0.00; 2 hidden layers with 111, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.69e+03 logL: -4.70e+03 KL: 4.14e+00 MMD: 3.34e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.65e+03 logL: -4.55e+03 KL: 4.90e+00 MMD: 3.75e-02\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.32e+03 logL: -4.51e+03 KL: 5.49e+00 MMD: 2.74e-02\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 5.71e+03 logL: -4.50e+03 KL: 5.63e+00 MMD: 4.08e-02\n",
      "Stopping\n",
      "====> Epoch: 40 VALIDATION Loss: 5.71e+03 logL: -4.50e+03 KL: 5.63e+00 MMD: 4.08e-02\n",
      "config 96, alpha = 0.0, lambda = 620.1, dropout = 0.00; 2 hidden layers with 132, 14 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 4.02e+03 logL: -2.69e+03 KL: 1.60e+02 MMD: 1.88e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.63e+03 logL: -1.36e+03 KL: 1.30e+02 MMD: 1.84e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.81e+03 logL: -1.43e+03 KL: 4.64e+01 MMD: 5.50e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.62e+03 logL: -1.40e+03 KL: 4.67e+01 MMD: 2.81e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.50e+03 logL: -1.31e+03 KL: 4.56e+01 MMD: 2.35e-01\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.45e+03 logL: -1.27e+03 KL: 4.52e+01 MMD: 2.17e-01\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.45e+03 logL: -1.27e+03 KL: 4.49e+01 MMD: 2.21e-01\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 1.45e+03 logL: -1.27e+03 KL: 4.49e+01 MMD: 2.26e-01\n",
      "config 97, alpha = 0.0, lambda = 75209.1, dropout = 0.00; 2 hidden layers with 7, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.21e+04 logL: -1.12e+04 KL: 1.27e+00 MMD: 1.23e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.45e+03 logL: -5.18e+03 KL: 3.84e+00 MMD: 3.64e-03\n",
      "====> Epoch: 30 VALIDATION Loss: 5.27e+03 logL: -4.71e+03 KL: 3.88e+00 MMD: 7.41e-03\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 5.55e+03 logL: -4.58e+03 KL: 4.14e+00 MMD: 1.28e-02\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.99e+03 logL: -4.58e+03 KL: 4.24e+00 MMD: 5.30e-03\n",
      "Stopping\n",
      "====> Epoch: 51 VALIDATION Loss: 5.59e+03 logL: -4.58e+03 KL: 4.24e+00 MMD: 1.34e-02\n",
      "config 97, alpha = 0.0, lambda = 18548.4, dropout = 0.00; 2 hidden layers with 42, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.51e+03 logL: -4.98e+03 KL: 3.77e+00 MMD: 2.82e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.15e+03 logL: -4.69e+03 KL: 4.78e+00 MMD: 2.44e-02\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 4.90e+03 logL: -4.58e+03 KL: 5.13e+00 MMD: 1.67e-02\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 39 VALIDATION Loss: 4.90e+03 logL: -4.56e+03 KL: 5.21e+00 MMD: 1.78e-02\n",
      "config 97, alpha = 0.0, lambda = 7304.1, dropout = 0.00; 2 hidden layers with 15, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.67e+03 logL: -6.13e+03 KL: 1.60e+01 MMD: 2.08e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.79e+03 logL: -5.41e+03 KL: 2.00e+01 MMD: 5.04e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.24e+03 logL: -3.94e+03 KL: 2.03e+01 MMD: 3.83e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.96e+03 logL: -3.73e+03 KL: 1.80e+01 MMD: 2.80e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 4.14e+03 logL: -3.61e+03 KL: 1.68e+01 MMD: 6.95e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 2.95e+03 logL: -2.66e+03 KL: 1.82e+01 MMD: 3.72e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 2.78e+03 logL: -2.57e+03 KL: 1.66e+01 MMD: 2.68e-02\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 2.74e+03 logL: -2.55e+03 KL: 1.72e+01 MMD: 2.50e-02\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 2.76e+03 logL: -2.55e+03 KL: 1.72e+01 MMD: 2.60e-02\n",
      "config 97, alpha = 0.0, lambda = 2308.4, dropout = 0.00; 2 hidden layers with 143, 65 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.62e+03 logL: -1.27e+03 KL: 3.72e+01 MMD: 1.34e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.19e+03 logL: -9.42e+02 KL: 3.80e+01 MMD: 9.28e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 1.08e+03 logL: -8.37e+02 KL: 3.69e+01 MMD: 9.07e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 9.92e+02 logL: -7.11e+02 KL: 3.75e+01 MMD: 1.05e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 8.62e+02 logL: -6.86e+02 KL: 3.63e+01 MMD: 6.08e-02\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 7.42e+02 logL: -6.24e+02 KL: 3.82e+01 MMD: 3.45e-02\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 7.53e+02 logL: -6.11e+02 KL: 3.81e+01 MMD: 4.49e-02\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 80 VALIDATION Loss: 7.54e+02 logL: -6.10e+02 KL: 3.82e+01 MMD: 4.61e-02\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 88 VALIDATION Loss: 7.57e+02 logL: -6.10e+02 KL: 3.82e+01 MMD: 4.70e-02\n",
      "config 97, alpha = 0.0, lambda = 23303.5, dropout = 0.00; 2 hidden layers with 130, 29 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.27e+03 logL: -4.86e+03 KL: 4.73e+00 MMD: 6.04e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 5.75e+03 logL: -4.26e+03 KL: 7.12e+00 MMD: 6.36e-02\n",
      "====> Epoch: 30 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 34 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "config 98, alpha = 0.0, lambda = 12745.2, dropout = 0.00; 2 hidden layers with 58, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.35e+03 logL: -3.92e+03 KL: 1.03e+01 MMD: 3.30e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.59e+03 logL: -3.51e+03 KL: 1.08e+01 MMD: 5.09e-03\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.53e+03 logL: -3.38e+03 KL: 1.20e+01 MMD: 1.07e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.45e+03 logL: -3.36e+03 KL: 1.18e+01 MMD: 6.03e-03\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 46 VALIDATION Loss: 3.46e+03 logL: -3.35e+03 KL: 1.20e+01 MMD: 7.86e-03\n",
      "config 98, alpha = 0.0, lambda = 5347.1, dropout = 0.00; 2 hidden layers with 183, 160 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.97e+03 logL: -2.00e+03 KL: 9.43e+01 MMD: 9.12e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 2.11e+03 logL: -1.93e+03 KL: 2.18e+01 MMD: 3.09e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.09e+03 logL: -1.88e+03 KL: 2.13e+01 MMD: 3.53e-02\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.86e+03 logL: -1.77e+03 KL: 2.12e+01 MMD: 1.42e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.86e+03 logL: -1.76e+03 KL: 2.12e+01 MMD: 1.60e-02\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.87e+03 logL: -1.74e+03 KL: 2.12e+01 MMD: 2.17e-02\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 66 VALIDATION Loss: 1.85e+03 logL: -1.73e+03 KL: 2.13e+01 MMD: 1.86e-02\n",
      "config 98, alpha = 0.0, lambda = 88915.0, dropout = 0.00; 2 hidden layers with 144, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.48e+03 logL: -6.11e+03 KL: 2.65e+00 MMD: 3.78e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 7.69e+03 logL: -4.80e+03 KL: 3.81e+00 MMD: 3.24e-02\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 6.88e+03 logL: -4.70e+03 KL: 4.39e+00 MMD: 2.45e-02\n",
      "Stopping\n",
      "====> Epoch: 33 VALIDATION Loss: 7.47e+03 logL: -4.68e+03 KL: 4.42e+00 MMD: 3.14e-02\n",
      "config 98, alpha = 0.0, lambda = 71.1, dropout = 0.00; 2 hidden layers with 133, 119 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.04e+03 logL: -8.19e+02 KL: 1.02e+02 MMD: 1.66e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.50e+02 logL: -6.75e+02 KL: 7.30e+01 MMD: 1.46e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.85e+02 logL: -6.29e+02 KL: 5.84e+01 MMD: 1.39e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.36e+02 logL: -6.18e+02 KL: 4.40e+01 MMD: 1.05e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.95e+02 logL: -6.13e+02 KL: 3.93e+01 MMD: 6.03e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 7.95e+02 logL: -7.39e+02 KL: 3.67e+01 MMD: 2.70e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 7.07e+02 logL: -6.56e+02 KL: 3.60e+01 MMD: 2.01e-01\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 6.33e+02 logL: -5.86e+02 KL: 3.57e+01 MMD: 1.58e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 6.33e+02 logL: -5.87e+02 KL: 3.55e+01 MMD: 1.50e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 6.29e+02 logL: -5.87e+02 KL: 3.50e+01 MMD: 1.04e-01\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 6.25e+02 logL: -5.82e+02 KL: 3.51e+01 MMD: 1.22e-01\n",
      "Epoch 00114: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 116 VALIDATION Loss: 6.25e+02 logL: -5.82e+02 KL: 3.52e+01 MMD: 1.15e-01\n",
      "config 98, alpha = 0.0, lambda = 5.1, dropout = 0.00; 2 hidden layers with 36, 36 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.09e+03 logL: -9.76e+02 KL: 1.06e+02 MMD: 1.88e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 20 VALIDATION Loss: 8.29e+02 logL: -7.50e+02 KL: 7.03e+01 MMD: 2.14e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.53e+02 logL: -5.82e+02 KL: 6.35e+01 MMD: 1.83e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.81e+02 logL: -5.17e+02 KL: 5.72e+01 MMD: 1.75e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.11e+02 logL: -4.49e+02 KL: 5.50e+01 MMD: 1.73e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.73e+02 logL: -4.16e+02 KL: 5.04e+01 MMD: 1.60e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.69e+02 logL: -4.15e+02 KL: 4.76e+01 MMD: 1.54e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.64e+02 logL: -4.12e+02 KL: 4.63e+01 MMD: 1.39e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.55e+02 logL: -4.06e+02 KL: 4.44e+01 MMD: 1.26e+00\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 4.50e+02 logL: -4.02e+02 KL: 4.34e+01 MMD: 1.24e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 4.49e+02 logL: -4.01e+02 KL: 4.32e+01 MMD: 1.25e+00\n",
      "Epoch 00116: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 4.48e+02 logL: -4.00e+02 KL: 4.31e+01 MMD: 1.32e+00\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 130 VALIDATION Loss: 4.48e+02 logL: -4.00e+02 KL: 4.30e+01 MMD: 1.23e+00\n",
      "Stopping\n",
      "====> Epoch: 132 VALIDATION Loss: 4.49e+02 logL: -4.00e+02 KL: 4.30e+01 MMD: 1.43e+00\n",
      "config 99, alpha = 0.0, lambda = 101.5, dropout = 0.00; 2 hidden layers with 9, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.88e+03 logL: -3.65e+03 KL: 1.55e+02 MMD: 7.18e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.77e+03 logL: -3.63e+03 KL: 6.29e+01 MMD: 7.42e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.74e+03 logL: -3.62e+03 KL: 3.92e+01 MMD: 7.88e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 3.73e+03 logL: -3.62e+03 KL: 2.85e+01 MMD: 7.64e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.71e+03 logL: -3.62e+03 KL: 2.11e+01 MMD: 7.12e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.69e+03 logL: -3.62e+03 KL: 1.56e+01 MMD: 5.31e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 3.65e+03 logL: -3.61e+03 KL: 1.22e+01 MMD: 2.97e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 3.64e+03 logL: -3.62e+03 KL: 1.09e+01 MMD: 1.42e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 3.62e+03 logL: -3.61e+03 KL: 1.00e+01 MMD: 8.72e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 3.62e+03 logL: -3.61e+03 KL: 9.94e+00 MMD: 8.96e-02\n",
      "Stopping\n",
      "====> Epoch: 100 VALIDATION Loss: 3.62e+03 logL: -3.61e+03 KL: 9.94e+00 MMD: 8.96e-02\n",
      "config 99, alpha = 0.0, lambda = 1.6, dropout = 0.00; 2 hidden layers with 94, 71 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.89e+03 logL: -1.83e+03 KL: 6.07e+01 MMD: 1.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.82e+03 logL: -1.77e+03 KL: 4.35e+01 MMD: 1.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.71e+03 logL: -1.67e+03 KL: 3.85e+01 MMD: 1.05e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.64e+03 logL: -1.60e+03 KL: 3.65e+01 MMD: 1.05e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.64e+03 logL: -1.60e+03 KL: 3.57e+01 MMD: 9.58e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.61e+03 logL: -1.58e+03 KL: 2.81e+01 MMD: 8.83e-01\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.55e+03 logL: -1.53e+03 KL: 2.89e+01 MMD: 8.20e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.55e+03 logL: -1.52e+03 KL: 2.73e+01 MMD: 9.09e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.54e+03 logL: -1.51e+03 KL: 2.72e+01 MMD: 8.54e-01\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 94 VALIDATION Loss: 1.54e+03 logL: -1.51e+03 KL: 2.68e+01 MMD: 7.98e-01\n",
      "config 99, alpha = 0.0, lambda = 42.4, dropout = 0.00; 2 hidden layers with 105, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.81e+03 logL: -3.65e+03 KL: 8.38e+01 MMD: 1.91e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.66e+03 logL: -2.52e+03 KL: 7.58e+01 MMD: 1.58e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.62e+03 logL: -2.50e+03 KL: 5.36e+01 MMD: 1.62e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.94e+03 logL: -1.83e+03 KL: 5.02e+01 MMD: 1.47e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.89e+03 logL: -1.80e+03 KL: 4.13e+01 MMD: 1.19e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.89e+03 logL: -1.81e+03 KL: 3.61e+01 MMD: 1.05e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.85e+03 logL: -1.77e+03 KL: 3.42e+01 MMD: 9.35e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.81e+03 logL: -1.75e+03 KL: 3.27e+01 MMD: 6.51e-01\n",
      "====> Epoch: 90 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 94 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "config 99, alpha = 0.0, lambda = 1.6, dropout = 0.00; 2 hidden layers with 44, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.72e+03 logL: -3.59e+03 KL: 1.33e+02 MMD: 1.89e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.73e+03 logL: -1.63e+03 KL: 1.01e+02 MMD: 1.68e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.24e+03 logL: -1.17e+03 KL: 7.34e+01 MMD: 1.61e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.01e+03 logL: -9.51e+02 KL: 6.08e+01 MMD: 1.68e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.92e+02 logL: -9.38e+02 KL: 5.36e+01 MMD: 1.67e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.03e+02 logL: -7.51e+02 KL: 5.19e+01 MMD: 1.55e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.02e+02 logL: -7.52e+02 KL: 4.87e+01 MMD: 1.51e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 7.96e+02 logL: -7.48e+02 KL: 4.69e+01 MMD: 1.48e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 7.16e+02 logL: -6.69e+02 KL: 4.62e+01 MMD: 1.52e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.46e+02 logL: -6.02e+02 KL: 4.32e+01 MMD: 1.41e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.53e+02 logL: -6.10e+02 KL: 4.19e+01 MMD: 1.35e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 6.48e+02 logL: -6.07e+02 KL: 3.98e+01 MMD: 1.20e+00\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 6.29e+02 logL: -5.89e+02 KL: 3.95e+01 MMD: 1.24e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 6.29e+02 logL: -5.89e+02 KL: 3.90e+01 MMD: 1.33e+00\n",
      "Stopping\n",
      "====> Epoch: 140 VALIDATION Loss: 6.29e+02 logL: -5.89e+02 KL: 3.90e+01 MMD: 1.33e+00\n",
      "config 99, alpha = 0.0, lambda = 41156.8, dropout = 0.00; 2 hidden layers with 132, 74 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.57e+03 logL: -2.52e+03 KL: 2.01e+01 MMD: 9.80e-02\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 3.46e+03 logL: -1.56e+03 KL: 1.97e+01 MMD: 4.56e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 4.09e+03 logL: -1.44e+03 KL: 2.30e+01 MMD: 6.39e-02\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 3.35e+03 logL: -1.39e+03 KL: 2.40e+01 MMD: 4.72e-02\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 47 VALIDATION Loss: 3.25e+03 logL: -1.38e+03 KL: 2.41e+01 MMD: 4.47e-02\n"
     ]
    }
   ],
   "source": [
    "for config, mdl_ncode in itertools.product(n_config_l,mdl_ncode_l):\n",
    "    alpha = 0\n",
    "    lambd = np.exp(np.random.uniform(0, np.log(1e5)))\n",
    "    dropout = 0#0.9*np.random.uniform()\n",
    "    dfac = 1./(1.-dropout)\n",
    "    nhidden = int(np.ceil(np.exp(np.random.uniform(np.log(dfac*mdl_ncode+1), np.log(dfac*2*nfeat)))))\n",
    "    nhidden2 = int(np.ceil(np.exp(np.random.uniform(np.log(dfac*mdl_ncode+1), np.log(nhidden)))))\n",
    "    print('config %i, alpha = %0.1f, lambda = %0.1f, dropout = %0.2f; 2 hidden layers with %i, %i nodes' % (config, alpha, lambd, dropout, nhidden, nhidden2))\n",
    "    model = InfoVAE(alpha=alpha, lambd=lambd, nfeat=nfeat, nhidden=nhidden, nhidden2=nhidden2, ncode=mdl_ncode, dropout=dropout)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5)\n",
    "    stopper = EarlyStopper(patience=10)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        valid_loss, valid_logL, valid_KLD, valid_MMD = train()\n",
    "        if epoch % log_interval == 0:\n",
    "            print('====> Epoch: {} VALIDATION Loss: {:.2e} logL: {:.2e} KL: {:.2e} MMD: {:.2e}'.format(\n",
    "                  epoch, valid_loss, valid_logL, valid_KLD, valid_MMD))\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "        if (not stopper.step(valid_loss)) or (epoch == epochs):\n",
    "            \n",
    "            if mdl_ncode == 2:\n",
    "                p_i = 0\n",
    "            elif mdl_ncode == 4:\n",
    "                p_i = 1\n",
    "            elif mdl_ncode == 6:\n",
    "                p_i = 2\n",
    "            elif mdl_ncode == 8:\n",
    "                p_i = 3\n",
    "            elif mdl_ncode == 10:\n",
    "                p_i = 4\n",
    "                \n",
    "            print('Stopping')\n",
    "            print('====> Epoch: {} VALIDATION Loss: {:.2e} logL: {:.2e} KL: {:.2e} MMD: {:.2e}'.format(\n",
    "                  epoch, valid_loss, valid_logL, valid_KLD, valid_MMD))\n",
    "            model.MSE = -valid_logL\n",
    "            model.KLD = valid_KLD\n",
    "            model.MMD = valid_MMD\n",
    "            mdl_MSE[config,p_i] = model.MSE\n",
    "            mdl_KLD[config,p_i] = model.KLD\n",
    "            mdl_MMD[config,p_i] = model.MMD\n",
    "            torch.save(model, tag +'/%i' % mdl_ncode +'/%04i.pth' % config  )\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676c17e5",
   "metadata": {},
   "source": [
    "# Saving Training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69553efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(tag+'/metrics.npz', MSE=mdl_MSE, KLD=mdl_KLD, MMD=mdl_MMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a785a7",
   "metadata": {},
   "source": [
    "# Identifying top models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2ef8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = np.load(tag+'/metrics.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17b5932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_models(tag):\n",
    "    metrics = np.load(tag+'/metrics.npz')\n",
    "    model_paths = []\n",
    "    for j in range(2,11,2):\n",
    "        if j == 2:\n",
    "            p_i = 0\n",
    "        elif j == 4:\n",
    "            p_i = 1\n",
    "        elif j == 6:\n",
    "            p_i = 2\n",
    "        elif j == 8:\n",
    "            p_i = 3\n",
    "        elif j == 10:\n",
    "            p_i = 4\n",
    "        order = np.argsort(metrics['MSE'][:,p_i])\n",
    "        i = order[0]\n",
    "        m = torch.load(tag +'/%i' % j +'/%04i.pth' % i)\n",
    "        model_paths.append(tag +'/%i' % j +'/%04i.pth' % i)\n",
    "        print('Dims %i: Model %04i MSE %0.3e KLD %0.2e MMD %0.2e lambda %0.2e nhidden %i %i' % (j, i, metrics['MSE'][i,p_i], metrics['KLD'][i,p_i], metrics['MMD'][i,p_i], m.lambd, m.encd.out_features, m.enc2.out_features))\n",
    "    return model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a3a689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims 2: Model 0048 MSE 2.876e+03 KLD 1.62e+01 MMD 2.12e-01 lambda 1.08e+02 nhidden 189 71\n",
      "Dims 4: Model 0067 MSE 1.447e+03 KLD 2.26e+01 MMD 6.69e-01 lambda 8.14e+00 nhidden 193 112\n",
      "Dims 6: Model 0051 MSE 8.936e+02 KLD 2.83e+01 MMD 2.52e-01 lambda 3.16e+01 nhidden 163 52\n",
      "Dims 8: Model 0098 MSE 5.819e+02 KLD 3.52e+01 MMD 1.15e-01 lambda 7.11e+01 nhidden 133 119\n",
      "Dims 10: Model 0074 MSE 3.913e+02 KLD 4.23e+01 MMD 1.36e+00 lambda 1.16e+00 nhidden 181 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/GRFx/2/0048.pth',\n",
       " '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/GRFx/4/0067.pth',\n",
       " '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/GRFx/6/0051.pth',\n",
       " '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/GRFx/8/0098.pth',\n",
       " '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/GRFx/10/0074.pth']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_top_models(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3ebfe",
   "metadata": {},
   "source": [
    "# Storing the top models in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98e79bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims 2: Model 0048 MSE 2.876e+03 KLD 1.62e+01 MMD 2.12e-01 lambda 1.08e+02 nhidden 189 71\n",
      "Dims 4: Model 0067 MSE 1.447e+03 KLD 2.26e+01 MMD 6.69e-01 lambda 8.14e+00 nhidden 193 112\n",
      "Dims 6: Model 0051 MSE 8.936e+02 KLD 2.83e+01 MMD 2.52e-01 lambda 3.16e+01 nhidden 163 52\n",
      "Dims 8: Model 0098 MSE 5.819e+02 KLD 3.52e+01 MMD 1.15e-01 lambda 7.11e+01 nhidden 133 119\n",
      "Dims 10: Model 0074 MSE 3.913e+02 KLD 4.23e+01 MMD 1.36e+00 lambda 1.16e+00 nhidden 181 20\n"
     ]
    }
   ],
   "source": [
    "npars = [2,4,6,8,10]\n",
    "models_npars = []\n",
    "models_paths = show_top_models(tag)\n",
    "for path in models_paths:\n",
    "    modelp = torch.load(path)\n",
    "    modelp.eval()\n",
    "    models_npars.append(modelp)\n",
    "models_npars.append(modelp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00bd5c9",
   "metadata": {},
   "source": [
    "# Reconstructing the dataset and calculating MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c33bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    MSE_VAE = np.zeros((1,len(npars)))\n",
    "    TSS_VAE = np.zeros((1,len(npars)))\n",
    "    R2 = np.zeros((1,len(npars)))\n",
    "    recon_VAE = np.zeros((len(npars), ML_GRF_stance_N_matrix.shape[0], ML_GRF_stance_N_matrix.shape[1]))\n",
    "    \n",
    "    for i in range(len(npars)):\n",
    "        modeli = models_npars[i]\n",
    "        _, mu_batch, _ = modeli(torch.tensor(ML_GRF_stance_N_matrix, dtype=torch.float32)) \n",
    "        recon_VAE[i,:,:] = modeli.decode(mu_batch).numpy()\n",
    "        MSE_VAE[0,i] = np.mean((recon_VAE[i,:,:] - ML_GRF_stance_N_matrix[:,:])**2)\n",
    "        TSS_VAE[0,i] = np.mean((ML_GRF_stance_N_matrix[:,:] - ML_GRF_stance_N_mean_array)**2)\n",
    "        R2[0,i] = (1-MSE_VAE[0,i]/TSS_VAE[0,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73237210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80739581, 0.90379728, 0.94076293, 0.96219426, 0.97483129]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c2327eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56.92618776, 28.4337218 , 17.50813845, 11.17388188,  7.43887719]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077bda1",
   "metadata": {},
   "source": [
    "# Reconstruction Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "321e2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLGRF_recon = []\n",
    "recon = []\n",
    "comb_fd = []\n",
    "orig = ML_GRF_stance_N_fd[10].data_matrix[0,:,:]\n",
    "for i in range(5):\n",
    "    MLGRF_recon.append(skfda.FDataGrid(data_matrix=recon_VAE[i,:,:],grid_points=grid_points_100))\n",
    "    recon.append(MLGRF_recon[i][10].data_matrix[0,:,:])\n",
    "    new = np.zeros((5,2,100,1))\n",
    "    new[i,0,:,:] = recon[i]\n",
    "    new[i,1,:,:] = orig\n",
    "    comb_temp = skfda.FDataGrid(data_matrix=new[i,:,:,:],grid_points=grid_points_100)\n",
    "    comb_fd.append(comb_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "077ad428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/pruthvibharadwaj/opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning:Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      " /Users/pruthvibharadwaj/opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning:Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      " /Users/pruthvibharadwaj/opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning:Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      " /Users/pruthvibharadwaj/opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning:Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      " /Users/pruthvibharadwaj/opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning:Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABNg0lEQVR4nO3dd3hUZfbA8e/JpAfSSAghIST0XkPHCriogKiIXbDXVXfd4jbXXcvPdV07FhQV7B2xrKsCIorSWwApAUJCAum9Z97fH3fQgAmEZJIpOZ/nyTPl3rn3jINz5r7lvGKMQSmllDqWj6sDUEop5Z40QSillGqQJgillFIN0gShlFKqQZoglFJKNUgThFJKqQZpglDKA4mIEZFejvvPicjfXB2T8j6aIJRHEpH9IlItIlHHPL/R8eWZ6Hj8iojc38gxRERuE5EtIlIuIodE5GsRuaTePl+LSKWIlIpIroh8ICKx9bbfKyI1ju1H/v7QhPgfEZHdIlIiIj+KyFXN/W9hjLnJGHNfc1+vVGM0QShPtg+49MgDERkMBJ/E658E7gTuAjoBccBfganH7HebMaYD0AvoADxyzPa3jTEd6v093IRzlwHTgTBgDvCEiIw/idiVanWaIJQnexWo/8t7DrCoKS8UkT7ALcAlxpgvjTEVxpg6Y8y3xpi5Db3GGFMILAaGtSDmI8f6uzHmR2OM3RizGlgJjDtOvL8XkSwRyRSRa47Z9tNVkoicLiIZIvIHEcl2vGamiJwjIrtEJF9E/lzvtaNFZJ2IFIvIYRF5tKXvTXkPTRDKk/0AhIpIfxGxAZcArzXxtWcC6caYdU09mYh0Ai4A9jRh38tEZEsTjxsEjAK2NbJ9KvA7YArQG5h8gkN2AQKxrojuAV4ArgBGAqcAfxORJMe+TwBPGGNCgZ7AO02JWbUPmiCUpztyFTEF2AEcbOLrooBD9Z9w/PIudPQ5dK+36UkRKQJyHa/79THHmu143ZG/rsaYN4wxQ5oYy3PAZuB/jWyfDbxsjEkxxpQB957geDXAA8aYGuAtR8xPGGNKjDHbgO3A0Hr79hKRKGNMqTHmhybGrNoBTRDK070KXAbMpYnNSw55QGz9J4wx8VhfpgGA1Nt0uzEmDBgCRADxxxzrHWNMeL2/zKYGISL/BgYBs03jlTO7Aun1Hqed4LB5xpg6x/0Kx+3hetsrsPpSAK4F+gA/ishaEZnW1NiV99MEoTyaMSYNq7P6HOCDk3jpMiBeRJJP4lxbgfuBeSIiJ9r/RETkH8DZwFnGmOLj7JoFdKv3OKGl5z7CGLPbGHMp0Bn4F/CeiIQ46/jKs2mCUN7gWuBMR/NLQ2wiEljvz98YsxN4HnhLRKaISJCjH+NEI4kWAjHAjJYELCJ/wrrymWyMyTvB7u8Ac0VkgIgEA39vybmPieMKEYk2xtiBQsfTdmcdX3k2TRDK4xljUk/Q2Xw3VrPKkb9ljudvxRrq+iiQD2QA9wEXAwcaOVc1VsfucSemicjlItJgp7PDg1hXAnvqzZ/4c0M7GmP+CzzuiHtPvfidYSqwTURKsd7XJcaYihO8RrUTogsGKaWUaoheQSillGqQJgillFIN0gShlFKqQZoglFJKNcjX1QE4S1RUlElMTHR1GEop5VHWr1+fa4yJbmibyxOEiIQDL2LNJjXANcBO4G0gEdiPNcu04HjHSUxMZN26JpfVUUopBYhIozPz3aGJ6Qngc2NMP6z6MDuwxq0vNcb0BpY6HiullGpDLk0QIhIGnAosAGsSkqOk8nlYM1Zx3M50RXxKKdWeufoKIgnIAV52rAT2oqMOTIwxJsuxzyGs0ga/ICI3OGrZr8vJyWmjkJVSqn1wdYLwBUYAzxpjhmOtsnVUc5KjwmWD072NMfONMcnGmOTo6Ab7WJRSSjWTqxNEBpDhWFEL4D2shHH4yLq/jttsF8WnlFLtlksThDHmEJAuIn0dT03CWsxkCdbykThuP3JBeEop1a65fJgr1upcr4uIP7AXuBorcb0jItdiLY4y24XxKaVUu+TyBGGM2QQ0tGjLpDYORTXV/m+h6CAMugBsfq6ORinVSlyeIJSH2f8dLJwOxg5b34XZi8A/2NVRKaVagas7qZUnyd4Bb10KkT3hzL/Cnq/g9VlQebzVMpVSnkqvIFTTFKbDa7PANxCueB8iukNEEnx4Izx/Coy+AQZeAKGxro5UKeUkegWhTqw4CxbNgKoSuPxdKzkADJ4FV3wAHWLgf3+GR/vDR7e6NlallNPoFYT6BbvdsD2rmG/35FKQtZ9rUm8ntDaP+yMeZN8nFdh8VhPdIYABXUMZEDuQwZd/SseSvfDdk7DxNRh+FSSMcfXbUEq1kCYI9ZOMgnKeXraHL7YfJr+smljyeCfwATpQxD8j7mNfYH9qa+3U2A27DpfwwcaDAIhA35iOjI6/ir/6LqZi1Ut0jB+Nj4+4+B0ppVpCE4SioKyaZ77ew8JVaSAwbXAsp/XswLmrLsW3rByu+Jj/6zbqF6/LKaliW2YRm9IL2XigkMUpBYyoG8KpOz5hzP3/Y2zvGKYNiWVSv8742rQ1UylPowminduSUcg1r6wjr6yKC0fE89spfegaHgSf/wnyd1l9DA0kB4DojgGc3rczp/ftDFhNU9lrioj8/HquiT/IglRfPt6cSUxoABcnd+Pi0QnEhQe15dtTSrWAJoh27Mvth7n9zY106uDPJ7+eyMCuYdaG1OXwwzMw6nro1fT5ij4+QpcR02BpMDd33sb1c65m+c4cXl+dxlPL9/D08j1MGRDDrWf0Ykh8eOu8KeW5yvJg/UtQeACCO1l/CeMhfqSrI2u3xCqW6vmSk5ONrijXdK/+kMbfP0phUFwYC+aMIrpjgLWhPB+eHQ8BHeGGFc2bBPfOVXDgB/jtj+BjNS2l55fz1toDvPp9GsWVtZzWJ5o/Tu3HgK6hTnxXyuMYAzk7Ye0LsPF1qK2AkM5QUQD2GhAfOO1uOPV34GNzdbReSUTWG2MaqmahCaI9WrErhzkvrWFSv848ddlwgv19rdIZu7+wRiFlbYbrl0Ls0OadYOt78P61cPXn0H3cUZtKKmt47YcDzP8mlcKKGi4Z1Y3fTun7c4JS7UPaKtjyDuxZCkUHwOYPQ2bDuF9D534UllWx/se9JK6/n56Zn5AZOZrt4/7DmMH96Rio5V2cSROE+kl2cSVnP7GSqA4BfHTbBAJ9feCD662yGQChcXDGX2D45c0/SVUJPNwTkq+Bsx9qcJei8hqeXLabhav2E+hn449n9+Py0Qk68snb2e2w8j+w/AHwD4Eep0OvSdT1PpvNRYF8syuHFbty2JxeiN0AGC6yreCfvq8gGL4yyezofC79J5zHtOEJLn4z3kEThAKgzm644sXVbEovZMltE+gd0xH2rrAmwSVfA6NvhOi+1rjVlnrjEji0FX6Tctzj7c0p5Z6PtvHtnlzG9ojk4QuHktBJazt5papSWHwT7PgYBs+G6U9QVOfHI//byZLNmRRV1CACQ+PDObVPNKf2jiIxKgQBbPl7qFr1LB13f0RwXTH77DF8N/gBLp81C3HGv9d2TBOEAuCJr3bz2Fe7+PesIVyU3M16ctFMyN4Od2wBv0DnnWzTG7D4Zrhu2Qk7GY0xvL02nQc+3YEIPH3ZCE7toysEepXMTVZZltxdMOWfMO42vtqRzV8WbyWnpIrzhsVxRr/OnNIriogQ/8aPU1tF3Y7PKF5yN6HVh1kafSWnX/8I/gHaRNlcx0sQOji9ndifW8aTy3Yzc1hXZo2Mt57M3Ah7l8PYW5ybHAD6ng0+vrDjxGs9iQiXjE7gsztOoWt4EHNfXsOLK/fiLT9e2rW6WljxMLw4CSoK4YoPyB96I3e8vYnrFq0jItifxbdO4LGLhzFjaNfjJwcA3wBsg88n/Ler2RlzLmflLmLfo5Oora5sk7fT3miCaCeeWLobP5vw53P7/3xJ/u3jEBBmNS85W1AEJJ0G2z+yRqo0QbfIYN6/eTxTBsRw/6c7+POHKdjtmiQ8VkEavHKu1d8wYCbmlu9ZUtqXsx5bwadbsrhzcm+W3DaxWUOeJSicAbe8zneD/knfqq2se+X3zo9faYJoD3YfLmHxpoPMGZdI546OK4W8VOvLe9S1ENhKQ00HzICC/VZfRBOFBPjy7OUjuem0nry55gD3LEnRKwlXqiiEkkMn/7qt78FzE+HwNjh/PgcnP801b+/h9jc3EhcexMe/nsidk/vg79uyr6AJs+5gdcR0Rh98lQ0rlrToWOqXdKJcO/D4V7sJ9rNx42k9f37yu8etoYVjb269E/ebBp/8BnYsgdghTX6Zj4/wx6l9MRieX7EXP5sP90wboJ2RbW33V/D+NdaotEn3wIQ7fx5wUHLIWlkQwL+DNV+mpsJKKLs+h20fQPxo6s6fz8Id8MijKwD427QBzB2fiM2Jo9WGXvcMWf9ZT+zyO8nonUx8165OO3Z7pwnCy23PLObTrVn8+sxeRB5p362ptMagD70EOnRuvZOHREH3CbB9ibXA0EkQEe6e2o/qWjsvf7efAF8bf5zaV5NEW7DbYeUjsPxBiOoNscPgq3utGc6T7oFVT8P386xJbQ3xDYLT7iZ7xK+5+Y0trE8r4PS+0dw/cxDxEc4foRYYEorPrBeJemc63y+8ia5//EiHSzuJJggv99hXu+gY6Mt1E3v8/GT6aqithL7ntH4A/WfAf38P2T9C534n9VIR4Z5pA6ips/PcilT8fX347ZQ+rRSooqrEanZc/wpkrIUhF8O0x6wv/GX/hG8fg3UvWfsOmgXjfw1+QVBdCtXl1v3AcAiN5cf8Oq59dg35ZdU8dvFQZg6La9XkHjtgAtv73MSpu+bxw8r/Mva0Nvi33Q5ogvBiaXllfLn9MHdO7k1YcL3Zp/tWgNig+/jWD6L/dCtB7Fhy0gkCrCTxzxmDqK618+TS3QT4+nDrGb1aIdB2zG6Hr/8PfngWqkusJWVnPAXDr/y5SWnyvRAzyPpxMfQSiGt86PLyH7O57Y0NdAj05d2bxjEoLqxN3kbfC/5E4UOv4rfyYcypZ+vVphO4PEGIyH6gBKgDao0xySISCbwNJAL7gdnGmAJXxeipFm/MRARmH5nzcMTeFdb/4K3VOV1faCx0G2M1M532h2YdwsdH+L8LhlBTZ/j3/3YS4OvDdaf0OPEL1YnZ62DJ7bDpNRgwE8bdCvGjGp7cOHiW9Xccr3y3j39+sp3+saEsmDOKLmFOHj59HLbAjqT3v56RO/7D2pWfM+rUs9vs3N7KXUYxnWGMGVZvssbdwFJjTG9gqeOxOgnGGBZvOsjYpE5W+e4jKosgcwP0OK3tguk/Aw5vhfy9zT6EzUf496whnDs4lvs/3cGHGzOcGGA7VVdjlVnZ9JpVEO+iV6Db6GbNpK+ts/P3j1K49+PtTOofwzs3jmvT5HBE/xm/IZ8w/FY+pKPfnMBdEsSxzgMWOu4vBGa6LhTPtDmjiH25ZcwcfsyIjrRVYOzWHIW20n+6dbttcYsO42vz4dGLhzKuRyd+/+4WVu7OaXls7VVtFbw7F1Let5qPzvhTs0usVNbUcfPrG1j4fRo3nNqD564YSUiAaxonfIM6cqD/9Qyr2cTG7/7rkhi8iTskCAN8ISLrReQGx3Mxxpgsx/1DQExDLxSRG0RknYisy8nRL4v6Fm88iL+vD1MHxR69Ye8K8A20mhHaSkR3SBgHGxZZ7d0tEOBr4/mrRtKrcwduenU9KQeLnBRkO1JTAW9dDj9+Amc/DBN/0+xDlVXVcvXLa/ly+2H+MWMgfz6nv1OHsDbHgBm/IY9wfFfoVURLuUOCmGiMGQGcDdwqIqfW32isT7jBT9kYM98Yk2yMSY6O1to9R9TU2fl4cyaT+3cmLOiY0sj7VkDCWOeX1jiRUddBwT7Yu6zFhwoN9GPhNaMJD/bn6lfWkp5f7oQA24m6Gis57PkKpj8BY25s9qGqauu44dV1rNmfz+MXD2PO+ETnxdkC/kEd2NfnGobUbCZ16/euDsejuTxBGGMOOm6zgQ+B0cBhEYkFcNxmuy5Cz/Pt7lzyyqqZOSzu6A2l2VZhvrZsXjqi/3QIiYa1C5xyuJjQQBZeM4rqWjtzXlpDQVm1U47r1YyBJb+G1KUw40kYObfZh6qts3PHm5v4bk8eD184hJnD4078ojbUe+rNVBh/ir55ztWheDSXJggRCRGRjkfuA2cBKcASYI5jtznAiSu+qZ8s3nSQ8GC/n9aK/sm+b6zbtuygPsI3AEZcZc2yLTzglEP26tyRF+ckk1FQwY2vrqeqts4px/Vay+6HzW/C6X+2PotmstsNf/pgK59vO8Q90wZw4ZHij24kLLIzG8MmMSD3c6pLdQBkc7n6CiIG+FZENgNrgE+NMZ8DDwFTRGQ3MNnxWDVBRXUd/9t2iHMGx/6yzs2+FVZxvthhLomNkVdbt+tfcdohRyVG8sjsoazZn88f3tuibc6NWfeSNTt6xJxmDzcGa3TcA5/t4N31GdwxqTfXTExyYpDO5Tf2eoKoIvWrF10disdyaYIwxuw1xgx1/A00xjzgeD7PGDPJGNPbGDPZGJPvyjg9ydr9+VTW2DlrwDH9+sZYHdSJE123tm94N+hzNqxfaI2icZIZQ7vy+1/15aNNmTz21W6nHddr/PgZfHoX9D4Lzn20RQtCPb1sDwu+3cfc8YncObm3E4N0vuFjziCF3oRvW9TkisLqaK6+glBOtio1D18fYXRS5NEb9n8LhWnQd6prAjti1LVQnmtNnHOiW07vyezkeJ5cupv31usciZ+kr4X3rrHWF5/1MtiaP/x00ff7+c+Xu7hgRJxHFE/0tfmwN/FiYmsOUPzjcleH45E0QXiZ7/fmMaxbOMH+x3wRfD8PgjvB4ItcE9gRPc6AyB6w1rmX/SLCA+cPZkKvTvzpgy2sSs116vE9Ul4qvHkxdIyBy96FgA7NPtTijQe556NtTO4fw8MXDvGYYnh9Jl1FgelA/vJ5rg7FI2mC8CLFlTVszShkfM9OR2/I3QO7/msNNfULavjFbcXHx4oj/Qc4lOLUQ/vZfHjm8pEkdgrhplfXsye7xKnH9yilOfDahdb9Kz6ADs0fBr7sx8Pc9e5mxvXoxNOXDcfX5jlfG/26xbAs6Cy6ZS+D4qwTv0AdxXM+aXVCa/bmYzcwrmfU0RtWP2ut/TDqOtcEdqyhl4ItANa/7PRDhwX58dLcUfj7+nD1K2vJLXVeX4fHKM+HNy6y1my47B3o1PPEr2lEysEibntjI/1jO/LCnGQC/VzUf9UCdSOuxoadgm9fcHUoHkcThBdZlZpHgK8PwxPCf36yPB82vQGDZ7fu2g8nIzgSBp5vrUlRXeb0w3eLDObFOaPIKani+kXrqKxpR8Nf8/fCi5Ph8HartlJ8g2vRN8mhokquXbiW8CA/Xpozig4uKp/RUuOSk/m6bij+mxdZEwVVk2mC8CLf780jOTHi6F9561+BmnIYd4vL4mrQyLlQVWzVAmoFw7qF8/jFw9mUXshv39nUPta2zlgPL06Biny46qMWDUgoq6rl2oVrKauqY8HcUXQObfvCe87SLTKYZR2nE1KVAzs+dnU4HkUThJfIL6tmR1Yx43rU63+oq4U186HH6RAz0GWxNShhLET3c+qciGNNHdSFv5zTn8+2HuKBz3a02nncwo+fwSvngn8IXPsldB/X7EPV2Q13vLWRHw+V8PRlw+kf2wZl4VtZh8HnsN90oXbV0zrk9SRogvASP+zNA47pf0j7FkqyIPkaF0V1HCLWxLmD6yFrS6ud5tqJScwdn8iCb/fx4srmlxt3a2tegLcvh8794bql1jKhzWSM4R8fb+OrHdncO2PgL2fje6hJA2JZUDsV38z1kL7G1eF4DE0QXmJVai4h/jaGxNdbvWv7R+AXYk2QckdDL7Yqy66Z32qnEBH+Nm0AZw/qwgOf7eDTLV40ksVuhy/+Cp/9DvpMhbmftGi0EsALK/ey6Ps0bjy1B1eO7e6kQF1vWLcIlgZMpsynI3z/tKvD8RiaILzEqtQ8RidF4ndkCKK9zmpv7XOW64e2NiYowir9sOkNyG29GdA2H+Gxi4cxMiGC37y9idWOqy2PVlMJ718Dq56CUdfDxa9ZzUst8PHmTB787EemDYnlj1NPfnlYd2bzEcb1S+Bt+5mYHz+BMp0n0xSaILxAdkkle3PKGFd//kPaKijLsZaRdGen/t5KYF/d26qnCfSz8eKcZLpFBnH9onXsPuxBcyQK0+Hrf8GOT6C63BqZ9upM2PYhTLkPzvl3i8unrN6bx13vbGZ0YiSPXDTUYybCnYzJ/TvzRfVgxNghc5Orw/EImiC8wJFFc4YnRPz85PbF4Bfsvs1LR3SIhgl3WIvXHFjdqqcKD/bnlatHE+BnY85La8gqqmjV8znFvm/guYnw9YNWP8PDPeDxIVbfzayXYcLtLaqtBLAnu4TrF62jW2QQ868a6ZFzHZrilD7R7BLHWuZZG10bjIfQBOEFUg4WI8LPo03sdVato95TwD/YtcE1xbhboUMMfPX3Vh9h0i0ymJfnjqK4spY5L62hqNyNx8VvfQ9evcD6b3PrWrhqiVWmO3GiNYx10AUtPkV2SSVzXlqLv6+NV662FmHyVh0CfBnYI54MiYWsza4OxyNogvACKQeLSOoU8vNEpv3fQlk2DLrQtYE1lX8InH43HPgedrb+OsKD4sKYf+VI9ueWc9XLbpok1rwA718H3UbDtV9AdB9rHY9zHobL3oLu41t8itKqWq55ZS35ZdW8NDeZbpEe8GOihSb168zG2u7UZugVRFNogvAC2zKLGdC13lj1lPfBv4P7Ny/VN/wq6NTb6ouoq231043vFcXTlw1ne2YRl734g+tWpMvdY1VctTtmexsDy//PGpnU92y44n0ICnf6aUurapn70hp2ZJUw7/LhDIl3/jnc0cTe0aTYk/AtybD6ctRxaYLwcAVl1RwsrGBQnGN4a2017FgC/c5139FLDbH5wuS/Q+7OVqnR1JCzBnZh/pXJ7M4u5dIXfiCnpI3qNlWXwbePw7MT4emRsGAyPDEMvnkEProVVjwEw66A2a+2ymdYUlnDnJfWsDG9kKcuHc6Z/WJO/CIv0TM6hIygPtYDbWY6IU0QHm57VjEAA49cQez9GioKPKd5qb5+06xZ30vvs9bPbgNn9OvMy3NHsT+vjEvmf8/h4srWPaHdDu9cZfW3+AXC1H/BBS9AZCIsuw82vW512p/3dIvWbmhMcWUNV720hs3phTx96XDOGRzr9HO4MxEhoodVn8quI5lOSBOEh9uWaY1gGnCkgzrlfQgMt9Zd8DQicM5/oLbCmgDWRib0imLRNWM4VFTJ7Oe/52BhK45u2vwG7PkKznkErvsKxt4EQ2bDnI/h9o1wyw8w5Z8tHpnUkOLKGq5asIatGUU8fdkIzm5nyeGIoX2SOGCPpnTfOleH4vY0QXi4HVkldAkNpFOHAKipgB8/hQEzwNdDR6NE9bJ+QW95G/atbLPTjk6K5NXrxpBfVs3s577nQF65809SUQBf3gPdxkLytb/cHtnDKpfRCooqarhywRq2ZRbxzOUjmDqoS6ucxxNM6BXFVpOkTUxNoAnCw23PLKZ/bEfrwe4vobrEM5uX6jvlLgjvbrXHt2FH4oiECN68fixl1bXMfv579uaUOvcEy+63ksS5j1gLJ7WRovIarlqwmu2ZRTx7+UjOGth+kwNA1/AgDgb1JbQi3fo8VKM0QXiwypo69uSU/jyCKeU9COkMiae4NrCW8guCCxdAcaY11NPedus5DIoL460bxlJTZ2f28z/8NAmxxTLWwdoFVlmMLoOdc8wmSM8v54Jnv2NHVgnPXTGSyQPaT4f08fjGjwCg7uAm1wbi5twiQYiITUQ2isgnjsdJIrJaRPaIyNsi4qHtJa1rT3YpdXbDgNgwqCqBXf+DgTNbXHbBLXQbZZWQSF0Kyx9o01P36xLK2zeOI8DXh4uf/55vduW07IB7lsKbl0BoHJzxZ+cE2QSb0gs5/5nvyC2t5tVrRzOpvyaHI7oNtMqhZ+343sWRuDe3SBDAHUD9gv3/Ah4zxvQCCoAGGmzV9kxrBFP/2I7WBLPaSs9vXqov+Wpr5vDK/7T5Qi+9Onfgg1vG0y0ymLkvr+Hxr3ZRd7KLDlWVwKd3wWsXQFAkXPlBq8xpaMg7a9OZ/fz3BPrZeP/m8Yypv06IYmS/nqTZO1ORph3Vx+PyBCEi8cC5wIuOxwKcCbzn2GUhMNMlwbm57VnFBPvb6N4pxBq9FBoP8aNdHZZznfMIxI2ED2+CnJ1teuqY0EDev3k8M4fH8fhXu7n8xR+aNgy2uhy+ewKeGAprX4Sxt8KNKyC6b6vHXFlTx58+2MIf3t/CqMQIPrp1Ar06d2j183qayBB/0gL6ElqwzdWhuDWXJwjgceAPgN3xuBNQaIw5Mp02A4hr6IUicoOIrBORdTk5LWwG8EDbs4rp16UjtsoCqxlj0Plt2vnZJnwDrAljvoHw1uVQWdympw8J8OXR2cN45KKhbE4v4pwnVvL1zmxr0ZnDDXy5bH3PSgxf3gOxw+C6ZTD1wTaZtJieX87s57/nzTXp3HJ6TxZdM8Ya3aYaVBMzhJi6Q1QUtr/vjqZy6beJiEwDso0x65vzemPMfGNMsjEmOTq6ZQuleBpjDDuyiq0CfetfBnsNDL3M1WG1jrA4mL0Q8vdaVxJ2+4lf42SzRsbz8a8nEN0xgLcWzoMFU+DZ8bDasdhRbZXVnPT+tRDRHa7+3GpSih/Z6rEZY3hj9QGmPv4N+3LKeP7Kkfxhaj9sXliy25kie1tX2/tTvnNxJO7L+VM1T84EYIaInAMEAqHAE0C4iPg6riLigYMujNEtZRRUUFJZy6AuQbDqBWtiXMwAV4fVehInwln3w//+BMvvhzP/1vhksvS1cGAVjLnZqfNBenXuyOKbx1D66K3kVIaR6tuLsf/9vVVksGA/ZG6AcbfB5HvB5ue08x5Pak4p9y7ZxsrduUzsFcW/Zg0hLtyDSqy4UNKgCbAcivasgYkzXR2OW3JpgjDG/An4E4CInA78zhhzuYi8C8wC3gLmAB+5KkZ3daTExrhKx7rT0590cURtYOzNcDjF6rQ+vA3OewZCjul8PbjBWkynuhR+/My68ujoGPdvt0NdVYuaewJT3iKw6gA7Tn+O3/8QxVXVr3D9tg8w/h2Q2a9akxTbQEllDU8t28NL3+4jyN/GfecN5PIx3b1yoZ/WEt4pmgzpil+2TphrjKuvIBrzR+AtEbkf2AgscHE8bmdHVjEihm67XraqoPaa7OqQWp8InDcPYodapTiemwDTn4BeU6y+l9zd8PosCI60fsV/eQ88fyqc9gc4uBF2f2FNjBo5x5qMF9q14fNUlUBAx18+X1MBXz8E8aPof/olfDquln9+3JnTN5yB3SeK8w/249K4SrqEBbbaf4Ly6lreX5/Bk8v2kFtaxeyR3fj91L5EaV9DsxwOHURC0RqM3Y54W/+dE7hNgjDGfA187bi/F/Cy4TjOtT2zmGnh6diyNsG5//G+zunGiMCYGyFhHLx3DbwxGyISrUJ/W98F8YErF0OnntB9Arx9hdU3EBhmJVG/IFj/Cmx4FZKvsY4VmWSV2U77Dlb8y1rFLaybNeGwx2kQPwrC4mH1c1CSCRe+ACKEBvrxyEVDWT0ynhdW7uWJpbt5atluzuzXmUtHJ3Ban2h8bc75XNLzy3l99QHeXHOAoooaRnaP4MWrkhnaLdwpx2+vTLcxRBd9QdreHXTvNdDV4bgdMa28gldbSU5ONuvWtZ8xzaf9ezmPyWOMqN0Mv93e4gXrPVJNpVXafOOr1pd6QCjM/cS6wjiiqhTy9kDMoJ+roxbshxX/hs1vgqmDsAQIjrBq84R0huGXQ/4+65gV9Ut9iFVG/ZLXGwwnLa+Mt9em8866DHJLq+gSGsjM4XGMSAhnUFwYsWGBSBOL8GUXV7J2fwFr9+ezdn8+2zKL8RGYOqgL105MYkRCRJOPpRp3YPtqEt45i9XDHmTMzFtdHY5LiMh6Y0xyg9s0QXie8upapt/7Ml/6/wGfCbfDlH+4OiTXK822hsQGhjX9NQX7YdcXsH8lFKXDsMth+BU/91HY7XB4q9XfUZRhjVQad6vVhHUcNXV2lu7I5q21B/hmVw5H5td1CvFnYFwYPaJC6BoeiK+PDyIgQFWtnYLyGg4XV7I+rYAD+VaxwCA/G8MTwpnYO4oZQ7sSH+H9q761JXttLWX3x7Mt8izG3r7I1eG4hCYIL7MlvYCS+ecyOvAAfrevh45aQsFdVVTXsT2rmG2ZRWzNKGJbZjFpeWWUVf+yvpSfTYjqEMDQ+HCSEyMYlRjJgK6h+DmpmUo1bOtDk+hQnUPSPVtcHYpLHC9BuE0fhGq6ivVvMcG2jdzx/0eUJge3FuRvY2T3CEZ2j/jpOWMMpVW12O1gNwYD+Pv6EOJv02YjFyiLSWbg/ucpys8lLDLK1eG4Ff1p4mkqChiY8i82m55EnHKDq6NRzSAidAz0IyzYj4gQfyJD/OkQ4KvJwUU69J6Ajxj2b/7a1aG4HU0QnmbpPwmuLeLF8Duw+eoFoFItlTT0VGqND+WpOqP6WJogPEnublj3Mm/7nItf3NAT76+UOqGQjuHs900iNLtZFX+8miYIT7L3a8Awr2ISfWMamMillGqW3MjhJFX9SG11latDcSuaIDzJgR+oDu5ChommTxdNEEo5i637OIKliv3bV7s6FLeiCcKTHFxHVsfBgOgVhFJOFDPoNACKdmk/RH2aIDxFZREU7GeXTw86BvgS24r1fpRqb+ITepFJFH4H17g6FLeiCcJTOBanWVcZT58uHXVIpFJO5OMj7A8cSExJiqtDcSuaIDzFoa0ALC/sTB9tXlLK6SqiBxNjz6a6ONfVobgNTRCe4tAW7EGd2FXRgb4xusawUs4W1N1a/S9zx/cujsR9aILwFIdSKArrD4iOYFKqFcT3HwtAUepaF0fiPjRBeIK6GsjeQXpATwAdwaRUK+jWNZYDxGA7vMnVobgNTRDuoq7GWn+goeq6ubuhroptdQlEdfCnk64eppTTiQgZgf2ILtnh6lDchiYId/Hd47BwurUs5rEcHdTflXXVDmqlWlGldlQfRROEu9iz1Lrd+Novtx3agrEFsCIvTBOEUq0oUDuqj6IJwh3Y6+Cgo1DYzs+g5PDR2w+nUNOpLyXV0Fc7qJVqNdpRfTRNEO6gJAvqqmHcbWCvtdZYPsIYOLSVnJA+AHoFoVQr+rmjerOrQ3ELLk0QIhIoImtEZLOIbBORfzieTxKR1SKyR0TeFhF/V8bZ6grSrNtekyDxFNiw0FoPGazkUZ5Hqq0HAH10DoRSrUZEOBjUl+iS7a4OxS24+gqiCjjTGDMUGAZMFZGxwL+Ax4wxvYAC4FrXhdgGCvZbtxGJkHw1FB6A1GXWczs/A2BtTRJx4UF0DPRzSYhKtRcVUUO0o9rhhAlCRL6od/9Pzjy5sZQ6Hvo5/gxwJvCe4/mFwExnntftFKaB+EBYN+g3HUI6w+d/hPmnw6e/g/jRfFkYp1cPSrUBnVH9s6ZcQUTXu3+RswMQEZuIbAKygS+BVKDQGFPr2CUDiGvktTeIyDoRWZeTk+Ps0NpOwX4IjQebH/j6w/THoTAdbP5wxl+ouWgRe3PLdQa1Um0gboB2VB/RlEWNG5i55TzGmDpgmIiEAx8C/U7itfOB+QDJycmtGmerKtgPEd1/ftzvXPjrYXBUbE3LLqG6zq4zqJVqA91irY5qH+2oblKC6CEiSwCpd/8nxpgZzgjEGFMoIsuBcUC4iPg6riLigYPOOIfbKkiD3pOPfq5eOe+dh6xWOB3BpFTrO9JR3UM7qpuUIM6rd/8RZ55cRKKBGkdyCAKmYHVQLwdmAW8Bc4CPnHlet1JTAaWHIDyx0V12Hi7BR6BXZ+2DUKotVEQNJib9G2pKcvHrGOXqcFzmhAnCGLOisW0iMqGF548FFoqIDas/5B1jzCcish14S0TuBzYCC1p4HvdVeMC6jUhsdJfdh0vo3imEQD9b28SkVDsXkDAC0q2O6u6jp7s6HJc5YYJwfHnPxuoo/twYkyIi04A/A0HA8Oae3BizpaHXG2P2AqObe1yPUn+IayNSc0rpGa1XD0q1ldi+Y+E7KN67FjRBHNcCoBuwBnhSRDKBZOBuY8ziVoytffgpQXRvcHNtnZ39ueWc0a9z28WkVDvXPT6OAyYG26H23VHdlASRDAwxxthFJBA4BPQ0xuS1bmjtREEa+AVDSHSDmzMKKqius+sVhFJtyOYjpAf2pXc7L/3dlHkQ1cYYO4AxphLYq8nBiQr2Q3j3o0Yt1ZeaY41g0gShVNsq6zSIznWHsZe236+7piSIfiKyxfG3td7jrSKypbUD9HqFaSfsfwDopQlCqTbl320EAId3/uDiSFynKU1M/Vs9ivbKGOsKInFio7ukZpcR1SGAsGCtwaRUW+rcdzSshsLUtcSOPNfV4bhEU4a5prVFIO1SeR5Ulx73CmJPTik9o0PaLialFAC9ErqRZmKQrE2uDsVlmlKs71oR+X29xwdFpFhESkTkptYNz8sdKfPdSIIwxrAnu5SeOkFOqTbn7+tDWkAfOhW33xnVTemDuAl4qd7jbGNMKFYRv0tbJar2omCfdRve8BDX/LJqiipqtINaKRcpiRhIdN1hTFn77KhuSoKQY0YtvQs/jWgKapWo2ou8VEAgMqnBzak5ZQDaxKSUi/g5OqrzU9e4OBLXaEqCCK//wBjzIICI+ADtt0iJM+TuhPAE8Gs4z+oQV6Vcq3Mfq6BD/m5NEI35wlET6Vj/BL5o4HnVVLm7IKpPo5tTs0sJ8PUhLlwv1JRyhd7du5FmOkPmJleH4hJNSRC/B3o61od+3/G3B+gF/K51w/Ni9jrI3XP8BJFTSo/oDvj4NDyJTinVukICfNnn15uIovbZUd2UYa5lwKUi0gMY6Hh6uzEmtf5+IjLQGLOtFWL0Tnl7oLYCugxudJfUnDKGdgtvu5iUUr9QFD6IqNzvoDwfgiNdHU6basoVBGBVWDXGfOz4S21gl1edGJf3O3LJGju0wc2VNXWkF5RrB7VSLmaLGwY4Kru2M01OEE2g7SAnI2sz+AY12sS0L7cMY7SDWilXi+ozBoC8Pe2vo9qZCcJz14R2haxN0GUQ2Bpu5dMRTEq5h35J3cg0kdRktb8WdGcmCNVUdjtkbWm0eQmsGkwikBSlTUxKuVJ4sD8HbN0JKtzt6lDaXIsShIh0rfewuoWxtB8F+6C6BGKHNbpLak4pceFBBPnrMqNKuVpxx150rjpgjT5sR1p6BfFTHVxjzNgWHqv9yNxo3R7vCkKXGVXKbUjn/gRQTfnhPa4OpU21NEFox3RzZG0Gmz9E92tws91u2JtTpglCKTcR3t0ajp65a4OLI2lbLU0Q2jHdHFmbIWYg+Po3vLm4koqaOnppFVel3EJC35EAFB/Y6uJI2tYJJ8qJyFM0nAiEY+o0nSwR6QYsAmIc55hvjHlCRCKBt4FEYD8w2xhT0JJzuQ1jrAQxcGaju6RmWyOYeugcCKXcQueoSDKJRnJ+dHUobaopK8qta+a2pqgF7jLGbBCRjsB6EfkSmAssNcY8JCJ3A3cDf2zhudxDYRpUFh63gzotz6riqiOYlHIPIkJ2YBLhZQ3NEfZeTSm1sbC1Tm6MyQKyHPdLRGQHEAecB5zu2G0h8DXekiBOMIMa4EB+OQG+PkR3CGibmJRSJ1QZ0ZcBmRuoqq4iwL99/L/ZlCamJcfbboyZ4YxARCQRGA6sBmIcyQPgEFYTVEOvuQG4ASAhIcEZYbS+A99bM6hjBja+S345CZHBWqRPKTcS0HUA/lm17NqdQp+BI10dTptoShPTOCAdeBPry9vp31oi0gF4H7jTGFMs8vMpjDFGRBrsDDfGzAfmAyQnJ3tGh3nqMug+Hnwb/wWSlmclCKWU+4jpORzWQ07qxnaTIJoyiqkL8GdgEPAEMAXINcasMMasaGkAIuKHlRxeN8Z84Hj6sIjEOrbHAtktPY9byFhnrQHR+6xGdzHGkJ5fTkInTRBKuZMuPYdgN0JVZvsp/X3CBGGMqTPGfG6MmQOMBfYAX4vIbS09uViXCguAHcaYR+ttWgLMcdyfA3zU0nO5heUPQnAnGH5Fo7vklVVTVl2nVxBKuRmfgBCyfbsQULDT1aG0maY0MSEiAcC5wKVYQ0+fBD50wvknAFcCW0Vkk+O5PwMPAe+IyLVAGjDbCedyrfQ1kLoUJv8DAhqf33AgvxxAE4RSbqiwQ09iCvdTZzfY2kEfYVM6qRdhNS99BvzDGJPirJMbY76l8T6NSc46j1tY/iAER8Ho64+724E8K0F01yYmpdxPdH+6F37PvsP59Irt5OpoWl1T+iCuAHoDdwCrRKTY8VciIsWtG56XSPse9i6HCXeA//HnNhy5goiP0AShlLvpmDAYP6njwO72MaO6KX0QPsaYjo6/0Hp/HY0xoW0RpMcyBvYshcU3QWgcjLr2hC9JyyunS2gggX5axVUpd9O55zAACvdvcW0gbaRJfRDqJNjtVjnvAz/A2hesyq2hcXDRKye8egCsEUza/6CUW/Lr3Jc6fCBnh6tDaROaIJxpy7vwyW+stR4AOvWGaY/DsMuOO++hvrT8Mk7pHd16MSqlms8vkHz/OEJLUjHGUH/OljfSBOEsdTXw5T0Q0R3G3Ahdhlh/Pk0vmFtZU8fh4iq9glDKjVVE9CEpazsZBRV08/L/V3XJUWfJ2gIlmXDKXTDiKug67KSSA1jNS6AjmJRyZ/5dBpAoh9iRnuPqUFqdJghnyXdUeTxOjaUTOTKCydt/lSjlySKThmATw6G93t9RrQnCWfJSAYGIxGYfIu3IHAhNEEq5Lf9Y60dgxcFtLo6k9WmCcJb8VAjr1uTO6IYcyC8nxN9GZEjDK80ppdxAp17UYcM/3/tLbmiCcJa8VOjUo0WHOJBfTkKnEK8fGaGUR/MNoDg4gbiaNHJKqlwdTavSBOEMxlhXEJ16tegw1joQQU4KSinVWuxR/eglB9mWWeTqUFqVJghnKM+DyiKI7NnsQ9jtVpnv7p10mVGl3F1I/EC6y2F+zPDukUyaIJwhzzGCqVPzE0R2SRVVtXYdwaSUBwjsOgCbGPLSvLujWhOEMxwZ4tqCJqYjQ1x1BJNSHiC6v3V7yGnFrd2SJghnyNsDYoPw5q+LnZZXBug6EEp5hOi+VNlCSCxPobiyxtXRtBpNEM6Ql2qV2LD5NfsQ6fnl+AjERWgntVJuz8dGaedkRvn8yPZM7131QBOEMzhhBFNafjldw4Pws+lHopQn8O9zJn18DpKx23tnVOu3UUsZA3l7WzSCCaw+CK3BpJTn6Jh8CbX4EL7rXVeH0mo0QbRUySGoKWvRCCawlhrV/gelPEjHLqQEjWJo/udgr3N1NK1CE0RL5bd8iGtpVS15ZdU6xFUpD3Og20yiTR7Vu5e5OpRWoQmipfL2WLctaGI68FORPp0kp5QnCRh4LoUmhNLVi1wdSqtweYIQkZdEJFtEUuo9FykiX4rIbsdthCtjPK68VLD5Q1h8sw9xZA6ENjEp5VmGJcXwUd14Qvf/DyoKXR2O07k8QQCvAFOPee5uYKkxpjew1PHYPeXvhcge4GNr9iEO5DvmQGgntVIeJSY0kBVBU/C1V8G2D10djtO5PEEYY74B8o95+jxgoeP+QmBmW8Z0UvL2OGUEU1iQH2FBzZ9HoZRyjaDuyeyVbrDpDVeH4nQuTxCNiDHGZDnuHwJiGtpJRG4QkXUisi4nxwVFs+x1kL+vxWW+0/J0iKtSnmp49wjerD4FMtZA7m5Xh+NU7pogfmKMMYBpZNt8Y0yyMSY5Ojq6jSMDijKgrqrFk+TS88t1BJNSHmp4QjiL6yZgF5vXXUW4a4I4LCKxAI7bbBfH07DcXdZtp97NPkRtnZ2Mggot0qeUhxrYNYxCWyR7w8bC5re8ak6EuyaIJcAcx/05wEcujKVxhxxT7GMGNvsQWUWV1NqNjmBSykMF+tkYEBvKx3I6lGTCvhWuDslpXJ4gRORN4Hugr4hkiMi1wEPAFBHZDUx2PHY/h1IgLAGCwpt9iJ+GuGofhFIea3hCBC/n9scEhntVM5OvqwMwxlzayKZJbRpIcxxOgS6DWnQInQOhlOcb26MTr6zaz+Hu0+my411rhcnAsJ93qKuBVU9ZlRdO/YNV/dkDuPwKwmPVVFhDXGNaniD8bEJsmJb5VspTje/VCZuP8KX/JKithA31ZlaXHIKF02HpP2Dja/D8qbDrC9cFexI0QTRX9nYw9pZfQeSVEx8RjM1HnBSYUqqthQb6MbxbOO9mRkGvKfDF3+Dp0fD+9fDcKZC1GS5cAL/eAGHd4I2LYOk/rR+abkwTRHMdWWrQCVcQ2ryklOc7tU80WzOLKZg6D874C0Qkwr5vrOak65bC4FlWUc/rvoThV8DK/8DjQ+Dbx6GqxNXhN8jlfRAe63AK+HeAiKQWHSYtr4xh3cKdE5NSymVO6R3Fo1/uYuXBOmac9vvGd/QLgvPmwdDLYOUj8NXfrWSRMA7iR0G/c1o0MtKZ9AqiuQ6lWB+iT/P/ExaV11BcWauzqJXyAkPiwwkL8uObXU2s6pA4Aa78EK5fBgNnQmEaLL8fnp0A2xa3ZqhNplcQzWEMHN5mXTK2QJqjSJ/OolbK89l8hIm9oli5OwdjDCJN7FeMG2n9AZTlweuzYMmvIXYoRLashaKl9AqiOQrToKpIh7gqpY5yap8oDhdXsetwafMOENIJLnoFEHjvGqittn6QbvsQ3rjk577PNqJXEM2Rtdm6jR3aosPsy7GuILSJSSnvcEpvqybcyt059O3SsXkHiegO5z0N71wJn/7GGia75ytrW1E63PA12Nqm8rNeQTRH1hYQG3RuWUfS3twyuoYFEuyveVopb9A1PIhenTuwoqn9EI0ZMANGXWfNmzjwA5z9MMx+1Roc893jTom1KfSbqb7dX1ptfzd+Ax06N75f1mbo3B/8Alt0ur25ZfSI7tCiYyil3MupvaN5fXUaZVW1hAS04Cv2rAcgqq81qunIipUDz4cVD0P/GRDd1zkBH4deQdS3+nkoyYItbx9/v6zN0GVIi05ljGFvTilJUboOtVLe5NwhsVTV2vl4c2bLDuQXCGNuOHo547MfBv8Q64es3d6y4zeBJogjSg5D6lLr/qY3rY6hBvc7BGXZLe5/yC2tpqSylh7RmiCU8iYjEsLpE9OBN9cccP7BO3SGqQ9B+mrY9Jrzj38MTRBHpLxnlc4YfSNkb/u5lPex9q20buNGtOh0e3OsUQ7axKSUdxERLh2dwOaMIrZlFjn/BEMuhq7Drcl1dbXOP349miCO2PwmdB0Bp98NPn6w5Z2G99vyllVLJS65Rafbl2uNYOqhTUxKeZ3zh8cR4OvDW2vSnX9wETjld1CwH7Z94Pzj16MJAqxJb4e2wtBLIDgSek2GlPd/uTJUySFIXWZl8BbMoAarg9rf14eu4VrFVSlvEx7sz7mDY1m88SDl1c3/lZ9bWsW+3DJ2Hiphe2YxlTWO76S+50B0f+sqohX7InQUE1jLBPr4wqALrceDZ8Gu/0LaKkg65ef9tjqaoYZe0uJT7s0pJalTiFZxVcpLXTomgQ82HmTJpkwuGZ3Q5NdlF1fy8ZYslmw6yOaMo5uoAnx9SE6MYHzPKOaOuYOQT26CnZ9C/+nODh/QBGFdJWx91yrRGxJlPdf3bPALsZ6vnyA2v2VNiY9q/hrUR+zNKWv+RBqllNtL7h7BoLhQ5n29hxnDup5wvlNRRQ2PfbmLV39Io85uGBQXyh+m9iU2LJAAXxt2Y9iQVsiq1Fz+/b+dvBMexReh3Qn45hHoN81qenIyTRD7VlhDW6f+38/P+YdAv3Nh+2L41QMQ0NGa4n54K5zzSItPWVNn50B+OVMHdWnxsZRS7klE+PPZ/bliwWp+/cZGnr9yJL62XzZN19bZeXNtOo9/uYv88mouHZ3ANROS6NX5lwNYpg3pCsDGAwXc/NoG7iv8FfcXz7dGYPaa7PT3oH0QZbnQqRf0Ofvo50fOtWq0v3Cm1T+x+U2r83rgBS0+ZXp+ObV2oyOYlPJy43tF8Y/zBrH0x2z+9lEKpt7weWMMX24/zK8e/4a/LU6hZ+cOfHzbRB48f3CDyaG+4QkRfPzriaR2nc4me0++2pTaKvHrFcSQ2TD4ol9eniVOgCsXw/vXwfzTweZvzWgM6dTiU+511GDSORBKeb8rx3bnUFEF85anUlheQ/dOIVTW1LEts4i1+wvoER3C/CtHMmVATNMrwALRHQNYdP1EHvvidS4d0zprXGuCgMbb7nqcBreuhs/vhpQPYPwdTjmdDnFtXE1NDRkZGVRWVro6FHUcgYGBxMfH4+fXNkXjPN3vzupLbZ1hwbf78BEhwM+HyBB/7jtvIJeMTsCvgaanpvCz+fCHs/s7OdqfuW2CEJGpwBOADXjRGPNQW57fGENFTR3BwZFwwXyY/oS1EpQT7M0tJTLEn/Bgf6ccz5tkZGTQsWNHEhMTT+rXlGo7xhjy8vLIyMggKcm16xV4ChHhT+f05+6z+3nUv2u3TBAiYgPmAVOADGCtiCwxxmx39rnsdkNxZQ0F5TUUlFeTnl/Ot7tzWbk7l/yyaj6/8xSrr8BJyQEgNadMrx4aUVlZqcnBzYkInTp1IienhRVL2yFP+3ftlgkCGA3sMcbsBRCRt4DzAKcniJe+28f9n+446rmwID/G9ejEF9sPsXjjQX57lnOrJqZmlzK5f4xTj+lNPO1/ovZIP6P2wV0TRBxQf456BjDm2J1E5AbgBoCEhKZPRKlvbI9O/PXc/kQE+xMR4kfnjoH0jw3F5iNc9sIPLNmcyW+m9HHa/xB5pVXklVXTO0ZHMCml3JtHD3M1xsw3xiQbY5Kjo6ObdYxBcWFcd0oPLhwZz5n9YhgUF/bT7ObzhnVlf145WzKcV3DryFKEfWJ0kpy7stlsDBs2jEGDBjF9+nQKCwtdFsvXX3/NqlWrnHa8xYsXs337yV+Id+igP2jaI3dNEAeBbvUexzuea1NTB8biZxM+25rltGPuyS4B0CsINxYUFMSmTZtISUkhMjKSefPmuSyW4yWI2tqTr/HT3ASh2id3bWJaC/QWkSSsxHAJcFlbBxEW7MeEXlF8ujXLaaMPdh0upWOAL11CW7YaXXvwj4+3sT2z2KnHHNA1lL9Pb/pSsePGjWPLFqv0e2pqKrfeeis5OTkEBwfzwgsv0K9fPw4fPsxNN93E3r17AXj22WcZP348jz76KC+99BIA1113HXfeeSf79+/n7LPPZuLEiaxatYq4uDg++ugjgoKCePLJJ3nuuefw9fVlwIABPPTQQzz33HPYbDZee+01nnrqKRYsWEBgYCAbN25kwoQJhIaG0qFDB373u98BMGjQID755BMSExNZtGgRjzzyCCLCkCFDuPnmm1myZAkrVqzg/vvv5/333wdo8D3t27ePyy67jNLSUs477zxnfgTKg7hlgjDG1IrIbcD/sIa5vmSM2eaKWM4ZFMsf3t9CysFiBseHtfh4uw6X0Dumg3byeYC6ujqWLl3KtddeC8ANN9zAc889R+/evVm9ejW33HILy5Yt4/bbb+e0007jww8/pK6ujtLSUtavX8/LL7/M6tWrMcYwZswYTjvtNCIiIti9ezdvvvkmL7zwArNnz+b999/niiuu4KGHHmLfvn0EBARQWFhIeHg4N91001EJYMGCBWRkZLBq1SpsNhv33ntvg7Fv27aN+++/n1WrVhEVFUV+fj6RkZHMmDGDadOmMWvWLAAmTZrU4Hu64447uPnmm7nqqqtcegWlXMstEwSAMeYz4DNXx3HWwBj+/KHw6dYspySIPTqCqclO5pe+M1VUVDBs2DAOHjxI//79mTJlCqWlpaxatYqLLrrop/2qqqoAWLZsGYsWLQKs/ouwsDC+/fZbzj//fEJCrOHMF1xwAStXrmTGjBkkJSUxbNgwAEaOHMn+/fsBGDJkCJdffjkzZ85k5syZjcZ30UUXYbPZjvseli1bxkUXXURUlFWAMjIy8hf7HO89fffddz9dYVx55ZX88Y9/PO75lHdy1z4ItxEe7M+4np34b0rWUXVUmuNQUSV5ZdVaxdXNHemDSEtLwxjDvHnzsNvthIeHs2nTpp/+duzYceKDNSAgIOCn+zab7ae+hE8//ZRbb72VDRs2MGrUqEb7GI4kHQBfX1/s9dYDOJkZ6Cd6T3qVqzRBNMG5g2NJyytnY3phi46zPq0AgJHdI5wQlWptwcHBPPnkk/znP/8hODiYpKQk3n33XcCaTbx582bAaqZ59tlnAatZqqioiFNOOYXFixdTXl5OWVkZH374Iaecckqj57Lb7aSnp3PGGWfwr3/9i6KiIkpLS+nYsSMlJSWNvi4xMZENGzYAsGHDBvbt2wfAmWeeybvvvkteXh4A+fn5AEcdLzQ0tNH3NGHCBN566y0AXn/99Wb811PeQBNEE0wb2pXQQF9e+GZvi46zLi2fQD8fBnQNdVJkqrUNHz6cIUOG8Oabb/L666+zYMEChg4dysCBA/noo48AeOKJJ1i+fDmDBw9m5MiRbN++nREjRjB37lxGjx7NmDFjuO666xg+fHij56mrq+OKK65g8ODBDB8+nNtvv53w8HCmT5/Ohx9+yLBhw1i5cuUvXnfhhReSn5/PwIEDefrpp+nTpw8AAwcO5C9/+QunnXYaQ4cO5be//S0Al1xyCf/+978ZPnw4qampx31P8+bNY/DgwRw82OYDCJWbkJY2m7iL5ORks27dulY7/iP/28m8r/fw1W9Po2czy3TPePpbgvxsvH3jOCdH5z127NhB//6tV3xMOY9+Vt5BRNYbY5Ib2qZXEE00d0Ii/jafZl9FlFfXsi2zmOREbV5SSnkGTRBNFNUhgIuS4/lgw0EOF598KerN6UXU2Y32PyilPIYmiJNwwyk9qbXbWfDtvpN+7YYDVgf1iARNEEopz6AJ4iQkdApmxtCuvPp9GjklVSf12nX78+nduYOuAaGU8hiaIE7SHZP7UF1n59mvm74GrN1u2HCgUJuXlFIeRRPESUqKCmHWiHhe+yGN/Y6lQ08kNaeUoooaTRBKKY+iCaIZ7jqrD3424YHPmjaTdp1OkPMoGRkZnHfeefTu3ZuePXtyxx13UF1d/Yv9MjMzf6ppdDznnHNOs0uG33vvvTzyyCPNeq1SLaUJohk6hwZy25m9+XL7YVbuPvGyi1/vzCY2LJAkXWbU7RljuOCCC5g5cya7d+9m165dlJaW8pe//OWo/Wpra+natSvvvffeCY/52WefER4e3koRK9V63LZYn7u7ZmIib645wD8/3s5/7zgFX1vDubasqpaVu3M5f3ic1rY5WevvhIJNzj1mxDAY+Xijm5ctW0ZgYCBXX301YNVKeuyxx0hKSiIpKYnPP/+c0tJS6urqWLhwIdOmTSMlJYXy8nLmzp1LSkoKffv2JTMzk3nz5pGcnExiYiLr1q2jtLS00VLfL7zwAvPnz6e6uppevXrx6quvEhwc7Nz3rtRJ0iuIZgrwtfGXc/uzO7uUR7/c1eh+z3+zl/LqOmaNjG/D6FRzbdu2jZEjRx71XGhoKAkJCdTW1rJhwwbee+89VqxYcdQ+zzzzDBEREWzfvp377ruP9evXN3j83bt3c+utt7Jt2zbCw8N/qph6wQUXsHbtWjZv3kz//v1ZsGBB67xBpU6CXkG0wK8GduHS0d145utUBseFcfbg2KO2ZxVVMP+bVKYP7cpwnf9w8o7zS99VpkyZ0mDp7G+//ZY77rgDsBbtGTJkSIOvb6zUd0pKCn/9618pLCyktLSUX/3qV60Sv1InQ68gWujeGQMZ1i2cu97dzI+Hjl797OHPd2I38MepfV0UnTpZAwYM+MWv/+LiYg4cOICvr+9Rpbabo7FS33PnzuXpp59m69at/P3vfz+pst1KtRZNEC0U4GvjuStG0iHAl2tfWcf6tAIqa+rYeKCADzce5LqJScRHaFuyp5g0aRLl5eU/LQBUV1fHXXfdxdy5c4/bJzBhwgTeeecdALZv387WrVtP6rwlJSXExsZSU1Oj5bWV29AE4QRdwgJ54apkiipquPDZVfS/53Mumf8DMaEB3HJGL1eHp06CiPDhhx/y7rvv0rt3b/r06UNgYCAPPvjgcV93yy23kJOTw4ABA/jrX//KwIEDCQtr+gqE9913H2PGjGHChAn069evpW9DKafQct9OVFBWzXepuezJLiW7pIq54xPpE6Orx50MTy0hXVdXR01NDYGBgaSmpjJ58mR27tyJv7/3llbx1M9KHe145b61k9qJIkL8mTakq6vDUC5QXl7OGWecQU1NDcYYnnnmGa9ODqp90AShlBN07NgRV1/BKuVsLuuDEJGLRGSbiNhFJPmYbX8SkT0islNEdLxfO+MtzZ7eTD+j9sGVndQpwAXAN/WfFJEBwCXAQGAq8IyI2No+POUKgYGB5OXl6ReQGzPGkJeXR2BgoKtDUa3MZU1MxpgdQEPlJ84D3jLGVAH7RGQPMBr4vm0jVK4QHx9PRkYGOTknrnGlXCcwMJD4eK0O4O3csQ8iDvih3uMMx3O/ICI3ADcAJCQktH5kqtX5+fmRlJTk6jCUUrRyghCRr4AuDWz6izHmo5Ye3xgzH5gP1jDXlh5PKaXUz1o1QRhjJjfjZQeBbvUexzueU0op1YbccSb1EuASEQkQkSSgN7DGxTEppVS747KZ1CJyPvAUEA0UApuMMb9ybPsLcA1QC9xpjPlvE46XA6SdRAhRQO5Jhu0N2uP7bo/vGdrn+26P7xla9r67G2OiG9rgNaU2TpaIrGtserk3a4/vuz2+Z2if77s9vmdovfftjk1MSiml3IAmCKWUUg1qzwlivqsDcJH2+L7b43uG9vm+2+N7hlZ63+22D0IppdTxtecrCKWUUsehCUIppVSD2mWCEJGpjlLie0TkblfH0xpEpJuILBeR7Y6y6nc4no8UkS9FZLfjNsLVsTqbiNhEZKOIfOJ4nCQiqx2f99si4nUr+YhIuIi8JyI/isgOERnXTj7r3zj+faeIyJsiEuhtn7eIvCQi2SKSUu+5Bj9bsTzpeO9bRGRES87d7hKEo3T4POBsYABwqaPEuLepBe4yxgwAxgK3Ot7n3cBSY0xvYKnjsbe5A9hR7/G/gMeMMb2AAuBal0TVup4APjfG9AOGYr1/r/6sRSQOuB1INsYMAmxYSwV42+f9CtbSB/U19tmejVV9ojdWIdNnW3LidpcgsEqH7zHG7DXGVANvYZUY9yrGmCxjzAbH/RKsL4w4rPe60LHbQmCmSwJsJSISD5wLvOh4LMCZwHuOXbzxPYcBpwILAIwx1caYQrz8s3bwBYJExBcIBrLwss/bGPMNkH/M0419tucBi4zlByBcRGKbe+72mCDigPR6jxstJ+4tRCQRGA6sBmKMMVmOTYeAGFfF1UoeB/4A2B2POwGFxphax2Nv/LyTgBzgZUfT2osiEoKXf9bGmIPAI8ABrMRQBKzH+z9vaPyzder3W3tMEO2KiHQA3seqaVVcf5uxxjh7zThnEZkGZBtj1rs6ljbmC4wAnjXGDAfKOKY5yds+awBHu/t5WAmyKxDCL5tivF5rfrbtMUG0m3LiIuKHlRxeN8Z84Hj68JFLTsdttqviawUTgBkish+r6fBMrLb5cEcTBHjn550BZBhjVjsev4eVMLz5swaYDOwzxuQYY2qAD7D+DXj75w2Nf7ZO/X5rjwliLdDbMdLBH6tTa4mLY3I6R9v7AmCHMebRepuWAHMc9+cALV64yV0YY/5kjIk3xiRifa7LjDGXA8uBWY7dvOo9AxhjDgHpItLX8dQkYDte/Fk7HADGikiw49/7kfft1Z+3Q2Of7RLgKsdoprFAUb2mqJPWLmdSi8g5WG3VNuAlY8wDro3I+URkIrAS2MrP7fF/xuqHeAdIwCqPPtsYc2wHmMcTkdOB3xljpolID6wrikhgI3CFY81zryEiw7A65v2BvcDVWD8AvfqzFpF/ABdjjdrbCFyH1ebuNZ+3iLwJnI5V0vsw8HdgMQ18to5E+TRWU1s5cLUxZl2zz90eE4RSSqkTa49NTEoppZpAE4RSSqkGaYJQSinVIE0QSimlGqQJQimlVIM0QSh1kkSkk4hscvwdEpGDjvulIvKMq+NTyll0mKtSLSAi9wKlxphHXB2LUs6mVxBKOYmInF5vDYp7RWShiKwUkTQRuUBEHhaRrSLyuaMMCiIyUkRWiMh6EflfSypvKuVsmiCUaj09sepBzQBeA5YbYwYDFcC5jiTxFDDLGDMSeAnwuln9ynP5nngXpVQz/dcYUyMiW7HKunzueH4rkAj0BQYBX1oVErBhla1Wyi1oglCq9VQBGGPsIlJjfu7ws2P9vyfANmPMOFcFqNTxaBOTUq6zE4gWkXFglWcXkYEujkmpn2iCUMpFHEvezgL+JSKbgU3AeJcGpVQ9OsxVKaVUg/QKQimlVIM0QSillGqQJgillFIN0gShlFKqQZoglFJKNUgThFJKqQZpglBKKdWg/wfY4HJRh8K4+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEWCAYAAACOv5f1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABMrElEQVR4nO3dd3hUVfrA8e/JpJGQQhokhBJCSygJEDpKt9JEwAIIlrW3Xd3fuqu7sq667qprbyAqWFBBKSJWmvQOAUKHQCZAKklIT2bO7487SKgJkMlMZt7P88wzM3fuvfNeJrxz59z3nKO01gghhHAvHo4OQAghRN2T5C+EEG5Ikr8QQrghSf5CCOGGJPkLIYQbkuQvhBBuSJK/EE5GKdVSKaWVUp625z8opSY5Oi7hWiT5C6eklEpVSpUrpcLOWr7Flhhb2p5/opR6/gL7UEqph5VSyUqpYqXUcaXUMqXUrVXWWaaUKlVKFSqlspVS3yqlIqu8PkUpVWF7/dTt/y7hOEKUUllKqZWX/I9go7W+Xms943K3F+J8JPkLZ3YIuO3UE6VUJ8DvErZ/E3gceAIIBZoCzwDXnbXew1rrhkBroCHwylmvf6W1bljl9t9LiOE/wK5LWF+IOiHJXzizT4E7qjyfBMysyYZKqbbAg8CtWutftNYlWmuL1nql1nry+bbRWucB84DEK4i5agx9gI7Ax9WsZ1JKvWL75XEQuPGs15cppe6xPZ6slFqllHpNKZWnlDqolOpjW56mlMqs2kSklLpBKZWilDqplEpXSj1ZG8cm6j9J/sKZrQUClVJxSikTcCvwWQ23HQSkaa031vTNlFKhwGhgfw3WvV0plXyR103A28DDQHVjqPwBGAZ0AZKAMdWs3xNIxvg18wXwJdAd45fLBOBtpVRD27rTgfu01gEYX0RLqtm3cBOS/IWzO3X2PxSj+SS9htuFAcerLlBKmW1ny6VKqRZVXnpTKZUPZNu2e+SsfY2zbXfqFqW1/kJr3fki7/8osE5rvakGsY4DXtdap2mtc4F/V7P+Ia31x1prC/AV0Ax4TmtdprX+GSjH+CIAqADilVKBWusTWuvNNYhHuAFJ/sLZfQrcDkymhk0+NjlAZNUFWutojOTuA6gqLz2qtQ4COgONgOiz9vW11jq4yu3oxd5YKRWFkfyfrmGsUUBaleeHq1k/o8rjEgCt9dnLTp353wzcABxWSi1XSvWuYUzCxUnyF05Na30Y48LvDcC3l7DpEiBaKZV0Ce+1HXgeeEcppapb/yJ6YHzxpCiljgNvAD1s1Uam86x/DOPs/ZTmV/DeZ9Bab9BajwQiMK5nfF1b+xb1myR/UR/cDQzSWhdd4HWTUsq3ys1ba70H+AD4Uik1VCnVwJZ4+1TzXjOAxsCIK4j3B6AlxoXjROAfwBYg0dZUc7avgUeVUtFKqUbAU1fw3r9TSnkrpcYrpYK01hVAAWCtjX2L+k+Sv3B6WusD1Vy4fQqjqePU7dRFzYcwyj3/B+QCZuBfwC3AkQu8VznGmfrfLxaTLanuvMA+yrTWx0/dgHygwvb4fKYBPwHbgM1c2i+c6kwEUpVSBcD9wPha3Leox5RM5iKEEO5HzvyFEMINSfIXQgg3JMlfCCHckCR/IYRwQ572fgOlVDDwIUbXcg3cBezB6JnYEkgFxmmtT1xsP2FhYbply5Z2jFQIIVzPpk2bsrXW4Wcvt3u1j1JqBrBCa/2hUsobY1TGvwG5WuuXlFJPAY201n+52H6SkpL0xo01HqZFCCEEoJTapLU+p7OjXZt9lFJBwNUYg0uhtS63jZw4EqMzDbb7UfaMQwghxJns3eYfA2QBH9sm4fhQKeUPNNZaH7OtcxyjR+U5lFL3KqU2KqU2ZmVl2TlUIYRwH/ZO/p5AV+A9rXUXoIizuq5ro93pvG1PWuupWuskrXVSePg5TVZCCCEuk72Tvxkwa63X2Z7PwfgyyDg1VZ7tPtPOcQghhKjCrsnfNpZJmlKqnW3RYCAFWIAxKxO2+/n2jEMIIcSZ7F7qiTExxue2Sp+DwJ0YXzpfK6Xuxhi7fFwdxCGEEMLG7slfa70VY2q6sw2293sLIYQ4P+nhK85ktcLGj+H4dkdHIoSwI0n+4ky/vQwLH4f3+8H+Xx0djRDCTiT5i9MOLIFl/4bWQ6FxJ/hqIpilV7UQrkiSvzDkp8M390B4exg3AyZ8Aw0bw+dj4MhaR0cnhKhlkvwFlBfDV+OhsgzGzQRvfwhoDBPngpc/fHQtTB0IGz+CynJHRyuEqAWS/N2d1QrzH4SjW2H0NAhve/q1kBh4aC1c95LxxbDwj7D0eYeFKoSoPZL83d3iKbBzLgz9J7S/4dzXfQKg1wPwwCqIGwGbZxpfBEKIek2SvzvbMB1WvQFJd0OfRy++rlKQOB5KTsDh1XUTnxDCbiT5u6v0TbDoSWhzLVz/XyO5VyfmKjB5SwmouCKVFitZJ8vYn3mSknKLo8NxW3UxvINwNlYrfP8k+EfAzR+CqYZ/Bt7+0Ly3URIqRA1YrZrFuzPZbs5j1/GT7Dl+krQTxZyaQ8rkoWjXOIDE5sGMTIiiR0wIqiYnIuKKSfJ3R1s/g6Ob4aap4Bt4adu2HgK//N0oDQ1qap/4hEtIyy3midnbWH8oFw8FrcIbMiiikOj2/vg1akKDhoEcyCximzmP77Ye5Yt1R0hoFsx9V7fi2g5NMHnIl4A9SfJ3N0U58Ms/jDP4zpcxnl7rwUbyP7AYut5R+/GJek9rzeyNZv753U6UUvz35s6MbJKNz/LnjSbDVNuKPkEw4Cm46wFKK63M2WRm2oqDPPj5ZkZ3bcqrYxPkV4AdSfJ3N4unQGkB3Pi/mrXzny0iHgIiYb8kf3Gu7eZ8nlu4kw2pJ+gbE8AbPfIJ2/cMfD8PfINh0N8hoAkUZkLqSvjpr3AiFd/r/s2EXi24rUdzXvtlL28v3U9seEMeGtja0YfksiT5u4OSPKOcM2U+HFwKfR6BxvGXty+ljLP/Xd+BpbLm1wuES8svqeA/P+5m1vojtGpQwpLYBcRkL0XNLwCfQLjqSePvrkHw6Y36Pm78ilzzNuQdgZvex9QgmCeuacvh3GJe/mkPseH+XNcx0lGH5dLkf66rO74dpl8LFUUQEgtXPWH8R7wSsYNhy2dGxVDznrUTp6iXtNYs2n6cKd/tJKewjMe6+/NI+rOYMtKg480QPxJaDQBPn3M39vCAa1+ARi3hh/+D/8VD4u2oHvfy8pjOpOUW88evthHdyI+OTYPq+tBcniR/V7f2PeNs/Q9LIKrr5TX1nK3VAFAeRru/JH+3lXWyjKe+SWbx7kw6NQ3i85vCafvTBKMvyIRvoWXfmu2oxx+gWU/jb3XzDNgwDd/uf2Dq+GcZ/u46/jwnme8e7ounSSrTa5P8a7qykhOw4xvoNBaadqudxA/gFwJNk6Te343tPJrPyLdXsupANs/cGMfcUb60XTQWygth8nc1T/ynRHaGm96DP6ZAzwdgwzQi5t7Ci0ObsOtYATPWHLbPgbgxSf6ubNtXUFkKSXfW/r5bD4b0zUb1kHAri3dlMPb9NWhgzv19uMfvNzw/uR5MPnDnDxDV5fJ33jAcrn/JGGcqfRODVt7C7TFF/O/nPRzPL621YxCS/F2X1rDpY6OpJzKh9vffegigjQvIwi1orflo5SH+MHMjseENmX9/dzpu/gd89xi06Av3LYeI9rXzZp3HwV0/oirLmVLxGtpayb++T6mdfQtAkr/rOrIWsnZD0l322X9UF2jQyCj5FC6v0mLl2QU7eW5hCkPiGvPVHe2ImH87bPoE+v3RmP/BL6R23zSqC9z4Ct7ZKbzfdhPfJx/jt71ZtfsebkySv6va+JFRYtdxtH3272GCVgONi76n+uqL+sVSCdu+hBOpF12tpNzCvZ9uYuaaw9x7dSvevz4IvxnXQto6uOkDGDLF+Huwh7gR0HooV5mn0iW4mNd/3Wuf93FDUu3jiopzjZr+rncY4/HYS+shsPNbyNgBTTrZ731E7assgzl3we6F4NnAOHM/dZFWazj0G2Ttphgf3luTgSmrjM+7B9PXfydMfxs8PGHSd9C8l33jVApu+C/q3d78r9HXDDw8mS1HTtCleSP7vq8bkOTvivb8AJYy6DLBvu/TerBxv/9XSf71SUkefDkeDq80Ol7t/Qk+HwsTv4WgZvD9E7D3BwD8gCcAvIDttu2b9YLRHxj1+XUhpBVc9QQxS1/gGp/ufLwqSpJ/LZDk74rS1oFvEDTpfFmbH8sv4URRBWWVFiqtmgqLFYtVY7Fq4qMCiQjwNVYMaAKNOxrt/v3+WIsHIOzCaoX9v8CvUyB7n1FR03kc9H4YPrkRProOPI3PNv+qZ5m0KYaCk0X8Z3gM3Zs2MK7x+IVe+mCAtaHvY7DlM/5esYCB2zvy1xvaExnUoO7jcCGS/F2ReQNEdzd6UFZDa435RAk7jxawIz2fX3dlsPv4yQuu7+mhuL1ncx4Z1IbwAB/j7H/Nu1B20pj1SzgfrWHrF7DiVcg9AA2bwPjZEDvQeD2gCUxaCN//CbwaUHb100z46hipRUV8cs8AurVwgrNsTx/o9QDNfnyKDhzg0zWt+L/raqmyyE1J8nc1pfmQuQs63HTBVfJLKvg++RiLth8j2ZxHQWklAB4KujRvxNM3xNEspAE+nia8TB6YPBReJoXFqpm/7SifrzvCnE1m7u4Xw/3N++O/6g04tOL800AKx/v1WWPGtsgEuHm6cRHV0/vMdQIj4bZZADz7TTLb0/P58I4k50j8pySOhyUv8NeGy3hgfTseGdSGBt52utDsBuye/JVSqcBJwAJUaq2TlFIhwFdAS4wBXsdprU/YOxa3YN4IaOPMv4pKi5Xf9mXxzeZ0fknJoLzSSuuIhgxLiKJDVCDxkYG0bxJY7X+mnq1C+cNVrXj15z28tWQ/XzWAVaYGqH2/4inJ3/msff/0VJ03vlptL++vN6Tx5YY0HhoYy5D4xnUUZA35BkKX8fTYMB3P4puYuyWd23s2d3RU9VZdnfkP1FpnV3n+FLBYa/2SUuop2/O/1FEsri1tvTHuTtNuAOQXV/DWkn3M25pOdmE5jfy8uL1Hc27uGk3HpoGXNV56TJg/b9/elfuuzue/P+1mWWoc8ZsWsqnZE4xIlAlenMbOufDjU9B+GNzwcrWJf7s5n2fm76Bf6zD+NLRdHQV5iXrci1r3AY8FreCbza0k+V8BRzX7jAQG2B7PAJYhyb92mNcbY+77BrI/8yT3zNiI+UQJQ+IaM7prUwa0i8Dbs3a6d3SKDuLTu3ty8PsRNN0whTu++p61h3rzj2Hx+HrJz3GHSl0J395rDJh284fV1uGn5RZz14wNhDf04Y1bE513Fq3QWFSbaxid+jPPHb6eY/klcuH3MtVFJy8N/KyU2qSUute2rLHW+pjt8XHgvL8vlVL3KqU2KqU2ZmVJz75qWa1Gs0+zHizelcGod1ZTWGbhy3t78f7EblzToUmtJf6qWl11CxrFP1vt4Yt1Rxj3wRoyCmQcFofJSIFZtxulmLfNAq+LJ8cTReVM+ng95ZVWZtzVndCG5xl+2Zn0uh+/ilyGe6zhh+3HHR1NvVUXyb+f1rorcD3wkFLq6qovaq01xhfEObTWU7XWSVrrpPDw8DoItZ7L2g1lBayvbM09MzcSE+bPgof7ktSylrvdny0wChVzFf1KljB1Qlf2ZxYy7oM1pOUW2/d9xbkKM+HzMeDtV6MhF0orLNw9YwPmEyV8OCmJ1hH1oGKr1UAIa8tdDZbzw45j1a8vzsvuyV9rnW67zwTmAj2ADKVUJIDtPtPecbgF83oAnlrfgL6xYcy+vzdRwXX0k7jTOMg9yDXB6Xx2T09OFJUz7oM1HMgqrJv3F2C1GL12i3Ph9q8h+OLt4Vpr/jwnmS1pebxxSyLd7X2SUFuUgs630MGyi2OH98qvzMtk1+SvlPJXSgWcegxcA+wAFgCTbKtNAubbMw53kbt7Jbk6AL8mrXl/Yre6bXePH2EM6Zv8NV2bN+LLe3tTYbFyywdrSDlaUHdxuLOlL0LqCqOqJ7L6Dn4zVqfy3bajPHlNO67vVM+mSuw0FoARHqv4Ybuc/V8Oe5/5NwZWKqW2AeuB77XWPwIvAUOVUvuAIbbn4goczSuhYN8qdpra89GdPWjoU8fX8n2DoN11xuQxlgriowL56r7eeJk8uHXqGrYckUpeu9r3C6x4xRjSo8v4alffkZ7PC4t2MSQuggf6x9ZBgLWsUQto1otbfNawKFmS/+Wwa/LXWh/UWifYbh201i/YludorQdrrdtorYdorXPtGYerKyyr5LGPFtOSo7TvPvj08At1rfMtUJwNB5cBEBvekK/v600jf28mfLiOdQdl4he7yEuDb/9gDLVxwyvVrl5UVsmjs7YQ6u/Dy2MS8HDWyp7qdB5HC2saRWlbyJSmn0smQzrXcxar5pEvNhOcswWA8Lirq9nCjloPBd9gSP7690XNQvz4+r7eNAnyZdLH6/leztJqV2U5zJ5sDM88bma1lT0AUxbs5FBOEa/dkkgjf+9q13daHW5Ce3gy0mMVP+6Uqp9LJcm/nnvtl70s3ZPF35qnGE0vts5dDuHpbQwrsXshlJ2+0Ns40Jev7utNXGQgD32xmX/M30FZpcVxcbqSX/4O6Rth1DsQWn3zzfvLDzB7k5mHBrSmd2xoHQRoR34hqNZDGe21hl92HHV0NPWOJP96bMnuDN5eup/JXYKJyVxsVNx4OajJ55TO46CiGPYsOmNxWEMfvrq3N/f0i2HmmsOMeU/6AlyxnXNh3fvGhOfxI6tdfdpvB3nph90MT4ji8SFt6iDAOtB5LGE6F3VkFaUVckJxKST511NpucX88attxEcG8nSLFNv4/dVf6LO7Zr2MMeGTvzrnJW9PD54ZFs/Uid04mFXIH2ZupKRc/sNeluz9MP8RYwynoc9Vu/rMNam8sGgXN3aO5LVxCXiaXOS/ftvrqfT050a9kg2pcunwUrjIX4B7Kau08NAXm7FqzXsTuuKV/IVxsS8y0dGhGcNIdxoDB5ZC0fkv8F7ToQlv3NqF7en5/HnONrRMA3lpyovh6zvA5AVjPzl3hM6zfLPJzD/m72RIXGNevyXRdRI/GJ3Z4oZzg2kda/akOzqaesWF/grcx3PfpZBszufVsQm0qEyFo5uNEr/LGKTNLjqOAW2BlHkXXGVIfGP+cl17FiYf4+0l++sutvqusgxmT4LMFGMylqDoi67+S0oG//dNMn1bh/L27V3wcqXEb+OZeAsBqoSKXT84OpR6xfX+Elzcou3H+HzdEe7r34prOjSBLZ+Dh5fR3u8sGneAsHaw49uLrnbf1a0Y3aUpr/6yl9kb0+oouHqsshy+ngT7foZhr0GbIRddfe3BHB76YjMdmwYxdWKS6w62F9OfIu9Qkgp+JaewzNHR1BuS/OuRknILzy9MoUNUIH++pp2RDJK/MiZR8Xeiyg2loOPNcHgVFFy4CkMpxYujO3FVmzD+PCeZab8drMMg6xlLBcy505hb94ZXIOnOi66+Iz2fP8zYSPMQPz6e3B3/uu70V5c8TBS1vYmBHlvYkLLP0dHUG5L865FpKw5yNL+UfwyLN9pt9/9idKpKtPNE7Zej482ANipSLsLXy8SHk5K4sVMkLyzaxb9/2CXXAM5mqYRv7jZKaK/7D/T4w0VXT80uYvLH6wnw9WTmXT0Iqc+1/DUU2u8uvJWF8s2zHB1KvSHJv57IKCjlvWUHuL5jE3q2sp3l715k1PbHDnJscOcT1tqYNnDHN9Wu6uNp4s3bujC+Z3M+WH6Qv3yTTKXFWgdB1gOWSph7L6TMh2tegF73X3T1zJOlTPxoHRarZubdPetuYD8HMzXpwCGf9nTIWIC2yt9OTUjyryde/mkPFqvmr9fHGQu0hv2/Gonf5KQ/6TuOgfRNkHOg2lVNHornR3Xk0cFt+HqjmQc/3yx121YLzH/Q+AId8k/o8/BFVy8ur+TuTzaSfbKcT+7sQeuIhnUUqHPIjB1LrD5CespqR4dSL0jyrwe2m/OZs8nMnf1a0jzUz1iYsQMKjxtDKjirTmPAwxPWfVCj1ZVS/GloW6YMj+fnlAzu/HgDhWWVdg7SSVmtsOAR45rOoL9Dv8cvunqlxcojX2xh59F83r69CwnNguskTGcS2W88pdqLwrUzHB1KvSDJvx74aNUhAnw9eWhg69ML9/1i3Lce7JigaiIwyhjsbfPMC9b8n8/kvjG8dksC61NzuX3aWo7nu1lPYKsVFj4GWz+HAX+Fq5+86Opaa6Z8t5PFuzN5bmRHBsc52cTrdaR5VCRbTR3xz9zo6FDqBUn+Tq6k3MLPO49zY6dIAn29Tr+w/1do0gkCmjguuJro+xhUlsD6mp39n3JTl2imTuzG/sxCbnxzBav2Z9spQAcrL4a5D8A7PWHxvyBzN3z/J+ML8+o/Q//qp7ae+ttBPlt7hPv7xzKhV4s6CNp5lTZqT+PyI1grKxwditOT5O/kFu/OoKjcwojEqNMLS/PhyFrnbvI5JbwdtLsR1k89Y7C3mhgc15gFD/elkb83E6ev4+0l+7BaXagSqKwQvhgH22aBMsHK/8G7PWHTx9DvjzDw6Yt23NNa8/GqQ/zbNl7P/13brg6Dd04NojviTSVpB3Y4OhSnJ8nfyc3fepTGgT70jKlSx39gqdGDtk09SP5gJLKSE8bZ7CVqHRHA/If6Mjwhild+3svdMzaQV1xuhyDrWHkRfD7W6Asxeho8uBr+tAtu/J/xfPCzF038ZZUWnvpmO//8LoWh8Y15eUzn+jsufy1q2rYrAEd2b3ZwJM5Pkr8Tyy+uYNmeTIZ1jsJU9T/23p+McfOjezgstkvSrDu06Atr3jE6pl0ifx9PXr8lkX+N7MDK/dnc+OZKks15tR9nXaksN8bmSVsLN38InY0pCQloAt3vNkZGvUjizzpZxvhp6/hqYxqPDGrNBxPqeMpOJ9a0TQJWFEXm7Y4OxelJ8ndiP+w4RoVFM7Jqk4/VanTvbzPUeUs8z6ffH6HADDvmXNbmSikm9m7J7Pv7ADDmvTV8tPJQ/esPYLXA3PuMazbDXrd1hqu5Hen5jHx7JTtsVT1PXNNOzvirUN7+ZHtG4p27x9GhOD1J/k5s/tajxIT506lp0OmFRzcbvXrbXOu4wC5H6yHGyKMrXzcS4GVKbBbMwkf60bd1KM8tTGHYWytZc8AJp4fM3AXfPwmvxsHUAUZPZ6sFFj0JO781hmHuNumSdvntZjNj3jdq2Ofc34dhnaOq2cI9lQS3IbrisMwXUQ1J/k7qeH4paw/lMCIhClW1CWDvj6A8nLvE83yUMkoWs/fA9tlXtKtG/t58NLk7747vysnSSm6btpaJ051kjmCtYd6D8G4v4xpHdDcoLTCmWnypBWz8CPo+blRB1VBphYW/zd3On77eRkJ0MPMf7kfHqicE4gy+TTsSo46z6WCGo0NxavWo3cC9fL/9GFpzZpUPGMm/WS/wC3FMYFcibiQ06QxLX4QOo6sdh/5ilFLc0CmSQe0jmLE6lWkrDnLL1LX0iAnhkUGt6dc67Mwvzbqy9XPj1vth6PcnY8A9q8UYnmH7bIjqWm3dflVpucU88PkmdqQX8MCAWJ4Y2ta1xuO3g9BWiXhus5C6Zxskunfp68XIX5GTWrI7g7aNGxIbXqWLfsFROL4d2tazJp9TPDxg8D8g7zBsufTKn/Px9TJxX/9YVv5lEFOGx3Mkp5iJ09dz07urWbwro24Hiassg2UvGfMoX/P86ZFWPUzQcTTcNgv6/7lG8y5UWKzMWJ3K9W+s4EhOMR/ekcRfrmsvib8GPBvHA1CYluzgSJybnPk7oeLySjYcOsGkPmedtez9ybivr8kfjLb/5n1g+cuQcLsxE1Mt8PUyMblvDLf1bM43m9J5d9l+7p6xkfjIQO4fEMsNHZvYP3FumgH5aTDizcueWKfSYmXulnTeWrKfI7nFXNUmjBdv6kSzkNr5d3ILYW2wYsIvbx+FZZU0dOXhrK+AnEY4obUHcyi3WLm6bfiZL+z9CYKbQ3h7xwRWG5Qyzv4Lj8Nv/6313ft4mri9Z3OWPjmAl8d0pqzSwqOztjDo1eV8uvYwZZV2GiyuvBhWvGKUtLYaeMmbl1VamL0xjSH/W86f5yQT2MCT6ZOSmHlXD0n8l8rTh9LAlrRVaWw5csLR0Tgt+Up0Qsv3ZOHr5UH3llXa9StK4OAy6DrReaZrvFwtekOXibDqDWh3AzSr/f4KXiYPxiY14+au0fyyK4P3lh3g7/N28M6S/TwwIJZbujer3dr49VOhMMOYU/cSPp+cwjI+W3uET9ceJruwjLjIQKZO7MbQ+MaOuWbhIryiOtI2fy3fpp7gqjbh1W/ghiT5O6Hf9mXTq1XomckpdaUxRk59bvKp6toX4eBymHs/3L8CvP3t8jYeHoprOzThmvjGrD6Qw+u/7uXZBTt5d9l+7rs6ljFJ0WeOmXQ5jm2D5f81htto0afa1SssVram5TFno5m5W9Mpr7QysF04d/WLcdyFahfjFdmR5rsXsP3gUaCto8NxSnWS/JVSJmAjkK61HqaUigG+BEKBTcBErbUL9Nm/ckdyijmUXcQdvc9u7/8RvPygRT/HBFbbfANh1DswYzj8OgVueNmub6eUom/rMPrEhrLmQA6vL97HcwtT+O9Pu+nSrBGdo4PoFB1E56bBNAtpUPMEvOMbmPcQNGhktPVfgPlEMcv2ZLFsTxZrDmRTVG7B18uDsd2iubNvjNuNvW93EXF4oCk5thOt+8sX6nnU1Zn/Y8AuIND2/D/Aa1rrL5VS7wN3A+/VUSxObfm+LIAz2/u1Ntr7Ww0EL18HRWYHMVdDzwdg3XvQ/kZoNcDub6mUok/rMPq0DmNrWh7ztqSz5cgJPl6VSrmtt3CwnxcJ0cEMah/ByMQogv3OU5JaVghLX4C17xqlt+NmQsDpoZTziytYczCHVfuzWbU/m4PZRQBEN2rAqC5NuapNGL1jwwhqcIW/OsT5RRgVP80qUjmcU0zLMPv8sqzP7J78lVLRwI3AC8CflPEVPAi43bbKDGAKkvwBo70/ulEDWlX9Y83cZVSRXP1nxwVmL4P/YcxFPO8hY3Az37rrvJTYLJhE26Qn5ZVW9macZJs5j+3mfDYePsGzC3bywve76NkqhMaBvniZFApIzP+F646+S2BFNuvDb+bbhg9yYl4aBSWHKCitoKC0gvQTJVg1+Hmb6BETwu09mzOgXTix4Q3lLLQuNGqJ1eRLu8o0ktPzJfmfR12c+b8O/B8QYHseCuRprU9N0WQGmp5vQ6XUvcC9AM2bN7dvlE6gwmJlzYFsRnZpemaC2PO9cd/mGscEZk/efnDTBzB9KPz4Vxj1rmPC8PSgY9Mgo+dsfAYc2Mnuht35MqWM9Ydy2Z9ZSANLIS9aXqWX3sZOWvFvHmdbVlsCCvIJ8PUisIEnTQJ9ads4gNFd/OjXJoyE6GC8PaWors55mCAijrh0M8vT8xmRIENhnM2uyV8pNQzI1FpvUkoNuNTttdZTgakASUlJLjSQ+/ntSM+nqNxCn9gqwzdrDVtnGSWEgZGOC86eopOM3rArXoH2w6D9DY6NZ8EjsO8n2gc3Z8qkhTCig9HB7rObIXsfXP8KHZLu4jMPGUnTmXk07kDc8e9525zv6FCckr1PSfoCI5RSqRgXeAcBbwDBSqlTXzzRQLqd46gX1h3KBThz7P60dZB7ABLHOyiqOtL/L9C4E3z3KBRVM2uXeaORjO0hcxfs+wli+htj8nxyI+z6Dj4cCnlpMGEO9PiDcWYpnFtEHCH6BOajaa41CVAtsWvy11r/VWsdrbVuCdwKLNFajweWAmNsq00C5tszjvpi7cEcYsP9CQ/wOb1wy2fg5Q/xIx0XWF3w9Iab3jdmKfv4Bsg9dP71Di43mog+6A/H7NB9f/VbRlXV2E9g0gIoL4SvJoC1Au5cVCcXpUUtsQ3zEF1+iMO5xQ4Oxvk4qjHyLxgXf/djXAOY7qA4nEalxcrG1BP0bFXlrL+8CHbOgw6jwMcNSgGbdISJc6EoE6YNgsNrznw9Lw3m3AmhrcHkbZyVp640msbMG+GXZ40hFqyXOcZ/wVFI/hq6TDAGzotMgLt/hev/C/cug8jOV3yIog417ghAnDrC9nRp+jlbnXXy0lovA5bZHh8E6sk0VHUj5VgBhWWV9Kqa/Hd9B+UnXb/Jp6qW/eCexcbctjNHGEMf97gXfAKMM3BLBdw6yyh5/XS0cQtoDHlHjKGutdWYE3fEWxDW5sx9aw0HlkC+2TiDb3RWX4p1HxjTY/Z68PSysNbGTdQ/DSPQAZF0zk9lh1z0PYf08HUSa21j0feKqTKkw5bPoFHLGvUadSmhsXD3L/DdY/Dby7DqTaNTWFGWkfhPJeO7fjQuzlaWwoC/GkNF7P4efvorvNcXku40+g807200I/30N6Os9JSIDsbF5cgE8I8wxtqPGwEhMY45blHrVGQCXYtT+LI+T/tpJ5L8ncS6g7m0CvMnItDWietEKqSugIFP1/+xfC6HXwjc8qlRXbP2XdjzIwz6+5mVQH4hcOvnZ27XZbwxcuhPfzOS+br3wSfQaLv3bmgMKxE7CPYvhj0/wIpXjV8LYDQlDfxb3R2jsL/IRKL3/szB9EysVi1TXlYhyd8JWKya9am5DOtcpZRz0wxAQcJtDovLKYS1gWGvGbeaCmgMY6YbvXAPLjPmPG4QDH0eBf8wY52IOOjzsHGBOeeA8WUb0ATC29nhIITDRCXigZXo8oMczi0mRjp7/U6SvxPYdayAk6WVp0s8CzON9uf4ERDczLHB1Wc+DSFumHG7EN8gaNrVuAnXE5kAQCePQySb8yT5VyFdD53Aqfb+nq1s7f2/vWK0Yw/6uwOjEsIFBESi/SNIMB1ih1T8nEGSvxNYdyiXFqF+RAY1MJofNn5klBueXa0ihLg0SqGiEunqJeWeZ5Pk72BWq2ZDai69TjX5LP230Xt0wFOODUwIVxGZQDNLGgeOZtftnM5OTpK/gx3KKSKvuIJuLRpBxk5I/gp63geBUpMsRK2ITMSEheiyAxzNL3V0NE5Dkr+DJdvqjxOaBcP6acaMVn0fd2RIQriWqEQAOniksutogWNjcSKS/B1sW1o+ft4mYyan9I0Q3d2oXxdC1I7ApugGoXRSh9h1TJL/KZL8HSzZnEfHqCBMljJjRMmoLo4OSQjXYrvo28XrMLuPn3R0NE5Dkr8DVVis7DxaQOfoIMhMAWvl7z9RhRC1KCqRWH2E/UerGS7cjUjyd6A9x09SVmmlc7NgyNpjLAyPc2hMQrikyARMWPA9sZvi8srq13cDkvwdKNk2w1BCdBBk7wEPTxlUTAh7iEwEoINKZY80/QA1SP5KqZ+rPP6rfcNxL8nmPIL9vGge4gdZeyEkFkxejg5LCNcT3ByrV0PaqjR2HZPkDzU78w+v8nisvQJxR9vM+XRqGmRM1p69B8LbOjokIVyTUqjG8cSb0qXix6YmyV+6xNlBSbmFvRknSYgOhspyY7z5MBlRUgh7URFxtPdIY9dRGeYBajaqZyul1AJAVXn8O631CLtE5uJSjuVjsWqj0if3gDGDlAwnLIT9RMQTqGeQlWGWsf2pWfKvOnP4K/YKxN1sS7Nd7G0WDGm2uWrDpNlHCLuJMCrpmlakkp5XQrMQPwcH5FjVJn+t9fILvaaU6lu74biPZHMejQN9aBzoa1zsBRnFUwh7iogHoJ1KI+VYgdsn/5pU+5iUUrcppZ5USnW0LRumlFoNvG33CF3Filfh52d+f5pszqdzdLDxJHsPBDU3xvURQthHw3C0XxjtPMxy0ZeaNftMB5oB64E3lVJHgSTgKa31PDvG5jqsVlj8nPG435/IVwEczC5idNemxrKsvVLpI0QdUBFxdCo9ylJJ/jVK/klAZ621VSnlCxwHYrXWOfYNzYVk7Dj9eNcCdgQZ0wp2jg4GqwVy9kGr/o6JTQh3EhFP7OGNMronNSv1LNdaWwG01qXAQUn8l+j4duNeecD2OWyzDePcOToI8o4YUzbKxV4h7C8iDl9dguXEEU6WVjg6GoeqSfJvr5RKtt22V3m+XSmVbO8AXUJBunHf51FIXcnhQwdoGepHsJ83ZNsu9kqZpxD2Z7vo29bD7PbDPNSk2UdGGrtS+WbwD4fE8bDqdZqYf6Rz6zuM104N6CZn/kLYX0R7wKj42XWsgKSW7jt3Rk1KPQ9f7s5t1wh+A3xs7zVHa/2sUioG+BIIBTYBE7XW5Zf7Pk6vIB0Cm0J4WyrCOzIg4zcCoh8xXju21fhikAlchLA/3yB0YDQd8tNZn1Ho6Ggcqialnncrpf5c5Xm6UqpAKXVSKXV/NZuXAYO01glAInCdUqoX8B/gNa11a+AEcPdlH0F9kJ8OQdEApEZeTxeP/fTxPQRbZ8GuhRA/spodCCFqi4qIo4NXOnsz3LvZpyZt/vcDH1V5nqm1DsQY8O22i22oDae+Xr1sNw0MAubYls8ARl1CzPVPQfrvE7Iv9h7ISd2A+O9Hw7z7IaAJ9H7YwQEK4UYi4mhmMXPgeB5au+/QZTVp81dnVffMBqPyRynVoNqNlTJhNO20Bt4BDgB5WutTMyqYgaYX2PZe4F6A5s2b1yBUJ1RaAGUFRrMPsCbTi2MNn+SfXQqh/TCI6goeMq2CEHUmIh4vXU5gmZnswnLCA3wcHZFD1CTrBFd9orV+EUAp5QGEVbex1tqitU4EooEeQPuaBqe1nqq1TtJaJ4WHh1e/gTM6VekTFI3WmmRzHiUxQ2HIFIhOksQvRF2zjfHTVpnZ58ZNPzXJPD8rpZ4/z/LngJ/Ps/y8tNZ5wFKgNxCslDr1qyMaSK/pfuqdfNuhBTbFfKKEE8UVp4d1EELUvfB2aBTtVJpbt/vXJPn/GYhVSu1XSn1ju+3HaMZ58mIbKqXClVLBtscNgKHALowvgTG21SYB8y8zfudXYDbug5r+3rkrQZK/EI7j1QBCWhkXfTPdt+KnJqWeRcBtSqlWQAfb4hSt9YGq6ymlOmitd561eSQww9bu7wF8rbVeqJRKAb60/aLYgjF+kGsqOAooCIgk2bwfb5MH7ZoEODoqIdyaiogjPn8b09z4zL8mF3wB0FofBA5eZJVPga5nbZMMdLnAvnrU9L3rtfx0o6LH5MW2tDziogLx9pR2fiEcKiKeyN0/kHo8F621MZWqm6nNLOR+/3o1UWCGwKZYrJod6fkkRAc5OiIhREQcJiyElR0h82SZo6NxiNpM/u5bMHsx+ekQ1JSDWYUUlVvkYq8QzuDUGD9ufNFX2h/sSWtbB69otplt0zbKmb8Qjhcai/bwop2Hmb1uOszDFSV/pVRUlaeuOzbP5So5ARXFRqVPWh7+3iZahTd0dFRCCJMXKqwtHT3T3bbW/0rP/NeeeqC17nWF+3I9pzp4BUaRbM6jY9MgTB5yaUQIpxARR3uTWZp9LpNksouxdfCq8I9i17GTJDYLdmw8QojTIuKIsGSQnpHllmP8XGnyd79/sUth6+C1ryyIcotVLvYK4UxsF32jylM5XlDq4GDqXrV1/kqptzh/klecNe6POEt+Onh4sjnXG7BN2yiEcA6nxvixzeoVGVTtOJUupSadvDZe5mui4CgERLLNfJIQf2+iG7nXH5cQTi24BdrLj3aVaezLKGRAuwhHR1SnajK8w4y6CMQl5RsdvJLN+XSODnLLXoRCOC0PD1R4ezocS2eOG170rUmzz4KLva61HlF74biYE6lUNu/Nvv0nubZjE0dHI4Q4W0Q87Y5/75YDvNWk2ac3kAbMAtYhFT41U1kGBelkmKKwauncJYRTiogj2PoZORnpbjfGT02qfZoAfwM6Am9gDMucrbVerrVebs/g6rW8I4BmX4Ux341U+gjhhGwXfaMrD5OeV+LgYOpWtcnfNhPXj1rrSUAvYD+wTCklE89eTO4hADafDCIqyNdtp4oTwqlVGeNnn5sN81CjOn+llI9SajTwGfAQ8CYw156B1Xs5+wBYkhVAYvNgx8YihDi/gCZYfYNpp8zscbOLvjW54DsTo8lnEfBPrfUOu0flCjJ3YW0Qyo4TPozoE+zoaIQQ56MUHhHxdChLZ6abJf+anPlPANoAjwGrlVIFtttJpVSBfcOrx7J2kx/QGpBpG4VwahFxtCGNfcfdK/nXpM5fhn2+VFpD5m4Oh1yHyUPRSSp9hHBeEXH46yIKstKwWjUebjL4oiR2e8g3Q/lJtpY1oW3jAPy8azxbphCiroUav9CjLGbMJ9yn4keSvz1k7QZg2YkwEpvJWb8QTi00FoCW6rhbDe8syd8eMlMA2FIaKcM4C+HsAqPRJh9aquNuVfEjyd8eMndT6hNGPg1JkOQvhHPz8ECFtKK9d5Zbzeolyd8esnaR7t0Sf28TbSICHB2NEKI6obG09jjuVvP5SvKvbVYrZO1hZ2VTOkXLtI1C1AshrWhsOcahrAIsVveYo0qSf23LOwwVxaw/GSFNPkLUF6GxeOoKQi1ZHMktdnQ0dUKSf22zVfqkWJrSRZK/EPWDrdzTnSp+7Jr8lVLNlFJLlVIpSqmdSqnHbMtDlFK/KKX22e4b2TOOOmWr9Nmno+XMX4j6IuR0uae7XPS195l/JfCE1joeY0TQh5RS8cBTwGKtdRtgse25a8jczQnPcPwCG7ndnKBC1FsBTcDLn86+Wexxk4u+dk3+WutjWuvNtscngV1AU2AkcGp6yBnAKHvGUacydrJXN5PxfISoT5SCkFa0886UM//appRqCXTBmA2ssdb6mO2l40DjC2xzr1Jqo1JqY1ZWVt0EeiVK8tCZKawqbSXDOAtR34S2opn1GAeziqi0WB0djd3VSfJXSjUEvgEe11qfMRKo1loD562t0lpP1Vonaa2TwsPD6yDSK2TegEKzQbeTnr1C1DehrQkuP4bFUkFqjutX/Ng9+SulvDAS/+da629tizOUUpG21yOBTHvHUSeSv6bM5M82HUunpjKmjxD1SkgsHrqSaOUePX3tXe2jgOnALq31/6q8tACYZHs8CZhvzzjqxMnjsHMuy/2uIToijABfL0dHJIS4FLYB3lp5HHOLnr72Hmu4LzAR2K6U2mpb9jfgJeBrpdTdwGFgnJ3jsL8N09HWSl47OZBunV2nclUIt2Er9+zil8ueTNc/87dr8tdarwQuNL7BYHu+d52qKIWNH1HQfAi79obxYGyYoyMSQlwq/zDwCaSjdzbfucGsXtLDtzbsmAPF2SxrNBqA3rGhDg5ICHHJlILQWGI8jnMou4jySteu+JHkf6UsFbD6LYiIZ3Z2K9o3CSCsoY+joxJCXI6QWCIq0qm0alJzihwdjV1J8r8SWXvgi1sgazflV/+VDYdP0EeafISov0Jj8Ss5ijcVLj/Gj0wue6nKCuHoZtj2FWybBd4N4bqX2NSgL2WVa+kjTT5C1F8hsShtpYVHpstX/EjyrymtYc6dkDIftBVMPtDjXrj6SfAPY/XPezB5KHq2CnF0pEKIy2Ur9+wReMLla/0l+deUeSPsnAsJt0PH0RDdHRoE//7yqv3ZdI4Okvp+IeqzkFYAJPrl8J4kfwHA7u/A5A03/Bd8zpya8WRpBdvM+dzfv5WDghNC1Aq/EGgQQhuvTA4fL6as0oKPp8nRUdmFXPCtqdxD0KjlOYkfYENqLharpq9c7BWi/guNpanlKBar5mCW61b8SPKvqbwjENz8vC+t2p+Dt6cHXVtIz14h6r2QWIJLjgC4dMWPJP+ayjsCQc3OWay1ZvneLJJaNMLXyzV/HgrhVkJj8So6hr9HuSR/t1dWCCW55z3zX3swl/2ZhYxIiHJAYEKIWmer+OkXcpKUowXVrFx/SfKvifw04/48yf+T1YcI9vNiVJemdRyUEMIubAO89Wl0gu3p+RhTjrgeSf41kWe0/xHc4ozFabnF/JKSwW09mkuTjxCuwlbu2dE3h+zCco7llzo4IPuQ5F8Tvyf/M8/8P1t7GKUUE3u1OM9GQoh6yTcQ/MNpoTIASDbnOzgg+5DkXxN5h8HTFxpG/L6ouLySWeuPcF2HJkQFN3BgcEKIWhfSikZlZjw9FNvT8xwdjV1I8q+JE4eNSh91emqCuVvSKSitZHLflo6LSwhhHyGtMOUepG3jADnzd2t5R6DR6aYdrTWfrEqlQ1QgSVLbL4TrCWsDJ4/SM9LDZS/6SvKvibM6eK0+kMO+zEIm92mJUheaqEwIUW9FdQGgn7+ZvOIKzCdKHBxQ7ZPkX52yk+fU+H+8KpVQf2+GS22/EK7Jlvw76P0AbE93vaYfSf7VyTuzxv9wThGLd2dwe08p7xTCZTVoBOFxRGStwdvk4ZLt/pL8q3NWjf/MNYcxKcX4nlLeKYRLixuGx5FVdI+wumTFjyT/6lSp8S8qq+TrDWlc3ymSJkG+jo1LCGFfcSNAWxntn0yy2fUu+kryr07eYfBsAP7hfLvZzMmySu6U8k4hXF+TThDcgt5lKzlZWsnhnGJHR1SrJPlXJ+8wBDfHquGT1akkRAfRpVmwo6MSQtibUhA/giY56wikiGQXu+gryb86tjLPFfuzOZBVxOS+Ut4phNuIG4GHtYKhXtvYbs5zdDS1yq7JXyn1kVIqUym1o8qyEKXUL0qpfbZ75+4lZUv+n6w6RHiADzd2kvJOIdxG0yQIiGRMg80uV/Fj7zP/T4Drzlr2FLBYa90GWGx77pxKC6DkBLneTVi6J4vxPZvj7Sk/loRwGx4eEDecbhWbOJCegdXqOhd97ZrJtNa/AblnLR4JzLA9ngGMsmcMV8Q2jv+SY754mRS39zz/NI5CCBcWNxxvXUZS5RYOZhc6Oppa44jT2MZa62O2x8eBxg6IoWZsZZ7fHDJxfcdIIgKkvFMIt9O8DxbfEK43rWfLkTxHR1NrHNqGoY3C2Qv+jlJK3auU2qiU2piVlVWHkdnYkv/e0kZM6iOduoRwSyZPPOJuZLBpC9tSMxwdTa1xRPLPUEpFAtjuMy+0otZ6qtY6SWudFB4eXmcB/v7+J1IpxYfGTaLp2ty5r0sLIexHxY+kISVwcJmjQ6k1ng54zwXAJOAl2/18B8RQI3lpKRyzNmGSlHfWioqKCsxmM6Wlrjktnivx9fUlOjoaLy8vR4fiHGKupszkT6eCFeSXPEZQg/r/72LX5K+UmgUMAMKUUmbgWYyk/7VS6m7gMDDOnjFciYqMPRzxiGFEgkzOXhvMZjMBAQG0bClfps5Ma01OTg5ms5mYmBhHh+McPH0oaDaEoYcWszU1i/5x9b/k297VPrdprSO11l5a62it9XStdY7WerDWuo3WeojW+uxqIKewZreZsIrjhMV0ooG3jN5ZG0pLSwkNDZXE7+SUUoSGhsovtLMEdL2JEFVIxo4ljg6lVkjR+nlYrZrPFi3BQ2k6J3Z3dDguRRJ//SCf07l8219LKT4Ep/547ouWCtjxLaRvqvvALpMj2vyd3vxt6ajsveAN3o3jHB2OEMIZePtxIKgXiXkrqayowPPU9ZCyQvh6IhxYAsoDhr8BXe9wbKw1IGf+ZymtsPDKT3vpHZSDRkForKNDErXIZDKRmJhIx44dGT58OHl5eQ6LZdmyZaxevbrW9jdv3jxSUlIuebuGDRvWWgyurrD9GCLUCfLm/wWOb4eTGTBjuFEFNPRfEHM1LHgElv0HnHwIaEn+Z/lkdSrpeSVcG5GPatQCvBo4OiRRixo0aMDWrVvZsWMHISEhvPPOOw6L5WLJv7Ky8pL3d7nJX9RcVI+b+dnSjbAd0+H9fvBqW8hMgVs+h76Pwu2zIeE2WPYifPsH48vBSUmzTxVpucW88es+hsRFEFZ8GMLaOjokl/XP73aScrSgVvcZHxXIs8M71Hj93r17k5ycDMCBAwd46KGHyMrKws/Pj2nTptG+fXsyMjK4//77OXjwIADvvfceffr04X//+x8fffQRAPfccw+PP/44qampXH/99fTr14/Vq1fTtGlT5s+fT4MGDXjzzTd5//338fT0JD4+npdeeon3338fk8nEZ599xltvvcX06dPx9fVly5Yt9O3bl8DAQBo2bMiTTz4JQMeOHVm4cCEtW7Zk5syZvPLKKyil6Ny5Mw888AALFixg+fLlPP/883zzzTcA5z2mQ4cOcfvtt1NYWMjIkSNr8yNwedEhftzs+zd+a1rB813yIfcgtL8RIhOMFTy9YdR70Kgl/PYK7F4E/R6H3g+Dt58jQz+HJH8brTXPzNuBh4J/Do+Dd/dDqwGODkvYicViYfHixdx9990A3Hvvvbz//vu0adOGdevW8eCDD7JkyRIeffRR+vfvz9y5c7FYLBQWFrJp0yY+/vhj1q1bh9aanj170r9/fxo1asS+ffuYNWsW06ZNY9y4cXzzzTdMmDCBl156iUOHDuHj40NeXh7BwcHcf//9ZyT36dOnYzabWb16NSaTiSlTppw39p07d/L888+zevVqwsLCyM3NJSQkhBEjRjBs2DDGjBkDwODBg897TI899hgPPPAAd9xxh0N/+dRHSikGx0Xw7dajPH372PNXAioFA56CTmPh12dh6Quw6g1o3hta9YfE8eAXUvfBn0WSv82CbUdZvjeLZ4fH05QsqCyVM387upQz9NpUUlJCYmIi6enpxMXFMXToUAoLC1m9ejVjx479fb2ysjIAlixZwsyZMwHjekFQUBArV67kpptuwt/fH4DRo0ezYsUKRowYQUxMDImJiQB069aN1NRUADp37sz48eMZNWoUo0aNumB8Y8eOxWS6eGnxkiVLGDt2LGFhYQCEhJybSC52TKtWrfr9l8HEiRP5y1/+ctH3E2canhDFrPVp/Lorg+EJF6n3D42FWz6DI2th+2w49Bv8/Axs+BAeWA3e/nUX9HlImz9woqic575LIaFZMHf0bgnZ+4wXwts5NC5R+061+R8+fBitNe+88w5Wq5Xg4GC2bt36+23Xrl2XtX8fH5/fH5tMpt/b7r///nseeughNm/eTPfu3S/Ypn/qCwXA09MTq9X6+/NLqbuv7piklPPy9YwJJSLAh++2Ha3ZBs17wY2vwsMbYMI3cCIVfnvZrjHWhNsnf6tV8/S87eSXVPDS6E6YPBRk7zFelDN/l+Xn58ebb77Jq6++ip+fHzExMcyePRswmgC3bdsGGE0n7733HmA0FeXn53PVVVcxb948iouLKSoqYu7cuVx11VUXfC+r1UpaWhoDBw7kP//5D/n5+RQWFhIQEMDJkycvuF3Lli3ZvHkzAJs3b+bQoUMADBo0iNmzZ5OTkwNAbq7RT7Lq/gIDAy94TH379uXLL78E4PPPP7+Mfz33ZvJQ3Ng5kmV7ssgvqbi0jVsPMZp9Vr8FmbtPL885AIdrr/KrJtw++f/nx90s2n6cP1/bjrjIQGNh5m7wj3CKdjlhP126dKFz587MmjWLzz//nOnTp5OQkECHDh2YP98YcuqNN95g6dKldOrUiW7dupGSkkLXrl2ZPHkyPXr0oGfPntxzzz106dLlgu9jsViYMGECnTp1okuXLjz66KMEBwczfPhw5s6dS2JiIitWrDhnu5tvvpnc3Fw6dOjA22+/Tdu2xslIhw4dePrpp+nfvz8JCQn86U9/AuDWW2/l5ZdfpkuXLhw4cOCix/TOO+/QqVMn0tPTa/uf1S2MSIii3GLl553HL33joc+Bd0NY9CRYLbDyNXirK3x8PWTsrP1gL0BpJ69FPSUpKUlv3LixVvf56drD/H3eDib2asFzIzuc/ik8bZDRHjfpu1p9P3e3a9cu4uKk01x9IZ/XhWmtufrlpbQM9efTu3te+g42fgQL/3j6easBRl+B2EEwcW5thQmAUmqT1jrp7OXudeZfVmjcgNUHspmyYCeD2kcwZUSVxK81ZO2BcPmjF0Kcn1KK4Z2jWH0gh+zCskvfQdfJ0HooePoaPYInzoNr/230Et73a22He17ulfxn3QpfjQfgXwt30TzEjzduTTTa+U/JN0N5oVzsFUJc1PCEKCxWzQ/bj1W/8tk8POC2WfDHFOg22SgP7X4PNIoxKoIsl97J75JDsPs7OIuTxyF1BRxczr6DB9h1rIDJfVoS4HvWuNxZtoswEXLmL4S4sPZNAmjbuCEz1xymwmKtfoOzmbzAP/T0c09vGPpPyNoFWz6tvUAvwH2S/+7vbQ80+1fMxuShGNY58tz1TiX/8PZ1FpoQov5RSvHkNe3Yl1nIjNWptbPTuBFGZ7ClL0KFfYfUdqPkvxBCWqEbxRCc+iP924YT2tDn3PWOJUPDxlLpI4So1tD4xgxsF87rv+4jo6AWkvWp3sFFmZAy78r3dxHukfxL8ozede2HcSxyCN2syYzpGHTuepZK2P+LDOsghKgRpRRTRnSg3GLlxUWX1zHwHDH9IbSN0RPYjtwj+e/7BayV0H4Y80sS8FYWBnvtOHe9I6uh5AS0H1b3MYo6YzabGTlyJG3atCE2NpbHHnuM8vLyc9Y7evTo7+PkXMwNN9xw2UNDT5kyhVdeeeWythXOoUWoP/f3j2X+1qOsOZBz5Ts8dfHXvAGObr3y/V2AeyT/3QvBP4LSJl354GAoRaYgfA78dO56uxYapVetB9d9jKJOaK0ZPXo0o0aNYt++fezdu5fCwkKefvrpM9arrKwkKiqKOXPmVLvPRYsWERwcbKeIRX3w4IBYohs14LEvt7DzaP6V7zDhVvDyg43Tr3xfF+D6A7tVlML+X6HTGJbsySavTFPUaiD++342mnlMtn8CrY2LwrGDHT7gklvY9Dic2Fq7+2yUCN1ev+gqS5YswdfXlzvvvBMwxt957bXXiImJISYmhh9//JHCwkIsFgszZsxg2LBh7Nixg+LiYiZPnsyOHTto164dR48e5Z133iEpKYmWLVuyceNGCgsLLzik87Rp05g6dSrl5eW0bt2aTz/9FD8/5xriV1w+Xy8TH03uzuSP1jPu/TW8O6Eb/duGk1FQyobUXFKzizhZVsnJ0koKSys5WVpBUZmFhr6ejOrSlGviG+PrVWVAvwbB0GkMJM82JolpEFzrMbv+mf+h5VBeiLXdMGauSSUiwIfQbqOgJBfM60+vd3QLFJiNsbmFy9q5cyfdunU7Y1lgYCDNmzensrKSzZs3M2fOHJYvX37GOu+++y6NGjUiJSWFf/3rX2zadP65Wvft28dDDz3Ezp07CQ4O/n30zNGjR7Nhwwa2bdtGXFwc06fb74xOOEbbxgF8+2BfmoX4cefH6+nxwq/0fHExD3+xhVd+3svHq1L5eedxks15ZBeW4+EBe46f5NFZW+j54mKmLNh55q+GpLuhsgS2zbJLvK5/5r/rO/AO4M2DTVh78AjPj+qIqXUj8PCCPYugRR9jvd0LQZmg3fWOjdddVHOG7ihDhw497xDJK1eu5LHHHgOMSVU6d+583u0vNKTzjh07eOaZZ8jLy6OwsJBrr73WLvELx2oS5Mvs+3vz2i/7OJpXQlLLRiS1DKF9k4Azz+xtrFbNqgPZfLUhjS/WHeGT1al0bBrIhJ4tGJeUgEd0d+PCb8/7jWsBtcj1k//x7RyLuIrXlx1hXFI043s2N/4RYwfB+g+h5VXQ9lqjyadFHynxdHHx8fHntOMXFBRw5MgRPD09zxhS+XKcPaRzSUkJAJMnT2bevHkkJCTwySefsGzZsit6H+G8Any9+Mfw+Bqt6+GhuKpNOFe1CSevuJx5W9L5aqOZp77dzk87j/N258n4r3zRGHkguFmtxunyzT77Rn7HqCNjSWgWzHMjO54ew2fk2xDe1hjyYeGfjM5dccMdG6ywu8GDB1NcXPz7BC0Wi4UnnniCyZMnX7QNvm/fvnz99dcApKSksH379kt635MnTxIZGUlFRYUMoyzOK9jPm8l9Y1j0aD+eG9mBVftzuO7XcDbdtLzWEz+4ePK3WDUPfLEFi3cgH0zodubProYRMHkRdLjJuKJu8oaO1Zf1ifpNKcXcuXOZPXs2bdq0oW3btvj6+vLiiy9edLsHH3yQrKws4uPjeeaZZ+jQoQNBQefpK3IB//rXv+jZsyd9+/alfXvpPS4uTCnFHb1bMueB3uDpxS3TNpCaXVT77+PqQzqvPZiDp4ciqeUFmnO0hq2fg7ZC1zuuMEpxMfV5iGCLxUJFRQW+vr4cOHCAIUOGsGfPHry9vR0dmt3U58/LVeSXVPDTjuOM6375Z/4XGtLZYW3+SqnrgDcAE/Ch1vole7xPr1ahF19BKegywR5vLVxIcXExAwcOpKKiAq017777rksnfuEcghp4XVHivxiHJH+llAl4BxgKmIENSqkFWusUR8QjRHUCAgKo7cmEhHAkR7X59wD2a60Paq3LgS+BkQ6KRdSh+tLM6O7kc3J9jkr+TYG0Ks/NtmXChfn6+pKTkyOJxclprcnJycHX19fRoQg7cuo6f6XUvcC9AM2bN3dwNOJKRUdHYzabycrKcnQoohq+vr5ER0c7OgxhR45K/ulA1asY0bZlZ9BaTwWmglHtUzehCXvx8vIiJibG0WEIIXBcs88GoI1SKkYp5Q3cCixwUCxCCOF2HHLmr7WuVEo9DPyEUer5kdZ6pyNiEUIId+SwNn+t9SJgkaPeXwgh3Fm96eGrlMoCDl/CJmFAtp3CcVbueMzgnsftjscM7nncV3rMLbTW4WcvrDfJ/1IppTaer0uzK3PHYwb3PG53PGZwz+O21zG79MBuQgghzk+SvxBCuCFXTv5THR2AA7jjMYN7Hrc7HjO453Hb5Zhdts1fCCHEhbnymb8QQogLkOQvhBBuyOWSv1LqOqXUHqXUfqXUU46Ox16UUs2UUkuVUilKqZ1Kqcdsy0OUUr8opfbZ7hs5OtbappQyKaW2KKUW2p7HKKXW2T7zr2xDhrgUpVSwUmqOUmq3UmqXUqq3q3/WSqk/2v62dyilZimlfF3xs1ZKfaSUylRK7aiy7LyfrTK8aTv+ZKVU18t9X5dK/lUmibkeiAduU0rFOzYqu6kEntBaxwO9gIdsx/oUsFhr3QZYbHvuah4DdlV5/h/gNa11a+AEcLdDorKvN4AftdbtgQSM43fZz1op1RR4FEjSWnfEGAbmVlzzs/4EuO6sZRf6bK8H2thu9wLvXe6bulTyx40midFaH9Nab7Y9PomRDJpiHO8M22ozgFEOCdBOlFLRwI3Ah7bnChgEzLGt4orHHARcDUwH0FqXa63zcPHPGmP4mQZKKU/ADziGC37WWuvfgNyzFl/osx0JzNSGtUCwUiryct7X1ZK/W04So5RqCXQB1gGNtdbHbC8dBxo7Ki47eR34P8Bqex4K5GmtK23PXfEzjwGygI9tzV0fKqX8ceHPWmudDrwCHMFI+vnAJlz/sz7lQp9treU4V0v+bkcp1RD4Bnhca11Q9TVt1PG6TC2vUmoYkKm13uToWOqYJ9AVeE9r3QUo4qwmHhf8rBthnOXGAFGAP+c2jbgFe322rpb8azRJjKtQSnlhJP7Ptdbf2hZnnPoZaLvPdFR8dtAXGKGUSsVo0huE0RYebGsaANf8zM2AWWu9zvZ8DsaXgSt/1kOAQ1rrLK11BfAtxufv6p/1KRf6bGstx7la8nebSWJsbd3TgV1a6/9VeWkBMMn2eBIwv65jsxet9V+11tFa65YYn+0SrfV4YCkwxraaSx0zgNb6OJCmlGpnWzQYSMGFP2uM5p5eSik/29/6qWN26c+6igt9tguAO2xVP72A/CrNQ5dGa+1SN+AGYC9wAHja0fHY8Tj7YfwUTAa22m43YLSBLwb2Ab8CIY6O1U7HPwBYaHvcClgP7AdmAz6Ojs8Ox5sIbLR93vOARq7+WQP/BHYDO4BPAR9X/KyBWRjXNSowfuXdfaHPFlAYFY0HgO0Y1VCX9b4yvIMQQrghV2v2EUIIUQOS/IUQwg1J8hdCCDckyV8IIdyQJH8hhHBDkvyFOItSKlQptdV2O66USrc9LlRKvevo+ISoDVLqKcRFKKWmAIVa61ccHYsQtUnO/IWoIaXUgCpzCExRSs1QSq1QSh1WSo1WSv1XKbVdKfWjbegNlFLdlFLLlVKblFI/Xe4IjELUNkn+Qly+WIzxhUYAnwFLtdadgBLgRtsXwFvAGK11N+Aj4AVHBStEVZ7VryKEuIAftNYVSqntGJON/Ghbvh1oCbQDOgK/GMPTYMLoxi+Ew0nyF+LylQFora1KqQp9+gKaFeP/lgJ2aq17OypAIS5Emn2EsJ89QLhSqjcYQ3ArpTo4OCYhAEn+QtiNNqYSHQP8Rym1DWPk1T4ODUoIGyn1FEIINyRn/kII4YYk+QshhBuS5C+EEG5Ikr8QQrghSf5CCOGGJPkLIYQbkuQvhBBu6P8BpcMfnTv1hlsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEWCAYAAACOv5f1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABJtElEQVR4nO3dd3xUVfrH8c+TSe+VXhIglNAF6Ygo2EVsrL1h15+4lm26q7u6u7rr2rGgqNg7FgQbIKBIrwmhhUBIKGmkkT45vz/ugBEDBMjkZmae9+s1r8zcmTv3e5nw5M65554jxhiUUkr5Fj+7AyillGp+WvyVUsoHafFXSikfpMVfKaV8kBZ/pZTyQVr8lVLKB2nxV6qFE5GHReRt1/1OIlImIg67cynPpsVftQgisl1EqkUk/pDlq0XEiEii6/EbIvLoYd5DROROEVknIuUiskdEfhCRy+q95gcRqXQV0HwR+VRE2tZ7/mERqXE9f+D2h0bknyQii13b/aGB5weIyErX8ytFZECj/3HqMcZkGWPCjTHO41lfqQO0+KuWJBO4/MADEekLhB7D+s8CdwP3AnFAe+BB4KxDXnenMSYc6AaEA08c8vwHrgJ74PafRmy7EHgaeOzQJ0QkEPgceBuIAWYAn7uWK2ULLf6qJXkLuKbe42uBNxuzooh0B24HLjPGfGeMqTDGOI0xPxpjrmtoHWNMEfAZMOAEMh94r++NMR8Cuxp4+lTAH3jaGFNljHkWEOC0ht5LRJJEZIGIlIrId0B8vecSXd+E/F2PfxCRR13fOspE5EsRiRORd0SkRESW1/vWJCLylIjkup5bLyJ9TnTflWfS4q9akiVApIj0crVpX4Z1tNwYpwE7jTErGrsxEYkDLgK2NuK1V4jIusa+9yF6A+vMr8dSWeda3pB3gZVYRf8RrD+CR3IZcDXWN52uwM/A60AskA485HrdGcApQHcgCpgEFBzjvigvocVftTQHjv7HYxWunEauFw/sqb9ARLJFpMjVxt+53lPPikgxkO9a7/8Oea9JrvUO3NoZY941xvQ7rj2ympaKD1lWDEQc+kIR6QScDPzV9S1hIfDlUd7/dWNMhjGmGJgDZLi+idQCHwEDXa+rcW2zJyDGmHRjzO7j3Cfl4bT4q5bmLeAK4Doa2eTjUgC0rb/AGNMBq7gHYTWzHHCXMSYK6IfVBt/hkPf60BgTXe/WUFPOsSgDIg9ZFgmUNvDadsA+Y8z+est2HOX999a7X9HA43AAY8w84HlgKpArItNE5NBcykdo8VctijFmB9aJ33OAT49h1XlABxEZfAzbWg88CkwVETna609AGtDvkG30cy0/1G4gRkTC6i3r1FRBjDHPGmMGASlYzT/3N9V7K8+ixV+1RJOB0w45+q3PISLB9W6BxphNwMvA+yIyXkRCXOcNRhxlWzOA1sCEEwksIg4RCcY6sevnyhXgevoHwAncJSJBInKna/m8Q9/H9cdvBfB3EQkUkVHA+SeSrV7Gk0VkqCvXfqASqGuK91aeR4u/anFc7ddHOnH7J6zmjAO3A0X0Dqzunk9idb3Mxjph+jsg6zDbqgaeAf56pEwicqWINHSkfsDVriwvAqNd91+pt42JWOcyioAbgImu5Q25Ahjq2oeHOLbmryOJdGXah9WUVAD8t4neW3kY0clclFLK9+iRv1JK+SAt/kop5YO0+CullA/S4q+UUj7I390bEJFo4FWgD2CwejpsAj4AEoHtwCRjzL4jvU98fLxJTEx0Y1KllPI+K1euzDfGJBy63O29fURkBrDIGPOqaxTDUOAvQKEx5jER+RMQY4z545HeZ/DgwWbFikYP26KUUgoQkZXGmN9c/OjWZh8RicIaSGo6WP2dXSMpXoB1cQ2unxPdmUMppdSvubvNPwnIA153Tcrxquuy9db1BpTag3WF5W+IyM0iskJEVuTl5bk5qlJK+Q53F39/4CTgRWPMQKxLyv9U/wWuYW4bbHsyxkwzxgw2xgxOSPhNk5VSSqnj5O7inw1kG2OWuh5/jPXHYO+BqfNcP3PdnEMppVQ9bi3+xpg9wE4R6eFadDqwAfiCXyaouBZrijullFLNxO1dPbEmynjH1dNnG3A91h+dD0VkMtYAU5OaIYdSSikXtxd/Y8waoKEx1k9397aVUko1TK/wVb9W54QVr8Oe9XYnUUq5kRZ/9WsL/gOz7oaXRsGW7+xOo5RyEy3+6hdbvocFj0P3s6BNP/jgashaYncqpZQbaPFXlqIs+PRGaN0bLnkdrvoUItvBu5MgYz7opD9KeRUt/gqqyuC9K6z2/klvQmAohCfANZ9BSAy8NRFeGA6Ln4eaCrvTKqWagBZ/H1dTU03x29dStzeNXeOnQlzXX56M7gS3/QznPQ2BYfDtAzD3H7ZlVUo1HS3+PmpvSSV3vrOSTx69kqid3/NQzTWM+NjBta8tY8X2wl9eGBgKg6+Hm+ZC30th9dtQXW5fcKVUk9Di74M27y3lwqk/kbTpVS6Tb8nofiPXTvkn95/Zg9ScYi556Wf+PSed3wz33f8yqCqBHT/ZE1wp1WSa4wpf1YIs3prPLW+vZIQjnXv93oU+F9P1ov+Cnx/dWnXj+pGJPPpVOi8v2IafCH88q+cvK3ceBf4hsPV7SB5v304opU6YHvn7kGWZhVz7+jLaR/jzfNQ7Vpv+BVPB75dfg9BAf/45sQ9XDu3Eiz9k8MZPmb+8QUAwJI3W/v9KeQEt/j5i3/5q7npvNe2jQ5g5aD0BhZvhrMchIOQ3rxUR/nFBH8antObvszYwZ/3uX57sNg4KM6BwWzOmV16hrg62/wg7fob8rVBZYncin6bNPj7AGMP9H6+lYH8Vs65JIuTjK60LuXqcfdh1HH7Cs5cN5MpXl3D3B2tIbh1Ot1YRVvEH2DoXhnRppj1QnqSovJrsfRXU1hnqjEGMoWvRj0Qufhz2pv76xcPvhPH/AD+HPWF9mBZ/H/DG4u18n57L387tRY/VfwVTB2f/B0SOuF5IoIOXrx7MGU8t4N4P1/LJbSPwj+sKMUlWu/+Qm5ppD1RL46wzbNxTQmpOMdsLyskqKCersJwdBfspqawFIIEiznQs52LHIiL9tpJFG76Mvp9eyd0Y1dYQuPMn+Pl5KMyEi1+xuhOrZqPF38ttzS3lw9lzebH1Ms5a/aDVXDPuYYjp3Kj1EyKCeGRiH+58dzUvL9zGHWO7WUf/a96B2irwD3LvDqgWY2dhOYu25LNoSx6LMwoorqgBwN9P6BATQqe4MAZ0jKZvSB7jtjxKTP5KBMP+iCQWdfgbcxxjWbmzjP8uKiUmNIArht7JHaf3InTeg/D6OXDRK5DQ3ea99B3ym+58LdTgwYPNihUr7I7hcaa+PoNbt0/Bz+FAksZA74kw4Mpj/pp9x7ur+DZtD1/cOYpepT9bwz5c/Rl0HeuW3KplKK2s4e0lWXywPIvtBdb1He2ighmVHM+IrvH07xhNx5gQ/B2u04e718JbFwEGht4Kvc6HhJ4Hv2UaY1iWWcj0HzP5Ln0vCeFBTBuax4Cl90LNfugyFobeAsln/qojgjp+IrLSGPObYfW1+HuxvSWVrH3iPEYEbiH87uUQ3uq436twfzVnPLWAVhHBfH7zAAKe6AJDboYz/9mEiVVLUVpZw6uLMnlj8XaKK2oY0TWOM1JaMyo5ga4JYUhDTYY7FsO7v4PgKOvAIL7bEbeRmlPMfR+tZeOeUq7pF8Jf2iwnePUbULrL+iNwyWsQGuuW/fMlhyv++qfVi33ywzJOk5U4+115QoUfIDYskEcn9mHD7hLeXV0AnUdY7f7K6yzLLOTsZxbxzNwtDE2K5fM7RvLuTcO4bmQS3VqFN1z4138Mb10IEW3ghq+PWvgB+rSP4os7R3HX6cm8k1rJVZtGU3H7ajj3f9aFhK+Mhb1pbthDBVr8vdb+qlpY9SZ+Yoga1TQnZs/s3YYRXeN46vvNVHQ+DfI2QtHOJnlvZb+qWiePzdnI76b9jMNP+OS24Uy7ZjD9O0YffqXaapj9B/hkMrQbCNfPgagOjd5moL8f94zvznOXD2Rl1j5uf38dNSfdANfNts4pvToOMuad+M6p39Di76U+XpbJhWYupe1PgdikJnlPEeHBc1Morqjh9b2uI7uMuU3y3spem/aUMnHqYl5akMFlJ3di9l2jGdT5KE0u+/PhzQmw7GUYdgdc+yWExR/X9s/p25Z/TuzL/E153P/RWuraD4abf7AuRPzsDqgqPa73VYenxd8LOesMmxZ9TFspJGr0LU363intIrns5I48uQZqw9vp1b4ezhjDaz9mcv5zP5JXWsn0awfz74v6EhZ0lI6Ae9Ng2ljYtRoung5n/QscASeU5Yqhnbj/zB58tmYXT8/dYjUhXTAVSnfD/H+f0Hur39Li74Xmb8zlzIrZVIS0sXpNNLF7xvcgOMCfn2QgbFsAzpom34Zyv+qSfH567gZ+mv0Wp3RP4Ju7T+H0Xq2PvuKmOTD9DHBWw/Wzoe8lTZbp9lO7ctFJ7Xl+3hZWZe2DDoOtUWWXvmj1JFJNRvv5e6ElK1fyoGMdziF/BkfTf8QJEUHcPrYr737bnTGBX8HOZZA4ssm3o9ynaM92Sl89n1G1WYwKBNOzDRLu6hBSVWYV27xNEBAKQRHWUX1FEZTsgi3fQtv+cPl71mxvTUhEeHhCb5ZuK+SeD9bw1V2jCTv9b5D+Jcz6PUz+Tq8GbiJ65O9lqmvrcGR8C4BjwOVu2871I5LYGDKQWhywVZt+PMnOjcupefk0YmryWDziVehxDjLnflj2Cmz+Fl4YBvMetf6ob/4GVrwOPz1rFeB922H4HdaJ3SYu/AdEBgfwv0n92VFYzqNfpVuzyZ35L8hZCavedMs2fZEe+XuZnzLy6e3cSGV4G4IbeRXv8QgJdHDlmD6s+L47/TZ8Q+i4h922LdVEirLI+e55YtLepJwQsiZ+woiBI6H2AvjoWph9n/W6+B5wwzfQaZhtUYd1iePm0V14eeE2xqe04rS+l8KyafDT03DSNXr03wT0yN/LfL1+D4P9thCQ5P7/uFcN68xyx0mEFm6A0j1u3546TpUlMPM26p4ZQOvUV1jlP5Da678hZaCrqc4/EC6dAeMfgXOegFsX2Vr4D7jnjO4ktwrn4S82UOWsswaB27fdOuegTpjbi7+IbBeR9SKyRkRWuJbFish3IrLF9TPG3Tl8Qa2zjjVpabSTfBydhrt9e6GB/rQZfD4A25d94fbtqeNQWwUfXEndug+ZUXsGt8ROp989n9Ou8yFj6PgHwsi7rMH6Wsh4TUH+Dh48L4WswnJmLN4OPc+DqI6w5EW7o3mF5jryH2uMGVDvEuM/AXONMcnAXNdjdYKWZhbStWqD9aDjkGbZ5jmnjyOPaPas/KpZtqeOQV0dzLwFMhdyX/VNzGk/hadvOZ/o0EC7kzXamO4JnNojgefmbqWgwmkNKbLjR+350wTsava5AJjhuj8DmGhTDq8yJ3U3Q/23YvxDoE3fZtlmWHAA+W1G03P/ctbsyG+WbapGMAa+/iOkzeRfNZeT1+VCZtwwhIjgE+uLb4cHz+1FeY2Tp77fbLX3B4TBkpfsjuXxmqP4G+BbEVkpIje7lrU2xhyYHmoP0IjOxepI6uoM36Tt5ZSQbUj7QSd8wc2xSBx2IdGyn29mz2y2baqjWPQ/WDaNV2rPIbP7ZF69djAhgZ55krRbqwiuGtqJd5dmsanYAQOvhNSPoXSv3dE8WnMU/1HGmJOAs4E7ROSU+k8aa1jRBocWFZGbRWSFiKzIy8trhqiea2XWPkpKS+lcvbXZmnwOCEk5i2q/EDrlfMXGPTo1n93Mqjdh3iPMdI4kNeU+XrhqEEH+nln4D7h7XHfCgvx5Zu5ma6hoZzWseM3uWB7N7cXfGJPj+pkLzASGAHtFpC2A62fuYdadZowZbIwZnJCQ4O6oHm3exlwGOjLxM7XQcWjzbjwwDHqdz7mOpUyfn96821a/YjIXYb64m4XOvizv+whPXnYSAQ7P79QXExbIZSd35Ju0vezxbw9dT4O171rnNdRxcetvhYiEiUjEgfvAGUAq8AVwretl1wKfuzOHL1i4OY/zY7OsBx1ObvbtB550OZFSTnnqbLL3lTf79hXUFe+m7J1ryKxrxU8nPcmjl5yEw+/IU3V6kquGdabOGN5dlgX9LoOiLNi51O5YHsvdhwStgR9FZC2wDPjKGPM18BgwXkS2AONcj9VxyiutIm1XCSMCMyAuGcLimj9E0hicYa2Z6PiRVxdlNv/2fZxx1rB92u9w1Oxn4cD/8aeJJ+PnRYUfoHNcGKd2T+DdpVlUJ59tDT2x7gO7Y3kstxZ/Y8w2Y0x/1623MeafruUFxpjTjTHJxphxxphCd+bwdj9uzQMMHcvWN3+TzwF+Dhz9LmWs3xq+Xp5G4f5qe3L4IGMMP0+bQpf9a5mf/Geuv/Dchidc8QLXjEgkv6yKOZtLrX7/aTOtaxnUMfP8xkDFws35DAwtwL9qX7Of7P2Vfr/Dn1rG1S22LspRzeLLD6YxYu87LI+/kHOuvNvuOG41JjmBznGhvPXzDuj3O6gssgaaU8dMi7+Hq6szLNqSx+UJrqYWOy/Lb9MXWqVwXfgyZvy8nfLqWvuy+IgPvp7PqekPkRXSk0E3v+S1R/wH+PkJVw/rzIod+0gLGQhhCdr0c5y0+Hu4DbtLyC+rZlzVd9CqN8R3P/pK7iIC/SbRrSqNyIps3l+mUzy60/uLN9Jv8V34OQJof9OH+AUG2x2pWVw6qCPBAX68tTQH+lxijTxasc/uWB5Hi7+HW7gljx6SRWxRKpx0tVWA7dT3UkC4I24l03/MpMapXfHcYeaqnfjPuZ8efjsJ+t10HLHuG8G1pYkKDeDCge35bE0OpT0utvr8b9AOg8dKi7+HW7g5j1siF4NfAPSdZHcca/LuxFGcLz+SU1TOl2t32Z3I63yduoflnz7NJY6F1I26j4AeZ9gdqdldObQzlTV1fLY3wfq2u+5DuyN5HC3+Hmx/VS3rd+RylnMB9DzXni6eDel7KaGl2zk3PpeXFmRQV9fgBdzqOCzcnMer73/Iw/4zqE0ai/9pf7Y7ki16t4uke+twPluzC/pNgh0/Wf3+VaNp8fdgP2cUMMasILS22GryaSlSJoBfAHe3WsPmvWXM39TgBdzqGC3LLOTZtz7gjYDHcES3x/+S6T47qYmIcOHADqzcsY+cDtaw4qz/yN5QHkaLvwdbuCWPywMWYCLbQ5exdsf5RUgMJI+nW953dIgK4qUFGXYn8njrsov43xvv84bjX4RExuG4blbL+aZnkwsGWNNIfrzNAZ2Gw9r3rdFMVaNo8fdg6Rs3MFLWIQOubHlHgH0uRkp38Zc++1i+fR8rtut1fMdr055S/j39PV6VRwmOjMVx/VcQ3dHuWLZrFx3CsC6xfLYmB9P/csjfDNnL7Y7lMbT4e6isgnIGlszDD2MNcdvS9LAuvx/vXERMaIAe/R+nzPz9PPLqe7xkHiUkIgb/67+C6E52x2oxLhzYnsz8/ayPPt0a518neG80Lf4easGWPMb6raUqrhfEJNod57cCw6DnuQRs+oLrh7Xn+/RcNu8ttTuVR8kpquDv095jau3fCQ2Pwv+G2RDjO106G+Psvm0J9Pfj07Ri6HOhNdxDVZndsTyCFn8PtWzjdk52bCKw51l2Rzm8PpdAxT5uiE8nJMDBywu22Z3IY+SWVvLQy+/zdNVDhIRFEjBZC39DIoMDOCU5ge/T92IGXg3VZdYfAHVUWvw9UI2zDkfmAvxxIsnj7Y5zeN3GQUwS4Sue53eDO/D5mhxyiirsTtXiFZVX89C0D/hv+V8JDYsgcPLslvntroUY2zOB7H0VZASlQFw3Lf6NpMXfA63asY9hzpXUBETYO5Db0Tj8YeQU2LWaO5Ny8BPhme83252q5cnfAmmfQVUpZVW1/G3aB/yr5AFCw8IIvHE2xCbZnbBFO7VHKwDmb8q35rLYm2ZzIs+gxd8DLdycy6mOtVb3zmacq/e4DLgCwtsQv/p5rh7emY9XZmvbf30bv4IXR8JH12L+m0za/87ln/vuJyQ0lKDJsyG2i90JW7z20SH0aB1hXU/SqheU7YFy7V12NFr8PVBW+nLayD7PuKzfPwiG3wGZC5nSs4SwQH8en7PR7lQtQ9pM+PAaaNOHwglv8IWMpUtVOn4RrQm+cQ7EdbU7occ4tUcCy7cXUhHTw1qQq9OJHo0Wfw+TW1JJ5/yF1oNkDyj+AIOvh+AoIlc8x62ndmXuxlyWbiuwO5W9Nn4FH0+GDieTec47nP9tFA9UX8fWa1YTdu8aLfzH6NQerahxGpbtb20tyNPifzRa/D3MvI25nO5YTUVCf4hobXecxgmKgCE3w8ZZTO5RTZvIYP49ZyPGV6/G3PYDfHQdtBvIhrHTueS1VCprnLx/8zCGd42zf2RWDzQ4MYbwIH++zvKDoEg98m8ELf4eZlnqRvr7ZRDc+1y7oxybobeCfwjBS5/j9+OTWbOziDmpe+xO5V6FmfDzC5D6KThdE9tkr4D3roC4biwfOY1Jb6QSHODgo1uH06d9lL15PViAw4/RyfHM35SPadVLi38j+NsdQDVeZY2T4O1z8fMz0KMF9+9vSFg8DLoOlk3j4pG/59VW4fz3m02MT2lNgMPLjkGylsCCxyFj3i/Lojtbcx0sfxXCE/iy31TufWczneNCeXPyENpGhdiX10uM7dGKOal7KOrajZjts61xfvRb1GF52f8677ZkWwGnmJVUhbSGNv3sjnPsRt8D/sH4L/gXfzyrJ5n5+3lvmZcNw5u/Bd66yDryPPXPMGUdXPau9cdv0ROYgFCeaf8E/zdrN0OSYvno1uFa+JvImB4JAKTWtLNm9irba3Oilk2Lvwf5IS2b0X7rcPQ82zOPaMJbwfDbIW0mp0fvYniXOP737WYKyqrsTtZ0vn/YGmTvpvlw6p+sq3J7ngs3zqXghqVMjniRp1ZUc8PIJN64/mSiQwPtTuw1WkcGk9I2knmF8daC3A32BmrhtPh7CGMMxenzCZMq/HudY3ec4zfi/yAkBpn7CP+4oDf7q2p5zFu6fmavhI2zrH2MbHtwsTGGOal7GPdGFj9lVfCfS/rxt/NT8Pe25q4WYGzPBGbtibYe7NXifyT62+chNu4ppX/FEmr9giHpFLvjHL/gKBh9L2TMJbl0GZNHJ/HRymzvGPJ53j8gNA6G3XZw0c7CcibPWMFt76yifUwIX901mkmDdThmdzm1Ryvy6iKoCorTk75HocXfQ3yftodxjlU4E0+BAA9vIz75JmsMli/u4q4RrWgXFcyDn6VSXevBk71vW2B14Rx9LwRFUFXr5Pl5Wxj35AKWbCvggXN6MfP2kXRrFW53Uq82oGM0YYEOsgMSIVeHeTgSLf4eYlPqMjpIPkGe1sWzIQHBcOE0KN1N2LwH+PsFfdi4p5Rn526xO9nxqa2Cbx+AyPbUDbqBz9fkMO7JBTzx7WZO79WKufeO4aZTunhfr6YWKMDhx8lJsayuage5G6HOaXekFqtZfhtFxCEiq0VklutxkogsFZGtIvKBiOhZryPYU1xJp7wDV/WeaW+YptJhkHWUvPY9xrOUSwZ14IUftrIqa5/dyY5NWZ7Vu2fPetIH/pUJL69gyvtrCA8K4M0bhvDClYO0N08zG9E1jqXlbaG2AvZttztOi9VchyJTgPoNcI8DTxljugH7gMnNlMMjfZe+l9Mcq6lM6PerE4keb8wfoG1/+HIKD4+No21UCPd+uJby6lq7kzVO9gqYNoa67OW8FPdHzv4mkn37a3hyUn+++r9RnNI9we6EPmlE13g21bnOq+gIn4fl9uIvIh2Ac4FXXY8FOA342PWSGcBEd+fwZEvWb+Ikvy0EpZxtd5Sm5Qiwmn+q9xP+7b08cUk/thfs594P1+Ksa8FDP+wvgFm/x0wfT365k/PLH+LFwsE8cE4v5t47hotO6oCfnwd2xfUSvdpGsicokTpEu3seQXMc+T8N/AE4cDYvDigyxhw4vMsG2je0oojcLCIrRGRFXl6e24O2RKWVNYTtmIsfBunhZcUfoFVPGPcwbP6a4SWzefDcFOak7uGvn6faO/ZPTcOTztStmEHNMwNxrniD12vO4LzqfzH21HEsvH8sN53SheAARzMHVYdy+AkDurQjR9rokf8RuLX4i8h5QK4xZuXxrG+MmWaMGWyMGZyQ4JtfoRdszuM8+YmqsHbQpr/dcdxj6K2QOBq+/jOTU+D2U7vy7tIsnvrOpolf5j0K/2wDM289OCZPbnE5q1+5Db9Zd7Gioj2XO/5Hwei/M/uP53PfmT2ICm3h8yr4mBFd40ir7UDNbi3+h+PusX1GAhNE5BwgGIgEngGiRcTfdfTfAchxcw6PtXzNOh5ypMKgP4Cfl/YW8fODiS/CiyPgs9u5/9ovKSir5tl5W/F3+PF/p3VD6l/R7Ky1Zglzh4p91mBsAGvfI2tvPo847mBi1r8517GUr8Mn4hz/T97u3Y5Afy/9PLzA8K7xfG06ckbRSutbnKd3j3YDt/72GmP+bIzpYIxJBC4D5hljrgTmA5e4XnYt8Lk7c3iqGmcdCds+wQ+D38Ar7Y7jXtEd4ezHIWsxMvte/jmhBxcNbM+T323mng/XUlXr6rJXshueHQDvXgbV+5s8RuXPr0DNfh5o/SKP1l5Fpz3f8UrORM51LKVg5N846943OLd/By38LVz31uHkBCbhRx3keckV5E3Mrt/gPwL3iMhWrHMA023K0aItzchnQt18CloNs8aI8Xb9L4eRd8PKN/B/71L+d35n7jujOzNX53DFK0vJ3VcMH14N+/NhyzcwY4J18hWsbwM7fj6urn3l1bV8sXYXt72xmLIFz7PA2Y9FpW0JGv1/5JzyH0zSGJj0JnHj7/XMMZV8kIgQ3slqJjV7Um1O0zI125DOxpgfgB9c97cBLXjm8ZZhy7I5jPLLo3rYo3ZHaR4iMP7vEN8dvpyCTB/HnRNfJCn+JO75cA0Ln76eS2Q5NRe/QYB/AHwyGV470xruIv0L2J8HjiAYc7/1R6Sh+Y0rS6C6jJLABH7aks+c1D18n76X8mont4QtIF6KaX/en1kw5FRXU1NP4JZm/odQTaFrz36UZQbjzFxB1ElX2x2nxdHx/FsoYwxtt31CuV8YoX0vsDtO8xp4pTVx+YdXw/TxnNvhZMb0aE14xlym1k7gk29juHFUF8646APiv7gG1rwL3c+EXudbA6vNexRSZ1q9iJJOgYBgTG01e+dNJWbpEwQ5y8gz7chyDmR/wBAm9RvOWf07M3TOXyF4IN2GeOioqepXhndNIM0k0nXnKrujtEha/Fuo9O3ZnOpcTHbnC+nmiyerOg+Hu9ZYhX3JC4RnL4du4+h10uPInM38ZeZ6/gL0afUao3rF0K5VHK0kmLiTTyMs7ky6LPsbwe9eSrVfCKlBA4ivyKQTe1jo7Mum8JMZF5jGTaXfckvdV5AKbImCqmK4/AMt/F4iKT6MxY5uDCz+zr2dBDyU/mu0ULsXvU2K1BA/+ga7o9gnKByG3gwnT7YmSYlJ5LSAYMb2asuW3DJ+2JTLD5vyeG1ZLtXO+lNCRhLEfxjul8Z4/7Wcxlpqg6NY1OdBeoy+hFMODLdQWQKZCyF/s3WuILqj582Qpg5LRCiP60tg/izI3wSte9sdqUXR4t8S1VTSd/t0tvon063bMLvT2M/PYV0M5iIidG8dQffWEdx8SlecdYbC/dXsLamkYH81IQEOwoIcRAafSduo4IPj5v9mIOXgSOh1XvPth2p2YUmDIB/Kt68kVIv/r2jxb4GKFr5Aq7o8lvX9B920CeKoHH5CQkQQCRFBdkdRLUxSj/6ULQumeOtSQodeY3ecFkU7K7c0lcUEL3mahc6+pIyaYHcapTxa/46xpJlEZM9au6O0OFr8W5qfniG4pph3I66nS4JO/KHUiQgL8icnpAdxZZsPDtWhLFr8W5LSPZifX+DLuhF07jPC7jRKeYXa1v0JMlXU5eqVvvVp8W9JlryAcdbw35pLObVHK7vTKOUVorqcDEDu5qU2J2lZtPi3JFlL2RHSi31B7RmcGGN3GqW8QnLKAMpMMKXbltkdpUXR4t9SOGsxu9eypLIzpyQn6HyvSjWRpIQINkoSgbnr7Y7SomiFaSnyNyO1FSyt7MTYntrko1RTEREKIlJoXbFFT/rWo8W/pcizpjjeaDpxag/fnLhGKbdpN4BgqinN0cldDtDi31LkbcaJH+HtehAfrhcrKdWUEroPBWDXhp9tTtJyaPFvIap2byCrLoFRvTrYHUUpr9M9ZQAVJpCyLL3Y64CjFn8R+bbe/T+7N47vqtydzlbTntO0vV+pJhceEsRO/04EFmpf/wMac+RfvwH6UncF8WnOWsJKM8nx70SfdlF2p1HKK5VGJtO6MpO6OmN3lBahMcVf/6XcrLZgG/7UEtyuF35+OpCbUu7gaNObVuwjK2en3VFahMaM6tlFRL4ApN79g4wxOvrYCdqWvoruQKfuJ9kdRSmvFdelP6TDzo2rSOzYye44tmtM8a8/h+AT7griy3ZvWUN3oM+AwXZHUcprtU0eBEBp1lpgoq1ZWoKjFn9jzILDPSciI5s2jhdbOg0qi63JxQ9RszedfEcC8VGxNgRTyjf4R7WjVMJx5OtJX2hE8RcRBzAJaA98bYxJFZHzgL8AIcBA90b0AsbAHFfRH3ozBP9yUjenqILWVdupjO9mUzilfIQIhWHdSCjdSq2z7uAMb76qMXs/HbgRiAOeFZG3sZp//mOM0cLfGLkbfrmfPutXT81P30NX2U14B51iTil3q0voRTd2smVvqd1RbNeY4j8YGG+M+TNwDnAeMNIY85k7g3mV3fUuLFn/0a+eWr1+PaFSRVSnPs0cSinfE9G5P5FSwdat2vTTmOJfbYypAzDGVALbjDEF7o3lZYpzrJ8j/g8yF0BZLgD79lezb0cqAJLQw650SvmM2MT+ABRk6pW+jSn+PUVkneu2vt7j9SKyzt0BvULxTgiNhwFXgamDtJkAfLthD13Itl6T0NPGgEr5Br/WKQCYvTrAW2O6evY63jcXkWBgIRDk2tbHxpiHRCQJeB/rPMJK4GpjTPXxbqfFK8mBqPbQqie07ms1/Qy9hVnrdnNdcBYmOAEJ1Z4+SrldSDQlga2IKd1KZY2T4ACH3Ylsc9Qjf2PMjiPdjrJ6FXCaMaY/MAA4S0SGAY8DTxljugH7gMknuB8tW3EORLoGbOt7MWQvp2TzT7TN/IQxdUuRXnqdnFLNpSq2J91lJ+m7S+yOYqvGDOw2WUTur/c4R0RKRKRURG490rrGUuZ6GOC6GeA04GPX8hl4+xUXJTkQ5Sr+/a+AoEgi3z2H//i/TF14G+tcgFKqWQS370NXySF1p2+fumxMm/+twGv1HucaYyKxBny7/Ggri4hDRNYAucB3QAZQZIw5MKVONtY1BA2te7OIrBCRFXl5eY2I2gJVFkNVidXsAxDRGi6ezmcRV3BLyBME/H4txCbZm1EpHxLesR9BUsuubb7d7t+Y4i+H9O75CA72/Ak52srGGKcxZgDQARgCNPrMpjFmmjFmsDFmcEKCh85udaCnT+Qvf98K2o3h3oLzSR5wCuLn2xeaKNXcxHXSt3pXqs1J7NWYyhNd/4Ex5l8AIuIHxDd2Q8aYImA+MByIFpEDJ5s7ADmNfR+PU+LatahfJmn5Jm0vzjrDuf3a2hRKKR8W34M6/Igq3cL+Kt+d07cxxf9bEXm0geX/AL5tYPlBIpIgItGu+yHAeCAd64/AJa6XXQt83tjAHqfY1ZWzXvH/eOVOuiSE0bNNhE2hlPJhAcFURHSmu2T79EnfxhT/+4GuIrJVRD5x3bYC3YD7jrJuW2C+63qA5cB3xphZwB+Be1zvE4c1hIR3KskB8YPwNgCsztrHqqwirhnWGREdu18pOzja9KG77CQ1p9juKLZpzKie+4HLRaQLcGAAmg3GmIz6rxOR3saYtEPWXUcDA78ZY7Zhtf97v+IciGgLDuuf+vWfthMR5M8lgzvaHEwp3xXUrjeJW2bxSnYu4JsdLhpzkRdwsGBvO8JL3gJ0NpJDFe88eLJ3d3EFs9fv5roRiYQHNfqfXinVxKR1CoKhdGcaMNTuOLZoyq4m2obRkANX9wJv/ryDOmO4dkSivZmU8nWtrEaM0KJNVNY4bQ5jj6Ys/jrX76GMgZJdENWBimon7y7N4oyUNnSMDbU7mVK+LTYJp18Q3djJZh8d3lk7mbtTeQHUVkJkBz5dnU1xRQ03jPLN9kWlWhQ/B8647vSULFJzfLPHzwkVfxFpV++h9w7Mdrxc3Tyrw9rywvwM+raP4uTEGJtDKaUAAtr1pocjm7Rdvtnj50SP/JccuGOMGXaC7+V9XMX/tdRqcooq+Ms5vbR7p1IthLTqTWv2sT3be68xPZITLf5ayY7EdXXvK2truGl0EsO7xtkcSCl1UKsDY/tvoMZZZ3OY5neixV9P8h7Bvt3bqDIBJHXqzB/O0slalGpRXGP8dDU7yMgrO8qLvc9RO5uLyHM0XOSFQ8b9Ub8wxrA+PZ0kieP5KwcR4NBz60q1KBFtcQZF0aN2J2k5JfRsE2l3ombVmCuNVhzncz7tx635hFbsJjC+I62jgu2Oo5Q6lAh+rVPotSObWbuKuXhQh6Ov40UaM7zDjOYI4m2en7eVF/xyie7kG6NYKOWJpFUKPXe+z+PZvtfjpzHNPl8c6XljjM5BeIjl2wtZn7mLuOAiiOtidxyl1OG0TiHM7Kdg9zbq6obj5+c7fVga0+wzHNgJvAcsRXv4HNXz87bSN7QQ6oAYvahLqRbL1eOnY+0OdhSWkxQfZnOg5tOYs5BtgL8AfYBnsMbkzzfGLDDGLHBnOE+0PruYBZvzuK6n6xy5TtGoVMvVqhcAPXxweOejFn/XNIxfG2OuBYYBW4EfROROt6fzQC8u2EpksD+nxRdZC2K72ppHKXUEITGYiHb0dGST6mNX+jaq/6GIBInIRcDbwB3As8BMdwbzRMXlNXy3YS+TBnckqHATRHWCYN/qPqaUp5HWKfQLyGHDLt8a46cxJ3zfxGrymQ383Rjj27MeH8G3G/ZQ4zSc378dfJl+8CulUqoFa9WLThkLSc8uxBjjM0OwNObI/yogGZgCLBaREtetVER860/lUfy4NZ/48CD6tQ2F/M1a/JXyBAm9CDDVRFTmsKu40u40zaYx/fz10tRGMMawOKOAEV3jkMJtUFejxV8pTxDXDYBE2UNqTjHto0NsDtQ8tLA3kYy8MvJKqxjRNQ7y0q2FWvyVavlcxb+L3x7SfKjdX4t/E/lpawEAI7vFQ246iB/Ed7c5lVLqqEJjITiaAaH5pPlQd08t/k1kcUY+HWJCrCkaczdAbBcI8I2vj0p5NBGI60oP/1yf6u6pxb8JOOsMP2cUMLJrvLUgNx0SdAhnpTxGXDfa1uWwt6SKvNIqu9M0Cy3+TWDDrhJKKmsZ0S0OaiqhcNvBy8aVUh4gtisRlXsIotpnpnXU4t8EfsrIB7Bm6srfDKZOT/Yq5UnirCvxO8tenznp69biLyIdRWS+iGwQkTQRmeJaHisi34nIFtdPj57VfHFGAcmtwmkVEWw1+YAe+SvlSVw9foZEFvrMGD/uPvKvBe41xqRgjQt0h4ikAH8C5hpjkoG5rsceqbq2juWZhVYvH7BO9voFHDySUEp5ANf/18HhhXrk3xSMMbuNMatc90uBdKA9cAFwYJKYGcBEd+ZwpzU7i6iocf4yOXtuOsQngyPA3mBKqcYLioDw1vQIyCWrsJzi8hq7E7lds7X5i0giMBBrToDWxpjdrqf2AK0Ps87NIrJCRFbk5eU1T9Bj9NPWfPwEhnWJA2Ng12po09fuWEqpYxXblXbOHADSdnt/00+zFH8RCQc+Ae42xvzqO5UxxtDwBPEYY6YZYwYbYwYnJCQ0Q9Jj93NGAX3aRxEVEmD18tmfC52G2R1LKXWs4roSsX8HAGk53t/04/biLyIBWIX/HWPMp67Fe0Wkrev5tkCuu3O4Q3l1Lat37mPEgf79ma65bTqNsC+UUur4xHXDrzyPbpFOn+ju6e7ePgJMB9KNMU/We+oL4FrX/WuBz92Zw12Wb99HjdNY4/kYA8tehdZ9IKGH3dGUUsfKddJ3THwJqT5w0tfdR/4jgauB00Rkjet2DvAYMF5EtgDjXI89zuKt+QQ4hJMTYyFzIeSmwbDbrMvFlVKexdXdc1D4PrbllVFeXWtzIPdqzATux80Y8yOHn/D9dHduuzkszihgYKcYQgIdsOQFCEuAPpfYHUspdTxikgChR8Be6kwX0neXMqizR1+CdER6he9xKiqvJnVXsTWeT0EGbP4aBk+GgGC7oymljkdAMER1pK1zF4DXt/tr8T9OS7YVYgzWeD5LXgRHIAy+we5YSqkTEdeFkJJMYsMCvb7Hjxb/47Q4I5/QQAcDZCusmgH9JkFEg5crKKU8RVw3pCCD3m0jvH54Zy3+x2nrlnRejphOwOtnQEgMnP6Q3ZGUUicqrhtUFTO4lWHz3lKqa+vsTuQ2bj3h63V2r4UNn1O9fSlvlP6Mw88PRtwJo++DkGi70ymlTlSsa4yfiAJqnMLmvaX0aR9lcyj30OLfWBVF8NpZUFvF/siefOg8izFX/I2ePXTSFqW8hquvfw//XKA1abuKvbb4a7NPY6V+AjXlcOP3/Lvji7wQcB3dk/ViLqW8SnRn8PMnrnIn4UH+pHrxSV8t/o21axWEtcK0HcBPWwsY3iUOPz+9mEspr+Lwh5hEZF8GKe0ivbq7pxb/xirKgphEdu6rJKeogpHd4uxOpJRyh7huUJBBn3ZRbNhdgrOuwXEnPZ4W/8YqyoLoTvWmbIy3OZBSyi1iu7qKfziVNXVk5JXZncgttPg3Rp0TirMhuhOLMwpoHRlE14Qwu1MppdwhrivUVjAwpgKAddne2fSjxb8xSndDXS0mqiM/Z+Qzoms8ooO3KeWdDkzmbnYTHuTPuuwie/O4iRb/xijKAiCbVuSXVVtDOCulvJNrdE+/wgz6tI9k7c4ie/O4iRb/xthnze6zfF84wC/z9SqlvE9EO/APhsJt9O8YzYbdJVTVOu1O1eS0+DeG68j/h71BtI8OoUNMqM2BlFJu4+dnDe9cuI3+HaKpcRo27i61O1WT0+LfGEU7MBFtWbyjjKFdYu1Oo5Ryt9guB4/8AdZ6Ybu/Fv/GKMqiMqwD+WXVDE3S4q+U14vrCoXbaBfuID48kLU7va/Hjxb/xti3gz1+rQAYmqTt/Up5vbb9wVmN5G2kf4doPfL3Sc5aKMlhU1UsrSKC6Byn7f1Keb12A62fu1bRr0M0GXlllFbW2JupiWnxP5qSbDBOVhZFMLRLnPbvV8oXxHaBkFjY8TP9O0ZhDKzP8a6mHy3+R+Pq6ZNaEcMQbe9XyjeIQPezYPMc+rW1vu1725W+WvyPxtXHP9vEM0yLv1K+I2UCVBYTu3cJneNCWbljn92JmpQW/6MpyqIOP6pC2tKtVbjdaZRSzaXLWAiMgPTPOTkxluXbC6nzohE+tfgfTdEOciWOgUkJ2t6vlC8JCIbuZ8LGrxjWOZKi8hq25HrPCJ9a/I+iKn87253x2sVTKV+UcgGUF3BK0BYAlmYW2Byo6WjxP4q6wu1kmwS9slcpX9RtHASEkrDzG9pGBbN0W6HdiZqMW4u/iLwmIrkiklpvWayIfCciW1w/Y9yZ4YTUVhFUmcteR2t6tom0O41SqrkFhkK3ccjGLxmWGM3SzAKM8Y52f3cf+b8BnHXIsj8Bc40xycBc1+OWqTgbPwxB8Uk4dL5epXxTygVQtpdzYnaSX1bNtvz9didqEm4t/saYhcCh35MuAGa47s8AJrozw4ko3r0VgISOyTYnUUrZpvuZ4AhiSMUiAK9p+rGjzb+1MWa36/4eoPXhXigiN4vIChFZkZeX1zzp6snathGALskpzb5tpVQLERQB3U4nMvNr4sMCveakr60nfI3VeHbYBjRjzDRjzGBjzOCEhIRmTGYpytlKjXHQM7lHs29bKdWC9JqAlGQzqV0uS7cVekW7vx3Ff6+ItAVw/cy1IUOjSOFW8gPaEBAQYHcUpZSdepwFfv6c61jGnpJKdhZW2J3ohNlR/L8ArnXdvxb43IYMR1VcXkNCVRYVkV3tjqKUsltIDHQ5leTC+YBhiRc0/fi7881F5D3gVCBeRLKBh4DHgA9FZDKwA5jkzgzHa8W2XEbLbvLanmt3FK9RU1NDdnY2lZWVdkdRRxEcHEyHDh30W299vSYQuPUuhofm8HNGByYN7mh3ohPi1uJvjLn8ME+d7s7tNoUtm1I5XZwkdOlndxSvkZ2dTUREBImJiTpURgtmjKGgoIDs7GySkpLsjtNy9DwXZt3N5Nj1/H5DIpU1ToIDHL88X5BhfUMI9YwLQvUK38Mo2LEegMDWvWxO4j0qKyuJi9M5EVo6ESEuLk6/oR0qLB4SRzGi+idKq2qYt9F1urKuDr7+Czx3knXLWmJvzkbS4t+A/VW1BBRaY3kQr338m5IWfs+gn9Nh9LmE0JJt3Bj2I7NXZkBtFXx6IyyZCr0mQHA0zJgAG1rkqcxf0eLfgFVZ++gqOVSFtIZgHdZBKeUy4ApolcKDzhd5MnMCdU/1gdRPYNzDMOlNuHGuNf/vh9fCwiegttruxIelxb8BS7cV0k124d9a+/d7G4fDwYABA+jTpw/nn38+RUVFtmX54YcfWLx4cZO932effcaGDRuOeb3wcJ2notEcAXDj92Se+QbTneeQE94PLp0Bo35vzf4VFgfXfmENCTHvEZh6MqR+Ci3wugAt/g1Ytq2AZMcuHK162h1FNbGQkBDWrFlDamoqsbGxTJ061bYsRyr+tbW1x/x+x1v81TEKDCNx2EQ+ir6R+x33Qe+Jv34+IAQmzYCrPoGAMPj4epg6BGb/ATZ+Bc6WMRG8W3v7eKKKaie7s7cRGlAB8d3tjuO1/v5lGht2lTTpe6a0i+Sh83s3+vXDhw9n3bp1AGRkZHDHHXeQl5dHaGgor7zyCj179mTv3r3ceuutbNu2DYAXX3yRESNG8OSTT/Laa68BcOONN3L33Xezfft2zj77bEaNGsXixYtp3749n3/+OSEhITz77LO89NJL+Pv7k5KSwmOPPcZLL72Ew+Hg7bff5rnnnmP69OkEBwezevVqRo4cSWRkJOHh4dx3330A9OnTh1mzZpGYmMibb77JE088gYjQr18/brvtNr744gsWLFjAo48+yieffALQ4D5lZmZyxRVXUFZWxgUXXNCUH4HPEBEuGNCep+duZndxBW2jQn77om7jrNnA1r4P6z+CVW/Cspeh/WC44WvrW4SNtPgfYsm2AjqbbOtBgjb7eCun08ncuXOZPHkyADfffDMvvfQSycnJLF26lNtvv5158+Zx1113MWbMGGbOnInT6aSsrIyVK1fy+uuvs3TpUowxDB06lDFjxhATE8OWLVt47733eOWVV5g0aRKffPIJV111FY899hiZmZkEBQVRVFREdHQ0t95666+K+/Tp08nOzmbx4sU4HA4efvjhBrOnpaXx6KOPsnjxYuLj4yksLCQ2NpYJEyZw3nnncckllwBw+umnN7hPU6ZM4bbbbuOaa66x9ZuPp7tgQDue+n4zX6zZxS1jDnMxqJ8DBl5p3WqrrD8As++DJS/CyLuaN/AhtPgfYv6mXFL8d1kP4rX4u8uxHKE3pYqKCgYMGEBOTg69evVi/PjxlJWVsXjxYi699NKDr6uqqgJg3rx5vPnmm4B1viAqKooff/yRCy+8kLCwMAAuuugiFi1axIQJE0hKSmLAgAEADBo0iO3btwPQr18/rrzySiZOnMjEiRMPm+/SSy/F4XAc9vkDmS699FLi4+MBiI39bb/yI+3TTz/9dPCbwdVXX80f//jHI25PNSwxPoxBnWN48+cdXDcykSD/I39u+AfByTfC1rnww2PQ+0KIdl0oVl4IFfsgrvlGFNA2/3qMMczbmMvIqHzrYo3wVnZHUk3sQJv/jh07MMYwdepU6urqiI6OZs2aNQdv6enpx/X+QUFBB+87HI6DbfdfffUVd9xxB6tWreLkk08+bJv+gT8oAP7+/tTV1R18fCz97o+2T9qVs2ncPS6ZnKIK3vp5R+NWEIGzHwdTB1+7pjJZ+z48kQwvDIPCbe4Lewgt/vVk5O0ne18FKY4caJVifVDKK4WGhvLss8/yv//9j9DQUJKSkvjoo48A6yBg7dq1gNV08uKLLwJWU1FxcTGjR4/ms88+o7y8nP379zNz5kxGjx592G3V1dWxc+dOxo4dy+OPP05xcTFlZWVERERQWlp62PUSExNZtWoVAKtWrSIzMxOA0047jY8++oiCAmt8mcJCa3z5+u8XGRl52H0aOXIk77//PgDvvPPOcfzrqQNGJycwqls8z8/fSkllI0/kxnSGMX+AjbPgkVYw8xaI7gTOavj+Ybfmrc+3ir+z5ohn2n/YlAsY4sq3QYL29PF2AwcOpF+/frz33nu88847TJ8+nf79+9O7d28+/9y6SOeZZ55h/vz59O3bl0GDBrFhwwZOOukkrrvuOoYMGcLQoUO58cYbGThw4GG343Q6ueqqq+jbty8DBw7krrvuIjo6mvPPP5+ZM2cyYMAAFi1a9Jv1Lr74YgoLC+nduzfPP/883btbHRB69+7NAw88wJgxY+jfvz/33HMPAJdddhn//e9/GThwIBkZGUfcp6lTp9K3b19ycnKa+p/V5/zp7J4Uldfw0g8ZjV9p+J3QYQg4q+DUv8CdK2DsA9bFYTuarvvvkYinjEs9ePBgs2LFihN7k3cmWSdgLn+vwaevfHUJpng375ZeB+c8AUNuOrHtqV9JT0+nVy8dLsNT6OfVeFPeX803aXv44b6xtIkKbtxKVWVQtveXdv7qcnhuEES0hhvngV/THJuLyEpjzOBDl/vOkX9ZHmz9DjZ/Y51cOfTpqlqWZRYyoV2xtUCP/JVSjXTfGT2oq4N/zErDWdfIA+qg8F+f4A0MhXEPwa7VVtdQN/Od4r9ptnWSxTit+4f4cUs+NU7DsHDXYE2t9IhHKdU4HWNDmTIumdnr93D3B2uocdYdfaWG9J0E7QbC3L+7fWgI3yn+G2dBdGeI6gTpX/7m6e/T9xIR7E+nmgwIjbdG8FNKqUa6Y2w3/nhWT75cu4vb3l7JnuJKKmucxzblo58fjH0QSnIg/Qv3hcVX+vlXlsC2H2DIzdYYG8tfgapSa2JmoNZZx/fpezmjZxx+W7+zrspTSqljdNupXQkP9uehz1MZ9u+5B5cHB/gRHOAg2N9Bp7hQrh+RyBm92+Dwa6BHYdfTILYLLHsF+l7itqy+Ufy3fGt1o+p5ntXss2QqZMyHlAkALMsspKi8ht+1yoaNBdDrfJsDK6U81dXDOjOwYzRLMwuprHFSVeOksraOyhonFdVOlmQWcNs7q+gcF8rkUUlcMqgDoYH1SrGfHwyeDN8+AHvWQ5u+bsnpG8V/4ywIS4COQ6wj/+Boq90/ZQJ1dYZ3lmUR5O/HwPKfwD/YGpNDKaWOU5/2UfRpH9Xgc846w7dpe5i2aBt/+zyNJ7/bzDXDE7lhZCLRoYHWiwZeCfMetY7+Jzzrloze3+ZfUwlbvrOmYPNzgMMfup8Jm7/BWVvDnz5dx1frdnPz6CQCNs+GrqdbZ+GV18rOzuaCCy4gOTmZrl27MmXKFKqrf3tybdeuXQfHyTmSc84557iHhn744Yd54oknjmtd5ZkcfsLZfdsy8/aRfHLbcIYkxvLs3C2M+e8PzFm/23pRSIzV5LP+I6gocksO7y/+mQugugx61mvK6XEOVBTyzBvv8OGKbKacnsw9fcqhJBt6nWdfVuV2xhguuugiJk6cyJYtW9i8eTNlZWU88MADv3pdbW0t7dq14+OPPz7qe86ePZvo6Gg3JVbebFDnWKZdM5g5U0aTGBfKbe+s4o8fr2N/Va11nVFNOax51y3b9v5mn/QvICgSkk45uKikwymEEED49m954JxHuemULvD930Ec0P0sG8P6kJV3w741TfueMQNg0NNHfMm8efMIDg7m+uuvB6zxd5566imSkpJISkri66+/pqysDKfTyYwZMzjvvPNITU2lvLyc6667jtTUVHr06MGuXbuYOnUqgwcPJjExkRUrVlBWVnbYIZ1feeUVpk2bRnV1Nd26deOtt94iNDS0afdfeaxebSP5+LYRPP39Zl74IYOlmQU8f8VJ9OkwBJa/CkNvbbKLvg7w/iP/3I2QfAb4W21pWQXlXDR9PQvq+nFD4PfcFJ9qvS79S0gaDaG/HSFReY+0tDQGDRr0q2WRkZF06tSJ2tpaVq1axccff8yCBQt+9ZoXXniBmJgYNmzYwCOPPMLKlSsbfP8tW7Zwxx13kJaWRnR09MHRMy+66CKWL1/O2rVr6dWrF9OnT3fPDiqPFeDw4/4ze/LeTcOorq3j0pd+Zn37SVBXY3X9bGLef+R/4/dQvR+wevXc8tYKDBD1u5fx//k2+PBq6H85FGyBYbfam9WXHOUI3S7jx49vcIjkH3/8kSlTpgDWpCr9+vVrcP3DDemcmprKgw8+SFFREWVlZZx55pluya8837AucXx+5ygmz1jORQtb89fzPuOaA0M/NyHvP/IXgaBwPl6ZzZWvLiEmNJCZt4/k5N7JcO2XcNI1sPY9q5dPn4vtTqvcLCUl5TdH7SUlJWRlZeHv7/+rIZWPx+GGdL7uuut4/vnnWb9+PQ899NAxDc+sfE9CRBDv3zyMU3u145HZm9iev7/Jt+H1xb+uzvDYnI3c99FahiTFMvP2kSTFu/6DB4TAhOesCZgnvmCdYVde7fTTT6e8vPzgBC1Op5N7772X66677oht8CNHjuTDDz8EYMOGDaxfv/6YtltaWkrbtm2pqanRYZRVo4QG+vPSVYP46NYRJMaf2EFJQ2wr/iJylohsEpGtIvInd2zDWWe47Z2VvLQggyuGduKN64cQFdrAvJm9J+pRv48QEWbOnMlHH31EcnIy3bt3Jzg4mH/9619HXO/2228nLy+PlJQUHnzwQXr37k1UVMP9uBvyyCOPMHToUEaOHEnPnjpooGoch58woGO0W97bliGdRcQBbAbGA9nAcuByY8yGw61zvEM6P/71RhLCg7h+ZKLOXmQzTx4i2Ol0UlNTQ3BwMBkZGYwbN45NmzYRGBhodzS38eTPS/3icEM623XCdwiw1RizDUBE3gcuAA5b/I/XH8/Soyx14srLyxk7diw1NTUYY3jhhRe8uvAr72dX8W8P7Kz3OBsYalMWpY4qIiKCE55MSKkWpEWf8BWRm0VkhYisyMvLszuOagKeMnOcr9PPyfvZVfxzgPodVzu4lv2KMWaaMWawMWZwQkJCs4VT7hEcHExBQYEWlhbOGENBQQHBwY2cjlB5JLuafZYDySKShFX0LwOusCmLaiYdOnQgOzsb/RbX8gUHB9OhQwe7Yyg3sqX4G2NqReRO4BvAAbxmjEmzI4tqPgEBASQlJdkdQymFjcM7GGNmA7+dTFcppZTbtegTvkoppdxDi79SSvkgW67wPR4ikgfsOIZV4oF8N8VpqXxxn8E399sX9xl8c79PdJ87G2N+013SY4r/sRKRFQ1d0uzNfHGfwTf32xf3GXxzv921z9rso5RSPkiLv1JK+SBvLv7T7A5gA1/cZ/DN/fbFfQbf3G+37LPXtvkrpZQ6PG8+8ldKKXUYWvyVUsoHeV3xb47pIVsCEekoIvNFZIOIpInIFNfyWBH5TkS2uH563cTEIuIQkdUiMsv1OElElro+8w9ExOtmWRGRaBH5WEQ2iki6iAz39s9aRH7v+t1OFZH3RCTYGz9rEXlNRHJFJLXesgY/W7E869r/dSJy0vFu16uKv2t6yKnA2UAKcLmIpNibym1qgXuNMSnAMOAO177+CZhrjEkG5roee5spQHq9x48DTxljugH7gMm2pHKvZ4CvjTE9gf5Y+++1n7WItAfuAgYbY/pgDQB5Gd75Wb8BnHXIssN9tmcDya7bzcCLx7tRryr+1Jse0hhTDRyYHtLrGGN2G2NWue6XYhWD9lj7O8P1shnARFsCuomIdADOBV51PRbgNOBj10u8cZ+jgFOA6QDGmGpjTBFe/lljDTwZIiL+QCiwGy/8rI0xC4HCQxYf7rO9AHjTWJYA0SLS9ni2623Fv6HpIdvblKXZiEgiMBBYCrQ2xux2PbUHaG1XLjd5GvgDUOd6HAcUGWNqXY+98TNPAvKA113NXa+KSBhe/FkbY3KAJ4AsrKJfDKzE+z/rAw732TZZjfO24u9zRCQc+AS42xhTUv85Y/Xj9Zq+vCJyHpBrjFlpd5Zm5g+cBLxojBkI7OeQJh4v/KxjsI5yk4B2QBi/bRrxCe76bL2t+DdqekhvISIBWIX/HWPMp67Few98DXT9zLUrnxuMBCaIyHasJr3TsNrCo11NA+Cdn3k2kG2MWep6/DHWHwNv/qzHAZnGmDxjTA3wKdbn7+2f9QGH+2ybrMZ5W/E/OD2kqxfAZcAXNmdyC1db93Qg3RjzZL2nvgCudd2/Fvi8ubO5izHmz8aYDsaYRKzPdp4x5kpgPnCJ62Vetc8Axpg9wE4R6eFadDqwAS/+rLGae4aJSKjrd/3APnv1Z13P4T7bL4BrXL1+hgHF9ZqHjo0xxqtuwDnAZiADeMDuPG7cz1FYXwXXAWtct3Ow2sDnAluA74FYu7O6af9PBWa57ncBlgFbgY+AILvzuWF/BwArXJ/3Z0CMt3/WwN+BjUAq8BYQ5I2fNfAe1nmNGqxveZMP99kCgtWjMQNYj9Ub6ri2q8M7KKWUD/K2Zh+llFKNoMVfKaV8kBZ/pZTyQVr8lVLKB2nxV0opH6TFX6lDiEiciKxx3faISI7rfpmIvGB3PqWagnb1VOoIRORhoMwY84TdWZRqSnrkr1Qjicip9eYQeFhEZojIIhHZISIXich/RGS9iHztGnoDERkkIgtEZKWIfHO8IzAq1dS0+Ct1/LpijS80AXgbmG+M6QtUAOe6/gA8B1xijBkEvAb8066wStXnf/SXKKUOY44xpkZE1mNNNvK1a/l6IBHoAfQBvrOGp8GBdRm/UrbT4q/U8asCMMbUiUiN+eUEWh3W/y0B0owxw+0KqNThaLOPUu6zCUgQkeFgDcEtIr1tzqQUoMVfKbcx1lSilwCPi8harJFXR9gaSikX7eqplFI+SI/8lVLKB2nxV0opH6TFXymlfJAWf6WU8kFa/JVSygdp8VdKKR+kxV8ppXzQ/wP/DvBEcUjAJQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "titles = ['MLGRF: 2 dims','MLGRF: 4 dims','MLGRF: 6 dims','MLGRF: 8 dims','MLGRF: 10 dims']\n",
    "\n",
    "legend_elements = [Line2D([0], [0], label='Reconstructed'), \n",
    "                   Line2D([0], [0], color='orange', label='Original')]\n",
    "\n",
    "for i in range(5):\n",
    "    fig = comb_fd[i].plot()\n",
    "    fig.show()\n",
    "    plt.title(titles[i])\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"ML_GRF\")\n",
    "    plt.legend(handles = legend_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700a3ae",
   "metadata": {},
   "source": [
    "# Latent Space Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7dd88e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    recondata, latentmu, latentlv = models_npars[0](torch.tensor(ML_GRF_stance_N_matrix, dtype=torch.float32))\n",
    "    recondata = recondata.numpy()\n",
    "    latentmu = latentmu.numpy()\n",
    "    latentlv = latentlv.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b2f06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_PCA = PCA()\n",
    "latent_PCA.fit(latentmu[trainidx])\n",
    "latentmu_PCA = latent_PCA.transform(latentmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44def7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor()\n",
    "lof.fit(latentmu_PCA)\n",
    "outidx = np.argsort(lof.negative_outlier_factor_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7e35b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O1 | ID : 1832 | Trial: 2 | Leg: LEFT | Trial Length: 778\n",
      "O2 | ID : 217 | Trial: 2 | Leg: RIGHT | Trial Length: 761\n",
      "O3 | ID : 1983 | Trial: 5 | Leg: RIGHT | Trial Length: 869\n",
      "O4 | ID : 1479 | Trial: 4 | Leg: RIGHT | Trial Length: 698\n",
      "O5 | ID : 612 | Trial: 3 | Leg: RIGHT | Trial Length: 842\n",
      "O6 | ID : 1321 | Trial: 3 | Leg: LEFT | Trial Length: 817\n",
      "O7 | ID : 1546 | Trial: 2 | Leg: RIGHT | Trial Length: 905\n",
      "O8 | ID : 2244 | Trial: 4 | Leg: LEFT | Trial Length: 662\n",
      "O9 | ID : 1257 | Trial: 2 | Leg: LEFT | Trial Length: 792\n",
      "O10 | ID : 1493 | Trial: 4 | Leg: LEFT | Trial Length: 677\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    j = outidx[i]\n",
    "    print('O%i | ID : %i | Trial: %i | Leg: %s | Trial Length: %i' % (i+1, int(ID_info_matrix[j,0]), int(ID_info_matrix[j,2]), ID_info_matrix[j,1] , int(ID_info_matrix[j,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8ec13ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(subsel, offset=3):\n",
    "    ML_GRF_stance_N_out = ML_GRF_stance_N_matrix[outidx[subsel]].copy()\n",
    "#    weigout = spec_weig[outidx[subsel]]\n",
    "\n",
    "    ML_GRF_stance_N_out /= np.mean(np.abs(ML_GRF_stance_N_out), axis=1)[:,None]\n",
    "    ML_GRF_stance_N_outfil = ML_GRF_stance_N_out.copy()\n",
    "#    specoutfil[weigout == 0] = float('nan')\n",
    "\n",
    "    prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "    colors = prop_cycle.by_key()['color']\n",
    "    for i in range(len(subsel)):\n",
    "        plt.plot(grid_points_100, ML_GRF_stance_N_out[i] + offset*i, zorder=-i-0.01, c=colors[subsel[i]], alpha=0.5)\n",
    "        plt.plot(grid_points_100, ML_GRF_stance_N_outfil[i] + offset*i, zorder=-i, c=colors[subsel[i]])\n",
    "        plt.text(grid_points_100[-1], ML_GRF_stance_N_out[i,-1] + i*offset, 'O%i' % (subsel[i]+1), color=colors[subsel[i]], fontsize='small')\n",
    "#    plt.xlim((3250,8800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f02095f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1350 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7.2,4.5), dpi=300)\n",
    "fig.subplots_adjust(hspace=0, wspace=0)\n",
    "plt.subplot(1,2,1)\n",
    "plt.ylabel('APGRF + Offset')\n",
    "plt.yticks([])\n",
    "plt.xlabel('Time')\n",
    "plot_outliers(range(0,5))\n",
    "plt.subplot(1,2,2)\n",
    "plot_outliers(range(5,10))\n",
    "plt.yticks([])\n",
    "plt.xlabel('Time')\n",
    "#plt.savefig('figures/outliers.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b41c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
