{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778863dc",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0fc7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "import os\n",
    "import skfda\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import itertools \n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib as mpl\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0fa96e",
   "metadata": {},
   "source": [
    "# Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73a0610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wf(x):\n",
    "    return '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Data/' + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "250c5bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_GRF_stance_N = pd.read_csv(wf('V_GRF_stance_N.csv'), header = None)\n",
    "ML_GRF_stance_N = pd.read_csv(wf('ML_GRF_stance_N.csv'), header = None)\n",
    "AP_GRF_stance_N = pd.read_csv(wf('AP_GRF_stance_N.csv'), header = None)\n",
    "ID_info = pd.read_csv(wf('IDinfo.csv'), header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "13bf172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_GRF_stance_N_matrix = V_GRF_stance_N.to_numpy()\n",
    "ML_GRF_stance_N_matrix = ML_GRF_stance_N.to_numpy()\n",
    "AP_GRF_stance_N_matrix = AP_GRF_stance_N.to_numpy()\n",
    "ID_info_matrix = ID_info.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "11ab2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a list of grid points\n",
    "grid_points_100 = list(range(1,101))\n",
    "grid_points_2990 = list(range(1,2991))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5c424dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_GRF_stance_N_fd = skfda.FDataGrid(data_matrix=V_GRF_stance_N_matrix,grid_points=grid_points_100)\n",
    "V_GRF_stance_N_mean = skfda.exploratory.stats.mean(V_GRF_stance_N_fd)\n",
    "\n",
    "ML_GRF_stance_N_fd = skfda.FDataGrid(data_matrix=ML_GRF_stance_N_matrix,grid_points=grid_points_100)\n",
    "ML_GRF_stance_N_mean = skfda.exploratory.stats.mean(ML_GRF_stance_N_fd)\n",
    "\n",
    "AP_GRF_stance_N_fd = skfda.FDataGrid(data_matrix=AP_GRF_stance_N_matrix,grid_points=grid_points_100)\n",
    "AP_GRF_stance_N_mean = skfda.exploratory.stats.mean(AP_GRF_stance_N_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "108eb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_GRF_stance_N_mean_array = np.zeros((15696,100))\n",
    "for j in range(0,15696):\n",
    "    for i in range(0,100):\n",
    "        val = V_GRF_stance_N_mean.data_matrix[0,i,0]\n",
    "        V_GRF_stance_N_mean_array[j,i] = val\n",
    "    \n",
    "ML_GRF_stance_N_mean_array = np.zeros((15696,100))\n",
    "for j in range(0,15696):\n",
    "    for i in range(0,100):\n",
    "        val = ML_GRF_stance_N_mean.data_matrix[0,i,0]\n",
    "        ML_GRF_stance_N_mean_array[j,i] = val\n",
    "    \n",
    "AP_GRF_stance_N_mean_array = np.zeros((15696,100))\n",
    "for j in range(0,15696):\n",
    "    for i in range(0,100):\n",
    "        val = AP_GRF_stance_N_mean.data_matrix[0,i,0]\n",
    "        AP_GRF_stance_N_mean_array[j,i] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b9e7a",
   "metadata": {},
   "source": [
    "# Defining VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf3f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoVAE(nn.Module):\n",
    "    def __init__(self,nfeat=100, ncode=5, alpha=0, lambd=10000, nhidden=128, nhidden2=35, dropout=0.2):\n",
    "        super(InfoVAE, self).__init__()\n",
    "        \n",
    "        self.ncode = int(ncode)\n",
    "        self.alpha = float(alpha)\n",
    "        self.lambd = float(lambd)\n",
    "        \n",
    "        self.encd = nn.Linear(nfeat, nhidden)\n",
    "        self.d1 = nn.Dropout(p=dropout)\n",
    "        self.enc2 = nn.Linear(nhidden, nhidden2)\n",
    "        self.d2 = nn.Dropout(p=dropout)\n",
    "        self.mu = nn.Linear(nhidden2, ncode)\n",
    "        self.lv = nn.Linear(nhidden2, ncode)\n",
    "        \n",
    "        self.decd = nn.Linear(ncode, nhidden2)\n",
    "        self.d3 = nn.Dropout(p=dropout)\n",
    "        self.dec2 = nn.Linear(nhidden2, nhidden)\n",
    "        self.d4 = nn.Dropout(p=dropout)\n",
    "        self.outp = nn.Linear(nhidden, nfeat)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.d1(F.leaky_relu(self.encd(x)))\n",
    "        x = self.d2(F.leaky_relu(self.enc2(x)))\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.lv(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.d3(F.leaky_relu(self.decd(x)))\n",
    "        x = self.d4(F.leaky_relu(self.dec2(x)))\n",
    "        x = self.outp(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    # https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n",
    "    def compute_kernel(self, x, y):\n",
    "        x_size = x.size(0)\n",
    "        y_size = y.size(0)\n",
    "        dim = x.size(1)\n",
    "        x = x.unsqueeze(1) # (x_size, 1, dim)\n",
    "        y = y.unsqueeze(0) # (1, y_size, dim)\n",
    "        tiled_x = x.expand(x_size, y_size, dim)\n",
    "        tiled_y = y.expand(x_size, y_size, dim)\n",
    "        # The example code divides by (dim) here, making <kernel_input> ~ 1/dim\n",
    "        # excluding (dim) makes <kernel_input> ~ 1\n",
    "        kernel_input = (tiled_x - tiled_y).pow(2).mean(2)#/float(dim)\n",
    "        return torch.exp(-kernel_input) # (x_size, y_size)\n",
    "    \n",
    "    # https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n",
    "    def compute_mmd(self, x, y):\n",
    "        xx_kernel = self.compute_kernel(x,x)\n",
    "        yy_kernel = self.compute_kernel(y,y)\n",
    "        xy_kernel = self.compute_kernel(x,y)\n",
    "        return torch.mean(xx_kernel) + torch.mean(yy_kernel) - 2*torch.mean(xy_kernel)\n",
    "    \n",
    "    def loss(self, x, epoch):\n",
    "        recon_x, mu, logvar = self.forward(x)\n",
    "        MSE = torch.sum(0.5 *  (x - recon_x).pow(2))\n",
    "        \n",
    "        # KL divergence (Kingma and Welling, https://arxiv.org/abs/1312.6114, Appendix B)\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        #return MSE + self.beta*KLD, MSE\n",
    "                \n",
    "        # https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n",
    "        true_samples = Variable(torch.randn(200, self.ncode), requires_grad=False)\n",
    "        z = self.reparameterize(mu, logvar) #duplicate call\n",
    "        # compute MMD ~ 1, so upweight to match KLD which is ~ n_batch x n_code\n",
    "        MMD = self.compute_mmd(true_samples,z) * x.size(0) * self.ncode\n",
    "        return MSE + (1-self.alpha)*KLD + (self.lambd+self.alpha-1)*MMD, MSE, KLD, MMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6391eac9",
   "metadata": {},
   "source": [
    "# Preparing Test and Train datasets for model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf5caf4",
   "metadata": {},
   "source": [
    "# APGRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3099c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfrac = 0.75\n",
    "ntrain = int(AP_GRF_stance_N_matrix.shape[0] * trainfrac)\n",
    "nvalid = AP_GRF_stance_N_matrix.shape[0] - ntrain\n",
    "nfeat = AP_GRF_stance_N_matrix.shape[1]\n",
    "np.random.seed(20190425) # make validation set deterministic\n",
    "permutation = np.random.permutation(AP_GRF_stance_N_matrix.shape[0])\n",
    "np.random.seed()\n",
    "trainidx = permutation[0:ntrain]\n",
    "valididx = permutation[-1-nvalid:-1]\n",
    "\n",
    "train_apgrf = AP_GRF_stance_N_matrix[trainidx,:]\n",
    "valid_apgrf = AP_GRF_stance_N_matrix[valididx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2c9f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.tensor(train_apgrf, dtype=torch.float32)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "valdloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.tensor(valid_apgrf, dtype=torch.float32)),\n",
    "    batch_size=nvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea9f88",
   "metadata": {},
   "source": [
    "## Defining Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cc0c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(): #model, optimizer, epoch, min_valid_loss, badepochs\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_logL = 0\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        spectrum = data[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss, logL, KLD, MMD = model.loss(spectrum, epoch)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        train_logL += logL.item()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(dataloader.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_logL = 0\n",
    "        valid_KLD = 0\n",
    "        valid_MMD = 0\n",
    "\n",
    "        for valddata in valdloader:\n",
    "            spectrum = valddata[0]\n",
    "            loss, logL, KLD, MMD = model.loss(spectrum, epoch)\n",
    "            valid_loss += loss.item()\n",
    "            valid_logL += logL.item()\n",
    "            valid_KLD += KLD.item()\n",
    "            valid_MMD += MMD.item()\n",
    "        \n",
    "        valid_loss /= len(valdloader.dataset)\n",
    "        valid_logL /= -len(valdloader.dataset)\n",
    "        valid_KLD  /= len(valdloader.dataset)\n",
    "        valid_MMD  /= len(valdloader.dataset)\n",
    "    return valid_loss, valid_logL, valid_KLD, valid_MMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97b38cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, precision=1e-3, patience=10):\n",
    "        self.precision = precision\n",
    "        self.patience = patience\n",
    "        self.badepochs = 0\n",
    "        self.min_valid_loss = float('inf')\n",
    "        \n",
    "    def step(self, valid_loss):\n",
    "        if valid_loss < self.min_valid_loss*(1-self.precision):\n",
    "            self.badepochs = 0\n",
    "            self.min_valid_loss = valid_loss\n",
    "        else:\n",
    "            self.badepochs += 1\n",
    "        return not (self.badepochs == self.patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a724bb",
   "metadata": {},
   "source": [
    "# Setting training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcfac0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "log_interval = 10\n",
    "mdl_ncode_l = range(2,11,2)\n",
    "n_config_l = range(100)\n",
    "test_list = [n_config_l,mdl_ncode_l]\n",
    "\n",
    "mdl_MSE = np.zeros((100, 5))\n",
    "mdl_KLD = np.zeros((100, 5))\n",
    "mdl_MMD = np.zeros((100, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5ee1882",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/GRFy'\n",
    "# Parent Directory path\n",
    "parent_dir = '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/'\n",
    "# Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851fb58a",
   "metadata": {},
   "source": [
    "# Training 100 models for 5 different encoder shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "853f5e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config 0, alpha = 0.0, lambda = 28183.0, dropout = 0.00; 2 hidden layers with 11, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.41e+04 logL: -1.38e+04 KL: 7.08e+02 MMD: 6.96e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.37e+04 logL: -1.36e+04 KL: 4.83e+02 MMD: 6.96e-01\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.39e+04 logL: -1.29e+04 KL: 4.22e+02 MMD: 7.29e-01\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 36 VALIDATION Loss: 3.14e+04 logL: -1.27e+04 KL: 4.24e+02 MMD: 6.49e-01\n",
      "config 0, alpha = 0.0, lambda = 570.6, dropout = 0.00; 2 hidden layers with 11, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.07e+04 logL: -1.91e+04 KL: 9.83e+02 MMD: 1.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.75e+04 logL: -1.61e+04 KL: 8.99e+02 MMD: 1.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.56e+04 logL: -1.45e+04 KL: 5.37e+02 MMD: 9.68e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.29e+04 logL: -1.20e+04 KL: 3.19e+02 MMD: 1.07e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.15e+04 logL: -1.06e+04 KL: 2.86e+02 MMD: 9.92e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.11e+04 logL: -1.02e+04 KL: 2.36e+02 MMD: 1.04e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.07e+04 logL: -9.84e+03 KL: 1.98e+02 MMD: 1.11e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 8.85e+03 logL: -8.06e+03 KL: 1.66e+02 MMD: 1.10e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 8.08e+03 logL: -7.33e+03 KL: 1.49e+02 MMD: 1.06e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 7.88e+03 logL: -7.14e+03 KL: 1.29e+02 MMD: 1.08e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 7.79e+03 logL: -7.10e+03 KL: 1.12e+02 MMD: 1.02e+00\n",
      "Epoch 00112: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 116 VALIDATION Loss: 7.79e+03 logL: -7.08e+03 KL: 1.08e+02 MMD: 1.06e+00\n",
      "config 0, alpha = 0.0, lambda = 69819.8, dropout = 0.00; 2 hidden layers with 8, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.23e+04 logL: -1.52e+04 KL: 5.52e+00 MMD: 1.02e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.58e+04 logL: -1.36e+04 KL: 5.27e+00 MMD: 3.12e-02\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.66e+04 logL: -1.36e+04 KL: 5.40e+00 MMD: 4.40e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.70e+04 logL: -1.36e+04 KL: 5.80e+00 MMD: 4.95e-02\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 45 VALIDATION Loss: 1.57e+04 logL: -1.35e+04 KL: 5.76e+00 MMD: 3.16e-02\n",
      "config 0, alpha = 0.0, lambda = 51960.3, dropout = 0.00; 2 hidden layers with 43, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.38e+05 logL: -3.49e+05 KL: 1.13e+05 MMD: 1.46e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.33e+05 logL: -3.32e+05 KL: 3.13e+04 MMD: 1.34e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.19e+05 logL: -1.78e+04 KL: 1.25e+04 MMD: 1.70e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.00e+05 logL: -1.54e+04 KL: 5.67e+03 MMD: 1.53e+00\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.03e+05 logL: -1.49e+04 KL: 2.85e+03 MMD: 1.64e+00\n",
      "Stopping\n",
      "====> Epoch: 53 VALIDATION Loss: 1.04e+05 logL: -1.48e+04 KL: 2.77e+03 MMD: 1.66e+00\n",
      "config 0, alpha = 0.0, lambda = 52867.1, dropout = 0.00; 2 hidden layers with 46, 45 nodes\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 10 VALIDATION Loss: 1.01e+05 logL: -3.44e+03 KL: 4.50e+02 MMD: 1.83e+00\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 20 VALIDATION Loss: 1.14e+05 logL: -3.00e+03 KL: 4.20e+02 MMD: 2.08e+00\n",
      "Stopping\n",
      "====> Epoch: 21 VALIDATION Loss: 1.02e+05 logL: -3.00e+03 KL: 4.19e+02 MMD: 1.86e+00\n",
      "config 1, alpha = 0.0, lambda = 27.6, dropout = 0.00; 2 hidden layers with 8, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.53e+04 logL: -1.46e+04 KL: 6.86e+02 MMD: 7.55e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.44e+04 logL: -1.38e+04 KL: 5.34e+02 MMD: 7.35e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.41e+04 logL: -1.37e+04 KL: 4.01e+02 MMD: 7.11e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.39e+04 logL: -1.36e+04 KL: 2.88e+02 MMD: 7.51e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.38e+04 logL: -1.36e+04 KL: 2.02e+02 MMD: 7.15e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.55e+02 MMD: 7.83e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.36e+04 logL: -1.34e+04 KL: 1.17e+02 MMD: 7.63e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 8.97e+01 MMD: 7.98e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 7.26e+01 MMD: 8.29e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.34e+04 logL: -1.34e+04 KL: 5.86e+01 MMD: 8.76e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.34e+04 logL: -1.33e+04 KL: 4.87e+01 MMD: 9.22e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 9.69e+03 logL: -9.60e+03 KL: 7.40e+01 MMD: 7.43e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 9.59e+03 logL: -9.51e+03 KL: 6.27e+01 MMD: 7.81e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 9.56e+03 logL: -9.49e+03 KL: 5.35e+01 MMD: 7.44e-01\n",
      "Stopping\n",
      "====> Epoch: 146 VALIDATION Loss: 9.55e+03 logL: -9.48e+03 KL: 4.95e+01 MMD: 8.22e-01\n",
      "config 1, alpha = 0.0, lambda = 6.9, dropout = 0.00; 2 hidden layers with 14, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.40e+04 logL: -1.36e+04 KL: 3.97e+02 MMD: 1.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.31e+04 logL: -1.28e+04 KL: 2.69e+02 MMD: 1.01e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.70e+03 logL: -9.56e+03 KL: 1.43e+02 MMD: 1.10e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.61e+03 logL: -9.51e+03 KL: 9.84e+01 MMD: 1.19e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.40e+03 logL: -7.29e+03 KL: 1.05e+02 MMD: 1.05e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.18e+03 logL: -7.10e+03 KL: 7.78e+01 MMD: 1.14e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.16e+03 logL: -7.09e+03 KL: 6.59e+01 MMD: 1.13e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.40e+03 logL: -5.31e+03 KL: 8.56e+01 MMD: 9.77e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 5.24e+03 logL: -5.16e+03 KL: 7.06e+01 MMD: 1.10e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.20e+03 logL: -5.13e+03 KL: 6.30e+01 MMD: 1.19e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.19e+03 logL: -5.13e+03 KL: 5.74e+01 MMD: 1.09e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.16e+03 logL: -5.10e+03 KL: 5.44e+01 MMD: 1.21e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 5.17e+03 logL: -5.11e+03 KL: 5.16e+01 MMD: 1.01e+00\n",
      "Epoch 00131: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 5.14e+03 logL: -5.08e+03 KL: 5.02e+01 MMD: 1.11e+00\n",
      "Epoch 00140: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 142 VALIDATION Loss: 5.14e+03 logL: -5.08e+03 KL: 5.02e+01 MMD: 1.13e+00\n",
      "config 1, alpha = 0.0, lambda = 2338.3, dropout = 0.00; 2 hidden layers with 12, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.84e+04 logL: -1.49e+04 KL: 5.27e+02 MMD: 1.26e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.82e+04 logL: -1.45e+04 KL: 4.18e+02 MMD: 1.38e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.52e+04 logL: -1.19e+04 KL: 3.20e+02 MMD: 1.28e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.30e+04 logL: -9.43e+03 KL: 2.61e+02 MMD: 1.41e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.14e+04 logL: -8.00e+03 KL: 2.12e+02 MMD: 1.36e+00\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.13e+04 logL: -7.79e+03 KL: 1.94e+02 MMD: 1.40e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.11e+04 logL: -7.71e+03 KL: 1.91e+02 MMD: 1.35e+00\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.09e+04 logL: -7.63e+03 KL: 1.87e+02 MMD: 1.33e+00\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 1.10e+04 logL: -7.63e+03 KL: 1.87e+02 MMD: 1.36e+00\n",
      "config 1, alpha = 0.0, lambda = 115.1, dropout = 0.00; 2 hidden layers with 18, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.30e+05 logL: -3.25e+05 KL: 4.74e+03 MMD: 1.89e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.63e+04 logL: -3.90e+04 KL: 7.05e+03 MMD: 1.66e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.52e+04 logL: -2.98e+04 KL: 5.27e+03 MMD: 1.56e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.98e+04 logL: -2.49e+04 KL: 4.68e+03 MMD: 1.69e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.82e+04 logL: -2.36e+04 KL: 4.36e+03 MMD: 1.70e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.63e+04 logL: -2.21e+04 KL: 3.98e+03 MMD: 1.52e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.33e+04 logL: -1.97e+04 KL: 3.37e+03 MMD: 1.67e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.88e+04 logL: -1.61e+04 KL: 2.51e+03 MMD: 1.60e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.46e+04 logL: -1.27e+04 KL: 1.67e+03 MMD: 1.69e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 100 VALIDATION Loss: 1.24e+04 logL: -1.11e+04 KL: 1.04e+03 MMD: 1.78e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.09e+04 logL: -1.00e+04 KL: 6.42e+02 MMD: 1.68e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.01e+04 logL: -9.54e+03 KL: 4.10e+02 MMD: 1.75e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 9.50e+03 logL: -9.00e+03 KL: 3.13e+02 MMD: 1.64e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 9.03e+03 logL: -8.61e+03 KL: 2.52e+02 MMD: 1.55e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 7.93e+03 logL: -7.53e+03 KL: 2.17e+02 MMD: 1.63e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 6.42e+03 logL: -6.06e+03 KL: 1.80e+02 MMD: 1.60e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 4.10e+03 logL: -3.73e+03 KL: 1.75e+02 MMD: 1.65e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 3.49e+03 logL: -3.14e+03 KL: 1.44e+02 MMD: 1.77e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 3.27e+03 logL: -2.94e+03 KL: 1.25e+02 MMD: 1.75e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 3.13e+03 logL: -2.82e+03 KL: 1.12e+02 MMD: 1.72e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 3.13e+03 logL: -2.82e+03 KL: 1.12e+02 MMD: 1.72e+00\n",
      "config 1, alpha = 0.0, lambda = 38473.4, dropout = 0.00; 2 hidden layers with 35, 24 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.23e+04 logL: -3.74e+03 KL: 4.85e+02 MMD: 1.77e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.75e+04 logL: -2.80e+03 KL: 2.96e+02 MMD: 1.93e+00\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 27 VALIDATION Loss: 7.88e+04 logL: -2.70e+03 KL: 2.68e+02 MMD: 1.97e+00\n",
      "config 2, alpha = 0.0, lambda = 6831.3, dropout = 0.00; 2 hidden layers with 40, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.41e+04 logL: -1.43e+04 KL: 4.79e+03 MMD: 7.31e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.96e+04 logL: -1.37e+04 KL: 1.32e+03 MMD: 6.68e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.83e+04 logL: -1.25e+04 KL: 7.26e+02 MMD: 7.43e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.53e+04 logL: -9.87e+03 KL: 5.39e+02 MMD: 7.22e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.46e+04 logL: -9.68e+03 KL: 3.11e+02 MMD: 6.81e-01\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.48e+04 logL: -9.64e+03 KL: 2.75e+02 MMD: 7.12e-01\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 67 VALIDATION Loss: 1.43e+04 logL: -9.63e+03 KL: 2.69e+02 MMD: 6.45e-01\n",
      "config 2, alpha = 0.0, lambda = 133.6, dropout = 0.00; 2 hidden layers with 50, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.48e+04 logL: -1.38e+04 KL: 9.05e+02 MMD: 1.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+04 logL: -1.35e+04 KL: 3.01e+02 MMD: 1.14e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.93e+03 logL: -9.59e+03 KL: 1.93e+02 MMD: 1.05e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.64e+03 logL: -5.33e+03 KL: 1.67e+02 MMD: 1.06e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.45e+03 logL: -5.19e+03 KL: 1.17e+02 MMD: 1.07e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.36e+03 logL: -5.14e+03 KL: 9.26e+01 MMD: 9.82e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 5.28e+03 logL: -5.07e+03 KL: 7.90e+01 MMD: 9.64e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 5.21e+03 logL: -5.02e+03 KL: 7.11e+01 MMD: 9.40e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 5.14e+03 logL: -4.94e+03 KL: 6.77e+01 MMD: 9.82e-01\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 5.09e+03 logL: -4.89e+03 KL: 6.54e+01 MMD: 1.02e+00\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 5.09e+03 logL: -4.88e+03 KL: 6.50e+01 MMD: 1.06e+00\n",
      "Epoch 00111: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 115 VALIDATION Loss: 5.08e+03 logL: -4.88e+03 KL: 6.50e+01 MMD: 1.03e+00\n",
      "config 2, alpha = 0.0, lambda = 4.1, dropout = 0.00; 2 hidden layers with 133, 54 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.17e+03 logL: -2.90e+03 KL: 2.64e+02 MMD: 1.25e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.91e+03 logL: -2.75e+03 KL: 1.52e+02 MMD: 1.42e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.81e+03 logL: -2.69e+03 KL: 1.08e+02 MMD: 1.31e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.81e+03 logL: -2.71e+03 KL: 9.51e+01 MMD: 1.33e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.73e+03 logL: -2.65e+03 KL: 8.20e+01 MMD: 1.26e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.68e+03 logL: -2.60e+03 KL: 7.39e+01 MMD: 1.33e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.76e+03 logL: -2.69e+03 KL: 6.43e+01 MMD: 1.35e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.57e+03 logL: -2.50e+03 KL: 5.96e+01 MMD: 1.13e+00\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 2.49e+03 logL: -2.43e+03 KL: 5.62e+01 MMD: 1.33e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.48e+03 logL: -2.43e+03 KL: 5.57e+01 MMD: 1.25e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.48e+03 logL: -2.42e+03 KL: 5.41e+01 MMD: 1.31e+00\n",
      "Epoch 00111: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00118: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 120 VALIDATION Loss: 2.48e+03 logL: -2.42e+03 KL: 5.41e+01 MMD: 1.29e+00\n",
      "Stopping\n",
      "====> Epoch: 122 VALIDATION Loss: 2.48e+03 logL: -2.42e+03 KL: 5.40e+01 MMD: 1.38e+00\n",
      "config 2, alpha = 0.0, lambda = 126.6, dropout = 0.00; 2 hidden layers with 120, 26 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.37e+04 logL: -1.31e+04 KL: 3.64e+02 MMD: 1.82e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.17e+03 logL: -5.72e+03 KL: 2.42e+02 MMD: 1.61e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.14e+03 logL: -3.74e+03 KL: 1.90e+02 MMD: 1.68e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.19e+03 logL: -2.84e+03 KL: 1.50e+02 MMD: 1.58e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.59e+03 logL: -2.24e+03 KL: 1.31e+02 MMD: 1.75e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.46e+03 logL: -2.14e+03 KL: 1.11e+02 MMD: 1.63e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.94e+03 logL: -1.64e+03 KL: 1.05e+02 MMD: 1.51e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.83e+03 logL: -1.54e+03 KL: 9.31e+01 MMD: 1.53e+00\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.79e+03 logL: -1.51e+03 KL: 8.54e+01 MMD: 1.61e+00\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 99 VALIDATION Loss: 1.78e+03 logL: -1.50e+03 KL: 8.34e+01 MMD: 1.57e+00\n",
      "config 2, alpha = 0.0, lambda = 31147.1, dropout = 0.00; 2 hidden layers with 44, 24 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.46e+04 logL: -7.55e+03 KL: 4.20e+02 MMD: 1.82e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.15e+04 logL: -3.70e+03 KL: 3.18e+02 MMD: 1.84e+00\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 29 VALIDATION Loss: 6.03e+04 logL: -3.62e+03 KL: 2.47e+02 MMD: 1.81e+00\n",
      "config 3, alpha = 0.0, lambda = 84.5, dropout = 0.00; 2 hidden layers with 27, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.98e+03 logL: -9.59e+03 KL: 3.31e+02 MMD: 7.12e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.74e+03 logL: -9.53e+03 KL: 1.53e+02 MMD: 7.27e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.69e+03 logL: -9.53e+03 KL: 1.02e+02 MMD: 7.12e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.65e+03 logL: -9.51e+03 KL: 8.01e+01 MMD: 7.01e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.55e+03 logL: -9.42e+03 KL: 6.81e+01 MMD: 7.64e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.53e+03 logL: -9.40e+03 KL: 6.10e+01 MMD: 7.59e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.30e+03 logL: -9.18e+03 KL: 6.01e+01 MMD: 7.99e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.04e+03 logL: -8.93e+03 KL: 4.94e+01 MMD: 6.86e-01\n",
      "Stopping\n",
      "====> Epoch: 89 VALIDATION Loss: 9.26e+03 logL: -9.15e+03 KL: 5.25e+01 MMD: 7.59e-01\n",
      "config 3, alpha = 0.0, lambda = 1628.5, dropout = 0.00; 2 hidden layers with 79, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.47e+03 logL: -7.42e+03 KL: 4.07e+02 MMD: 1.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.24e+03 logL: -5.42e+03 KL: 1.95e+02 MMD: 1.00e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.99e+03 logL: -5.24e+03 KL: 1.26e+02 MMD: 9.98e-01\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 7.05e+03 logL: -5.17e+03 KL: 1.04e+02 MMD: 1.09e+00\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 6.87e+03 logL: -5.15e+03 KL: 1.02e+02 MMD: 9.92e-01\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 60 VALIDATION Loss: 6.95e+03 logL: -5.15e+03 KL: 1.02e+02 MMD: 1.05e+00\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 66 VALIDATION Loss: 6.95e+03 logL: -5.15e+03 KL: 1.02e+02 MMD: 1.04e+00\n",
      "config 3, alpha = 0.0, lambda = 811.6, dropout = 0.00; 2 hidden layers with 10, 10 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 1.72e+04 logL: -1.52e+04 KL: 8.75e+02 MMD: 1.40e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.56e+04 logL: -1.41e+04 KL: 4.14e+02 MMD: 1.41e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.50e+04 logL: -1.37e+04 KL: 2.37e+02 MMD: 1.37e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.41e+03 logL: -8.10e+03 KL: 2.85e+02 MMD: 1.27e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.65e+03 logL: -7.38e+03 KL: 1.97e+02 MMD: 1.32e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.49e+03 logL: -7.21e+03 KL: 1.47e+02 MMD: 1.39e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.44e+03 logL: -7.20e+03 KL: 1.17e+02 MMD: 1.39e+00\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 8.37e+03 logL: -7.09e+03 KL: 1.00e+02 MMD: 1.46e+00\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 8.26e+03 logL: -7.08e+03 KL: 9.86e+01 MMD: 1.33e+00\n",
      "Stopping\n",
      "====> Epoch: 92 VALIDATION Loss: 8.34e+03 logL: -7.08e+03 KL: 9.86e+01 MMD: 1.43e+00\n",
      "config 3, alpha = 0.0, lambda = 97.3, dropout = 0.00; 2 hidden layers with 13, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.67e+04 logL: -1.55e+04 KL: 1.09e+03 MMD: 1.79e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.47e+04 logL: -1.40e+04 KL: 4.72e+02 MMD: 1.79e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.34e+04 logL: -1.30e+04 KL: 2.79e+02 MMD: 1.68e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.83e+03 logL: -8.38e+03 KL: 2.93e+02 MMD: 1.71e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.68e+03 logL: -7.31e+03 KL: 2.21e+02 MMD: 1.50e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.33e+03 logL: -5.99e+03 KL: 1.71e+02 MMD: 1.81e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.95e+03 logL: -5.66e+03 KL: 1.37e+02 MMD: 1.62e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.22e+03 logL: -3.92e+03 KL: 1.52e+02 MMD: 1.60e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.01e+03 logL: -3.70e+03 KL: 1.32e+02 MMD: 1.78e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.90e+03 logL: -3.62e+03 KL: 1.17e+02 MMD: 1.65e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.14e+03 logL: -2.87e+03 KL: 1.13e+02 MMD: 1.63e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.04e+03 logL: -2.78e+03 KL: 1.02e+02 MMD: 1.60e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.02e+03 logL: -2.78e+03 KL: 9.48e+01 MMD: 1.50e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.01e+03 logL: -2.77e+03 KL: 8.70e+01 MMD: 1.62e+00\n",
      "Epoch 00140: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00149: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 150 VALIDATION Loss: 3.01e+03 logL: -2.74e+03 KL: 8.63e+01 MMD: 1.82e+00\n",
      "Epoch 00159: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 160 VALIDATION Loss: 2.98e+03 logL: -2.74e+03 KL: 8.60e+01 MMD: 1.52e+00\n",
      "Epoch 00166: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 170 VALIDATION Loss: 2.99e+03 logL: -2.74e+03 KL: 8.60e+01 MMD: 1.71e+00\n",
      "Stopping\n",
      "====> Epoch: 170 VALIDATION Loss: 2.99e+03 logL: -2.74e+03 KL: 8.60e+01 MMD: 1.71e+00\n",
      "config 3, alpha = 0.0, lambda = 2641.3, dropout = 0.00; 2 hidden layers with 16, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.06e+06 logL: -4.64e+05 KL: 2.59e+06 MMD: 1.96e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.56e+05 logL: -3.96e+05 KL: 5.55e+05 MMD: 1.87e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.84e+05 logL: -3.44e+05 KL: 2.35e+05 MMD: 1.93e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.39e+05 logL: -3.21e+05 KL: 1.13e+05 MMD: 1.87e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.25e+05 logL: -5.64e+04 KL: 6.37e+04 MMD: 2.00e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.64e+04 logL: -1.49e+04 KL: 3.64e+04 MMD: 1.93e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.13e+04 logL: -1.51e+04 KL: 2.13e+04 MMD: 1.87e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.32e+04 logL: -1.52e+04 KL: 1.30e+04 MMD: 1.86e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.77e+04 logL: -1.48e+04 KL: 7.97e+03 MMD: 1.86e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.44e+04 logL: -1.46e+04 KL: 4.75e+03 MMD: 1.93e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.19e+04 logL: -1.45e+04 KL: 2.61e+03 MMD: 1.81e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.06e+04 logL: -1.43e+04 KL: 1.25e+03 MMD: 1.89e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.74e+04 logL: -1.19e+04 KL: 6.17e+02 MMD: 1.84e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.60e+04 logL: -1.07e+04 KL: 3.19e+02 MMD: 1.89e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.34e+04 logL: -8.54e+03 KL: 2.23e+02 MMD: 1.77e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.32e+04 logL: -7.55e+03 KL: 1.91e+02 MMD: 2.06e+00\n",
      "Epoch 00165: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 169 VALIDATION Loss: 1.26e+04 logL: -7.42e+03 KL: 1.77e+02 MMD: 1.88e+00\n",
      "config 4, alpha = 0.0, lambda = 2.0, dropout = 0.00; 2 hidden layers with 71, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.93e+03 logL: -9.60e+03 KL: 3.24e+02 MMD: 7.57e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.76e+03 logL: -9.59e+03 KL: 1.63e+02 MMD: 6.86e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.76e+03 logL: -9.64e+03 KL: 1.17e+02 MMD: 7.29e-01\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 9.60e+03 logL: -9.50e+03 KL: 1.05e+02 MMD: 7.23e-01\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 9.59e+03 logL: -9.49e+03 KL: 9.94e+01 MMD: 8.37e-01\n",
      "Stopping\n",
      "====> Epoch: 50 VALIDATION Loss: 9.59e+03 logL: -9.49e+03 KL: 9.94e+01 MMD: 8.37e-01\n",
      "config 4, alpha = 0.0, lambda = 37716.9, dropout = 0.00; 2 hidden layers with 30, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.00e+04 logL: -8.64e+03 KL: 4.75e+02 MMD: 1.08e+00\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 4.40e+04 logL: -7.17e+03 KL: 3.39e+02 MMD: 9.68e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 4.74e+04 logL: -6.87e+03 KL: 3.24e+02 MMD: 1.07e+00\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 4.53e+04 logL: -6.73e+03 KL: 3.22e+02 MMD: 1.01e+00\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 46 VALIDATION Loss: 4.52e+04 logL: -6.71e+03 KL: 3.22e+02 MMD: 1.01e+00\n",
      "config 4, alpha = 0.0, lambda = 342.7, dropout = 0.00; 2 hidden layers with 38, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.47e+04 logL: -1.38e+04 KL: 3.22e+02 MMD: 1.51e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.45e+04 logL: -1.36e+04 KL: 2.67e+02 MMD: 1.70e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.43e+04 logL: -1.35e+04 KL: 2.09e+02 MMD: 1.60e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.41e+04 logL: -1.34e+04 KL: 1.62e+02 MMD: 1.61e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.02e+04 logL: -9.57e+03 KL: 1.44e+02 MMD: 1.56e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.02e+04 logL: -9.55e+03 KL: 1.20e+02 MMD: 1.61e+00\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.01e+04 logL: -9.46e+03 KL: 1.13e+02 MMD: 1.56e+00\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.01e+04 logL: -9.45e+03 KL: 1.10e+02 MMD: 1.45e+00\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 1.01e+04 logL: -9.45e+03 KL: 1.10e+02 MMD: 1.45e+00\n",
      "config 4, alpha = 0.0, lambda = 220.8, dropout = 0.00; 2 hidden layers with 28, 26 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.78e+03 logL: -7.08e+03 KL: 3.23e+02 MMD: 1.73e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.55e+03 logL: -2.97e+03 KL: 2.22e+02 MMD: 1.63e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.72e+03 logL: -2.23e+03 KL: 1.59e+02 MMD: 1.51e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.65e+03 logL: -2.18e+03 KL: 1.23e+02 MMD: 1.59e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.14e+03 logL: -1.65e+03 KL: 1.09e+02 MMD: 1.73e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.04e+03 logL: -1.59e+03 KL: 9.31e+01 MMD: 1.66e+00\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.03e+03 logL: -1.55e+03 KL: 8.41e+01 MMD: 1.81e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.97e+03 logL: -1.55e+03 KL: 8.19e+01 MMD: 1.56e+00\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.99e+03 logL: -1.55e+03 KL: 8.15e+01 MMD: 1.66e+00\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 95 VALIDATION Loss: 2.00e+03 logL: -1.55e+03 KL: 8.14e+01 MMD: 1.68e+00\n",
      "config 4, alpha = 0.0, lambda = 968.1, dropout = 0.00; 2 hidden layers with 49, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.24e+04 logL: -9.84e+03 KL: 4.58e+02 MMD: 2.12e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 20 VALIDATION Loss: 7.69e+03 logL: -5.50e+03 KL: 3.17e+02 MMD: 1.93e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.66e+03 logL: -3.68e+03 KL: 2.10e+02 MMD: 1.83e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.85e+03 logL: -2.84e+03 KL: 1.76e+02 MMD: 1.90e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.80e+03 logL: -2.80e+03 KL: 1.42e+02 MMD: 1.92e+00\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 59 VALIDATION Loss: 4.71e+03 logL: -2.74e+03 KL: 1.30e+02 MMD: 1.91e+00\n",
      "config 5, alpha = 0.0, lambda = 683.4, dropout = 0.00; 2 hidden layers with 148, 134 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.03e+04 logL: -9.59e+03 KL: 2.06e+02 MMD: 6.76e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.01e+04 logL: -9.49e+03 KL: 1.25e+02 MMD: 7.05e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.00e+04 logL: -9.47e+03 KL: 1.11e+02 MMD: 6.45e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.84e+03 logL: -9.25e+03 KL: 1.23e+02 MMD: 6.91e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.51e+03 logL: -8.91e+03 KL: 1.28e+02 MMD: 6.84e-01\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 8.91e+03 logL: -8.30e+03 KL: 1.10e+02 MMD: 7.43e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.86e+03 logL: -8.26e+03 KL: 1.01e+02 MMD: 7.42e-01\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 8.85e+03 logL: -8.22e+03 KL: 9.83e+01 MMD: 7.69e-01\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 89 VALIDATION Loss: 8.82e+03 logL: -8.22e+03 KL: 9.82e+01 MMD: 7.36e-01\n",
      "config 5, alpha = 0.0, lambda = 104.1, dropout = 0.00; 2 hidden layers with 25, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.84e+05 logL: -3.51e+05 KL: 3.36e+04 MMD: 9.97e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.50e+05 logL: -3.28e+05 KL: 2.19e+04 MMD: 1.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.99e+05 logL: -2.85e+05 KL: 1.39e+04 MMD: 9.91e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.13e+05 logL: -1.02e+05 KL: 1.13e+04 MMD: 9.74e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.27e+04 logL: -1.57e+04 KL: 6.84e+03 MMD: 1.01e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.83e+04 logL: -1.44e+04 KL: 3.82e+03 MMD: 9.66e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.64e+04 logL: -1.42e+04 KL: 2.15e+03 MMD: 1.03e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.54e+04 logL: -1.40e+04 KL: 1.22e+03 MMD: 1.10e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.46e+04 logL: -1.38e+04 KL: 7.31e+02 MMD: 1.02e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.33e+04 logL: -1.27e+04 KL: 5.15e+02 MMD: 9.95e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.17e+04 logL: -1.13e+04 KL: 3.36e+02 MMD: 1.09e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.12e+04 logL: -1.09e+04 KL: 2.50e+02 MMD: 1.04e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.09e+04 logL: -1.06e+04 KL: 1.97e+02 MMD: 1.06e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 9.20e+03 logL: -8.87e+03 KL: 2.21e+02 MMD: 1.01e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 8.57e+03 logL: -8.27e+03 KL: 1.89e+02 MMD: 1.10e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 8.10e+03 logL: -7.83e+03 KL: 1.67e+02 MMD: 9.78e-01\n",
      "====> Epoch: 170 VALIDATION Loss: 6.24e+03 logL: -5.96e+03 KL: 1.74e+02 MMD: 1.01e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 5.77e+03 logL: -5.49e+03 KL: 1.60e+02 MMD: 1.11e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 5.57e+03 logL: -5.33e+03 KL: 1.41e+02 MMD: 9.67e-01\n",
      "====> Epoch: 200 VALIDATION Loss: 5.46e+03 logL: -5.22e+03 KL: 1.21e+02 MMD: 1.09e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 5.46e+03 logL: -5.22e+03 KL: 1.21e+02 MMD: 1.09e+00\n",
      "config 5, alpha = 0.0, lambda = 1.2, dropout = 0.00; 2 hidden layers with 27, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.92e+03 logL: -9.57e+03 KL: 3.49e+02 MMD: 1.46e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.42e+03 logL: -7.17e+03 KL: 2.53e+02 MMD: 1.31e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.81e+03 logL: -4.60e+03 KL: 2.10e+02 MMD: 1.38e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.83e+03 logL: -3.65e+03 KL: 1.77e+02 MMD: 1.31e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.73e+03 logL: -3.59e+03 KL: 1.41e+02 MMD: 1.29e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.90e+03 logL: -2.78e+03 KL: 1.25e+02 MMD: 1.31e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.87e+03 logL: -2.76e+03 KL: 1.07e+02 MMD: 1.38e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.86e+03 logL: -2.77e+03 KL: 8.89e+01 MMD: 1.31e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.84e+03 logL: -2.76e+03 KL: 7.95e+01 MMD: 1.37e+00\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 2.81e+03 logL: -2.74e+03 KL: 7.22e+01 MMD: 1.31e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.81e+03 logL: -2.73e+03 KL: 7.02e+01 MMD: 1.35e+00\n",
      "Stopping\n",
      "====> Epoch: 110 VALIDATION Loss: 2.81e+03 logL: -2.73e+03 KL: 7.02e+01 MMD: 1.35e+00\n",
      "config 5, alpha = 0.0, lambda = 28297.9, dropout = 0.00; 2 hidden layers with 12, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.35e+04 logL: -1.47e+04 KL: 8.05e+02 MMD: 1.70e+00\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 17 VALIDATION Loss: 6.12e+04 logL: -1.44e+04 KL: 7.19e+02 MMD: 1.63e+00\n",
      "config 5, alpha = 0.0, lambda = 514.9, dropout = 0.00; 2 hidden layers with 17, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.59e+04 logL: -1.41e+04 KL: 8.32e+02 MMD: 1.99e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.34e+04 logL: -1.21e+04 KL: 3.31e+02 MMD: 1.89e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.95e+03 logL: -7.62e+03 KL: 2.44e+02 MMD: 2.12e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.62e+03 logL: -5.45e+03 KL: 2.00e+02 MMD: 1.89e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.00e+03 logL: -3.84e+03 KL: 1.83e+02 MMD: 1.91e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.20e+03 logL: -2.97e+03 KL: 1.62e+02 MMD: 2.08e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.97e+03 logL: -2.84e+03 KL: 1.35e+02 MMD: 1.92e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.81e+03 logL: -2.80e+03 KL: 1.16e+02 MMD: 1.74e+00\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.81e+03 logL: -2.76e+03 KL: 1.07e+02 MMD: 1.83e+00\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 3.85e+03 logL: -2.76e+03 KL: 1.04e+02 MMD: 1.93e+00\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 109 VALIDATION Loss: 3.83e+03 logL: -2.76e+03 KL: 1.04e+02 MMD: 1.88e+00\n",
      "config 6, alpha = 0.0, lambda = 67222.5, dropout = 0.00; 2 hidden layers with 9, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.49e+04 logL: -1.01e+04 KL: 5.63e+02 MMD: 6.58e-01\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 5.71e+04 logL: -9.76e+03 KL: 5.72e+02 MMD: 6.95e-01\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 25 VALIDATION Loss: 5.51e+04 logL: -9.75e+03 KL: 5.71e+02 MMD: 6.65e-01\n",
      "config 6, alpha = 0.0, lambda = 1.8, dropout = 0.00; 2 hidden layers with 184, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.02e+04 logL: -9.77e+03 KL: 4.54e+02 MMD: 1.07e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.78e+03 logL: -9.59e+03 KL: 1.93e+02 MMD: 1.17e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.61e+03 logL: -9.48e+03 KL: 1.24e+02 MMD: 1.25e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.15e+03 logL: -7.04e+03 KL: 1.05e+02 MMD: 1.27e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.08e+03 logL: -6.99e+03 KL: 8.92e+01 MMD: 1.11e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.02e+03 logL: -6.93e+03 KL: 8.02e+01 MMD: 1.14e+00\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 6.96e+03 logL: -6.88e+03 KL: 7.71e+01 MMD: 1.18e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.95e+03 logL: -6.88e+03 KL: 7.36e+01 MMD: 1.12e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.96e+03 logL: -6.89e+03 KL: 7.10e+01 MMD: 1.22e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.93e+03 logL: -6.86e+03 KL: 6.80e+01 MMD: 1.14e+00\n",
      "Stopping\n",
      "====> Epoch: 101 VALIDATION Loss: 6.94e+03 logL: -6.87e+03 KL: 6.82e+01 MMD: 1.10e+00\n",
      "config 6, alpha = 0.0, lambda = 13121.4, dropout = 0.00; 2 hidden layers with 9, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.43e+05 logL: -3.42e+05 KL: 5.72e+00 MMD: 5.87e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.22e+04 logL: -1.49e+04 KL: 2.73e+02 MMD: 1.29e+00\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.19e+04 logL: -1.49e+04 KL: 2.79e+02 MMD: 1.28e+00\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 3.23e+04 logL: -1.48e+04 KL: 2.73e+02 MMD: 1.31e+00\n",
      "Stopping\n",
      "====> Epoch: 41 VALIDATION Loss: 3.32e+04 logL: -1.48e+04 KL: 2.73e+02 MMD: 1.38e+00\n",
      "config 6, alpha = 0.0, lambda = 34.6, dropout = 0.00; 2 hidden layers with 74, 33 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.96e+03 logL: -7.47e+03 KL: 4.32e+02 MMD: 1.52e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.58e+03 logL: -5.27e+03 KL: 2.52e+02 MMD: 1.68e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.05e+03 logL: -2.80e+03 KL: 1.99e+02 MMD: 1.57e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.13e+03 logL: -2.92e+03 KL: 1.55e+02 MMD: 1.67e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.71e+03 logL: -2.51e+03 KL: 1.50e+02 MMD: 1.76e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.82e+03 logL: -1.64e+03 KL: 1.25e+02 MMD: 1.65e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.77e+03 logL: -1.61e+03 KL: 1.08e+02 MMD: 1.61e+00\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.71e+03 logL: -1.55e+03 KL: 1.01e+02 MMD: 1.76e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.69e+03 logL: -1.55e+03 KL: 9.55e+01 MMD: 1.59e+00\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.69e+03 logL: -1.54e+03 KL: 9.22e+01 MMD: 1.59e+00\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.69e+03 logL: -1.54e+03 KL: 9.18e+01 MMD: 1.69e+00\n",
      "Epoch 00112: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 116 VALIDATION Loss: 1.69e+03 logL: -1.54e+03 KL: 9.18e+01 MMD: 1.70e+00\n",
      "config 6, alpha = 0.0, lambda = 2022.7, dropout = 0.00; 2 hidden layers with 35, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.49e+05 logL: -3.53e+05 KL: 1.92e+05 MMD: 1.97e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.10e+05 logL: -3.30e+05 KL: 1.76e+05 MMD: 1.98e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.71e+05 logL: -3.23e+05 KL: 1.44e+05 MMD: 1.90e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.14e+05 logL: -3.11e+05 KL: 9.94e+04 MMD: 2.03e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.59e+05 logL: -2.97e+05 KL: 5.82e+04 MMD: 1.91e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.08e+05 logL: -2.74e+05 KL: 3.02e+04 MMD: 1.89e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.34e+05 logL: -2.14e+05 KL: 1.56e+04 MMD: 1.95e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.28e+05 logL: -1.16e+05 KL: 8.32e+03 MMD: 1.96e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.91e+04 logL: -3.07e+04 KL: 4.69e+03 MMD: 1.85e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.13e+04 logL: -1.49e+04 KL: 2.64e+03 MMD: 1.86e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.96e+04 logL: -1.44e+04 KL: 1.52e+03 MMD: 1.83e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.85e+04 logL: -1.37e+04 KL: 9.06e+02 MMD: 1.91e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.69e+04 logL: -1.23e+04 KL: 5.69e+02 MMD: 2.00e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.43e+04 logL: -1.04e+04 KL: 3.77e+02 MMD: 1.77e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.29e+04 logL: -8.66e+03 KL: 2.72e+02 MMD: 1.98e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.11e+04 logL: -7.02e+03 KL: 2.38e+02 MMD: 1.92e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 9.57e+03 logL: -5.57e+03 KL: 2.37e+02 MMD: 1.86e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 8.94e+03 logL: -4.81e+03 KL: 2.28e+02 MMD: 1.93e+00\n",
      "Epoch 00183: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 190 VALIDATION Loss: 8.88e+03 logL: -4.56e+03 KL: 2.20e+02 MMD: 2.03e+00\n",
      "Epoch 00192: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 196 VALIDATION Loss: 8.63e+03 logL: -4.54e+03 KL: 2.20e+02 MMD: 1.91e+00\n",
      "config 7, alpha = 0.0, lambda = 14135.4, dropout = 0.00; 2 hidden layers with 117, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.90e+04 logL: -9.54e+03 KL: 4.12e+02 MMD: 6.40e-01\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 1.87e+04 logL: -9.44e+03 KL: 2.97e+02 MMD: 6.37e-01\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 27 VALIDATION Loss: 1.97e+04 logL: -9.44e+03 KL: 2.94e+02 MMD: 7.08e-01\n",
      "config 7, alpha = 0.0, lambda = 1667.7, dropout = 0.00; 2 hidden layers with 48, 29 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.77e+03 logL: -5.68e+03 KL: 3.40e+02 MMD: 1.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.13e+03 logL: -5.25e+03 KL: 1.61e+02 MMD: 1.03e+00\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 7.03e+03 logL: -5.13e+03 KL: 1.27e+02 MMD: 1.06e+00\n",
      "Stopping\n",
      "====> Epoch: 31 VALIDATION Loss: 6.96e+03 logL: -5.13e+03 KL: 1.26e+02 MMD: 1.02e+00\n",
      "config 7, alpha = 0.0, lambda = 892.6, dropout = 0.00; 2 hidden layers with 22, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.53e+04 logL: -1.37e+04 KL: 3.52e+02 MMD: 1.44e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.50e+04 logL: -1.36e+04 KL: 2.45e+02 MMD: 1.35e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.10e+04 logL: -9.66e+03 KL: 1.99e+02 MMD: 1.27e+00\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.10e+04 logL: -9.51e+03 KL: 1.36e+02 MMD: 1.47e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.08e+04 logL: -9.51e+03 KL: 1.33e+02 MMD: 1.30e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.08e+04 logL: -9.50e+03 KL: 1.26e+02 MMD: 1.35e+00\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 64 VALIDATION Loss: 1.09e+04 logL: -9.50e+03 KL: 1.25e+02 MMD: 1.44e+00\n",
      "config 7, alpha = 0.0, lambda = 14.6, dropout = 0.00; 2 hidden layers with 61, 32 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.86e+03 logL: -8.42e+03 KL: 4.09e+02 MMD: 1.71e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.07e+03 logL: -4.79e+03 KL: 2.52e+02 MMD: 1.50e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.62e+03 logL: -2.41e+03 KL: 1.95e+02 MMD: 1.56e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.91e+03 logL: -1.72e+03 KL: 1.63e+02 MMD: 1.69e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.80e+03 logL: -1.64e+03 KL: 1.39e+02 MMD: 1.82e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.77e+03 logL: -1.62e+03 KL: 1.20e+02 MMD: 1.60e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.77e+03 logL: -1.64e+03 KL: 1.05e+02 MMD: 1.59e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.65e+03 logL: -1.54e+03 KL: 9.31e+01 MMD: 1.49e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.66e+03 logL: -1.55e+03 KL: 8.78e+01 MMD: 1.60e+00\n",
      "Stopping\n",
      "====> Epoch: 90 VALIDATION Loss: 1.66e+03 logL: -1.55e+03 KL: 8.78e+01 MMD: 1.60e+00\n",
      "config 7, alpha = 0.0, lambda = 14.1, dropout = 0.00; 2 hidden layers with 41, 21 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.15e+05 logL: -2.06e+05 KL: 8.65e+03 MMD: 2.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.74e+04 logL: -1.47e+04 KL: 2.69e+03 MMD: 1.92e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.52e+04 logL: -1.41e+04 KL: 1.07e+03 MMD: 2.17e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.45e+04 logL: -1.39e+04 KL: 5.68e+02 MMD: 1.98e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.39e+04 logL: -1.35e+04 KL: 3.57e+02 MMD: 2.05e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.18e+04 logL: -1.15e+04 KL: 2.94e+02 MMD: 2.02e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.70e+03 logL: -8.40e+03 KL: 2.72e+02 MMD: 1.83e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.19e+03 logL: -5.92e+03 KL: 2.43e+02 MMD: 1.84e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.20e+03 logL: -3.94e+03 KL: 2.30e+02 MMD: 2.00e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.78e+03 logL: -2.54e+03 KL: 2.05e+02 MMD: 2.02e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.44e+03 logL: -2.25e+03 KL: 1.64e+02 MMD: 1.96e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.97e+03 logL: -1.80e+03 KL: 1.48e+02 MMD: 1.92e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.77e+03 logL: -1.62e+03 KL: 1.20e+02 MMD: 2.14e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.33e+03 logL: -1.20e+03 KL: 1.12e+02 MMD: 1.66e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.28e+03 logL: -1.16e+03 KL: 9.72e+01 MMD: 1.90e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.27e+03 logL: -1.15e+03 KL: 8.91e+01 MMD: 1.94e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 1.24e+03 logL: -1.13e+03 KL: 8.60e+01 MMD: 1.87e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 1.20e+03 logL: -1.09e+03 KL: 8.10e+01 MMD: 1.93e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 9.83e+02 logL: -8.77e+02 KL: 8.26e+01 MMD: 1.83e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 9.33e+02 logL: -8.33e+02 KL: 7.66e+01 MMD: 1.75e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 9.33e+02 logL: -8.33e+02 KL: 7.66e+01 MMD: 1.75e+00\n",
      "config 8, alpha = 0.0, lambda = 3.3, dropout = 0.00; 2 hidden layers with 49, 20 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 9.75e+03 logL: -9.57e+03 KL: 1.73e+02 MMD: 6.65e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.59e+03 logL: -9.49e+03 KL: 9.41e+01 MMD: 7.47e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.51e+03 logL: -9.44e+03 KL: 6.89e+01 MMD: 7.47e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.45e+03 logL: -9.38e+03 KL: 6.05e+01 MMD: 7.70e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.20e+03 logL: -9.14e+03 KL: 5.52e+01 MMD: 7.48e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.62e+03 logL: -9.58e+03 KL: 4.23e+01 MMD: 8.46e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.05e+03 logL: -9.01e+03 KL: 3.81e+01 MMD: 7.86e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.38e+03 logL: -9.35e+03 KL: 3.43e+01 MMD: 8.42e-01\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 85 VALIDATION Loss: 8.97e+03 logL: -8.93e+03 KL: 3.59e+01 MMD: 8.48e-01\n",
      "config 8, alpha = 0.0, lambda = 69924.3, dropout = 0.00; 2 hidden layers with 37, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.94e+04 logL: -1.54e+04 KL: 2.80e+00 MMD: 5.75e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 1.68e+04 logL: -1.37e+04 KL: 4.44e+00 MMD: 4.44e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 1.57e+04 logL: -1.35e+04 KL: 5.11e+00 MMD: 3.21e-02\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.47e+04 logL: -1.34e+04 KL: 5.34e+00 MMD: 1.81e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.50e+04 logL: -1.34e+04 KL: 5.50e+00 MMD: 2.27e-02\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 59 VALIDATION Loss: 1.47e+04 logL: -1.33e+04 KL: 5.64e+00 MMD: 1.93e-02\n",
      "config 8, alpha = 0.0, lambda = 55.2, dropout = 0.00; 2 hidden layers with 65, 21 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.64e+05 logL: -2.80e+05 KL: 8.39e+04 MMD: 1.28e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.12e+04 logL: -2.54e+04 KL: 2.57e+04 MMD: 1.35e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.41e+04 logL: -1.41e+04 KL: 9.95e+03 MMD: 1.37e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.81e+04 logL: -1.38e+04 KL: 4.26e+03 MMD: 1.34e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.57e+04 logL: -1.37e+04 KL: 1.94e+03 MMD: 1.35e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.46e+04 logL: -1.36e+04 KL: 9.26e+02 MMD: 1.35e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.38e+04 logL: -1.33e+04 KL: 4.71e+02 MMD: 1.35e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.02e+04 logL: -9.86e+03 KL: 2.85e+02 MMD: 1.28e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 7.94e+03 logL: -7.67e+03 KL: 2.03e+02 MMD: 1.29e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.86e+03 logL: -6.63e+03 KL: 1.48e+02 MMD: 1.40e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.50e+03 logL: -6.31e+03 KL: 1.14e+02 MMD: 1.33e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 6.28e+03 logL: -6.10e+03 KL: 1.07e+02 MMD: 1.24e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 4.94e+03 logL: -4.72e+03 KL: 1.50e+02 MMD: 1.25e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.99e+03 logL: -3.75e+03 KL: 1.64e+02 MMD: 1.39e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 3.49e+03 logL: -3.28e+03 KL: 1.48e+02 MMD: 1.22e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 3.28e+03 logL: -3.08e+03 KL: 1.27e+02 MMD: 1.42e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 3.10e+03 logL: -2.92e+03 KL: 1.16e+02 MMD: 1.25e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 3.03e+03 logL: -2.85e+03 KL: 1.04e+02 MMD: 1.29e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 2.96e+03 logL: -2.79e+03 KL: 9.38e+01 MMD: 1.41e+00\n",
      "Epoch 00194: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 200 VALIDATION Loss: 2.89e+03 logL: -2.74e+03 KL: 8.93e+01 MMD: 1.20e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 2.89e+03 logL: -2.74e+03 KL: 8.93e+01 MMD: 1.20e+00\n",
      "config 8, alpha = 0.0, lambda = 28366.4, dropout = 0.00; 2 hidden layers with 12, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.63e+04 logL: -1.23e+04 KL: 6.74e+02 MMD: 1.53e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.51e+04 logL: -6.81e+03 KL: 4.12e+02 MMD: 1.69e+00\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.25e+04 logL: -5.24e+03 KL: 3.20e+02 MMD: 1.66e+00\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 39 VALIDATION Loss: 4.79e+04 logL: -5.23e+03 KL: 3.14e+02 MMD: 1.49e+00\n",
      "config 8, alpha = 0.0, lambda = 4.9, dropout = 0.00; 2 hidden layers with 51, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.93e+03 logL: -5.62e+03 KL: 3.06e+02 MMD: 1.93e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.10e+03 logL: -2.89e+03 KL: 2.01e+02 MMD: 2.01e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.97e+03 logL: -2.82e+03 KL: 1.45e+02 MMD: 1.86e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.39e+03 logL: -2.26e+03 KL: 1.25e+02 MMD: 2.09e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.29e+03 logL: -2.17e+03 KL: 1.11e+02 MMD: 1.97e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.78e+03 logL: -1.66e+03 KL: 1.15e+02 MMD: 1.90e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.71e+03 logL: -1.60e+03 KL: 1.02e+02 MMD: 1.98e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.70e+03 logL: -1.60e+03 KL: 9.36e+01 MMD: 2.09e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.69e+03 logL: -1.60e+03 KL: 8.64e+01 MMD: 1.96e+00\n",
      "Epoch 00095: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.65e+03 logL: -1.56e+03 KL: 8.33e+01 MMD: 1.86e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.65e+03 logL: -1.56e+03 KL: 8.11e+01 MMD: 1.93e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.64e+03 logL: -1.56e+03 KL: 7.83e+01 MMD: 2.05e+00\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00127: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.64e+03 logL: -1.55e+03 KL: 7.83e+01 MMD: 1.99e+00\n",
      "Stopping\n",
      "====> Epoch: 131 VALIDATION Loss: 1.64e+03 logL: -1.55e+03 KL: 7.83e+01 MMD: 1.83e+00\n",
      "config 9, alpha = 0.0, lambda = 39.8, dropout = 0.00; 2 hidden layers with 7, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.45e+04 logL: -1.38e+04 KL: 6.99e+02 MMD: 8.56e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.42e+04 logL: -1.36e+04 KL: 5.02e+02 MMD: 7.72e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.39e+04 logL: -1.35e+04 KL: 3.21e+02 MMD: 8.48e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 2.00e+02 MMD: 8.84e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.47e+02 MMD: 9.46e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.12e+02 MMD: 1.02e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 9.02e+01 MMD: 1.07e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 7.68e+01 MMD: 1.02e+00\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 6.95e+01 MMD: 1.11e+00\n",
      "Stopping\n",
      "====> Epoch: 97 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 6.81e+01 MMD: 1.06e+00\n",
      "config 9, alpha = 0.0, lambda = 351.7, dropout = 0.00; 2 hidden layers with 98, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.45e+05 logL: -3.42e+05 KL: 2.55e+03 MMD: 1.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.30e+05 logL: -3.24e+05 KL: 6.11e+03 MMD: 1.05e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.91e+05 logL: -2.83e+05 KL: 7.69e+03 MMD: 1.03e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.08e+05 logL: -9.91e+04 KL: 8.08e+03 MMD: 1.00e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.98e+04 logL: -1.50e+04 KL: 4.42e+03 MMD: 1.05e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.61e+04 logL: -1.37e+04 KL: 2.11e+03 MMD: 9.92e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.50e+04 logL: -1.36e+04 KL: 1.09e+03 MMD: 9.81e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.44e+04 logL: -1.33e+04 KL: 6.34e+02 MMD: 1.06e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.38e+04 logL: -1.30e+04 KL: 4.05e+02 MMD: 1.12e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.09e+04 logL: -1.02e+04 KL: 3.39e+02 MMD: 1.01e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 9.02e+03 logL: -8.36e+03 KL: 2.73e+02 MMD: 1.13e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 8.63e+03 logL: -8.04e+03 KL: 2.02e+02 MMD: 1.12e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 8.34e+03 logL: -7.80e+03 KL: 1.67e+02 MMD: 1.07e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 7.71e+03 logL: -7.15e+03 KL: 2.06e+02 MMD: 9.96e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 6.65e+03 logL: -6.07e+03 KL: 2.19e+02 MMD: 1.01e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 6.35e+03 logL: -5.86e+03 KL: 1.65e+02 MMD: 9.33e-01\n",
      "====> Epoch: 170 VALIDATION Loss: 6.20e+03 logL: -5.72e+03 KL: 1.31e+02 MMD: 9.99e-01\n",
      "====> Epoch: 180 VALIDATION Loss: 6.06e+03 logL: -5.57e+03 KL: 1.12e+02 MMD: 1.07e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 190 VALIDATION Loss: 5.75e+03 logL: -5.29e+03 KL: 1.00e+02 MMD: 1.03e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 5.60e+03 logL: -5.14e+03 KL: 8.74e+01 MMD: 1.07e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 5.60e+03 logL: -5.14e+03 KL: 8.74e+01 MMD: 1.07e+00\n",
      "config 9, alpha = 0.0, lambda = 93823.1, dropout = 0.00; 2 hidden layers with 83, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.30e+05 logL: -1.17e+04 KL: 8.32e+02 MMD: 1.25e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.35e+05 logL: -8.07e+03 KL: 5.11e+02 MMD: 1.35e+00\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.43e+05 logL: -7.31e+03 KL: 5.21e+02 MMD: 1.44e+00\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.26e+05 logL: -7.28e+03 KL: 5.02e+02 MMD: 1.26e+00\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 47 VALIDATION Loss: 1.23e+05 logL: -7.27e+03 KL: 5.00e+02 MMD: 1.23e+00\n",
      "config 9, alpha = 0.0, lambda = 81062.3, dropout = 0.00; 2 hidden layers with 17, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.41e+04 logL: -1.53e+04 KL: 7.38e+00 MMD: 1.08e-01\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 2.10e+04 logL: -1.38e+04 KL: 8.82e+00 MMD: 8.88e-02\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 2.13e+04 logL: -1.37e+04 KL: 9.15e+00 MMD: 9.38e-02\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 38 VALIDATION Loss: 2.25e+04 logL: -1.37e+04 KL: 9.19e+00 MMD: 1.08e-01\n",
      "config 9, alpha = 0.0, lambda = 75922.7, dropout = 0.00; 2 hidden layers with 12, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.97e+04 logL: -1.48e+04 KL: 9.77e+00 MMD: 6.39e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 2.00e+04 logL: -1.36e+04 KL: 1.13e+01 MMD: 8.41e-02\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.75e+04 logL: -1.35e+04 KL: 1.26e+01 MMD: 5.30e-02\n",
      "Stopping\n",
      "====> Epoch: 32 VALIDATION Loss: 1.78e+04 logL: -1.35e+04 KL: 1.26e+01 MMD: 5.65e-02\n",
      "config 10, alpha = 0.0, lambda = 18.8, dropout = 0.00; 2 hidden layers with 120, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.77e+03 logL: -9.52e+03 KL: 2.44e+02 MMD: 7.03e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.95e+03 logL: -9.82e+03 KL: 1.17e+02 MMD: 7.35e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.47e+03 logL: -9.37e+03 KL: 9.22e+01 MMD: 6.87e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.08e+03 logL: -9.00e+03 KL: 7.45e+01 MMD: 6.91e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 8.41e+03 logL: -8.34e+03 KL: 6.18e+01 MMD: 7.43e-01\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 8.29e+03 logL: -8.22e+03 KL: 5.71e+01 MMD: 7.62e-01\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 8.13e+03 logL: -8.06e+03 KL: 5.46e+01 MMD: 7.99e-01\n",
      "Stopping\n",
      "====> Epoch: 72 VALIDATION Loss: 8.13e+03 logL: -8.06e+03 KL: 5.46e+01 MMD: 7.43e-01\n",
      "config 10, alpha = 0.0, lambda = 8.1, dropout = 0.00; 2 hidden layers with 25, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 2.68e+02 MMD: 1.28e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.34e+03 logL: -7.13e+03 KL: 1.98e+02 MMD: 9.78e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.71e+03 logL: -5.56e+03 KL: 1.47e+02 MMD: 1.03e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.55e+03 logL: -5.45e+03 KL: 9.41e+01 MMD: 1.10e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.45e+03 logL: -5.37e+03 KL: 7.28e+01 MMD: 1.08e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.18e+03 logL: -5.11e+03 KL: 6.33e+01 MMD: 1.08e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.14e+03 logL: -5.07e+03 KL: 5.76e+01 MMD: 1.08e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.07e+03 logL: -5.01e+03 KL: 5.41e+01 MMD: 1.03e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.00e+03 logL: -4.94e+03 KL: 5.06e+01 MMD: 1.11e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.96e+03 logL: -4.91e+03 KL: 4.87e+01 MMD: 1.06e+00\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 4.91e+03 logL: -4.85e+03 KL: 4.75e+01 MMD: 1.03e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.90e+03 logL: -4.85e+03 KL: 4.73e+01 MMD: 1.12e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 4.88e+03 logL: -4.83e+03 KL: 4.72e+01 MMD: 1.01e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 4.87e+03 logL: -4.81e+03 KL: 4.72e+01 MMD: 1.12e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 4.87e+03 logL: -4.81e+03 KL: 4.70e+01 MMD: 1.13e+00\n",
      "Epoch 00151: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 160 VALIDATION Loss: 4.86e+03 logL: -4.80e+03 KL: 4.70e+01 MMD: 1.10e+00\n",
      "Stopping\n",
      "====> Epoch: 163 VALIDATION Loss: 4.86e+03 logL: -4.80e+03 KL: 4.70e+01 MMD: 1.02e+00\n",
      "config 10, alpha = 0.0, lambda = 180.1, dropout = 0.00; 2 hidden layers with 26, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.41e+04 logL: -1.36e+04 KL: 1.90e+02 MMD: 1.42e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.02e+04 logL: -9.77e+03 KL: 2.09e+02 MMD: 1.40e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.98e+03 logL: -9.58e+03 KL: 1.42e+02 MMD: 1.47e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.57e+03 logL: -7.19e+03 KL: 1.46e+02 MMD: 1.35e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.54e+03 logL: -7.14e+03 KL: 1.06e+02 MMD: 1.64e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.47e+03 logL: -7.13e+03 KL: 8.89e+01 MMD: 1.44e+00\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 7.42e+03 logL: -7.08e+03 KL: 8.10e+01 MMD: 1.44e+00\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 7.38e+03 logL: -7.06e+03 KL: 7.88e+01 MMD: 1.33e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 7.38e+03 logL: -7.05e+03 KL: 7.85e+01 MMD: 1.42e+00\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 95 VALIDATION Loss: 7.38e+03 logL: -7.05e+03 KL: 7.85e+01 MMD: 1.44e+00\n",
      "config 10, alpha = 0.0, lambda = 3351.7, dropout = 0.00; 2 hidden layers with 37, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.58e+04 logL: -9.72e+03 KL: 4.49e+02 MMD: 1.68e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.28e+04 logL: -7.23e+03 KL: 2.75e+02 MMD: 1.59e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.14e+04 logL: -5.57e+03 KL: 2.32e+02 MMD: 1.67e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.27e+03 logL: -3.66e+03 KL: 1.97e+02 MMD: 1.61e+00\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 9.30e+03 logL: -3.59e+03 KL: 1.68e+02 MMD: 1.65e+00\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 58 VALIDATION Loss: 9.14e+03 logL: -3.59e+03 KL: 1.65e+02 MMD: 1.61e+00\n",
      "config 10, alpha = 0.0, lambda = 476.4, dropout = 0.00; 2 hidden layers with 31, 31 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.29e+03 logL: -7.81e+03 KL: 4.78e+02 MMD: 2.12e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.95e+03 logL: -3.81e+03 KL: 2.51e+02 MMD: 1.86e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.82e+03 logL: -2.76e+03 KL: 1.82e+02 MMD: 1.84e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.88e+03 logL: -1.76e+03 KL: 1.59e+02 MMD: 2.02e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.72e+03 logL: -1.67e+03 KL: 1.32e+02 MMD: 1.92e+00\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.58e+03 logL: -1.56e+03 KL: 1.28e+02 MMD: 1.89e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.33e+03 logL: -1.30e+03 KL: 1.28e+02 MMD: 1.91e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.28e+03 logL: -1.24e+03 KL: 1.22e+02 MMD: 1.93e+00\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 2.30e+03 logL: -1.22e+03 KL: 1.21e+02 MMD: 2.02e+00\n",
      "Epoch 00099: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 100 VALIDATION Loss: 2.22e+03 logL: -1.22e+03 KL: 1.20e+02 MMD: 1.86e+00\n",
      "Stopping\n",
      "====> Epoch: 103 VALIDATION Loss: 2.30e+03 logL: -1.21e+03 KL: 1.20e+02 MMD: 2.02e+00\n",
      "config 11, alpha = 0.0, lambda = 90439.4, dropout = 0.00; 2 hidden layers with 80, 37 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.85e+04 logL: -9.69e+03 KL: 6.57e+02 MMD: 6.43e-01\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 16 VALIDATION Loss: 7.38e+04 logL: -9.52e+03 KL: 6.21e+02 MMD: 7.04e-01\n",
      "config 11, alpha = 0.0, lambda = 20.8, dropout = 0.00; 2 hidden layers with 7, 6 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.37e+04 KL: 4.75e+02 MMD: 1.52e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+04 logL: -1.36e+04 KL: 2.58e+02 MMD: 1.69e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 1.51e+02 MMD: 1.90e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 1.16e+02 MMD: 1.91e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 9.34e+01 MMD: 2.07e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.36e+04 logL: -1.34e+04 KL: 7.82e+01 MMD: 2.18e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 6.59e+01 MMD: 2.21e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 5.61e+01 MMD: 2.34e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.34e+04 logL: -1.33e+04 KL: 5.27e+01 MMD: 2.15e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.64e+03 logL: -9.52e+03 KL: 9.67e+01 MMD: 1.38e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 9.58e+03 logL: -9.48e+03 KL: 7.21e+01 MMD: 1.53e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 9.53e+03 logL: -9.44e+03 KL: 5.88e+01 MMD: 1.74e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 9.54e+03 logL: -9.45e+03 KL: 5.24e+01 MMD: 1.63e+00\n",
      "Epoch 00131: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 9.51e+03 logL: -9.43e+03 KL: 5.06e+01 MMD: 1.72e+00\n",
      "Stopping\n",
      "====> Epoch: 143 VALIDATION Loss: 9.51e+03 logL: -9.43e+03 KL: 5.03e+01 MMD: 1.79e+00\n",
      "config 11, alpha = 0.0, lambda = 15.3, dropout = 0.00; 2 hidden layers with 50, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.49e+04 logL: -1.40e+04 KL: 8.83e+02 MMD: 1.50e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 2.77e+02 MMD: 1.47e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.00e+04 logL: -9.77e+03 KL: 2.05e+02 MMD: 1.48e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.10e+03 logL: -7.88e+03 KL: 1.98e+02 MMD: 1.21e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.94e+03 logL: -4.76e+03 KL: 1.65e+02 MMD: 1.40e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.07e+03 logL: -3.92e+03 KL: 1.34e+02 MMD: 1.21e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.07e+03 logL: -2.92e+03 KL: 1.32e+02 MMD: 1.25e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.97e+03 logL: -2.84e+03 KL: 1.07e+02 MMD: 1.43e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.88e+03 logL: -2.77e+03 KL: 9.11e+01 MMD: 1.35e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.85e+03 logL: -2.75e+03 KL: 8.05e+01 MMD: 1.42e+00\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.79e+03 logL: -2.70e+03 KL: 7.84e+01 MMD: 1.42e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.79e+03 logL: -2.69e+03 KL: 7.62e+01 MMD: 1.40e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.78e+03 logL: -2.69e+03 KL: 7.37e+01 MMD: 1.39e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.78e+03 logL: -2.69e+03 KL: 7.20e+01 MMD: 1.35e+00\n",
      "Epoch 00141: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 150 VALIDATION Loss: 2.77e+03 logL: -2.68e+03 KL: 7.15e+01 MMD: 1.29e+00\n",
      "Stopping\n",
      "====> Epoch: 153 VALIDATION Loss: 2.77e+03 logL: -2.68e+03 KL: 7.13e+01 MMD: 1.39e+00\n",
      "config 11, alpha = 0.0, lambda = 37530.0, dropout = 0.00; 2 hidden layers with 20, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.68e+05 logL: -3.48e+05 KL: 2.04e+02 MMD: 5.23e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.58e+05 logL: -3.39e+05 KL: 1.71e+02 MMD: 4.89e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.42e+05 logL: -3.20e+05 KL: 1.14e+02 MMD: 5.75e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 7.76e+04 logL: -1.85e+04 KL: 5.99e+02 MMD: 1.56e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.55e+04 logL: -1.69e+04 KL: 4.95e+02 MMD: 1.81e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.41e+04 logL: -1.62e+04 KL: 4.33e+02 MMD: 1.80e+00\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 67 VALIDATION Loss: 7.58e+04 logL: -1.59e+04 KL: 4.11e+02 MMD: 1.59e+00\n",
      "config 11, alpha = 0.0, lambda = 75864.5, dropout = 0.00; 2 hidden layers with 43, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.89e+04 logL: -1.46e+04 KL: 6.59e+00 MMD: 5.57e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 1.65e+04 logL: -1.05e+04 KL: 1.15e+01 MMD: 7.88e-02\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.36e+04 logL: -9.99e+03 KL: 1.15e+01 MMD: 4.76e-02\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.34e+04 logL: -9.88e+03 KL: 1.22e+01 MMD: 4.57e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.48e+04 logL: -9.86e+03 KL: 1.23e+01 MMD: 6.43e-02\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 56 VALIDATION Loss: 1.52e+04 logL: -9.85e+03 KL: 1.23e+01 MMD: 7.08e-02\n",
      "config 12, alpha = 0.0, lambda = 206.4, dropout = 0.00; 2 hidden layers with 125, 117 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.93e+03 logL: -9.56e+03 KL: 2.23e+02 MMD: 7.23e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.71e+03 logL: -9.43e+03 KL: 1.34e+02 MMD: 6.96e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.70e+03 logL: -9.45e+03 KL: 1.06e+02 MMD: 7.00e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.22e+03 logL: -8.97e+03 KL: 9.41e+01 MMD: 7.58e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 8.85e+03 logL: -8.62e+03 KL: 7.61e+01 MMD: 7.51e-01\n",
      "Epoch 00058: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 8.67e+03 logL: -8.44e+03 KL: 6.92e+01 MMD: 7.96e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.44e+03 logL: -8.21e+03 KL: 6.47e+01 MMD: 7.90e-01\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 8.38e+03 logL: -8.16e+03 KL: 6.10e+01 MMD: 7.62e-01\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 90 VALIDATION Loss: 8.37e+03 logL: -8.16e+03 KL: 6.08e+01 MMD: 7.28e-01\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 8.38e+03 logL: -8.16e+03 KL: 6.08e+01 MMD: 7.74e-01\n",
      "config 12, alpha = 0.0, lambda = 1194.6, dropout = 0.00; 2 hidden layers with 29, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.75e+04 logL: -1.41e+04 KL: 2.20e+03 MMD: 1.07e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.59e+04 logL: -1.39e+04 KL: 7.39e+02 MMD: 1.07e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.52e+04 logL: -1.35e+04 KL: 3.82e+02 MMD: 1.09e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.27e+04 logL: -1.12e+04 KL: 2.61e+02 MMD: 1.06e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.03e+04 logL: -8.79e+03 KL: 2.29e+02 MMD: 1.05e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.53e+03 logL: -8.05e+03 KL: 1.88e+02 MMD: 1.08e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.16e+03 logL: -7.71e+03 KL: 1.62e+02 MMD: 1.08e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 8.80e+03 logL: -7.37e+03 KL: 1.49e+02 MMD: 1.08e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 7.19e+03 logL: -5.78e+03 KL: 1.87e+02 MMD: 1.02e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.85e+03 logL: -5.46e+03 KL: 1.53e+02 MMD: 1.03e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.66e+03 logL: -5.33e+03 KL: 1.23e+02 MMD: 1.01e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 6.61e+03 logL: -5.21e+03 KL: 1.01e+02 MMD: 1.09e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 6.46e+03 logL: -5.16e+03 KL: 8.68e+01 MMD: 1.02e+00\n",
      "Epoch 00131: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 135 VALIDATION Loss: 6.52e+03 logL: -5.13e+03 KL: 8.45e+01 MMD: 1.09e+00\n",
      "config 12, alpha = 0.0, lambda = 22938.3, dropout = 0.00; 2 hidden layers with 9, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.98e+04 logL: -9.91e+03 KL: 4.60e+02 MMD: 1.28e+00\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 4.32e+04 logL: -9.63e+03 KL: 3.20e+02 MMD: 1.45e+00\n",
      "Stopping\n",
      "====> Epoch: 21 VALIDATION Loss: 4.11e+04 logL: -9.62e+03 KL: 3.12e+02 MMD: 1.36e+00\n",
      "config 12, alpha = 0.0, lambda = 9.2, dropout = 0.00; 2 hidden layers with 131, 32 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.14e+03 logL: -3.79e+03 KL: 3.34e+02 MMD: 1.69e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.52e+03 logL: -2.30e+03 KL: 2.02e+02 MMD: 1.61e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.79e+03 logL: -1.63e+03 KL: 1.45e+02 MMD: 1.53e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.86e+03 logL: -1.73e+03 KL: 1.19e+02 MMD: 1.70e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.68e+03 logL: -1.57e+03 KL: 1.03e+02 MMD: 1.65e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.71e+03 logL: -1.60e+03 KL: 9.59e+01 MMD: 1.78e+00\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.62e+03 logL: -1.51e+03 KL: 9.21e+01 MMD: 1.71e+00\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 80 VALIDATION Loss: 1.61e+03 logL: -1.51e+03 KL: 9.08e+01 MMD: 1.60e+00\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 86 VALIDATION Loss: 1.61e+03 logL: -1.51e+03 KL: 9.07e+01 MMD: 1.62e+00\n",
      "config 12, alpha = 0.0, lambda = 26.0, dropout = 0.00; 2 hidden layers with 151, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.46e+04 logL: -1.37e+04 KL: 8.83e+02 MMD: 2.16e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.13e+04 logL: -1.07e+04 KL: 5.05e+02 MMD: 1.99e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.40e+03 logL: -8.05e+03 KL: 2.98e+02 MMD: 1.93e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.48e+03 logL: -5.21e+03 KL: 2.19e+02 MMD: 1.87e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.88e+03 logL: -3.66e+03 KL: 1.70e+02 MMD: 1.86e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.73e+03 logL: -3.55e+03 KL: 1.36e+02 MMD: 1.86e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.06e+03 logL: -2.88e+03 KL: 1.26e+02 MMD: 1.88e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.91e+03 logL: -2.75e+03 KL: 1.10e+02 MMD: 1.91e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.87e+03 logL: -2.71e+03 KL: 1.05e+02 MMD: 2.04e+00\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 2.80e+03 logL: -2.65e+03 KL: 1.00e+02 MMD: 1.98e+00\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.79e+03 logL: -2.64e+03 KL: 9.82e+01 MMD: 1.99e+00\n",
      "Epoch 00110: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 113 VALIDATION Loss: 2.79e+03 logL: -2.64e+03 KL: 9.81e+01 MMD: 1.99e+00\n",
      "config 13, alpha = 0.0, lambda = 62532.5, dropout = 0.00; 2 hidden layers with 30, 29 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.80e+04 logL: -9.65e+03 KL: 5.74e+02 MMD: 7.64e-01\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 20 VALIDATION Loss: 5.53e+04 logL: -9.57e+03 KL: 5.55e+02 MMD: 7.23e-01\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.40e+04 logL: -9.56e+03 KL: 5.52e+02 MMD: 7.02e-01\n",
      "Stopping\n",
      "====> Epoch: 32 VALIDATION Loss: 5.39e+04 logL: -9.56e+03 KL: 5.52e+02 MMD: 7.01e-01\n",
      "config 13, alpha = 0.0, lambda = 24599.2, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.99e+04 logL: -1.49e+04 KL: 1.11e+02 MMD: 1.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.05e+04 logL: -1.37e+04 KL: 2.84e+02 MMD: 1.08e+00\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.95e+04 logL: -1.36e+04 KL: 2.83e+02 MMD: 1.04e+00\n",
      "Stopping\n",
      "====> Epoch: 31 VALIDATION Loss: 4.07e+04 logL: -1.36e+04 KL: 2.84e+02 MMD: 1.09e+00\n",
      "config 13, alpha = 0.0, lambda = 74647.8, dropout = 0.00; 2 hidden layers with 166, 107 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.01e+05 logL: -2.82e+03 KL: 2.91e+02 MMD: 1.32e+00\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 9.65e+04 logL: -2.74e+03 KL: 2.59e+02 MMD: 1.25e+00\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 26 VALIDATION Loss: 1.03e+05 logL: -2.73e+03 KL: 2.52e+02 MMD: 1.34e+00\n",
      "config 13, alpha = 0.0, lambda = 327.5, dropout = 0.00; 2 hidden layers with 18, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.46e+04 logL: -1.36e+04 KL: 4.77e+02 MMD: 1.64e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.03e+04 logL: -9.55e+03 KL: 2.48e+02 MMD: 1.68e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.05e+03 logL: -5.33e+03 KL: 2.11e+02 MMD: 1.57e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.36e+03 logL: -3.70e+03 KL: 1.77e+02 MMD: 1.49e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.58e+03 logL: -2.94e+03 KL: 1.52e+02 MMD: 1.50e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.44e+03 logL: -2.82e+03 KL: 1.30e+02 MMD: 1.51e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.38e+03 logL: -2.77e+03 KL: 1.17e+02 MMD: 1.54e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.38e+03 logL: -2.74e+03 KL: 1.04e+02 MMD: 1.64e+00\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 86 VALIDATION Loss: 3.36e+03 logL: -2.71e+03 KL: 1.02e+02 MMD: 1.68e+00\n",
      "config 13, alpha = 0.0, lambda = 107.5, dropout = 0.00; 2 hidden layers with 60, 49 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.85e+03 logL: -5.34e+03 KL: 3.12e+02 MMD: 1.83e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.59e+03 logL: -2.20e+03 KL: 1.78e+02 MMD: 1.96e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.97e+03 logL: -1.62e+03 KL: 1.37e+02 MMD: 2.01e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.60e+03 logL: -1.28e+03 KL: 1.14e+02 MMD: 1.90e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.24e+03 logL: -9.29e+02 KL: 1.05e+02 MMD: 1.92e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.17e+03 logL: -8.73e+02 KL: 9.40e+01 MMD: 1.92e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.27e+03 logL: -9.92e+02 KL: 8.42e+01 MMD: 1.85e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.32e+03 logL: -1.04e+03 KL: 8.05e+01 MMD: 1.86e+00\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.06e+03 logL: -8.08e+02 KL: 7.13e+01 MMD: 1.70e+00\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.07e+03 logL: -8.06e+02 KL: 6.99e+01 MMD: 1.81e+00\n",
      "Stopping\n",
      "====> Epoch: 101 VALIDATION Loss: 1.07e+03 logL: -8.06e+02 KL: 7.00e+01 MMD: 1.83e+00\n",
      "config 14, alpha = 0.0, lambda = 3920.6, dropout = 0.00; 2 hidden layers with 104, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.76e+04 logL: -1.40e+04 KL: 2.22e+02 MMD: 8.49e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.71e+04 logL: -1.37e+04 KL: 3.36e+02 MMD: 7.93e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.65e+04 logL: -1.36e+04 KL: 3.10e+02 MMD: 6.83e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.61e+04 logL: -1.29e+04 KL: 2.90e+02 MMD: 7.53e-01\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.72e+04 logL: -1.28e+04 KL: 1.30e+03 MMD: 7.76e-01\n",
      "Stopping\n",
      "====> Epoch: 51 VALIDATION Loss: 1.70e+04 logL: -1.28e+04 KL: 1.30e+03 MMD: 7.26e-01\n",
      "config 14, alpha = 0.0, lambda = 13.6, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.44e+04 logL: -1.37e+04 KL: 6.83e+02 MMD: 1.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 2.69e+02 MMD: 1.20e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.28e+02 MMD: 1.23e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 7.20e+01 MMD: 1.40e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.35e+04 logL: -1.35e+04 KL: 4.78e+01 MMD: 1.39e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 3.64e+01 MMD: 1.46e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 2.98e+01 MMD: 1.37e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.34e+04 logL: -1.34e+04 KL: 2.62e+01 MMD: 1.34e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.02e+04 logL: -1.02e+04 KL: 5.51e+01 MMD: 1.32e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.56e+03 logL: -9.49e+03 KL: 4.96e+01 MMD: 1.45e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 9.54e+03 logL: -9.48e+03 KL: 3.96e+01 MMD: 1.48e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 9.53e+03 logL: -9.48e+03 KL: 3.55e+01 MMD: 1.49e+00\n",
      "Epoch 00124: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 9.52e+03 logL: -9.46e+03 KL: 3.38e+01 MMD: 1.46e+00\n",
      "Stopping\n",
      "====> Epoch: 135 VALIDATION Loss: 9.52e+03 logL: -9.47e+03 KL: 3.36e+01 MMD: 1.56e+00\n",
      "config 14, alpha = 0.0, lambda = 2360.1, dropout = 0.00; 2 hidden layers with 16, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.53e+05 logL: -2.48e+05 KL: 1.41e+03 MMD: 1.45e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.02e+04 logL: -1.55e+04 KL: 1.35e+03 MMD: 1.43e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.83e+04 logL: -1.43e+04 KL: 7.28e+02 MMD: 1.37e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.75e+04 logL: -1.37e+04 KL: 5.10e+02 MMD: 1.41e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.54e+04 logL: -1.17e+04 KL: 4.10e+02 MMD: 1.37e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.21e+04 logL: -8.42e+03 KL: 3.80e+02 MMD: 1.40e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.12e+04 logL: -7.72e+03 KL: 2.81e+02 MMD: 1.35e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.05e+04 logL: -7.33e+03 KL: 2.28e+02 MMD: 1.23e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 8.97e+03 logL: -5.63e+03 KL: 2.30e+02 MMD: 1.32e+00\n",
      "Epoch 00095: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 100 VALIDATION Loss: 8.75e+03 logL: -5.44e+03 KL: 2.00e+02 MMD: 1.32e+00\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 109 VALIDATION Loss: 8.86e+03 logL: -5.41e+03 KL: 1.97e+02 MMD: 1.38e+00\n",
      "config 14, alpha = 0.0, lambda = 655.5, dropout = 0.00; 2 hidden layers with 13, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.51e+04 logL: -1.37e+04 KL: 3.36e+02 MMD: 1.69e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.18e+04 logL: -1.02e+04 KL: 3.79e+02 MMD: 1.78e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.22e+03 logL: -7.82e+03 KL: 2.68e+02 MMD: 1.73e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.54e+03 logL: -7.16e+03 KL: 1.86e+02 MMD: 1.82e+00\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 46 VALIDATION Loss: 8.45e+03 logL: -7.13e+03 KL: 1.77e+02 MMD: 1.74e+00\n",
      "config 14, alpha = 0.0, lambda = 1.9, dropout = 0.00; 2 hidden layers with 15, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.88e+04 logL: -3.70e+04 KL: 1.84e+03 MMD: 2.23e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.51e+04 logL: -1.44e+04 KL: 7.04e+02 MMD: 2.62e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.47e+04 logL: -1.43e+04 KL: 3.82e+02 MMD: 2.71e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.45e+04 logL: -1.43e+04 KL: 2.49e+02 MMD: 3.03e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.43e+04 logL: -1.41e+04 KL: 1.81e+02 MMD: 2.98e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.18e+04 logL: -1.16e+04 KL: 1.47e+02 MMD: 2.85e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.12e+04 logL: -1.11e+04 KL: 1.13e+02 MMD: 3.14e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.02e+04 logL: -1.01e+04 KL: 9.48e+01 MMD: 3.08e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.97e+03 logL: -9.89e+03 KL: 8.56e+01 MMD: 2.76e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.30e+03 logL: -9.21e+03 KL: 9.02e+01 MMD: 2.49e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 7.97e+03 logL: -7.87e+03 KL: 1.05e+02 MMD: 2.20e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 6.91e+03 logL: -6.79e+03 KL: 1.15e+02 MMD: 2.11e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 6.10e+03 logL: -5.97e+03 KL: 1.22e+02 MMD: 2.26e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 4.59e+03 logL: -4.45e+03 KL: 1.38e+02 MMD: 2.09e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 3.92e+03 logL: -3.78e+03 KL: 1.36e+02 MMD: 2.24e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 3.53e+03 logL: -3.40e+03 KL: 1.28e+02 MMD: 1.95e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 3.23e+03 logL: -3.11e+03 KL: 1.21e+02 MMD: 1.85e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 2.68e+03 logL: -2.56e+03 KL: 1.16e+02 MMD: 2.12e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 2.50e+03 logL: -2.39e+03 KL: 1.05e+02 MMD: 1.99e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 2.36e+03 logL: -2.26e+03 KL: 9.75e+01 MMD: 2.14e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 2.36e+03 logL: -2.26e+03 KL: 9.75e+01 MMD: 2.14e+00\n",
      "config 15, alpha = 0.0, lambda = 14156.1, dropout = 0.00; 2 hidden layers with 96, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.92e+04 logL: -9.66e+03 KL: 5.99e+02 MMD: 6.31e-01\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 1.94e+04 logL: -9.56e+03 KL: 3.60e+02 MMD: 6.72e-01\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 28 VALIDATION Loss: 1.94e+04 logL: -9.55e+03 KL: 3.45e+02 MMD: 6.73e-01\n",
      "config 15, alpha = 0.0, lambda = 64879.5, dropout = 0.00; 2 hidden layers with 38, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.66e+04 logL: -1.19e+04 KL: 5.84e+02 MMD: 1.14e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.08e+04 logL: -5.58e+03 KL: 3.24e+02 MMD: 1.00e+00\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 7.73e+04 logL: -5.40e+03 KL: 2.80e+02 MMD: 1.10e+00\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 40 VALIDATION Loss: 7.41e+04 logL: -5.39e+03 KL: 2.79e+02 MMD: 1.05e+00\n",
      "Stopping\n",
      "====> Epoch: 43 VALIDATION Loss: 7.16e+04 logL: -5.39e+03 KL: 2.79e+02 MMD: 1.02e+00\n",
      "config 15, alpha = 0.0, lambda = 80.1, dropout = 0.00; 2 hidden layers with 35, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.58e+03 logL: -4.08e+03 KL: 3.92e+02 MMD: 1.37e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.86e+03 logL: -3.60e+03 KL: 1.68e+02 MMD: 1.27e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.02e+03 logL: -2.77e+03 KL: 1.34e+02 MMD: 1.38e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.00e+03 logL: -2.79e+03 KL: 1.02e+02 MMD: 1.38e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.93e+03 logL: -2.74e+03 KL: 8.63e+01 MMD: 1.31e+00\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.90e+03 logL: -2.72e+03 KL: 7.85e+01 MMD: 1.29e+00\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.89e+03 logL: -2.72e+03 KL: 7.65e+01 MMD: 1.22e+00\n",
      "Stopping\n",
      "====> Epoch: 73 VALIDATION Loss: 2.90e+03 logL: -2.72e+03 KL: 7.64e+01 MMD: 1.35e+00\n",
      "config 15, alpha = 0.0, lambda = 9134.3, dropout = 0.00; 2 hidden layers with 139, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.98e+04 logL: -5.99e+03 KL: 4.99e+02 MMD: 1.46e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.74e+04 logL: -3.01e+03 KL: 2.71e+02 MMD: 1.55e+00\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 28 VALIDATION Loss: 1.85e+04 logL: -2.87e+03 KL: 2.32e+02 MMD: 1.68e+00\n",
      "config 15, alpha = 0.0, lambda = 8348.5, dropout = 0.00; 2 hidden layers with 95, 32 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.80e+04 logL: -1.17e+04 KL: 6.95e+02 MMD: 1.87e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.19e+04 logL: -5.31e+03 KL: 3.87e+02 MMD: 1.94e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.12e+04 logL: -3.17e+03 KL: 2.87e+02 MMD: 2.13e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.87e+04 logL: -2.46e+03 KL: 2.38e+02 MMD: 1.92e+00\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.95e+04 logL: -2.05e+03 KL: 2.21e+02 MMD: 2.07e+00\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 57 VALIDATION Loss: 1.80e+04 logL: -1.98e+03 KL: 2.20e+02 MMD: 1.89e+00\n",
      "config 16, alpha = 0.0, lambda = 3493.5, dropout = 0.00; 2 hidden layers with 97, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.25e+04 logL: -9.57e+03 KL: 2.71e+02 MMD: 7.53e-01\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 1.20e+04 logL: -9.48e+03 KL: 1.91e+02 MMD: 6.56e-01\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.22e+04 logL: -9.47e+03 KL: 1.84e+02 MMD: 7.35e-01\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 1.22e+04 logL: -9.47e+03 KL: 1.83e+02 MMD: 7.32e-01\n",
      "config 16, alpha = 0.0, lambda = 1.0, dropout = 0.00; 2 hidden layers with 39, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.80e+03 logL: -7.37e+03 KL: 4.31e+02 MMD: 1.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.61e+03 logL: -5.39e+03 KL: 2.24e+02 MMD: 1.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.40e+03 logL: -5.27e+03 KL: 1.34e+02 MMD: 1.01e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.30e+03 logL: -5.20e+03 KL: 9.92e+01 MMD: 1.19e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.20e+03 logL: -5.12e+03 KL: 8.17e+01 MMD: 9.91e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 5.09e+03 logL: -5.02e+03 KL: 7.25e+01 MMD: 1.00e+00\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.04e+03 logL: -4.97e+03 KL: 6.91e+01 MMD: 1.01e+00\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 5.03e+03 logL: -4.97e+03 KL: 6.84e+01 MMD: 1.03e+00\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 5.03e+03 logL: -4.97e+03 KL: 6.83e+01 MMD: 1.07e+00\n",
      "config 16, alpha = 0.0, lambda = 1210.3, dropout = 0.00; 2 hidden layers with 8, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.58e+04 logL: -1.37e+04 KL: 4.09e+02 MMD: 1.39e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.45e+04 logL: -1.26e+04 KL: 2.85e+02 MMD: 1.35e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.14e+04 logL: -9.58e+03 KL: 1.59e+02 MMD: 1.37e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.43e+04 logL: -1.24e+04 KL: 2.19e+02 MMD: 1.36e+00\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 44 VALIDATION Loss: 1.39e+04 logL: -1.21e+04 KL: 2.14e+02 MMD: 1.28e+00\n",
      "config 16, alpha = 0.0, lambda = 31.2, dropout = 0.00; 2 hidden layers with 12, 10 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 1.40e+04 logL: -1.35e+04 KL: 3.75e+02 MMD: 2.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.87e+03 logL: -9.59e+03 KL: 2.21e+02 MMD: 1.98e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.71e+03 logL: -9.51e+03 KL: 1.39e+02 MMD: 2.04e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.33e+03 logL: -7.14e+03 KL: 1.27e+02 MMD: 2.07e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.28e+03 logL: -7.12e+03 KL: 9.88e+01 MMD: 1.99e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.26e+03 logL: -7.11e+03 KL: 8.43e+01 MMD: 2.07e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.61e+03 logL: -5.46e+03 KL: 9.99e+01 MMD: 1.77e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.35e+03 logL: -5.22e+03 KL: 8.07e+01 MMD: 1.80e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.34e+03 logL: -5.21e+03 KL: 7.09e+01 MMD: 1.97e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.29e+03 logL: -5.17e+03 KL: 6.51e+01 MMD: 1.85e+00\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 5.27e+03 logL: -5.15e+03 KL: 6.25e+01 MMD: 1.82e+00\n",
      "Stopping\n",
      "====> Epoch: 115 VALIDATION Loss: 5.27e+03 logL: -5.15e+03 KL: 6.20e+01 MMD: 1.85e+00\n",
      "config 16, alpha = 0.0, lambda = 21031.2, dropout = 0.00; 2 hidden layers with 34, 27 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.16e+04 logL: -3.70e+03 KL: 3.96e+02 MMD: 1.78e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.06e+04 logL: -2.26e+03 KL: 2.51e+02 MMD: 1.81e+00\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 28 VALIDATION Loss: 4.36e+04 logL: -2.13e+03 KL: 2.21e+02 MMD: 1.96e+00\n",
      "config 17, alpha = 0.0, lambda = 13033.4, dropout = 0.00; 2 hidden layers with 43, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.83e+04 logL: -9.55e+03 KL: 4.40e+02 MMD: 6.41e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.93e+04 logL: -9.52e+03 KL: 2.75e+02 MMD: 7.29e-01\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.84e+04 logL: -9.45e+03 KL: 2.71e+02 MMD: 6.70e-01\n",
      "Stopping\n",
      "====> Epoch: 32 VALIDATION Loss: 1.95e+04 logL: -9.45e+03 KL: 2.70e+02 MMD: 7.50e-01\n",
      "config 17, alpha = 0.0, lambda = 673.7, dropout = 0.00; 2 hidden layers with 54, 39 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.87e+05 logL: -3.53e+05 KL: 3.30e+04 MMD: 9.93e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.38e+05 logL: -3.21e+05 KL: 1.59e+04 MMD: 1.12e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.16e+05 logL: -3.06e+05 KL: 8.72e+03 MMD: 9.96e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.96e+05 logL: -2.89e+05 KL: 6.28e+03 MMD: 9.99e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.71e+05 logL: -2.66e+05 KL: 4.26e+03 MMD: 1.04e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.27e+05 logL: -2.24e+05 KL: 3.19e+03 MMD: 9.85e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.43e+04 logL: -8.95e+04 KL: 4.09e+03 MMD: 9.46e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 2.00e+04 logL: -1.62e+04 KL: 3.18e+03 MMD: 1.03e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.56e+04 logL: -1.30e+04 KL: 1.84e+03 MMD: 1.03e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.44e+04 logL: -1.26e+04 KL: 1.07e+03 MMD: 1.01e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.31e+04 logL: -1.19e+04 KL: 6.61e+02 MMD: 9.10e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.17e+04 logL: -1.06e+04 KL: 4.30e+02 MMD: 1.01e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.06e+04 logL: -9.63e+03 KL: 3.21e+02 MMD: 9.82e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 8.77e+03 logL: -7.84e+03 KL: 2.70e+02 MMD: 9.80e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 8.41e+03 logL: -7.51e+03 KL: 1.94e+02 MMD: 1.05e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 6.90e+03 logL: -5.99e+03 KL: 2.34e+02 MMD: 1.00e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 6.32e+03 logL: -5.46e+03 KL: 1.72e+02 MMD: 1.02e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 6.11e+03 logL: -5.31e+03 KL: 1.34e+02 MMD: 9.95e-01\n",
      "====> Epoch: 190 VALIDATION Loss: 5.99e+03 logL: -5.22e+03 KL: 1.06e+02 MMD: 9.87e-01\n",
      "Epoch 00199: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 200 VALIDATION Loss: 5.93e+03 logL: -5.15e+03 KL: 8.82e+01 MMD: 1.02e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 5.93e+03 logL: -5.15e+03 KL: 8.82e+01 MMD: 1.02e+00\n",
      "config 17, alpha = 0.0, lambda = 391.5, dropout = 0.00; 2 hidden layers with 187, 40 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.36e+03 logL: -3.60e+03 KL: 2.68e+02 MMD: 1.27e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.44e+03 logL: -2.80e+03 KL: 1.54e+02 MMD: 1.25e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.39e+03 logL: -2.77e+03 KL: 1.26e+02 MMD: 1.26e+00\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 3.34e+03 logL: -2.68e+03 KL: 1.14e+02 MMD: 1.40e+00\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.29e+03 logL: -2.67e+03 KL: 1.07e+02 MMD: 1.31e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.25e+03 logL: -2.67e+03 KL: 1.05e+02 MMD: 1.22e+00\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 64 VALIDATION Loss: 3.30e+03 logL: -2.67e+03 KL: 1.05e+02 MMD: 1.34e+00\n",
      "config 17, alpha = 0.0, lambda = 27726.7, dropout = 0.00; 2 hidden layers with 12, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.33e+04 logL: -1.70e+04 KL: 2.15e+03 MMD: 1.60e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.08e+04 logL: -1.54e+04 KL: 1.02e+03 MMD: 1.60e+00\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 6.08e+04 logL: -1.52e+04 KL: 9.41e+02 MMD: 1.61e+00\n",
      "Stopping\n",
      "====> Epoch: 33 VALIDATION Loss: 6.18e+04 logL: -1.52e+04 KL: 9.38e+02 MMD: 1.65e+00\n",
      "config 17, alpha = 0.0, lambda = 110.1, dropout = 0.00; 2 hidden layers with 56, 28 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.28e+03 logL: -7.74e+03 KL: 3.34e+02 MMD: 1.94e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.91e+03 logL: -3.49e+03 KL: 2.09e+02 MMD: 1.95e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.36e+03 logL: -2.98e+03 KL: 1.61e+02 MMD: 1.95e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.50e+03 logL: -2.15e+03 KL: 1.43e+02 MMD: 1.89e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.95e+03 logL: -1.63e+03 KL: 1.25e+02 MMD: 1.78e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.78e+03 logL: -1.44e+03 KL: 1.12e+02 MMD: 2.02e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.49e+03 logL: -1.18e+03 KL: 9.94e+01 MMD: 1.88e+00\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.43e+03 logL: -1.13e+03 KL: 9.18e+01 MMD: 1.90e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.43e+03 logL: -1.13e+03 KL: 8.89e+01 MMD: 1.99e+00\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 98 VALIDATION Loss: 1.42e+03 logL: -1.13e+03 KL: 8.72e+01 MMD: 1.90e+00\n",
      "config 18, alpha = 0.0, lambda = 830.2, dropout = 0.00; 2 hidden layers with 172, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.02e+04 logL: -9.48e+03 KL: 1.87e+02 MMD: 6.86e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.01e+04 logL: -9.39e+03 KL: 1.23e+02 MMD: 7.24e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.67e+03 logL: -8.98e+03 KL: 9.31e+01 MMD: 7.15e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.60e+03 logL: -8.92e+03 KL: 9.26e+01 MMD: 7.02e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.21e+03 logL: -8.55e+03 KL: 7.95e+01 MMD: 7.10e-01\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 56 VALIDATION Loss: 9.14e+03 logL: -8.44e+03 KL: 7.80e+01 MMD: 7.49e-01\n",
      "config 18, alpha = 0.0, lambda = 1.1, dropout = 0.00; 2 hidden layers with 14, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.51e+04 logL: -1.43e+04 KL: 8.21e+02 MMD: 1.14e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.42e+04 logL: -1.38e+04 KL: 3.86e+02 MMD: 1.10e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.94e+03 logL: -9.72e+03 KL: 2.15e+02 MMD: 1.16e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.68e+03 logL: -9.56e+03 KL: 1.19e+02 MMD: 1.20e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.04e+03 logL: -7.94e+03 KL: 9.82e+01 MMD: 9.86e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 7.66e+03 logL: -7.59e+03 KL: 7.26e+01 MMD: 1.22e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.54e+03 logL: -5.46e+03 KL: 7.60e+01 MMD: 1.07e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.40e+03 logL: -5.34e+03 KL: 6.48e+01 MMD: 1.06e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.27e+03 logL: -5.21e+03 KL: 5.65e+01 MMD: 1.04e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.24e+03 logL: -5.18e+03 KL: 5.17e+01 MMD: 1.11e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.20e+03 logL: -5.15e+03 KL: 4.85e+01 MMD: 1.07e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 120 VALIDATION Loss: 5.13e+03 logL: -5.09e+03 KL: 4.73e+01 MMD: 1.13e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 5.16e+03 logL: -5.11e+03 KL: 4.55e+01 MMD: 1.04e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 5.10e+03 logL: -5.05e+03 KL: 4.41e+01 MMD: 1.04e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 5.05e+03 logL: -5.01e+03 KL: 4.24e+01 MMD: 1.13e+00\n",
      "Epoch 00156: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 160 VALIDATION Loss: 5.04e+03 logL: -5.00e+03 KL: 4.20e+01 MMD: 1.06e+00\n",
      "Epoch 00163: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 167 VALIDATION Loss: 5.04e+03 logL: -5.00e+03 KL: 4.19e+01 MMD: 1.16e+00\n",
      "config 18, alpha = 0.0, lambda = 963.8, dropout = 0.00; 2 hidden layers with 23, 18 nodes\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 10 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "Stopping\n",
      "====> Epoch: 10 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "config 18, alpha = 0.0, lambda = 56.8, dropout = 0.00; 2 hidden layers with 157, 28 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.53e+03 logL: -7.19e+03 KL: 2.54e+02 MMD: 1.70e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.17e+03 logL: -2.92e+03 KL: 1.68e+02 MMD: 1.58e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.41e+03 logL: -2.19e+03 KL: 1.30e+02 MMD: 1.59e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.18e+03 logL: -1.98e+03 KL: 1.15e+02 MMD: 1.57e+00\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.73e+03 logL: -1.54e+03 KL: 1.00e+02 MMD: 1.65e+00\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.72e+03 logL: -1.54e+03 KL: 9.63e+01 MMD: 1.56e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.72e+03 logL: -1.53e+03 KL: 9.54e+01 MMD: 1.65e+00\n",
      "Stopping\n",
      "====> Epoch: 70 VALIDATION Loss: 1.72e+03 logL: -1.53e+03 KL: 9.54e+01 MMD: 1.65e+00\n",
      "config 18, alpha = 0.0, lambda = 3146.8, dropout = 0.00; 2 hidden layers with 13, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.48e+07 logL: -1.32e+06 KL: 1.35e+07 MMD: 1.91e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.00e+06 logL: -4.71e+05 KL: 4.53e+06 MMD: 1.97e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.37e+06 logL: -3.96e+05 KL: 1.97e+06 MMD: 1.97e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+06 logL: -3.85e+05 KL: 9.71e+05 MMD: 1.82e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.39e+05 logL: -3.43e+05 KL: 4.90e+05 MMD: 1.89e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.87e+05 logL: -3.34e+05 KL: 2.46e+05 MMD: 1.98e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.39e+05 logL: -3.09e+05 KL: 1.24e+05 MMD: 1.90e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.37e+05 logL: -2.71e+05 KL: 6.09e+04 MMD: 1.85e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.58e+05 logL: -1.20e+05 KL: 3.12e+04 MMD: 2.07e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.19e+04 logL: -1.95e+04 KL: 1.62e+04 MMD: 1.95e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.90e+04 logL: -1.43e+04 KL: 8.62e+03 MMD: 1.96e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.50e+04 logL: -1.42e+04 KL: 4.93e+03 MMD: 1.89e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.32e+04 logL: -1.40e+04 KL: 3.02e+03 MMD: 1.97e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.19e+04 logL: -1.39e+04 KL: 2.02e+03 MMD: 1.91e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.10e+04 logL: -1.38e+04 KL: 1.35e+03 MMD: 1.84e+00\n",
      "Epoch 00154: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 160 VALIDATION Loss: 2.06e+04 logL: -1.38e+04 KL: 1.08e+03 MMD: 1.82e+00\n",
      "Epoch 00164: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 168 VALIDATION Loss: 2.07e+04 logL: -1.37e+04 KL: 1.03e+03 MMD: 1.89e+00\n",
      "config 19, alpha = 0.0, lambda = 1389.4, dropout = 0.00; 2 hidden layers with 108, 42 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.93e+05 logL: -1.44e+05 KL: 4.78e+04 MMD: 6.46e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.81e+04 logL: -2.33e+04 KL: 1.38e+04 MMD: 7.42e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.05e+04 logL: -1.51e+04 KL: 4.49e+03 MMD: 6.79e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.69e+04 logL: -1.43e+04 KL: 1.71e+03 MMD: 6.35e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.56e+04 logL: -1.37e+04 KL: 9.22e+02 MMD: 6.72e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.50e+04 logL: -1.35e+04 KL: 5.19e+02 MMD: 7.00e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.26e+04 logL: -1.11e+04 KL: 4.22e+02 MMD: 7.62e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.12e+04 logL: -1.01e+04 KL: 3.00e+02 MMD: 6.31e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.10e+04 logL: -9.85e+03 KL: 1.90e+02 MMD: 7.09e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.09e+04 logL: -9.75e+03 KL: 1.29e+02 MMD: 7.06e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.07e+04 logL: -9.66e+03 KL: 9.31e+01 MMD: 6.52e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.04e+04 logL: -9.56e+03 KL: 5.22e+01 MMD: 5.69e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 1.00e+04 logL: -9.62e+03 KL: 2.46e+01 MMD: 2.56e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 9.75e+03 logL: -9.56e+03 KL: 2.19e+01 MMD: 1.26e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 9.65e+03 logL: -9.53e+03 KL: 2.11e+01 MMD: 7.72e-02\n",
      "====> Epoch: 160 VALIDATION Loss: 9.65e+03 logL: -9.49e+03 KL: 2.05e+01 MMD: 9.92e-02\n",
      "====> Epoch: 170 VALIDATION Loss: 9.54e+03 logL: -9.43e+03 KL: 2.06e+01 MMD: 6.54e-02\n",
      "====> Epoch: 180 VALIDATION Loss: 9.45e+03 logL: -9.31e+03 KL: 2.02e+01 MMD: 7.95e-02\n",
      "====> Epoch: 190 VALIDATION Loss: 9.32e+03 logL: -9.15e+03 KL: 2.17e+01 MMD: 1.07e-01\n",
      "====> Epoch: 200 VALIDATION Loss: 9.31e+03 logL: -9.11e+03 KL: 2.41e+01 MMD: 1.28e-01\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 9.31e+03 logL: -9.11e+03 KL: 2.41e+01 MMD: 1.28e-01\n",
      "config 19, alpha = 0.0, lambda = 2.8, dropout = 0.00; 2 hidden layers with 14, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.43e+04 logL: -1.36e+04 KL: 6.35e+02 MMD: 1.07e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.37e+04 logL: -1.34e+04 KL: 2.29e+02 MMD: 1.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.55e+03 logL: -8.21e+03 KL: 3.34e+02 MMD: 1.00e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.96e+03 logL: -5.73e+03 KL: 2.21e+02 MMD: 1.06e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.45e+03 logL: -5.29e+03 KL: 1.58e+02 MMD: 1.03e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.35e+03 logL: -5.23e+03 KL: 1.18e+02 MMD: 1.05e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.30e+03 logL: -5.20e+03 KL: 9.64e+01 MMD: 9.84e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 5.26e+03 logL: -5.17e+03 KL: 8.14e+01 MMD: 1.10e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.22e+03 logL: -5.15e+03 KL: 7.35e+01 MMD: 9.56e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 5.19e+03 logL: -5.12e+03 KL: 6.62e+01 MMD: 1.11e+00\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 5.14e+03 logL: -5.07e+03 KL: 6.56e+01 MMD: 1.02e+00\n",
      "Stopping\n",
      "====> Epoch: 114 VALIDATION Loss: 5.14e+03 logL: -5.07e+03 KL: 6.48e+01 MMD: 1.05e+00\n",
      "config 19, alpha = 0.0, lambda = 47188.3, dropout = 0.00; 2 hidden layers with 26, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.55e+04 logL: -1.67e+04 KL: 3.80e+03 MMD: 1.38e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.36e+04 logL: -1.62e+04 KL: 1.95e+03 MMD: 1.39e+00\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 25 VALIDATION Loss: 8.49e+04 logL: -1.60e+04 KL: 1.78e+03 MMD: 1.42e+00\n",
      "config 19, alpha = 0.0, lambda = 856.7, dropout = 0.00; 2 hidden layers with 116, 56 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.59e+04 logL: -1.35e+04 KL: 1.02e+03 MMD: 1.62e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.47e+04 logL: -1.28e+04 KL: 4.36e+02 MMD: 1.73e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.03e+04 logL: -8.59e+03 KL: 3.60e+02 MMD: 1.57e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.22e+03 logL: -5.60e+03 KL: 2.84e+02 MMD: 1.56e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.71e+03 logL: -3.09e+03 KL: 2.62e+02 MMD: 1.58e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.42e+03 logL: -2.87e+03 KL: 1.93e+02 MMD: 1.58e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.70e+03 logL: -1.97e+03 KL: 1.78e+02 MMD: 1.82e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.38e+03 logL: -1.75e+03 KL: 1.54e+02 MMD: 1.72e+00\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.23e+03 logL: -1.66e+03 KL: 1.39e+02 MMD: 1.68e+00\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 3.23e+03 logL: -1.66e+03 KL: 1.39e+02 MMD: 1.67e+00\n",
      "config 19, alpha = 0.0, lambda = 4.5, dropout = 0.00; 2 hidden layers with 13, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.15e+04 logL: -1.10e+04 KL: 5.23e+02 MMD: 2.04e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 20 VALIDATION Loss: 7.41e+03 logL: -7.21e+03 KL: 1.91e+02 MMD: 2.21e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.92e+03 logL: -6.78e+03 KL: 1.35e+02 MMD: 2.23e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.79e+03 logL: -3.64e+03 KL: 1.41e+02 MMD: 1.97e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.72e+03 logL: -3.61e+03 KL: 1.09e+02 MMD: 2.06e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.71e+03 logL: -3.61e+03 KL: 9.19e+01 MMD: 2.12e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.69e+03 logL: -3.61e+03 KL: 8.19e+01 MMD: 2.00e+00\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.67e+03 logL: -3.58e+03 KL: 8.13e+01 MMD: 2.18e+00\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 3.67e+03 logL: -3.58e+03 KL: 8.11e+01 MMD: 2.08e+00\n",
      "config 20, alpha = 0.0, lambda = 114.6, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.45e+04 logL: -1.37e+04 KL: 6.84e+02 MMD: 6.94e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.40e+04 logL: -1.35e+04 KL: 3.74e+02 MMD: 8.54e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 1.87e+02 MMD: 8.80e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 1.10e+02 MMD: 9.60e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 7.21e+01 MMD: 1.08e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.36e+04 logL: -1.34e+04 KL: 5.18e+01 MMD: 1.11e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 3.64e+01 MMD: 8.97e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 3.02e+01 MMD: 8.42e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 2.47e+01 MMD: 7.61e-01\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.35e+04 logL: -1.33e+04 KL: 2.20e+01 MMD: 7.65e-01\n",
      "Stopping\n",
      "====> Epoch: 101 VALIDATION Loss: 1.35e+04 logL: -1.33e+04 KL: 2.20e+01 MMD: 8.16e-01\n",
      "config 20, alpha = 0.0, lambda = 8.7, dropout = 0.00; 2 hidden layers with 8, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.93e+04 logL: -1.79e+04 KL: 1.46e+03 MMD: 1.00e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.60e+04 logL: -1.50e+04 KL: 9.81e+02 MMD: 1.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.46e+04 logL: -1.40e+04 KL: 5.82e+02 MMD: 9.35e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.41e+04 logL: -1.37e+04 KL: 3.40e+02 MMD: 9.73e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.38e+04 logL: -1.36e+04 KL: 2.08e+02 MMD: 1.07e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.41e+02 MMD: 1.17e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 1.03e+02 MMD: 1.20e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 8.07e+01 MMD: 1.33e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 6.35e+01 MMD: 1.28e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.35e+04 logL: -1.35e+04 KL: 6.73e+01 MMD: 1.47e+00\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 107 VALIDATION Loss: 1.34e+04 logL: -1.34e+04 KL: 6.07e+01 MMD: 1.56e+00\n",
      "config 20, alpha = 0.0, lambda = 230.1, dropout = 0.00; 2 hidden layers with 53, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.17e+03 logL: -5.55e+03 KL: 3.18e+02 MMD: 1.33e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.63e+03 logL: -5.18e+03 KL: 1.53e+02 MMD: 1.32e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.01e+03 logL: -3.59e+03 KL: 1.25e+02 MMD: 1.27e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.98e+03 logL: -3.57e+03 KL: 9.97e+01 MMD: 1.37e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.86e+03 logL: -3.47e+03 KL: 8.88e+01 MMD: 1.31e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.56e+03 logL: -3.17e+03 KL: 8.47e+01 MMD: 1.34e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.09e+03 logL: -2.73e+03 KL: 7.74e+01 MMD: 1.21e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.07e+03 logL: -2.70e+03 KL: 6.91e+01 MMD: 1.34e+00\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.03e+03 logL: -2.66e+03 KL: 6.23e+01 MMD: 1.34e+00\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 98 VALIDATION Loss: 3.03e+03 logL: -2.66e+03 KL: 6.17e+01 MMD: 1.36e+00\n",
      "config 20, alpha = 0.0, lambda = 11.2, dropout = 0.00; 2 hidden layers with 15, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.39e+04 logL: -1.36e+04 KL: 3.53e+02 MMD: 2.26e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.49e+03 logL: -7.22e+03 KL: 2.56e+02 MMD: 1.66e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.71e+03 logL: -5.51e+03 KL: 1.80e+02 MMD: 1.63e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.53e+03 logL: -5.38e+03 KL: 1.28e+02 MMD: 1.82e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.35e+03 logL: -5.22e+03 KL: 1.05e+02 MMD: 1.81e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.79e+03 logL: -3.64e+03 KL: 1.26e+02 MMD: 1.70e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.69e+03 logL: -3.57e+03 KL: 9.66e+01 MMD: 1.78e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.65e+03 logL: -3.55e+03 KL: 8.47e+01 MMD: 1.70e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.63e+03 logL: -3.53e+03 KL: 7.84e+01 MMD: 1.76e+00\n",
      "Epoch 00099: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 3.61e+03 logL: -3.52e+03 KL: 7.26e+01 MMD: 1.73e+00\n",
      "Epoch 00106: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 3.61e+03 logL: -3.52e+03 KL: 7.21e+01 MMD: 1.82e+00\n",
      "Stopping\n",
      "====> Epoch: 110 VALIDATION Loss: 3.61e+03 logL: -3.52e+03 KL: 7.21e+01 MMD: 1.82e+00\n",
      "config 20, alpha = 0.0, lambda = 37471.3, dropout = 0.00; 2 hidden layers with 149, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.56e+04 logL: -1.41e+04 KL: 1.48e+03 MMD: 1.87e+00\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 9.06e+04 logL: -1.40e+04 KL: 1.12e+03 MMD: 2.01e+00\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 24 VALIDATION Loss: 9.08e+04 logL: -1.40e+04 KL: 1.11e+03 MMD: 2.02e+00\n",
      "config 21, alpha = 0.0, lambda = 38.4, dropout = 0.00; 2 hidden layers with 43, 40 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.83e+03 logL: -9.58e+03 KL: 2.23e+02 MMD: 7.05e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.56e+03 logL: -9.43e+03 KL: 1.08e+02 MMD: 6.69e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.33e+03 logL: -9.23e+03 KL: 7.68e+01 MMD: 6.82e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 8.93e+03 logL: -8.85e+03 KL: 5.62e+01 MMD: 7.96e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.18e+04 logL: -1.17e+04 KL: 4.96e+01 MMD: 8.57e-01\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 8.71e+03 logL: -8.64e+03 KL: 4.60e+01 MMD: 7.63e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.57e+03 logL: -8.50e+03 KL: 4.44e+01 MMD: 8.74e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.54e+03 logL: -8.47e+03 KL: 4.24e+01 MMD: 8.13e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 8.56e+03 logL: -8.48e+03 KL: 4.23e+01 MMD: 7.73e-01\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 96 VALIDATION Loss: 8.55e+03 logL: -8.47e+03 KL: 4.19e+01 MMD: 8.00e-01\n",
      "config 21, alpha = 0.0, lambda = 2309.8, dropout = 0.00; 2 hidden layers with 27, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.05e+04 logL: -8.10e+03 KL: 3.96e+02 MMD: 8.77e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 8.14e+03 logL: -5.58e+03 KL: 1.85e+02 MMD: 1.03e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.07e+03 logL: -5.53e+03 KL: 1.28e+02 MMD: 1.05e+00\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 36 VALIDATION Loss: 7.91e+03 logL: -5.43e+03 KL: 1.24e+02 MMD: 1.02e+00\n",
      "config 21, alpha = 0.0, lambda = 5.3, dropout = 0.00; 2 hidden layers with 68, 38 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.97e+03 logL: -9.64e+03 KL: 3.26e+02 MMD: 1.44e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.64e+03 logL: -4.42e+03 KL: 2.12e+02 MMD: 1.36e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.10e+03 logL: -2.93e+03 KL: 1.73e+02 MMD: 1.41e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.12e+03 logL: -2.97e+03 KL: 1.39e+02 MMD: 1.42e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.90e+03 logL: -2.78e+03 KL: 1.13e+02 MMD: 1.32e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.83e+03 logL: -2.73e+03 KL: 9.65e+01 MMD: 1.23e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.82e+03 logL: -2.73e+03 KL: 8.54e+01 MMD: 1.43e+00\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 2.74e+03 logL: -2.66e+03 KL: 7.92e+01 MMD: 1.30e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.74e+03 logL: -2.65e+03 KL: 7.68e+01 MMD: 1.26e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 100 VALIDATION Loss: 2.73e+03 logL: -2.65e+03 KL: 7.50e+01 MMD: 1.26e+00\n",
      "Epoch 00106: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.72e+03 logL: -2.64e+03 KL: 7.32e+01 MMD: 1.38e+00\n",
      "Epoch 00115: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 117 VALIDATION Loss: 2.72e+03 logL: -2.64e+03 KL: 7.30e+01 MMD: 1.32e+00\n",
      "config 21, alpha = 0.0, lambda = 61296.2, dropout = 0.00; 2 hidden layers with 63, 28 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.08e+05 logL: -7.81e+03 KL: 5.91e+02 MMD: 1.63e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.07e+05 logL: -4.84e+03 KL: 3.60e+02 MMD: 1.66e+00\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 24 VALIDATION Loss: 1.04e+05 logL: -4.75e+03 KL: 3.56e+02 MMD: 1.61e+00\n",
      "config 21, alpha = 0.0, lambda = 5.7, dropout = 0.00; 2 hidden layers with 36, 35 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.83e+03 logL: -7.36e+03 KL: 4.59e+02 MMD: 2.02e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.33e+03 logL: -3.07e+03 KL: 2.52e+02 MMD: 2.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.98e+03 logL: -1.76e+03 KL: 2.12e+02 MMD: 1.89e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.63e+03 logL: -1.47e+03 KL: 1.53e+02 MMD: 1.85e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.32e+03 logL: -1.18e+03 KL: 1.29e+02 MMD: 1.97e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.29e+03 logL: -1.17e+03 KL: 1.11e+02 MMD: 1.82e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.22e+03 logL: -1.11e+03 KL: 1.03e+02 MMD: 1.85e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.10e+03 logL: -9.95e+02 KL: 9.92e+01 MMD: 1.70e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.04e+03 logL: -9.37e+02 KL: 9.32e+01 MMD: 1.82e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.05e+03 logL: -9.57e+02 KL: 8.85e+01 MMD: 1.86e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.03e+03 logL: -9.43e+02 KL: 8.17e+01 MMD: 1.86e+00\n",
      "Epoch 00111: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 9.74e+02 logL: -8.84e+02 KL: 8.10e+01 MMD: 1.96e+00\n",
      "Stopping\n",
      "====> Epoch: 125 VALIDATION Loss: 9.73e+02 logL: -8.84e+02 KL: 7.98e+01 MMD: 2.02e+00\n",
      "config 22, alpha = 0.0, lambda = 36.3, dropout = 0.00; 2 hidden layers with 6, 5 nodes\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 10 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "Stopping\n",
      "====> Epoch: 10 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "config 22, alpha = 0.0, lambda = 889.4, dropout = 0.00; 2 hidden layers with 23, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.10e+04 logL: -9.64e+03 KL: 4.21e+02 MMD: 1.09e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.30e+03 logL: -8.07e+03 KL: 3.05e+02 MMD: 1.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.23e+03 logL: -7.14e+03 KL: 1.68e+02 MMD: 1.04e+00\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 8.22e+03 logL: -7.09e+03 KL: 1.36e+02 MMD: 1.11e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.09e+03 logL: -7.09e+03 KL: 1.31e+02 MMD: 9.78e-01\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 56 VALIDATION Loss: 8.12e+03 logL: -7.08e+03 KL: 1.29e+02 MMD: 1.02e+00\n",
      "config 22, alpha = 0.0, lambda = 1.4, dropout = 0.00; 2 hidden layers with 12, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.00e+04 logL: -9.70e+03 KL: 3.11e+02 MMD: 1.41e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.69e+03 logL: -9.55e+03 KL: 1.38e+02 MMD: 1.74e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.26e+03 logL: -7.14e+03 KL: 1.25e+02 MMD: 1.44e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.54e+03 logL: -5.42e+03 KL: 1.13e+02 MMD: 1.37e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.76e+03 logL: -3.64e+03 KL: 1.14e+02 MMD: 1.26e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.71e+03 logL: -3.62e+03 KL: 8.54e+01 MMD: 1.42e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.71e+03 logL: -3.64e+03 KL: 7.22e+01 MMD: 1.46e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.65e+03 logL: -3.59e+03 KL: 6.39e+01 MMD: 1.36e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.64e+03 logL: -3.58e+03 KL: 5.83e+01 MMD: 1.39e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.65e+03 logL: -3.59e+03 KL: 5.38e+01 MMD: 1.32e+00\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 3.63e+03 logL: -3.57e+03 KL: 5.25e+01 MMD: 1.45e+00\n",
      "Epoch 00110: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 112 VALIDATION Loss: 3.62e+03 logL: -3.57e+03 KL: 5.26e+01 MMD: 1.47e+00\n",
      "config 22, alpha = 0.0, lambda = 36768.0, dropout = 0.00; 2 hidden layers with 138, 103 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.87e+04 logL: -2.47e+03 KL: 3.91e+02 MMD: 1.79e+00\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 19 VALIDATION Loss: 6.14e+04 logL: -1.65e+03 KL: 3.03e+02 MMD: 1.62e+00\n",
      "config 22, alpha = 0.0, lambda = 197.4, dropout = 0.00; 2 hidden layers with 53, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.83e+04 logL: -1.57e+04 KL: 2.18e+03 MMD: 2.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.60e+04 logL: -1.48e+04 KL: 8.12e+02 MMD: 2.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.49e+04 logL: -1.41e+04 KL: 4.09e+02 MMD: 1.95e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.29e+04 logL: -1.22e+04 KL: 3.22e+02 MMD: 1.97e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.34e+03 logL: -7.66e+03 KL: 2.85e+02 MMD: 2.01e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.54e+03 logL: -5.90e+03 KL: 2.42e+02 MMD: 2.03e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.42e+03 logL: -4.84e+03 KL: 2.05e+02 MMD: 1.89e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.87e+03 logL: -3.32e+03 KL: 1.74e+02 MMD: 1.91e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.52e+03 logL: -3.01e+03 KL: 1.48e+02 MMD: 1.86e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.97e+03 logL: -2.44e+03 KL: 1.38e+02 MMD: 2.02e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.78e+03 logL: -2.32e+03 KL: 1.17e+02 MMD: 1.75e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.30e+03 logL: -1.85e+03 KL: 1.09e+02 MMD: 1.74e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.18e+03 logL: -1.72e+03 KL: 9.64e+01 MMD: 1.82e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.14e+03 logL: -1.68e+03 KL: 8.68e+01 MMD: 1.91e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.03e+03 logL: -1.61e+03 KL: 8.11e+01 MMD: 1.76e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 2.06e+03 logL: -1.63e+03 KL: 7.43e+01 MMD: 1.79e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 1.95e+03 logL: -1.54e+03 KL: 6.89e+01 MMD: 1.73e+00\n",
      "Epoch 00170: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 180 VALIDATION Loss: 1.91e+03 logL: -1.50e+03 KL: 6.65e+01 MMD: 1.74e+00\n",
      "Epoch 00182: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 186 VALIDATION Loss: 1.91e+03 logL: -1.50e+03 KL: 6.62e+01 MMD: 1.75e+00\n",
      "config 23, alpha = 0.0, lambda = 216.8, dropout = 0.00; 2 hidden layers with 139, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.61e+04 logL: -1.44e+04 KL: 1.57e+03 MMD: 8.21e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.42e+04 logL: -1.36e+04 KL: 4.51e+02 MMD: 7.50e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.18e+04 logL: -1.14e+04 KL: 2.63e+02 MMD: 6.96e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.02e+04 logL: -9.88e+03 KL: 1.74e+02 MMD: 6.93e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.87e+03 logL: -9.60e+03 KL: 1.17e+02 MMD: 6.98e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.79e+03 logL: -9.54e+03 KL: 8.73e+01 MMD: 7.61e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.74e+03 logL: -9.50e+03 KL: 7.01e+01 MMD: 7.69e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.74e+03 logL: -9.50e+03 KL: 5.93e+01 MMD: 7.94e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.64e+03 logL: -9.42e+03 KL: 5.21e+01 MMD: 7.60e-01\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 9.59e+03 logL: -9.38e+03 KL: 4.94e+01 MMD: 7.38e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 9.58e+03 logL: -9.38e+03 KL: 4.90e+01 MMD: 7.19e-01\n",
      "Epoch 00115: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 119 VALIDATION Loss: 9.57e+03 logL: -9.36e+03 KL: 4.88e+01 MMD: 7.56e-01\n",
      "config 23, alpha = 0.0, lambda = 4838.0, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.99e+04 logL: -1.41e+04 KL: 8.05e+02 MMD: 1.04e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.99e+04 logL: -1.38e+04 KL: 5.99e+02 MMD: 1.13e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.92e+04 logL: -1.37e+04 KL: 4.60e+02 MMD: 1.05e+00\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.92e+04 logL: -1.36e+04 KL: 3.90e+02 MMD: 1.08e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 50 VALIDATION Loss: 1.89e+04 logL: -1.36e+04 KL: 3.84e+02 MMD: 1.03e+00\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.96e+04 logL: -1.36e+04 KL: 3.78e+02 MMD: 1.16e+00\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 64 VALIDATION Loss: 1.89e+04 logL: -1.36e+04 KL: 3.77e+02 MMD: 1.02e+00\n",
      "config 23, alpha = 0.0, lambda = 11815.3, dropout = 0.00; 2 hidden layers with 126, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.71e+05 logL: -3.31e+05 KL: 3.25e+05 MMD: 1.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.48e+05 logL: -2.32e+04 KL: 1.10e+05 MMD: 1.25e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.96e+04 logL: -1.53e+04 KL: 3.94e+04 MMD: 1.25e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.95e+04 logL: -1.40e+04 KL: 9.82e+03 MMD: 1.32e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.16e+04 logL: -1.39e+04 KL: 3.33e+03 MMD: 1.22e+00\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.05e+04 logL: -1.35e+04 KL: 1.37e+03 MMD: 1.32e+00\n",
      "Stopping\n",
      "====> Epoch: 63 VALIDATION Loss: 3.08e+04 logL: -1.35e+04 KL: 1.32e+03 MMD: 1.35e+00\n",
      "config 23, alpha = 0.0, lambda = 10.4, dropout = 0.00; 2 hidden layers with 53, 42 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.02e+03 logL: -3.71e+03 KL: 2.96e+02 MMD: 1.68e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.06e+03 logL: -2.89e+03 KL: 1.58e+02 MMD: 1.60e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.38e+03 logL: -2.24e+03 KL: 1.27e+02 MMD: 1.53e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.92e+03 logL: -1.79e+03 KL: 1.07e+02 MMD: 1.68e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.84e+03 logL: -1.73e+03 KL: 9.33e+01 MMD: 1.60e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.75e+03 logL: -1.64e+03 KL: 8.62e+01 MMD: 1.72e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.72e+03 logL: -1.62e+03 KL: 7.93e+01 MMD: 1.70e+00\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.64e+03 logL: -1.55e+03 KL: 7.57e+01 MMD: 1.72e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.64e+03 logL: -1.55e+03 KL: 7.39e+01 MMD: 1.72e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.63e+03 logL: -1.54e+03 KL: 7.08e+01 MMD: 1.80e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.63e+03 logL: -1.54e+03 KL: 6.82e+01 MMD: 1.82e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.62e+03 logL: -1.54e+03 KL: 6.61e+01 MMD: 1.60e+00\n",
      "Stopping\n",
      "====> Epoch: 128 VALIDATION Loss: 1.62e+03 logL: -1.54e+03 KL: 6.51e+01 MMD: 1.67e+00\n",
      "config 23, alpha = 0.0, lambda = 79.7, dropout = 0.00; 2 hidden layers with 14, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.43e+04 logL: -1.36e+04 KL: 4.76e+02 MMD: 2.25e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.22e+03 logL: -7.72e+03 KL: 3.38e+02 MMD: 2.07e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.02e+03 logL: -5.67e+03 KL: 2.01e+02 MMD: 1.98e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.88e+03 logL: -5.57e+03 KL: 1.40e+02 MMD: 2.06e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.71e+03 logL: -5.44e+03 KL: 1.21e+02 MMD: 1.87e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.89e+03 logL: -3.61e+03 KL: 1.20e+02 MMD: 2.01e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.87e+03 logL: -3.62e+03 KL: 1.04e+02 MMD: 1.92e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.88e+03 logL: -3.61e+03 KL: 9.53e+01 MMD: 2.21e+00\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.83e+03 logL: -3.58e+03 KL: 9.06e+01 MMD: 2.02e+00\n",
      "Epoch 00095: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 95 VALIDATION Loss: 3.83e+03 logL: -3.58e+03 KL: 8.98e+01 MMD: 2.05e+00\n",
      "config 24, alpha = 0.0, lambda = 95.3, dropout = 0.00; 2 hidden layers with 18, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.41e+04 logL: -1.35e+04 KL: 4.84e+02 MMD: 7.53e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.01e+04 logL: -9.71e+03 KL: 2.80e+02 MMD: 7.43e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.77e+03 logL: -9.57e+03 KL: 1.33e+02 MMD: 6.85e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.70e+03 logL: -9.54e+03 KL: 8.55e+01 MMD: 7.56e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.67e+03 logL: -9.53e+03 KL: 6.60e+01 MMD: 7.67e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.62e+03 logL: -9.49e+03 KL: 5.71e+01 MMD: 7.67e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.59e+03 logL: -9.47e+03 KL: 5.17e+01 MMD: 7.45e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.54e+03 logL: -9.42e+03 KL: 4.82e+01 MMD: 7.09e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.51e+03 logL: -9.39e+03 KL: 4.56e+01 MMD: 7.44e-01\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 9.48e+03 logL: -9.37e+03 KL: 4.34e+01 MMD: 7.80e-01\n",
      "Stopping\n",
      "====> Epoch: 105 VALIDATION Loss: 9.48e+03 logL: -9.37e+03 KL: 4.33e+01 MMD: 7.36e-01\n",
      "config 24, alpha = 0.0, lambda = 9.8, dropout = 0.00; 2 hidden layers with 76, 26 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.37e+04 logL: -1.32e+04 KL: 5.29e+02 MMD: 9.65e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 7.85e+03 logL: -7.58e+03 KL: 2.64e+02 MMD: 1.03e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.66e+03 logL: -5.50e+03 KL: 1.58e+02 MMD: 1.04e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.34e+03 logL: -5.22e+03 KL: 1.10e+02 MMD: 1.04e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.23e+03 logL: -5.13e+03 KL: 8.76e+01 MMD: 1.08e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.13e+03 logL: -5.04e+03 KL: 7.56e+01 MMD: 1.02e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.07e+03 logL: -4.99e+03 KL: 6.87e+01 MMD: 1.01e+00\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 5.01e+03 logL: -4.93e+03 KL: 6.74e+01 MMD: 9.95e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 5.00e+03 logL: -4.92e+03 KL: 6.58e+01 MMD: 1.09e+00\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00098: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 100 VALIDATION Loss: 4.99e+03 logL: -4.91e+03 KL: 6.59e+01 MMD: 1.00e+00\n",
      "Stopping\n",
      "====> Epoch: 101 VALIDATION Loss: 4.99e+03 logL: -4.91e+03 KL: 6.59e+01 MMD: 1.07e+00\n",
      "config 24, alpha = 0.0, lambda = 4.8, dropout = 0.00; 2 hidden layers with 18, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.47e+05 logL: -3.47e+05 KL: 6.15e+02 MMD: 1.43e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.40e+05 logL: -3.40e+05 KL: 4.37e+02 MMD: 1.43e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.24e+05 logL: -3.24e+05 KL: 4.19e+02 MMD: 1.44e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.58e+05 logL: -2.57e+05 KL: 5.59e+02 MMD: 1.43e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.54e+04 logL: -6.46e+04 KL: 7.86e+02 MMD: 1.32e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.47e+04 logL: -1.41e+04 KL: 5.54e+02 MMD: 1.51e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.39e+04 logL: -1.35e+04 KL: 3.69e+02 MMD: 1.36e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.34e+04 logL: -1.32e+04 KL: 2.67e+02 MMD: 1.38e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.23e+04 logL: -1.21e+04 KL: 2.18e+02 MMD: 1.32e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.31e+03 logL: -9.09e+03 KL: 2.10e+02 MMD: 1.35e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 7.74e+03 logL: -7.57e+03 KL: 1.71e+02 MMD: 1.28e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.62e+03 logL: -5.46e+03 KL: 1.54e+02 MMD: 1.38e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 5.34e+03 logL: -5.21e+03 KL: 1.23e+02 MMD: 1.38e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.88e+03 logL: -3.76e+03 KL: 1.16e+02 MMD: 1.33e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 3.78e+03 logL: -3.66e+03 KL: 1.10e+02 MMD: 1.31e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 3.15e+03 logL: -3.03e+03 KL: 1.17e+02 MMD: 1.29e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 2.94e+03 logL: -2.84e+03 KL: 9.69e+01 MMD: 1.31e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 2.89e+03 logL: -2.80e+03 KL: 8.22e+01 MMD: 1.43e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 2.87e+03 logL: -2.79e+03 KL: 7.29e+01 MMD: 1.37e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 2.81e+03 logL: -2.74e+03 KL: 6.55e+01 MMD: 1.28e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 2.81e+03 logL: -2.74e+03 KL: 6.55e+01 MMD: 1.28e+00\n",
      "config 24, alpha = 0.0, lambda = 26368.8, dropout = 0.00; 2 hidden layers with 37, 24 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.22e+04 logL: -1.36e+04 KL: 5.85e+02 MMD: 1.82e+00\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 5.78e+04 logL: -1.36e+04 KL: 5.08e+02 MMD: 1.65e+00\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 24 VALIDATION Loss: 5.68e+04 logL: -1.36e+04 KL: 5.03e+02 MMD: 1.62e+00\n",
      "config 24, alpha = 0.0, lambda = 24360.2, dropout = 0.00; 2 hidden layers with 17, 16 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 5.75e+04 logL: -8.45e+03 KL: 7.71e+02 MMD: 1.98e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.89e+04 logL: -4.09e+03 KL: 4.02e+02 MMD: 1.82e+00\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.13e+04 logL: -3.74e+03 KL: 3.07e+02 MMD: 1.94e+00\n",
      "Stopping\n",
      "====> Epoch: 30 VALIDATION Loss: 5.13e+04 logL: -3.74e+03 KL: 3.07e+02 MMD: 1.94e+00\n",
      "config 25, alpha = 0.0, lambda = 500.2, dropout = 0.00; 2 hidden layers with 68, 9 nodes\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 10 VALIDATION Loss: 1.02e+26 logL: -4.21e+25 KL: 6.03e+25 MMD: 6.36e-01\n",
      "Stopping\n",
      "====> Epoch: 11 VALIDATION Loss: 6.82e+25 logL: -7.96e+24 KL: 6.03e+25 MMD: 6.37e-01\n",
      "config 25, alpha = 0.0, lambda = 10228.6, dropout = 0.00; 2 hidden layers with 169, 131 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.58e+04 logL: -1.34e+04 KL: 1.12e+03 MMD: 1.10e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.03e+04 logL: -9.50e+03 KL: 6.40e+02 MMD: 9.93e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.65e+04 logL: -5.57e+03 KL: 5.01e+02 MMD: 1.02e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.59e+04 logL: -5.40e+03 KL: 2.90e+02 MMD: 1.00e+00\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 44 VALIDATION Loss: 1.60e+04 logL: -5.35e+03 KL: 2.86e+02 MMD: 1.01e+00\n",
      "config 25, alpha = 0.0, lambda = 3223.6, dropout = 0.00; 2 hidden layers with 43, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.51e+04 logL: -9.92e+03 KL: 6.64e+02 MMD: 1.41e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.03e+04 logL: -5.81e+03 KL: 2.75e+02 MMD: 1.32e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.21e+03 logL: -2.95e+03 KL: 2.01e+02 MMD: 1.26e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.26e+03 logL: -2.82e+03 KL: 1.39e+02 MMD: 1.34e+00\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 47 VALIDATION Loss: 7.13e+03 logL: -2.78e+03 KL: 1.28e+02 MMD: 1.31e+00\n",
      "config 25, alpha = 0.0, lambda = 2870.1, dropout = 0.00; 2 hidden layers with 46, 26 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.78e+03 logL: -3.62e+03 KL: 2.82e+02 MMD: 1.70e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.97e+03 logL: -2.25e+03 KL: 1.83e+02 MMD: 1.58e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.78e+03 logL: -2.20e+03 KL: 1.37e+02 MMD: 1.55e+00\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 35 VALIDATION Loss: 6.93e+03 logL: -2.17e+03 KL: 1.31e+02 MMD: 1.61e+00\n",
      "config 25, alpha = 0.0, lambda = 2.5, dropout = 0.00; 2 hidden layers with 24, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.10e+04 logL: -1.05e+04 KL: 4.23e+02 MMD: 2.27e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.11e+03 logL: -5.92e+03 KL: 1.89e+02 MMD: 1.92e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.61e+03 logL: -5.47e+03 KL: 1.28e+02 MMD: 2.12e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.75e+03 logL: -3.63e+03 KL: 1.14e+02 MMD: 2.08e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.49e+03 logL: -3.38e+03 KL: 1.06e+02 MMD: 2.03e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.86e+03 logL: -2.76e+03 KL: 9.18e+01 MMD: 2.02e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.84e+03 logL: -2.76e+03 KL: 8.21e+01 MMD: 2.16e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.84e+03 logL: -2.76e+03 KL: 7.73e+01 MMD: 2.09e+00\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 2.80e+03 logL: -2.72e+03 KL: 7.46e+01 MMD: 2.17e+00\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 2.80e+03 logL: -2.72e+03 KL: 7.48e+01 MMD: 2.06e+00\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 103 VALIDATION Loss: 2.80e+03 logL: -2.72e+03 KL: 7.48e+01 MMD: 2.16e+00\n",
      "config 26, alpha = 0.0, lambda = 13.9, dropout = 0.00; 2 hidden layers with 17, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.43e+04 logL: -1.39e+04 KL: 3.66e+02 MMD: 8.41e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+04 logL: -1.36e+04 KL: 2.42e+02 MMD: 9.37e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.37e+04 logL: -1.36e+04 KL: 1.52e+02 MMD: 9.30e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.17e+02 MMD: 1.04e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 9.37e+01 MMD: 1.08e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 7.95e+01 MMD: 1.10e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 7.06e+01 MMD: 1.09e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.34e+04 logL: -1.33e+04 KL: 5.99e+01 MMD: 1.12e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.86e+03 logL: -9.77e+03 KL: 8.32e+01 MMD: 8.02e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 9.56e+03 logL: -9.49e+03 KL: 6.31e+01 MMD: 7.93e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 9.53e+03 logL: -9.47e+03 KL: 4.90e+01 MMD: 8.43e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 9.49e+03 logL: -9.44e+03 KL: 4.17e+01 MMD: 8.16e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 9.46e+03 logL: -9.41e+03 KL: 3.84e+01 MMD: 8.59e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 9.46e+03 logL: -9.42e+03 KL: 3.41e+01 MMD: 7.76e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 9.41e+03 logL: -9.37e+03 KL: 3.34e+01 MMD: 8.13e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 9.46e+03 logL: -9.41e+03 KL: 3.62e+01 MMD: 7.58e-01\n",
      "====> Epoch: 170 VALIDATION Loss: 9.28e+03 logL: -9.23e+03 KL: 3.88e+01 MMD: 7.19e-01\n",
      "====> Epoch: 180 VALIDATION Loss: 9.28e+03 logL: -9.23e+03 KL: 3.54e+01 MMD: 7.92e-01\n",
      "Epoch 00189: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 190 VALIDATION Loss: 8.98e+03 logL: -8.94e+03 KL: 3.28e+01 MMD: 7.82e-01\n",
      "====> Epoch: 200 VALIDATION Loss: 8.92e+03 logL: -8.87e+03 KL: 3.28e+01 MMD: 8.76e-01\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 8.92e+03 logL: -8.87e+03 KL: 3.28e+01 MMD: 8.76e-01\n",
      "config 26, alpha = 0.0, lambda = 10.8, dropout = 0.00; 2 hidden layers with 32, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.43e+04 logL: -1.36e+04 KL: 6.18e+02 MMD: 1.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.30e+04 logL: -1.27e+04 KL: 2.71e+02 MMD: 1.14e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.07e+03 logL: -7.88e+03 KL: 1.78e+02 MMD: 1.04e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.33e+03 logL: -7.21e+03 KL: 1.16e+02 MMD: 1.01e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.18e+03 logL: -7.09e+03 KL: 8.39e+01 MMD: 1.05e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.32e+03 logL: -5.22e+03 KL: 8.64e+01 MMD: 9.99e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 5.24e+03 logL: -5.15e+03 KL: 7.42e+01 MMD: 1.02e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.21e+03 logL: -5.13e+03 KL: 6.64e+01 MMD: 1.13e+00\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 5.18e+03 logL: -5.11e+03 KL: 6.26e+01 MMD: 1.06e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.18e+03 logL: -5.11e+03 KL: 6.11e+01 MMD: 1.09e+00\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 106 VALIDATION Loss: 5.18e+03 logL: -5.11e+03 KL: 6.07e+01 MMD: 1.10e+00\n",
      "config 26, alpha = 0.0, lambda = 17.6, dropout = 0.00; 2 hidden layers with 16, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.45e+04 logL: -1.37e+04 KL: 7.61e+02 MMD: 1.52e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+04 logL: -1.36e+04 KL: 3.15e+02 MMD: 1.71e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.89e+03 logL: -9.64e+03 KL: 2.20e+02 MMD: 1.57e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.67e+03 logL: -9.52e+03 KL: 1.30e+02 MMD: 1.69e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.27e+03 logL: -7.13e+03 KL: 1.14e+02 MMD: 1.59e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.22e+03 logL: -7.11e+03 KL: 9.02e+01 MMD: 1.55e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.27e+03 logL: -7.16e+03 KL: 7.91e+01 MMD: 1.64e+00\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 7.16e+03 logL: -7.06e+03 KL: 7.16e+01 MMD: 1.66e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 7.15e+03 logL: -7.06e+03 KL: 7.02e+01 MMD: 1.55e+00\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 7.15e+03 logL: -7.05e+03 KL: 6.95e+01 MMD: 1.83e+00\n",
      "Stopping\n",
      "====> Epoch: 100 VALIDATION Loss: 7.15e+03 logL: -7.05e+03 KL: 6.95e+01 MMD: 1.83e+00\n",
      "config 26, alpha = 0.0, lambda = 2.0, dropout = 0.00; 2 hidden layers with 49, 37 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.52e+04 logL: -1.37e+04 KL: 1.50e+03 MMD: 1.74e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.33e+04 logL: -1.28e+04 KL: 5.10e+02 MMD: 1.79e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.99e+03 logL: -9.67e+03 KL: 3.21e+02 MMD: 1.59e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 7.79e+03 logL: -7.57e+03 KL: 2.22e+02 MMD: 1.64e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.30e+03 logL: -6.13e+03 KL: 1.71e+02 MMD: 1.60e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.44e+03 logL: -4.29e+03 KL: 1.48e+02 MMD: 1.69e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.91e+03 logL: -3.78e+03 KL: 1.29e+02 MMD: 1.75e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.97e+03 logL: -2.84e+03 KL: 1.30e+02 MMD: 1.65e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.33e+03 logL: -2.21e+03 KL: 1.26e+02 MMD: 1.46e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.05e+03 logL: -1.93e+03 KL: 1.19e+02 MMD: 1.65e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.92e+03 logL: -1.80e+03 KL: 1.09e+02 MMD: 1.57e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.92e+03 logL: -1.81e+03 KL: 1.03e+02 MMD: 1.70e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.75e+03 logL: -1.65e+03 KL: 9.67e+01 MMD: 1.61e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.68e+03 logL: -1.59e+03 KL: 9.15e+01 MMD: 1.61e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.64e+03 logL: -1.55e+03 KL: 8.61e+01 MMD: 1.60e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.63e+03 logL: -1.55e+03 KL: 7.96e+01 MMD: 1.49e+00\n",
      "Epoch 00167: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 170 VALIDATION Loss: 1.58e+03 logL: -1.50e+03 KL: 7.61e+01 MMD: 1.58e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 1.58e+03 logL: -1.50e+03 KL: 7.45e+01 MMD: 1.65e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 1.57e+03 logL: -1.50e+03 KL: 7.33e+01 MMD: 1.68e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 1.57e+03 logL: -1.50e+03 KL: 7.17e+01 MMD: 1.74e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 1.57e+03 logL: -1.50e+03 KL: 7.17e+01 MMD: 1.74e+00\n",
      "config 26, alpha = 0.0, lambda = 49.7, dropout = 0.00; 2 hidden layers with 129, 54 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.48e+03 logL: -4.02e+03 KL: 3.71e+02 MMD: 1.84e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.08e+03 logL: -1.80e+03 KL: 1.93e+02 MMD: 1.94e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.61e+03 logL: -1.38e+03 KL: 1.46e+02 MMD: 1.84e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.23e+03 logL: -1.01e+03 KL: 1.27e+02 MMD: 2.02e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.10e+03 logL: -8.87e+02 KL: 1.12e+02 MMD: 2.06e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.07e+03 logL: -8.65e+02 KL: 1.06e+02 MMD: 2.00e+00\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.01e+03 logL: -8.16e+02 KL: 1.01e+02 MMD: 1.99e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.92e+02 logL: -8.11e+02 KL: 9.56e+01 MMD: 1.74e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.92e+02 logL: -8.12e+02 KL: 9.06e+01 MMD: 1.82e+00\n",
      "Stopping\n",
      "====> Epoch: 90 VALIDATION Loss: 9.92e+02 logL: -8.12e+02 KL: 9.06e+01 MMD: 1.82e+00\n",
      "config 27, alpha = 0.0, lambda = 14906.2, dropout = 0.00; 2 hidden layers with 110, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.50e+04 logL: -1.38e+04 KL: 3.04e+02 MMD: 7.29e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 2.49e+04 logL: -1.35e+04 KL: 3.16e+02 MMD: 7.46e-01\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 2.44e+04 logL: -1.34e+04 KL: 3.16e+02 MMD: 7.15e-01\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 35 VALIDATION Loss: 2.47e+04 logL: -1.34e+04 KL: 3.20e+02 MMD: 7.30e-01\n",
      "config 27, alpha = 0.0, lambda = 13883.5, dropout = 0.00; 2 hidden layers with 7, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.89e+04 logL: -1.37e+04 KL: 7.06e+02 MMD: 1.05e+00\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 2.87e+04 logL: -1.35e+04 KL: 5.32e+02 MMD: 1.05e+00\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 2.83e+04 logL: -1.35e+04 KL: 5.00e+02 MMD: 1.03e+00\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 35 VALIDATION Loss: 2.78e+04 logL: -1.35e+04 KL: 5.00e+02 MMD: 9.98e-01\n",
      "config 27, alpha = 0.0, lambda = 2.3, dropout = 0.00; 2 hidden layers with 113, 28 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.10e+03 logL: -2.81e+03 KL: 2.87e+02 MMD: 1.32e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.88e+03 logL: -2.73e+03 KL: 1.57e+02 MMD: 1.34e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.93e+03 logL: -2.81e+03 KL: 1.14e+02 MMD: 1.41e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.76e+03 logL: -2.67e+03 KL: 9.07e+01 MMD: 1.34e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.75e+03 logL: -2.67e+03 KL: 7.93e+01 MMD: 1.20e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.73e+03 logL: -2.66e+03 KL: 7.10e+01 MMD: 1.49e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.65e+03 logL: -2.58e+03 KL: 6.54e+01 MMD: 1.34e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.65e+03 logL: -2.59e+03 KL: 6.11e+01 MMD: 1.40e+00\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 2.58e+03 logL: -2.52e+03 KL: 5.91e+01 MMD: 1.30e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.57e+03 logL: -2.52e+03 KL: 5.76e+01 MMD: 1.42e+00\n",
      "Stopping\n",
      "====> Epoch: 107 VALIDATION Loss: 2.58e+03 logL: -2.52e+03 KL: 5.67e+01 MMD: 1.37e+00\n",
      "config 27, alpha = 0.0, lambda = 13.5, dropout = 0.00; 2 hidden layers with 27, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.40e+04 logL: -1.37e+04 KL: 2.41e+02 MMD: 1.76e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+04 logL: -1.37e+04 KL: 1.57e+02 MMD: 1.99e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.37e+04 logL: -1.36e+04 KL: 1.23e+02 MMD: 2.14e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.78e+03 logL: -9.60e+03 KL: 1.49e+02 MMD: 2.07e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.75e+03 logL: -9.61e+03 KL: 1.12e+02 MMD: 2.30e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.69e+03 logL: -9.57e+03 KL: 8.90e+01 MMD: 2.36e+00\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.62e+03 logL: -9.51e+03 KL: 7.56e+01 MMD: 2.36e+00\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.62e+03 logL: -9.51e+03 KL: 7.40e+01 MMD: 2.54e+00\n",
      "Stopping\n",
      "====> Epoch: 80 VALIDATION Loss: 9.62e+03 logL: -9.51e+03 KL: 7.40e+01 MMD: 2.54e+00\n",
      "config 27, alpha = 0.0, lambda = 27.8, dropout = 0.00; 2 hidden layers with 37, 30 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.47e+04 logL: -1.37e+04 KL: 9.45e+02 MMD: 2.31e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.40e+04 logL: -1.36e+04 KL: 3.56e+02 MMD: 2.17e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.31e+04 logL: -1.28e+04 KL: 2.29e+02 MMD: 1.98e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.97e+03 logL: -9.74e+03 KL: 1.77e+02 MMD: 1.90e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.11e+03 logL: -4.87e+03 KL: 1.81e+02 MMD: 2.00e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.30e+03 logL: -4.10e+03 KL: 1.52e+02 MMD: 1.96e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.55e+03 logL: -3.36e+03 KL: 1.39e+02 MMD: 1.93e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.39e+03 logL: -3.21e+03 KL: 1.27e+02 MMD: 2.02e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.68e+03 logL: -2.50e+03 KL: 1.25e+02 MMD: 1.95e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.05e+03 logL: -1.87e+03 KL: 1.23e+02 MMD: 2.02e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.51e+03 logL: -1.34e+03 KL: 1.16e+02 MMD: 1.89e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.42e+03 logL: -1.26e+03 KL: 1.11e+02 MMD: 1.90e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.20e+03 logL: -1.05e+03 KL: 1.05e+02 MMD: 1.86e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.09e+03 logL: -9.40e+02 KL: 9.74e+01 MMD: 1.92e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.05e+03 logL: -9.05e+02 KL: 9.01e+01 MMD: 1.96e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.03e+03 logL: -8.96e+02 KL: 8.09e+01 MMD: 1.94e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 9.90e+02 logL: -8.65e+02 KL: 7.65e+01 MMD: 1.81e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 9.69e+02 logL: -8.46e+02 KL: 7.41e+01 MMD: 1.86e+00\n",
      "Epoch 00182: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00189: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 190 VALIDATION Loss: 9.32e+02 logL: -8.10e+02 KL: 7.23e+01 MMD: 1.85e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 9.35e+02 logL: -8.10e+02 KL: 7.23e+01 MMD: 1.98e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 9.35e+02 logL: -8.10e+02 KL: 7.23e+01 MMD: 1.98e+00\n",
      "config 28, alpha = 0.0, lambda = 3405.9, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.75e+04 logL: -1.38e+04 KL: 1.33e+03 MMD: 6.92e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.69e+04 logL: -1.37e+04 KL: 5.70e+02 MMD: 7.55e-01\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 30 VALIDATION Loss: 1.68e+04 logL: -1.37e+04 KL: 4.95e+02 MMD: 7.70e-01\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 38 VALIDATION Loss: 1.66e+04 logL: -1.37e+04 KL: 4.80e+02 MMD: 7.31e-01\n",
      "config 28, alpha = 0.0, lambda = 212.9, dropout = 0.00; 2 hidden layers with 28, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.06e+04 logL: -1.00e+04 KL: 3.25e+02 MMD: 1.07e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.87e+03 logL: -5.49e+03 KL: 1.65e+02 MMD: 1.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.49e+03 logL: -5.16e+03 KL: 1.07e+02 MMD: 1.06e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.45e+03 logL: -5.12e+03 KL: 8.44e+01 MMD: 1.15e+00\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 5.40e+03 logL: -5.09e+03 KL: 8.07e+01 MMD: 1.09e+00\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 54 VALIDATION Loss: 5.39e+03 logL: -5.09e+03 KL: 8.07e+01 MMD: 1.05e+00\n",
      "config 28, alpha = 0.0, lambda = 5621.2, dropout = 0.00; 2 hidden layers with 32, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.48e+04 logL: -6.44e+03 KL: 3.94e+02 MMD: 1.42e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.25e+04 logL: -5.43e+03 KL: 2.26e+02 MMD: 1.22e+00\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.22e+04 logL: -4.14e+03 KL: 2.55e+02 MMD: 1.38e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.12e+04 logL: -3.66e+03 KL: 2.40e+02 MMD: 1.30e+00\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.10e+04 logL: -3.61e+03 KL: 2.09e+02 MMD: 1.29e+00\n",
      "Stopping\n",
      "====> Epoch: 52 VALIDATION Loss: 1.10e+04 logL: -3.61e+03 KL: 2.08e+02 MMD: 1.27e+00\n",
      "config 28, alpha = 0.0, lambda = 468.6, dropout = 0.00; 2 hidden layers with 55, 42 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.22e+04 logL: -1.39e+04 KL: 7.63e+03 MMD: 1.50e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.68e+04 logL: -1.35e+04 KL: 2.51e+03 MMD: 1.71e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.33e+04 logL: -1.14e+04 KL: 1.11e+03 MMD: 1.65e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.33e+03 logL: -8.00e+03 KL: 6.04e+02 MMD: 1.54e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.95e+03 logL: -5.79e+03 KL: 3.55e+02 MMD: 1.74e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.85e+03 logL: -4.83e+03 KL: 2.64e+02 MMD: 1.60e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.74e+03 logL: -3.77e+03 KL: 2.04e+02 MMD: 1.64e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.04e+03 logL: -3.12e+03 KL: 1.79e+02 MMD: 1.58e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.65e+03 logL: -2.72e+03 KL: 1.74e+02 MMD: 1.63e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.84e+03 logL: -1.91e+03 KL: 1.70e+02 MMD: 1.63e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.77e+03 logL: -1.83e+03 KL: 1.47e+02 MMD: 1.70e+00\n",
      "Epoch 00115: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 2.55e+03 logL: -1.72e+03 KL: 1.38e+02 MMD: 1.48e+00\n",
      "Epoch 00125: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 129 VALIDATION Loss: 2.58e+03 logL: -1.71e+03 KL: 1.37e+02 MMD: 1.57e+00\n",
      "config 28, alpha = 0.0, lambda = 782.1, dropout = 0.00; 2 hidden layers with 16, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.57e+04 logL: -1.38e+04 KL: 3.12e+02 MMD: 2.02e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.54e+04 logL: -1.36e+04 KL: 2.11e+02 MMD: 2.05e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.51e+04 logL: -1.35e+04 KL: 1.44e+02 MMD: 1.86e+00\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.50e+04 logL: -1.35e+04 KL: 1.03e+02 MMD: 1.88e+00\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.50e+04 logL: -1.35e+04 KL: 9.81e+01 MMD: 1.90e+00\n",
      "Stopping\n",
      "====> Epoch: 51 VALIDATION Loss: 1.51e+04 logL: -1.35e+04 KL: 9.81e+01 MMD: 1.97e+00\n",
      "config 29, alpha = 0.0, lambda = 30.1, dropout = 0.00; 2 hidden layers with 10, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.35e+04 KL: 6.24e+02 MMD: 8.18e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 2.31e+02 MMD: 8.54e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.26e+04 logL: -1.24e+04 KL: 1.52e+02 MMD: 7.50e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.66e+03 logL: -9.53e+03 KL: 1.12e+02 MMD: 7.25e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.65e+03 logL: -9.55e+03 KL: 7.80e+01 MMD: 7.68e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.59e+03 logL: -9.51e+03 KL: 6.08e+01 MMD: 8.05e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.58e+03 logL: -9.50e+03 KL: 5.07e+01 MMD: 7.44e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.56e+03 logL: -9.49e+03 KL: 4.66e+01 MMD: 8.26e-01\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 85 VALIDATION Loss: 9.54e+03 logL: -9.47e+03 KL: 4.57e+01 MMD: 7.65e-01\n",
      "config 29, alpha = 0.0, lambda = 114.0, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.36e+04 KL: 5.24e+02 MMD: 1.07e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.06e+04 logL: -1.01e+04 KL: 3.26e+02 MMD: 1.03e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.87e+03 logL: -9.57e+03 KL: 1.71e+02 MMD: 1.11e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.80e+03 logL: -9.55e+03 KL: 1.16e+02 MMD: 1.21e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.77e+03 logL: -9.54e+03 KL: 9.11e+01 MMD: 1.21e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.75e+03 logL: -9.53e+03 KL: 7.83e+01 MMD: 1.29e+00\n",
      "Stopping\n",
      "====> Epoch: 63 VALIDATION Loss: 9.74e+03 logL: -9.52e+03 KL: 7.54e+01 MMD: 1.23e+00\n",
      "config 29, alpha = 0.0, lambda = 5.5, dropout = 0.00; 2 hidden layers with 95, 37 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.90e+03 logL: -5.61e+03 KL: 2.89e+02 MMD: 1.23e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.65e+03 logL: -3.46e+03 KL: 1.88e+02 MMD: 1.25e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.95e+03 logL: -2.80e+03 KL: 1.42e+02 MMD: 1.23e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.90e+03 logL: -2.78e+03 KL: 1.11e+02 MMD: 1.29e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.81e+03 logL: -2.71e+03 KL: 9.30e+01 MMD: 1.36e+00\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.75e+03 logL: -2.66e+03 KL: 8.59e+01 MMD: 1.50e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.74e+03 logL: -2.65e+03 KL: 8.18e+01 MMD: 1.45e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.73e+03 logL: -2.65e+03 KL: 7.68e+01 MMD: 1.41e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.72e+03 logL: -2.64e+03 KL: 7.37e+01 MMD: 1.40e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.72e+03 logL: -2.64e+03 KL: 7.07e+01 MMD: 1.40e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.70e+03 logL: -2.63e+03 KL: 6.82e+01 MMD: 1.35e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.70e+03 logL: -2.62e+03 KL: 6.68e+01 MMD: 1.37e+00\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00127: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 130 VALIDATION Loss: 2.69e+03 logL: -2.62e+03 KL: 6.62e+01 MMD: 1.40e+00\n",
      "Stopping\n",
      "====> Epoch: 131 VALIDATION Loss: 2.69e+03 logL: -2.62e+03 KL: 6.62e+01 MMD: 1.38e+00\n",
      "config 29, alpha = 0.0, lambda = 4340.9, dropout = 0.00; 2 hidden layers with 84, 39 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.69e+04 logL: -9.01e+03 KL: 4.87e+02 MMD: 1.71e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.13e+04 logL: -3.89e+03 KL: 2.96e+02 MMD: 1.63e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.89e+03 logL: -2.62e+03 KL: 2.17e+02 MMD: 1.62e+00\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 9.39e+03 logL: -2.35e+03 KL: 1.82e+02 MMD: 1.58e+00\n",
      "Stopping\n",
      "====> Epoch: 42 VALIDATION Loss: 9.38e+03 logL: -2.35e+03 KL: 1.81e+02 MMD: 1.58e+00\n",
      "config 29, alpha = 0.0, lambda = 1.9, dropout = 0.00; 2 hidden layers with 46, 34 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.27e+03 logL: -2.95e+03 KL: 3.24e+02 MMD: 1.95e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.33e+03 logL: -2.15e+03 KL: 1.81e+02 MMD: 1.92e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.86e+03 logL: -1.72e+03 KL: 1.44e+02 MMD: 1.92e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.33e+03 logL: -1.21e+03 KL: 1.23e+02 MMD: 2.00e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.25e+03 logL: -1.14e+03 KL: 1.06e+02 MMD: 1.97e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.27e+03 logL: -1.17e+03 KL: 9.73e+01 MMD: 1.94e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.19e+03 logL: -1.10e+03 KL: 8.81e+01 MMD: 2.04e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.64e+02 logL: -8.74e+02 KL: 8.75e+01 MMD: 1.94e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 90 VALIDATION Loss: 9.54e+02 logL: -8.71e+02 KL: 8.10e+01 MMD: 1.83e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.37e+02 logL: -8.59e+02 KL: 7.61e+01 MMD: 2.02e+00\n",
      "Epoch 00107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 9.23e+02 logL: -8.49e+02 KL: 7.30e+01 MMD: 1.99e+00\n",
      "Stopping\n",
      "====> Epoch: 119 VALIDATION Loss: 9.24e+02 logL: -8.50e+02 KL: 7.19e+01 MMD: 1.94e+00\n",
      "config 30, alpha = 0.0, lambda = 73.6, dropout = 0.00; 2 hidden layers with 5, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.46e+04 logL: -1.36e+04 KL: 9.21e+02 MMD: 7.94e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.40e+04 logL: -1.36e+04 KL: 3.51e+02 MMD: 8.40e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 2.06e+02 MMD: 9.42e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 1.51e+02 MMD: 9.15e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 1.15e+02 MMD: 1.02e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 9.31e+01 MMD: 9.97e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 8.02e+01 MMD: 1.05e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.36e+04 logL: -1.34e+04 KL: 7.38e+01 MMD: 1.04e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 6.65e+01 MMD: 1.12e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 6.13e+01 MMD: 1.07e+00\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.35e+04 logL: -1.33e+04 KL: 5.75e+01 MMD: 1.09e+00\n",
      "Stopping\n",
      "====> Epoch: 113 VALIDATION Loss: 1.35e+04 logL: -1.33e+04 KL: 5.72e+01 MMD: 1.14e+00\n",
      "config 30, alpha = 0.0, lambda = 10.4, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.41e+04 logL: -1.35e+04 KL: 5.00e+02 MMD: 1.45e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 1.92e+02 MMD: 1.74e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.10e+02 MMD: 2.00e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.35e+04 logL: -1.35e+04 KL: 7.93e+01 MMD: 2.09e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 6.24e+01 MMD: 2.32e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 5.36e+01 MMD: 2.37e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.34e+04 logL: -1.33e+04 KL: 4.94e+01 MMD: 1.84e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.65e+03 logL: -9.55e+03 KL: 8.26e+01 MMD: 1.50e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.60e+03 logL: -9.53e+03 KL: 6.13e+01 MMD: 1.49e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.59e+03 logL: -9.52e+03 KL: 5.13e+01 MMD: 1.45e+00\n",
      "Stopping\n",
      "====> Epoch: 104 VALIDATION Loss: 9.59e+03 logL: -9.52e+03 KL: 4.90e+01 MMD: 1.45e+00\n",
      "config 30, alpha = 0.0, lambda = 1331.8, dropout = 0.00; 2 hidden layers with 8, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.61e+04 logL: -1.36e+04 KL: 5.95e+02 MMD: 1.44e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.20e+04 logL: -9.77e+03 KL: 3.32e+02 MMD: 1.41e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.15e+04 logL: -9.56e+03 KL: 2.12e+02 MMD: 1.29e+00\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.14e+04 logL: -9.50e+03 KL: 1.87e+02 MMD: 1.29e+00\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.16e+04 logL: -9.49e+03 KL: 1.84e+02 MMD: 1.43e+00\n",
      "Stopping\n",
      "====> Epoch: 50 VALIDATION Loss: 1.16e+04 logL: -9.49e+03 KL: 1.84e+02 MMD: 1.43e+00\n",
      "config 30, alpha = 0.0, lambda = 507.3, dropout = 0.00; 2 hidden layers with 18, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.08e+04 logL: -9.64e+03 KL: 3.55e+02 MMD: 1.65e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.60e+03 logL: -5.55e+03 KL: 2.16e+02 MMD: 1.64e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.43e+03 logL: -4.37e+03 KL: 1.59e+02 MMD: 1.78e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.83e+03 logL: -2.88e+03 KL: 1.44e+02 MMD: 1.59e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.74e+03 logL: -2.78e+03 KL: 1.12e+02 MMD: 1.66e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.65e+03 logL: -2.78e+03 KL: 9.60e+01 MMD: 1.54e+00\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.67e+03 logL: -2.75e+03 KL: 8.65e+01 MMD: 1.66e+00\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.67e+03 logL: -2.75e+03 KL: 8.36e+01 MMD: 1.66e+00\n",
      "Stopping\n",
      "====> Epoch: 82 VALIDATION Loss: 3.63e+03 logL: -2.75e+03 KL: 8.36e+01 MMD: 1.58e+00\n",
      "config 30, alpha = 0.0, lambda = 285.9, dropout = 0.00; 2 hidden layers with 108, 75 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.80e+03 logL: -6.88e+03 KL: 4.02e+02 MMD: 1.81e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.69e+03 logL: -2.89e+03 KL: 2.66e+02 MMD: 1.88e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.15e+03 logL: -1.37e+03 KL: 2.18e+02 MMD: 2.00e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.64e+03 logL: -9.41e+02 KL: 1.71e+02 MMD: 1.85e+00\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.58e+03 logL: -8.97e+02 KL: 1.42e+02 MMD: 1.89e+00\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.58e+03 logL: -8.91e+02 KL: 1.38e+02 MMD: 1.94e+00\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 69 VALIDATION Loss: 1.57e+03 logL: -8.90e+02 KL: 1.37e+02 MMD: 1.89e+00\n",
      "config 31, alpha = 0.0, lambda = 12699.5, dropout = 0.00; 2 hidden layers with 12, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.44e+04 logL: -1.48e+04 KL: 1.28e+03 MMD: 6.53e-01\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 2.34e+04 logL: -1.38e+04 KL: 8.93e+02 MMD: 6.82e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.28e+04 logL: -1.37e+04 KL: 8.13e+02 MMD: 6.48e-01\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.32e+04 logL: -1.37e+04 KL: 7.51e+02 MMD: 6.87e-01\n",
      "Stopping\n",
      "====> Epoch: 40 VALIDATION Loss: 2.32e+04 logL: -1.37e+04 KL: 7.51e+02 MMD: 6.87e-01\n",
      "config 31, alpha = 0.0, lambda = 4410.6, dropout = 0.00; 2 hidden layers with 108, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.52e+05 logL: -3.51e+05 KL: 1.54e+00 MMD: 8.54e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 3.50e+05 logL: -3.50e+05 KL: 1.38e+00 MMD: 5.00e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 3.47e+05 logL: -3.47e+05 KL: 1.63e+00 MMD: 3.53e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 3.44e+05 logL: -3.44e+05 KL: 1.27e+00 MMD: 4.57e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 3.38e+05 logL: -3.38e+05 KL: 7.22e-01 MMD: 2.45e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 3.30e+05 logL: -3.30e+05 KL: 6.66e-01 MMD: 2.15e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 3.19e+05 logL: -3.19e+05 KL: 6.36e-01 MMD: 1.41e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 3.06e+05 logL: -3.06e+05 KL: 6.19e-01 MMD: 2.03e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 2.90e+05 logL: -2.90e+05 KL: 6.13e-01 MMD: 1.70e-02\n",
      "====> Epoch: 100 VALIDATION Loss: 2.72e+05 logL: -2.72e+05 KL: 8.78e-01 MMD: 2.56e-02\n",
      "====> Epoch: 110 VALIDATION Loss: 1.09e+05 logL: -1.04e+05 KL: 4.43e+02 MMD: 1.08e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.44e+04 logL: -1.92e+04 KL: 7.07e+02 MMD: 1.01e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.13e+04 logL: -1.67e+04 KL: 5.52e+02 MMD: 9.17e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 2.02e+04 logL: -1.50e+04 KL: 3.81e+02 MMD: 1.09e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.88e+04 logL: -1.37e+04 KL: 2.85e+02 MMD: 1.10e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.78e+04 logL: -1.29e+04 KL: 2.95e+02 MMD: 1.05e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 1.51e+04 logL: -1.03e+04 KL: 2.21e+02 MMD: 1.03e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 1.39e+04 logL: -9.20e+03 KL: 2.43e+02 MMD: 1.01e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 1.25e+04 logL: -7.51e+03 KL: 2.12e+02 MMD: 1.07e+00\n",
      "Epoch 00195: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 200 VALIDATION Loss: 1.23e+04 logL: -7.34e+03 KL: 1.89e+02 MMD: 1.07e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 1.23e+04 logL: -7.34e+03 KL: 1.89e+02 MMD: 1.07e+00\n",
      "config 31, alpha = 0.0, lambda = 269.5, dropout = 0.00; 2 hidden layers with 85, 53 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.16e+03 logL: -5.52e+03 KL: 2.78e+02 MMD: 1.34e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.52e+03 logL: -2.97e+03 KL: 1.84e+02 MMD: 1.35e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.26e+03 logL: -2.79e+03 KL: 1.37e+02 MMD: 1.25e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.19e+03 logL: -2.73e+03 KL: 1.15e+02 MMD: 1.29e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.15e+03 logL: -2.69e+03 KL: 1.03e+02 MMD: 1.33e+00\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 57 VALIDATION Loss: 3.13e+03 logL: -2.68e+03 KL: 1.01e+02 MMD: 1.28e+00\n",
      "config 31, alpha = 0.0, lambda = 41.0, dropout = 0.00; 2 hidden layers with 103, 29 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.27e+03 logL: -2.93e+03 KL: 2.73e+02 MMD: 1.69e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.86e+03 logL: -1.63e+03 KL: 1.70e+02 MMD: 1.71e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.76e+03 logL: -1.57e+03 KL: 1.23e+02 MMD: 1.66e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.75e+03 logL: -1.58e+03 KL: 1.02e+02 MMD: 1.72e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.73e+03 logL: -1.58e+03 KL: 8.63e+01 MMD: 1.64e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.69e+03 logL: -1.54e+03 KL: 7.77e+01 MMD: 1.65e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.69e+03 logL: -1.56e+03 KL: 6.87e+01 MMD: 1.66e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.64e+03 logL: -1.51e+03 KL: 6.62e+01 MMD: 1.57e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.63e+03 logL: -1.51e+03 KL: 6.20e+01 MMD: 1.53e+00\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.60e+03 logL: -1.47e+03 KL: 5.91e+01 MMD: 1.60e+00\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 104 VALIDATION Loss: 1.60e+03 logL: -1.47e+03 KL: 5.91e+01 MMD: 1.68e+00\n",
      "config 31, alpha = 0.0, lambda = 12.9, dropout = 0.00; 2 hidden layers with 18, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.71e+04 logL: -1.56e+04 KL: 1.57e+03 MMD: 1.96e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.50e+04 logL: -1.44e+04 KL: 5.06e+02 MMD: 1.88e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.65e+03 logL: -9.25e+03 KL: 3.77e+02 MMD: 1.95e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.33e+03 logL: -8.04e+03 KL: 2.62e+02 MMD: 1.89e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.32e+03 logL: -6.05e+03 KL: 2.51e+02 MMD: 1.87e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.81e+03 logL: -5.59e+03 KL: 1.95e+02 MMD: 1.86e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.66e+03 logL: -4.46e+03 KL: 1.78e+02 MMD: 1.84e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.07e+03 logL: -3.88e+03 KL: 1.71e+02 MMD: 1.79e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.37e+03 logL: -3.21e+03 KL: 1.45e+02 MMD: 1.76e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.09e+03 logL: -2.94e+03 KL: 1.23e+02 MMD: 1.89e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.83e+03 logL: -2.69e+03 KL: 1.12e+02 MMD: 1.90e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.36e+03 logL: -2.23e+03 KL: 1.07e+02 MMD: 1.78e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.28e+03 logL: -2.16e+03 KL: 9.68e+01 MMD: 1.88e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.26e+03 logL: -2.15e+03 KL: 8.83e+01 MMD: 1.85e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.22e+03 logL: -2.12e+03 KL: 8.12e+01 MMD: 2.08e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.79e+03 logL: -1.68e+03 KL: 8.32e+01 MMD: 1.94e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 1.68e+03 logL: -1.58e+03 KL: 7.74e+01 MMD: 1.84e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 1.67e+03 logL: -1.57e+03 KL: 7.46e+01 MMD: 1.86e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 1.65e+03 logL: -1.56e+03 KL: 7.17e+01 MMD: 1.87e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 1.65e+03 logL: -1.55e+03 KL: 6.99e+01 MMD: 1.88e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 1.65e+03 logL: -1.55e+03 KL: 6.99e+01 MMD: 1.88e+00\n",
      "config 32, alpha = 0.0, lambda = 8.3, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.50e+04 logL: -1.36e+04 KL: 1.40e+03 MMD: 7.37e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.40e+04 logL: -1.35e+04 KL: 4.61e+02 MMD: 8.21e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 2.12e+02 MMD: 8.68e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.13e+02 MMD: 9.81e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 7.18e+01 MMD: 1.07e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 5.64e+01 MMD: 1.08e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.34e+04 logL: -1.34e+04 KL: 4.37e+01 MMD: 1.21e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.34e+04 logL: -1.34e+04 KL: 3.65e+01 MMD: 1.23e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.34e+04 logL: -1.34e+04 KL: 3.17e+01 MMD: 1.29e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.34e+04 logL: -1.34e+04 KL: 2.87e+01 MMD: 1.33e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.34e+04 logL: -1.33e+04 KL: 2.63e+01 MMD: 1.32e+00\n",
      "Epoch 00111: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.34e+04 logL: -1.33e+04 KL: 2.58e+01 MMD: 1.29e+00\n",
      "Epoch 00122: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 123 VALIDATION Loss: 1.34e+04 logL: -1.33e+04 KL: 2.57e+01 MMD: 1.32e+00\n",
      "config 32, alpha = 0.0, lambda = 4.1, dropout = 0.00; 2 hidden layers with 29, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.02e+04 logL: -9.85e+03 KL: 3.15e+02 MMD: 1.04e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.63e+03 logL: -5.43e+03 KL: 1.97e+02 MMD: 1.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.42e+03 logL: -5.28e+03 KL: 1.37e+02 MMD: 9.79e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 5.34e+03 logL: -5.22e+03 KL: 1.08e+02 MMD: 9.94e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 5.16e+03 logL: -5.06e+03 KL: 9.51e+01 MMD: 1.08e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.17e+03 logL: -5.08e+03 KL: 8.51e+01 MMD: 1.10e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.04e+03 logL: -4.96e+03 KL: 7.72e+01 MMD: 1.02e+00\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 5.00e+03 logL: -4.92e+03 KL: 7.27e+01 MMD: 1.05e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.00e+03 logL: -4.92e+03 KL: 7.08e+01 MMD: 1.02e+00\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 5.01e+03 logL: -4.93e+03 KL: 7.17e+01 MMD: 1.06e+00\n",
      "config 32, alpha = 0.0, lambda = 3.4, dropout = 0.00; 2 hidden layers with 61, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.41e+04 logL: -1.37e+04 KL: 3.98e+02 MMD: 1.60e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.92e+03 logL: -8.67e+03 KL: 2.47e+02 MMD: 1.36e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.66e+03 logL: -5.49e+03 KL: 1.61e+02 MMD: 1.46e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.11e+03 logL: -2.97e+03 KL: 1.40e+02 MMD: 1.46e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.86e+03 logL: -2.75e+03 KL: 1.03e+02 MMD: 1.31e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.90e+03 logL: -2.81e+03 KL: 8.83e+01 MMD: 1.31e+00\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.79e+03 logL: -2.70e+03 KL: 8.61e+01 MMD: 1.23e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.78e+03 logL: -2.70e+03 KL: 8.30e+01 MMD: 1.42e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.78e+03 logL: -2.69e+03 KL: 7.99e+01 MMD: 1.43e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.77e+03 logL: -2.69e+03 KL: 7.74e+01 MMD: 1.37e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.77e+03 logL: -2.69e+03 KL: 7.49e+01 MMD: 1.40e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.76e+03 logL: -2.68e+03 KL: 7.31e+01 MMD: 1.28e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.75e+03 logL: -2.68e+03 KL: 7.11e+01 MMD: 1.35e+00\n",
      "Epoch 00136: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 140 VALIDATION Loss: 2.75e+03 logL: -2.67e+03 KL: 7.01e+01 MMD: 1.34e+00\n",
      "Epoch 00145: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 147 VALIDATION Loss: 2.74e+03 logL: -2.67e+03 KL: 7.01e+01 MMD: 1.27e+00\n",
      "config 32, alpha = 0.0, lambda = 28206.0, dropout = 0.00; 2 hidden layers with 17, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.35e+04 logL: -7.93e+03 KL: 6.68e+02 MMD: 1.59e+00\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 5.39e+04 logL: -5.60e+03 KL: 3.51e+02 MMD: 1.70e+00\n",
      "Stopping\n",
      "====> Epoch: 23 VALIDATION Loss: 5.03e+04 logL: -5.59e+03 KL: 3.43e+02 MMD: 1.57e+00\n",
      "config 32, alpha = 0.0, lambda = 24.2, dropout = 0.00; 2 hidden layers with 78, 53 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.95e+03 logL: -2.62e+03 KL: 2.84e+02 MMD: 2.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+03 logL: -1.17e+03 KL: 1.85e+02 MMD: 1.85e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.20e+03 logL: -1.01e+03 KL: 1.43e+02 MMD: 1.93e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.03e+03 logL: -8.68e+02 KL: 1.16e+02 MMD: 2.08e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.02e+03 logL: -8.76e+02 KL: 1.01e+02 MMD: 1.95e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.00e+03 logL: -8.61e+02 KL: 9.32e+01 MMD: 1.98e+00\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.53e+02 logL: -8.19e+02 KL: 9.00e+01 MMD: 1.86e+00\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.54e+02 logL: -8.19e+02 KL: 8.96e+01 MMD: 1.96e+00\n",
      "Stopping\n",
      "====> Epoch: 80 VALIDATION Loss: 9.54e+02 logL: -8.19e+02 KL: 8.96e+01 MMD: 1.96e+00\n",
      "config 33, alpha = 0.0, lambda = 6.1, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.36e+04 KL: 5.87e+02 MMD: 7.14e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.11e+04 logL: -1.06e+04 KL: 4.67e+02 MMD: 7.05e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.84e+03 logL: -9.58e+03 KL: 2.52e+02 MMD: 7.05e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.70e+03 logL: -9.55e+03 KL: 1.46e+02 MMD: 7.59e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.64e+03 logL: -9.54e+03 KL: 9.37e+01 MMD: 7.83e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.60e+03 logL: -9.53e+03 KL: 6.65e+01 MMD: 6.81e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.58e+03 logL: -9.52e+03 KL: 5.23e+01 MMD: 8.21e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.56e+03 logL: -9.51e+03 KL: 4.38e+01 MMD: 7.92e-01\n",
      "Stopping\n",
      "====> Epoch: 84 VALIDATION Loss: 9.58e+03 logL: -9.53e+03 KL: 4.16e+01 MMD: 7.86e-01\n",
      "config 33, alpha = 0.0, lambda = 228.8, dropout = 0.00; 2 hidden layers with 7, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.41e+04 logL: -1.34e+04 KL: 4.50e+02 MMD: 1.09e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.70e+03 logL: -9.18e+03 KL: 3.02e+02 MMD: 9.60e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 7.76e+03 logL: -7.35e+03 KL: 1.59e+02 MMD: 1.11e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.46e+03 logL: -7.12e+03 KL: 1.10e+02 MMD: 1.01e+00\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 7.42e+03 logL: -7.10e+03 KL: 8.87e+01 MMD: 1.03e+00\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 7.44e+03 logL: -7.10e+03 KL: 8.71e+01 MMD: 1.13e+00\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: 7.43e+03 logL: -7.10e+03 KL: 8.68e+01 MMD: 1.09e+00\n",
      "config 33, alpha = 0.0, lambda = 352.1, dropout = 0.00; 2 hidden layers with 24, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.48e+04 logL: -1.80e+04 KL: 6.27e+03 MMD: 1.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.91e+04 logL: -1.63e+04 KL: 2.29e+03 MMD: 1.31e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.73e+04 logL: -1.57e+04 KL: 1.08e+03 MMD: 1.30e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.58e+04 logL: -1.48e+04 KL: 5.50e+02 MMD: 1.33e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.45e+04 logL: -1.37e+04 KL: 3.31e+02 MMD: 1.35e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.14e+04 logL: -1.07e+04 KL: 2.99e+02 MMD: 1.32e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.72e+03 logL: -8.98e+03 KL: 2.75e+02 MMD: 1.33e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 7.69e+03 logL: -6.98e+03 KL: 2.71e+02 MMD: 1.23e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.68e+03 logL: -5.99e+03 KL: 2.23e+02 MMD: 1.34e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.33e+03 logL: -4.69e+03 KL: 1.99e+02 MMD: 1.24e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 4.51e+03 logL: -3.91e+03 KL: 1.64e+02 MMD: 1.26e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.37e+03 logL: -3.76e+03 KL: 1.31e+02 MMD: 1.36e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 4.30e+03 logL: -3.69e+03 KL: 1.06e+02 MMD: 1.43e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 4.22e+03 logL: -3.63e+03 KL: 9.26e+01 MMD: 1.42e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 4.14e+03 logL: -3.58e+03 KL: 8.23e+01 MMD: 1.35e+00\n",
      "Epoch 00150: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 160 VALIDATION Loss: 4.12e+03 logL: -3.54e+03 KL: 8.13e+01 MMD: 1.41e+00\n",
      "Epoch 00162: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 166 VALIDATION Loss: 4.09e+03 logL: -3.54e+03 KL: 8.07e+01 MMD: 1.35e+00\n",
      "config 33, alpha = 0.0, lambda = 320.8, dropout = 0.00; 2 hidden layers with 39, 26 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.54e+04 logL: -1.40e+04 KL: 8.09e+02 MMD: 1.72e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.20e+04 logL: -1.11e+04 KL: 3.30e+02 MMD: 1.64e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.35e+03 logL: -7.60e+03 KL: 2.38e+02 MMD: 1.59e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.61e+03 logL: -4.88e+03 KL: 2.23e+02 MMD: 1.58e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.37e+03 logL: -3.68e+03 KL: 1.86e+02 MMD: 1.59e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.05e+03 logL: -2.37e+03 KL: 1.69e+02 MMD: 1.61e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.59e+03 logL: -1.87e+03 KL: 1.47e+02 MMD: 1.78e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.37e+03 logL: -1.75e+03 KL: 1.23e+02 MMD: 1.57e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.27e+03 logL: -1.62e+03 KL: 1.06e+02 MMD: 1.67e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.24e+03 logL: -1.62e+03 KL: 9.31e+01 MMD: 1.65e+00\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.13e+03 logL: -1.55e+03 KL: 8.68e+01 MMD: 1.53e+00\n",
      "Epoch 00118: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 2.15e+03 logL: -1.55e+03 KL: 8.38e+01 MMD: 1.60e+00\n",
      "Stopping\n",
      "====> Epoch: 122 VALIDATION Loss: 2.16e+03 logL: -1.55e+03 KL: 8.36e+01 MMD: 1.64e+00\n",
      "config 33, alpha = 0.0, lambda = 1.4, dropout = 0.00; 2 hidden layers with 199, 46 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.43e+04 logL: -1.36e+04 KL: 6.78e+02 MMD: 2.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.36e+04 logL: -1.34e+04 KL: 2.59e+02 MMD: 2.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.90e+03 logL: -7.62e+03 KL: 2.80e+02 MMD: 2.05e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.07e+03 logL: -3.80e+03 KL: 2.75e+02 MMD: 1.80e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.92e+03 logL: -2.69e+03 KL: 2.32e+02 MMD: 1.97e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.11e+03 logL: -1.92e+03 KL: 1.98e+02 MMD: 1.86e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.51e+03 logL: -1.33e+03 KL: 1.79e+02 MMD: 1.98e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.49e+03 logL: -1.32e+03 KL: 1.65e+02 MMD: 1.83e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.36e+03 logL: -1.21e+03 KL: 1.49e+02 MMD: 1.89e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.29e+03 logL: -1.15e+03 KL: 1.35e+02 MMD: 1.96e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.01e+03 logL: -8.87e+02 KL: 1.25e+02 MMD: 1.87e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.00e+03 logL: -8.87e+02 KL: 1.15e+02 MMD: 1.87e+00\n",
      "Epoch 00127: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 9.09e+02 logL: -8.01e+02 KL: 1.07e+02 MMD: 1.80e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 9.01e+02 logL: -7.98e+02 KL: 1.02e+02 MMD: 1.85e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 8.98e+02 logL: -7.99e+02 KL: 9.79e+01 MMD: 1.99e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 8.88e+02 logL: -7.94e+02 KL: 9.36e+01 MMD: 1.90e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 8.80e+02 logL: -7.90e+02 KL: 8.94e+01 MMD: 1.81e+00\n",
      "Epoch 00171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 180 VALIDATION Loss: 8.68e+02 logL: -7.80e+02 KL: 8.79e+01 MMD: 1.96e+00\n",
      "Stopping\n",
      "====> Epoch: 187 VALIDATION Loss: 8.68e+02 logL: -7.80e+02 KL: 8.72e+01 MMD: 1.85e+00\n",
      "config 34, alpha = 0.0, lambda = 11239.5, dropout = 0.00; 2 hidden layers with 101, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.80e+04 logL: -9.67e+03 KL: 4.62e+02 MMD: 6.98e-01\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 1.73e+04 logL: -9.52e+03 KL: 3.13e+02 MMD: 6.64e-01\n",
      "Stopping\n",
      "====> Epoch: 22 VALIDATION Loss: 1.86e+04 logL: -9.52e+03 KL: 3.08e+02 MMD: 7.78e-01\n",
      "config 34, alpha = 0.0, lambda = 24.7, dropout = 0.00; 2 hidden layers with 118, 100 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.33e+03 logL: -5.12e+03 KL: 1.90e+02 MMD: 1.02e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.35e+03 logL: -5.21e+03 KL: 1.17e+02 MMD: 1.09e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.94e+03 logL: -4.83e+03 KL: 8.77e+01 MMD: 1.03e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.70e+03 logL: -4.60e+03 KL: 7.71e+01 MMD: 1.08e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.71e+03 logL: -4.62e+03 KL: 6.68e+01 MMD: 1.09e+00\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.29e+03 logL: -4.20e+03 KL: 5.97e+01 MMD: 1.01e+00\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 4.23e+03 logL: -4.15e+03 KL: 5.74e+01 MMD: 1.10e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 80 VALIDATION Loss: 4.24e+03 logL: -4.15e+03 KL: 5.72e+01 MMD: 1.09e+00\n",
      "Stopping\n",
      "====> Epoch: 80 VALIDATION Loss: 4.24e+03 logL: -4.15e+03 KL: 5.72e+01 MMD: 1.09e+00\n",
      "config 34, alpha = 0.0, lambda = 26.4, dropout = 0.00; 2 hidden layers with 12, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.41e+04 logL: -1.37e+04 KL: 3.68e+02 MMD: 1.68e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.38e+04 logL: -1.36e+04 KL: 1.86e+02 MMD: 1.71e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 1.09e+02 MMD: 1.93e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 7.88e+01 MMD: 2.24e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 6.59e+01 MMD: 2.36e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 5.86e+01 MMD: 1.79e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 5.00e+01 MMD: 1.92e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.71e+03 logL: -9.58e+03 KL: 7.75e+01 MMD: 1.97e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.62e+03 logL: -9.50e+03 KL: 6.30e+01 MMD: 2.05e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 7.25e+03 logL: -7.13e+03 KL: 7.81e+01 MMD: 1.63e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 7.20e+03 logL: -7.10e+03 KL: 6.47e+01 MMD: 1.58e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 7.22e+03 logL: -7.12e+03 KL: 5.72e+01 MMD: 1.69e+00\n",
      "Stopping\n",
      "====> Epoch: 123 VALIDATION Loss: 7.19e+03 logL: -7.09e+03 KL: 5.54e+01 MMD: 1.68e+00\n",
      "config 34, alpha = 0.0, lambda = 62.0, dropout = 0.00; 2 hidden layers with 31, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.40e+04 logL: -1.36e+04 KL: 2.59e+02 MMD: 2.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.92e+03 logL: -9.61e+03 KL: 1.86e+02 MMD: 2.11e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.86e+03 logL: -9.59e+03 KL: 1.31e+02 MMD: 2.31e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.81e+03 logL: -9.56e+03 KL: 1.09e+02 MMD: 2.34e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.00e+03 logL: -7.76e+03 KL: 1.23e+02 MMD: 1.86e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.35e+03 logL: -7.12e+03 KL: 1.01e+02 MMD: 2.12e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.31e+03 logL: -7.10e+03 KL: 8.69e+01 MMD: 2.00e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.06e+03 logL: -5.85e+03 KL: 8.63e+01 MMD: 1.94e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.49e+03 logL: -5.30e+03 KL: 8.06e+01 MMD: 1.83e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.38e+03 logL: -5.20e+03 KL: 7.17e+01 MMD: 1.89e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.35e+03 logL: -5.18e+03 KL: 6.35e+01 MMD: 1.73e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.33e+03 logL: -5.16e+03 KL: 5.79e+01 MMD: 1.81e+00\n",
      "Epoch 00125: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 5.30e+03 logL: -5.14e+03 KL: 5.43e+01 MMD: 1.73e+00\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 136 VALIDATION Loss: 5.29e+03 logL: -5.14e+03 KL: 5.40e+01 MMD: 1.65e+00\n",
      "config 34, alpha = 0.0, lambda = 1056.6, dropout = 0.00; 2 hidden layers with 12, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.64e+04 logL: -1.36e+04 KL: 5.84e+02 MMD: 2.14e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.21e+04 logL: -9.65e+03 KL: 2.93e+02 MMD: 2.08e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.99e+03 logL: -7.73e+03 KL: 2.32e+02 MMD: 1.93e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.66e+03 logL: -5.33e+03 KL: 1.89e+02 MMD: 2.02e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.28e+03 logL: -5.18e+03 KL: 1.36e+02 MMD: 1.85e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.43e+03 logL: -5.18e+03 KL: 1.07e+02 MMD: 2.04e+00\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 7.27e+03 logL: -5.16e+03 KL: 8.81e+01 MMD: 1.91e+00\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 74 VALIDATION Loss: 7.21e+03 logL: -5.16e+03 KL: 8.72e+01 MMD: 1.86e+00\n",
      "config 35, alpha = 0.0, lambda = 368.0, dropout = 0.00; 2 hidden layers with 7, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.50e+04 logL: -1.36e+04 KL: 1.17e+03 MMD: 7.21e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.42e+04 logL: -1.35e+04 KL: 3.94e+02 MMD: 7.09e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.39e+04 logL: -1.35e+04 KL: 2.01e+02 MMD: 7.22e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.02e+04 logL: -9.70e+03 KL: 2.19e+02 MMD: 7.17e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.93e+03 logL: -9.55e+03 KL: 1.29e+02 MMD: 7.04e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.89e+03 logL: -9.53e+03 KL: 9.59e+01 MMD: 7.25e-01\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.88e+03 logL: -9.52e+03 KL: 8.88e+01 MMD: 7.28e-01\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 74 VALIDATION Loss: 9.87e+03 logL: -9.52e+03 KL: 8.78e+01 MMD: 7.16e-01\n",
      "config 35, alpha = 0.0, lambda = 2104.6, dropout = 0.00; 2 hidden layers with 15, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.95e+04 logL: -1.64e+04 KL: 9.10e+02 MMD: 1.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.77e+04 logL: -1.50e+04 KL: 5.35e+02 MMD: 1.03e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.48e+04 logL: -1.24e+04 KL: 2.99e+02 MMD: 9.99e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.24e+04 logL: -1.01e+04 KL: 1.98e+02 MMD: 9.87e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.92e+03 logL: -7.79e+03 KL: 1.57e+02 MMD: 9.37e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.48e+03 logL: -7.21e+03 KL: 1.30e+02 MMD: 1.02e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.43e+03 logL: -7.13e+03 KL: 1.11e+02 MMD: 1.04e+00\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 73 VALIDATION Loss: 9.39e+03 logL: -7.13e+03 KL: 1.08e+02 MMD: 1.03e+00\n",
      "config 35, alpha = 0.0, lambda = 5536.6, dropout = 0.00; 2 hidden layers with 11, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.12e+04 logL: -1.37e+04 KL: 4.47e+02 MMD: 1.28e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.83e+04 logL: -9.98e+03 KL: 3.56e+02 MMD: 1.44e+00\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.72e+04 logL: -9.57e+03 KL: 2.42e+02 MMD: 1.34e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.73e+04 logL: -9.56e+03 KL: 2.28e+02 MMD: 1.36e+00\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 42 VALIDATION Loss: 1.73e+04 logL: -9.56e+03 KL: 2.27e+02 MMD: 1.35e+00\n",
      "config 35, alpha = 0.0, lambda = 757.3, dropout = 0.00; 2 hidden layers with 14, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.22e+21 logL: -9.54e+19 KL: 1.13e+21 MMD: 1.54e+00\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 15 VALIDATION Loss: 1.26e+21 logL: -1.34e+20 KL: 1.13e+21 MMD: 1.58e+00\n",
      "config 35, alpha = 0.0, lambda = 10744.6, dropout = 0.00; 2 hidden layers with 12, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.60e+06 logL: -7.70e+05 KL: 5.81e+06 MMD: 1.95e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.43e+06 logL: -6.95e+05 KL: 1.71e+06 MMD: 1.91e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.08e+06 logL: -3.77e+05 KL: 6.83e+05 MMD: 1.82e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.70e+05 logL: -3.47e+05 KL: 3.03e+05 MMD: 1.91e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.84e+05 logL: -3.20e+05 KL: 1.44e+05 MMD: 1.89e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.81e+05 logL: -2.88e+05 KL: 7.03e+04 MMD: 2.10e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.78e+05 logL: -2.21e+05 KL: 3.63e+04 MMD: 1.85e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.22e+05 logL: -8.12e+04 KL: 2.05e+04 MMD: 1.92e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.35e+04 logL: -2.10e+04 KL: 1.18e+04 MMD: 1.92e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.58e+04 logL: -1.89e+04 KL: 7.19e+03 MMD: 1.83e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 4.31e+04 logL: -1.88e+04 KL: 4.65e+03 MMD: 1.83e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.25e+04 logL: -1.85e+04 KL: 3.02e+03 MMD: 1.95e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.94e+04 logL: -1.76e+04 KL: 2.01e+03 MMD: 1.84e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.70e+04 logL: -1.60e+04 KL: 1.40e+03 MMD: 1.83e+00\n",
      "Epoch 00146: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 3.74e+04 logL: -1.50e+04 KL: 1.11e+03 MMD: 1.98e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 3.55e+04 logL: -1.48e+04 KL: 1.04e+03 MMD: 1.83e+00\n",
      "Epoch 00161: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 170 VALIDATION Loss: 3.70e+04 logL: -1.47e+04 KL: 1.02e+03 MMD: 1.98e+00\n",
      "Epoch 00170: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping\n",
      "====> Epoch: 174 VALIDATION Loss: 3.53e+04 logL: -1.47e+04 KL: 1.02e+03 MMD: 1.83e+00\n",
      "config 36, alpha = 0.0, lambda = 2.6, dropout = 0.00; 2 hidden layers with 122, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.72e+03 logL: -9.43e+03 KL: 2.85e+02 MMD: 6.77e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.52e+03 logL: -9.37e+03 KL: 1.50e+02 MMD: 6.86e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.09e+03 logL: -8.97e+03 KL: 1.16e+02 MMD: 7.14e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 8.85e+03 logL: -8.76e+03 KL: 9.44e+01 MMD: 7.68e-01\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 8.54e+03 logL: -8.44e+03 KL: 9.17e+01 MMD: 7.57e-01\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 57 VALIDATION Loss: 8.51e+03 logL: -8.42e+03 KL: 8.95e+01 MMD: 7.51e-01\n",
      "config 36, alpha = 0.0, lambda = 1386.2, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.75e+04 logL: -1.55e+04 KL: 4.60e+02 MMD: 1.12e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.62e+04 logL: -1.41e+04 KL: 5.12e+02 MMD: 1.13e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.58e+04 logL: -1.37e+04 KL: 3.43e+02 MMD: 1.27e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.53e+04 logL: -1.36e+04 KL: 2.16e+02 MMD: 1.08e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.52e+04 logL: -1.35e+04 KL: 1.32e+02 MMD: 1.10e+00\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.51e+04 logL: -1.34e+04 KL: 1.31e+02 MMD: 1.07e+00\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 64 VALIDATION Loss: 1.51e+04 logL: -1.34e+04 KL: 1.30e+02 MMD: 1.07e+00\n",
      "config 36, alpha = 0.0, lambda = 3998.5, dropout = 0.00; 2 hidden layers with 46, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.59e+04 logL: -9.78e+03 KL: 5.29e+02 MMD: 1.41e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.09e+04 logL: -5.33e+03 KL: 3.13e+02 MMD: 1.31e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.05e+04 logL: -5.33e+03 KL: 2.05e+02 MMD: 1.24e+00\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.02e+04 logL: -5.03e+03 KL: 2.01e+02 MMD: 1.25e+00\n",
      "Stopping\n",
      "====> Epoch: 43 VALIDATION Loss: 1.11e+04 logL: -5.02e+03 KL: 2.01e+02 MMD: 1.47e+00\n",
      "config 36, alpha = 0.0, lambda = 2.1, dropout = 0.00; 2 hidden layers with 31, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.72e+03 logL: -7.33e+03 KL: 3.85e+02 MMD: 1.74e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.23e+03 logL: -7.08e+03 KL: 1.56e+02 MMD: 1.76e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.02e+03 logL: -3.87e+03 KL: 1.53e+02 MMD: 1.69e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.18e+03 logL: -3.06e+03 KL: 1.19e+02 MMD: 1.64e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.90e+03 logL: -2.80e+03 KL: 9.75e+01 MMD: 1.77e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.85e+03 logL: -2.76e+03 KL: 8.52e+01 MMD: 1.66e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.81e+03 logL: -2.73e+03 KL: 7.66e+01 MMD: 1.76e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.27e+03 logL: -2.20e+03 KL: 7.47e+01 MMD: 1.59e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.24e+03 logL: -2.17e+03 KL: 6.90e+01 MMD: 1.73e+00\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 2.22e+03 logL: -2.15e+03 KL: 6.61e+01 MMD: 1.82e+00\n",
      "Stopping\n",
      "====> Epoch: 107 VALIDATION Loss: 2.22e+03 logL: -2.15e+03 KL: 6.54e+01 MMD: 1.69e+00\n",
      "config 36, alpha = 0.0, lambda = 16208.2, dropout = 0.00; 2 hidden layers with 67, 31 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.85e+04 logL: -5.53e+03 KL: 5.19e+02 MMD: 2.00e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.47e+04 logL: -2.19e+03 KL: 3.27e+02 MMD: 1.99e+00\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.27e+04 logL: -1.81e+03 KL: 2.98e+02 MMD: 1.89e+00\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 3.30e+04 logL: -1.75e+03 KL: 2.86e+02 MMD: 1.91e+00\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 46 VALIDATION Loss: 3.17e+04 logL: -1.75e+03 KL: 2.85e+02 MMD: 1.83e+00\n",
      "config 37, alpha = 0.0, lambda = 132.6, dropout = 0.00; 2 hidden layers with 11, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.44e+04 logL: -1.39e+04 KL: 3.91e+02 MMD: 7.20e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+04 logL: -1.36e+04 KL: 2.62e+02 MMD: 7.89e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.16e+04 logL: -1.13e+04 KL: 2.23e+02 MMD: 6.85e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.79e+03 logL: -9.57e+03 KL: 1.28e+02 MMD: 7.14e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.72e+03 logL: -9.55e+03 KL: 7.90e+01 MMD: 7.07e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.70e+03 logL: -9.53e+03 KL: 5.61e+01 MMD: 8.14e-01\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.66e+03 logL: -9.51e+03 KL: 5.22e+01 MMD: 7.28e-01\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 73 VALIDATION Loss: 9.67e+03 logL: -9.51e+03 KL: 5.18e+01 MMD: 7.85e-01\n",
      "config 37, alpha = 0.0, lambda = 1.3, dropout = 0.00; 2 hidden layers with 55, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.12e+03 logL: -7.73e+03 KL: 3.98e+02 MMD: 1.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.56e+03 logL: -5.39e+03 KL: 1.74e+02 MMD: 1.08e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.34e+03 logL: -5.23e+03 KL: 1.12e+02 MMD: 1.07e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.27e+03 logL: -5.18e+03 KL: 8.71e+01 MMD: 1.02e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.12e+03 logL: -5.04e+03 KL: 7.83e+01 MMD: 1.16e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.08e+03 logL: -5.01e+03 KL: 7.32e+01 MMD: 1.08e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.02e+03 logL: -4.95e+03 KL: 6.99e+01 MMD: 1.05e+00\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 4.92e+03 logL: -4.85e+03 KL: 6.92e+01 MMD: 9.51e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 4.90e+03 logL: -4.83e+03 KL: 6.68e+01 MMD: 9.95e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 4.90e+03 logL: -4.83e+03 KL: 6.57e+01 MMD: 1.03e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 4.88e+03 logL: -4.81e+03 KL: 6.43e+01 MMD: 1.01e+00\n",
      "Stopping\n",
      "====> Epoch: 114 VALIDATION Loss: 4.88e+03 logL: -4.82e+03 KL: 6.41e+01 MMD: 1.05e+00\n",
      "config 37, alpha = 0.0, lambda = 318.4, dropout = 0.00; 2 hidden layers with 156, 41 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.92e+04 logL: -1.42e+04 KL: 4.55e+03 MMD: 1.24e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.55e+04 logL: -1.39e+04 KL: 1.17e+03 MMD: 1.26e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.46e+04 logL: -1.36e+04 KL: 5.25e+02 MMD: 1.32e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+04 logL: -1.28e+04 KL: 3.64e+02 MMD: 1.34e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.54e+03 logL: -7.76e+03 KL: 3.67e+02 MMD: 1.28e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.86e+03 logL: -7.20e+03 KL: 2.38e+02 MMD: 1.32e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.05e+03 logL: -4.32e+03 KL: 2.48e+02 MMD: 1.54e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.47e+03 logL: -3.88e+03 KL: 1.85e+02 MMD: 1.26e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.57e+03 logL: -2.98e+03 KL: 1.59e+02 MMD: 1.36e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.39e+03 logL: -2.85e+03 KL: 1.33e+02 MMD: 1.29e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.33e+03 logL: -2.78e+03 KL: 1.15e+02 MMD: 1.40e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.29e+03 logL: -2.74e+03 KL: 1.02e+02 MMD: 1.42e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.19e+03 logL: -2.69e+03 KL: 9.07e+01 MMD: 1.29e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.27e+03 logL: -2.76e+03 KL: 8.23e+01 MMD: 1.35e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 3.12e+03 logL: -2.66e+03 KL: 7.34e+01 MMD: 1.23e+00\n",
      "Epoch 00151: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 160 VALIDATION Loss: 3.08e+03 logL: -2.59e+03 KL: 6.73e+01 MMD: 1.31e+00\n",
      "Epoch 00169: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 170 VALIDATION Loss: 3.06e+03 logL: -2.59e+03 KL: 6.24e+01 MMD: 1.28e+00\n",
      "Epoch 00178: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 180 VALIDATION Loss: 3.02e+03 logL: -2.59e+03 KL: 6.18e+01 MMD: 1.18e+00\n",
      "Stopping\n",
      "====> Epoch: 182 VALIDATION Loss: 3.02e+03 logL: -2.59e+03 KL: 6.18e+01 MMD: 1.19e+00\n",
      "config 37, alpha = 0.0, lambda = 288.8, dropout = 0.00; 2 hidden layers with 161, 55 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.99e+03 logL: -3.23e+03 KL: 2.79e+02 MMD: 1.66e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 20 VALIDATION Loss: 2.44e+03 logL: -1.80e+03 KL: 1.63e+02 MMD: 1.66e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.22e+03 logL: -1.63e+03 KL: 1.21e+02 MMD: 1.63e+00\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.12e+03 logL: -1.54e+03 KL: 1.06e+02 MMD: 1.63e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.07e+03 logL: -1.54e+03 KL: 1.00e+02 MMD: 1.52e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.11e+03 logL: -1.53e+03 KL: 9.36e+01 MMD: 1.69e+00\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.07e+03 logL: -1.52e+03 KL: 9.23e+01 MMD: 1.56e+00\n",
      "Stopping\n",
      "====> Epoch: 72 VALIDATION Loss: 2.06e+03 logL: -1.52e+03 KL: 9.22e+01 MMD: 1.55e+00\n",
      "config 37, alpha = 0.0, lambda = 54567.5, dropout = 0.00; 2 hidden layers with 56, 45 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.07e+05 logL: -3.61e+03 KL: 4.52e+02 MMD: 1.88e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.36e+04 logL: -1.70e+03 KL: 2.74e+02 MMD: 1.68e+00\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.10e+05 logL: -1.65e+03 KL: 2.10e+02 MMD: 1.98e+00\n",
      "Stopping\n",
      "====> Epoch: 30 VALIDATION Loss: 1.10e+05 logL: -1.65e+03 KL: 2.10e+02 MMD: 1.98e+00\n",
      "config 38, alpha = 0.0, lambda = 37.9, dropout = 0.00; 2 hidden layers with 16, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.36e+04 KL: 5.93e+02 MMD: 7.77e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.38e+04 logL: -1.36e+04 KL: 2.10e+02 MMD: 9.30e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.11e+02 MMD: 9.30e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.72e+03 logL: -9.55e+03 KL: 1.41e+02 MMD: 7.50e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.65e+03 logL: -9.53e+03 KL: 8.68e+01 MMD: 7.57e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.61e+03 logL: -9.52e+03 KL: 6.38e+01 MMD: 7.54e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.60e+03 logL: -9.52e+03 KL: 5.13e+01 MMD: 7.96e-01\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.58e+03 logL: -9.50e+03 KL: 5.03e+01 MMD: 8.30e-01\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 82 VALIDATION Loss: 9.58e+03 logL: -9.50e+03 KL: 5.03e+01 MMD: 7.66e-01\n",
      "config 38, alpha = 0.0, lambda = 187.1, dropout = 0.00; 2 hidden layers with 116, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.81e+03 logL: -7.29e+03 KL: 3.33e+02 MMD: 1.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.43e+03 logL: -7.05e+03 KL: 1.74e+02 MMD: 1.12e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.44e+03 logL: -6.09e+03 KL: 1.33e+02 MMD: 1.16e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.37e+03 logL: -5.07e+03 KL: 1.07e+02 MMD: 1.05e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.25e+03 logL: -4.98e+03 KL: 9.18e+01 MMD: 9.56e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 5.29e+03 logL: -5.02e+03 KL: 7.86e+01 MMD: 1.05e+00\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.12e+03 logL: -4.85e+03 KL: 7.40e+01 MMD: 1.05e+00\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 5.12e+03 logL: -4.84e+03 KL: 7.28e+01 MMD: 1.08e+00\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 5.12e+03 logL: -4.84e+03 KL: 7.26e+01 MMD: 1.09e+00\n",
      "config 38, alpha = 0.0, lambda = 961.0, dropout = 0.00; 2 hidden layers with 28, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.56e+03 logL: -5.13e+03 KL: 2.82e+02 MMD: 1.20e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.45e+03 logL: -2.89e+03 KL: 1.81e+02 MMD: 1.44e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.31e+03 logL: -2.78e+03 KL: 1.19e+02 MMD: 1.47e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.09e+03 logL: -2.76e+03 KL: 9.64e+01 MMD: 1.29e+00\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 45 VALIDATION Loss: 4.08e+03 logL: -2.74e+03 KL: 9.42e+01 MMD: 1.29e+00\n",
      "config 38, alpha = 0.0, lambda = 36.2, dropout = 0.00; 2 hidden layers with 106, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.32e+03 logL: -2.99e+03 KL: 2.73e+02 MMD: 1.58e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.07e+03 logL: -1.85e+03 KL: 1.68e+02 MMD: 1.52e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.78e+03 logL: -1.60e+03 KL: 1.24e+02 MMD: 1.56e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.77e+03 logL: -1.61e+03 KL: 1.00e+02 MMD: 1.58e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.77e+03 logL: -1.63e+03 KL: 8.65e+01 MMD: 1.54e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.70e+03 logL: -1.57e+03 KL: 7.97e+01 MMD: 1.60e+00\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.66e+03 logL: -1.53e+03 KL: 7.36e+01 MMD: 1.65e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.66e+03 logL: -1.53e+03 KL: 7.18e+01 MMD: 1.62e+00\n",
      "Stopping\n",
      "====> Epoch: 87 VALIDATION Loss: 1.66e+03 logL: -1.53e+03 KL: 6.95e+01 MMD: 1.64e+00\n",
      "config 38, alpha = 0.0, lambda = 2865.9, dropout = 0.00; 2 hidden layers with 88, 64 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.65e+05 logL: -2.97e+05 KL: 6.28e+04 MMD: 1.89e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.78e+05 logL: -2.42e+05 KL: 3.08e+04 MMD: 1.82e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.95e+05 logL: -1.70e+05 KL: 1.85e+04 MMD: 2.03e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.68e+04 logL: -7.93e+04 KL: 1.19e+04 MMD: 1.97e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.95e+04 logL: -1.73e+04 KL: 7.14e+03 MMD: 1.78e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.17e+04 logL: -1.26e+04 KL: 3.67e+03 MMD: 1.89e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.92e+04 logL: -1.21e+04 KL: 1.94e+03 MMD: 1.83e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.75e+04 logL: -1.08e+04 KL: 1.16e+03 MMD: 1.93e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.49e+04 logL: -8.42e+03 KL: 8.01e+02 MMD: 1.97e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.18e+04 logL: -5.81e+03 KL: 6.13e+02 MMD: 1.86e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 9.82e+03 logL: -3.89e+03 KL: 4.69e+02 MMD: 1.91e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 8.57e+03 logL: -2.96e+03 KL: 3.52e+02 MMD: 1.84e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 8.62e+03 logL: -2.60e+03 KL: 2.72e+02 MMD: 2.00e+00\n",
      "Epoch 00134: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 138 VALIDATION Loss: 8.08e+03 logL: -2.50e+03 KL: 2.45e+02 MMD: 1.86e+00\n",
      "config 39, alpha = 0.0, lambda = 9.3, dropout = 0.00; 2 hidden layers with 136, 32 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.65e+03 logL: -9.49e+03 KL: 1.50e+02 MMD: 6.70e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.54e+03 logL: -9.45e+03 KL: 8.24e+01 MMD: 7.73e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.51e+03 logL: -9.44e+03 KL: 6.07e+01 MMD: 7.67e-01\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 9.41e+03 logL: -9.35e+03 KL: 5.96e+01 MMD: 7.29e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.37e+03 logL: -9.31e+03 KL: 5.78e+01 MMD: 8.07e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.33e+03 logL: -9.27e+03 KL: 5.61e+01 MMD: 7.53e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.27e+03 logL: -9.21e+03 KL: 5.70e+01 MMD: 7.81e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.17e+03 logL: -9.11e+03 KL: 5.62e+01 MMD: 7.91e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.07e+03 logL: -9.01e+03 KL: 5.54e+01 MMD: 7.19e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 8.93e+03 logL: -8.86e+03 KL: 5.64e+01 MMD: 7.79e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 8.83e+03 logL: -8.77e+03 KL: 5.70e+01 MMD: 7.89e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 8.73e+03 logL: -8.67e+03 KL: 5.65e+01 MMD: 7.92e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 8.68e+03 logL: -8.62e+03 KL: 5.71e+01 MMD: 7.84e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 8.63e+03 logL: -8.57e+03 KL: 5.40e+01 MMD: 7.57e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 8.67e+03 logL: -8.60e+03 KL: 5.72e+01 MMD: 8.31e-01\n",
      "Epoch 00155: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 160 VALIDATION Loss: 8.49e+03 logL: -8.43e+03 KL: 5.30e+01 MMD: 8.13e-01\n",
      "====> Epoch: 170 VALIDATION Loss: 8.48e+03 logL: -8.42e+03 KL: 5.24e+01 MMD: 7.63e-01\n",
      "Stopping\n",
      "====> Epoch: 173 VALIDATION Loss: 8.48e+03 logL: -8.42e+03 KL: 5.22e+01 MMD: 8.12e-01\n",
      "config 39, alpha = 0.0, lambda = 85.0, dropout = 0.00; 2 hidden layers with 41, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.58e+03 logL: -7.22e+03 KL: 2.80e+02 MMD: 1.02e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.51e+03 logL: -5.27e+03 KL: 1.48e+02 MMD: 9.98e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.33e+03 logL: -5.14e+03 KL: 1.04e+02 MMD: 1.01e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 5.21e+03 logL: -5.04e+03 KL: 8.56e+01 MMD: 1.01e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.21e+03 logL: -5.04e+03 KL: 7.34e+01 MMD: 1.10e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.17e+03 logL: -5.02e+03 KL: 6.65e+01 MMD: 1.04e+00\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.08e+03 logL: -4.93e+03 KL: 6.35e+01 MMD: 1.06e+00\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 5.08e+03 logL: -4.93e+03 KL: 6.32e+01 MMD: 1.07e+00\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 84 VALIDATION Loss: 5.07e+03 logL: -4.93e+03 KL: 6.32e+01 MMD: 1.02e+00\n",
      "config 39, alpha = 0.0, lambda = 31.5, dropout = 0.00; 2 hidden layers with 18, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.82e+09 logL: -5.45e+08 KL: 3.28e+09 MMD: 1.40e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.51e+09 logL: -1.23e+08 KL: 1.38e+09 MMD: 1.26e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.69e+08 logL: -1.06e+08 KL: 6.63e+08 MMD: 1.48e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.56e+08 logL: -4.10e+07 KL: 3.15e+08 MMD: 1.27e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.58e+08 logL: -1.69e+07 KL: 1.41e+08 MMD: 1.28e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.79e+07 logL: -8.73e+06 KL: 6.92e+07 MMD: 1.35e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.99e+07 logL: -5.69e+06 KL: 3.42e+07 MMD: 1.32e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.83e+07 logL: -1.86e+06 KL: 1.64e+07 MMD: 1.44e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.88e+06 logL: -1.32e+06 KL: 8.56e+06 MMD: 1.35e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.92e+06 logL: -1.37e+06 KL: 4.55e+06 MMD: 1.21e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.68e+06 logL: -3.74e+05 KL: 2.31e+06 MMD: 1.40e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.19e+06 logL: -9.60e+05 KL: 1.23e+06 MMD: 1.23e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 9.49e+05 logL: -3.22e+05 KL: 6.28e+05 MMD: 1.35e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 6.11e+05 logL: -3.08e+05 KL: 3.04e+05 MMD: 1.28e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 4.48e+05 logL: -2.90e+05 KL: 1.58e+05 MMD: 1.37e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 3.63e+05 logL: -2.84e+05 KL: 7.90e+04 MMD: 1.24e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 3.06e+05 logL: -2.65e+05 KL: 4.14e+04 MMD: 1.33e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 2.79e+05 logL: -2.57e+05 KL: 2.22e+04 MMD: 1.39e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 2.28e+05 logL: -2.16e+05 KL: 1.20e+04 MMD: 1.36e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 1.04e+05 logL: -9.64e+04 KL: 7.48e+03 MMD: 1.27e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 1.04e+05 logL: -9.64e+04 KL: 7.48e+03 MMD: 1.27e+00\n",
      "config 39, alpha = 0.0, lambda = 9.1, dropout = 0.00; 2 hidden layers with 13, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.41e+04 logL: -1.37e+04 KL: 4.46e+02 MMD: 2.04e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.38e+04 logL: -1.36e+04 KL: 2.00e+02 MMD: 2.55e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.49e+02 MMD: 2.60e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.84e+03 logL: -7.63e+03 KL: 1.89e+02 MMD: 1.74e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.46e+03 logL: -5.31e+03 KL: 1.34e+02 MMD: 1.93e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.32e+03 logL: -5.20e+03 KL: 1.01e+02 MMD: 1.83e+00\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.26e+03 logL: -5.16e+03 KL: 9.21e+01 MMD: 1.92e+00\n",
      "Stopping\n",
      "====> Epoch: 75 VALIDATION Loss: 5.27e+03 logL: -5.16e+03 KL: 9.08e+01 MMD: 2.03e+00\n",
      "config 39, alpha = 0.0, lambda = 16813.4, dropout = 0.00; 2 hidden layers with 35, 29 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.37e+04 logL: -1.37e+04 KL: 1.54e+03 MMD: 1.70e+00\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 4.86e+04 logL: -1.35e+04 KL: 7.47e+02 MMD: 2.04e+00\n",
      "Stopping\n",
      "====> Epoch: 20 VALIDATION Loss: 4.86e+04 logL: -1.35e+04 KL: 7.47e+02 MMD: 2.04e+00\n",
      "config 40, alpha = 0.0, lambda = 17.6, dropout = 0.00; 2 hidden layers with 29, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.48e+05 logL: -3.48e+05 KL: 7.55e+01 MMD: 1.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.40e+05 logL: -3.40e+05 KL: 1.70e+02 MMD: 9.04e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.29e+05 logL: -3.28e+05 KL: 3.93e+02 MMD: 8.58e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.67e+04 logL: -2.31e+04 KL: 3.54e+03 MMD: 7.46e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.91e+04 logL: -1.70e+04 KL: 2.12e+03 MMD: 7.15e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.81e+04 logL: -1.68e+04 KL: 1.29e+03 MMD: 7.48e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.75e+04 logL: -1.67e+04 KL: 8.07e+02 MMD: 8.57e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.70e+04 logL: -1.64e+04 KL: 5.35e+02 MMD: 8.31e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.66e+04 logL: -1.62e+04 KL: 3.77e+02 MMD: 9.23e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.61e+04 logL: -1.58e+04 KL: 2.93e+02 MMD: 8.69e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.57e+04 logL: -1.54e+04 KL: 2.39e+02 MMD: 9.32e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.51e+04 logL: -1.48e+04 KL: 1.99e+02 MMD: 9.02e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 1.42e+04 logL: -1.41e+04 KL: 1.59e+02 MMD: 9.51e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 1.24e+02 MMD: 9.94e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 1.11e+04 logL: -1.08e+04 KL: 2.34e+02 MMD: 6.99e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 9.83e+03 logL: -9.66e+03 KL: 1.51e+02 MMD: 7.22e-01\n",
      "====> Epoch: 170 VALIDATION Loss: 9.71e+03 logL: -9.59e+03 KL: 1.07e+02 MMD: 6.87e-01\n",
      "====> Epoch: 180 VALIDATION Loss: 9.63e+03 logL: -9.53e+03 KL: 8.91e+01 MMD: 7.18e-01\n",
      "====> Epoch: 190 VALIDATION Loss: 9.58e+03 logL: -9.49e+03 KL: 7.55e+01 MMD: 7.11e-01\n",
      "====> Epoch: 200 VALIDATION Loss: 9.60e+03 logL: -9.52e+03 KL: 6.48e+01 MMD: 7.87e-01\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 9.60e+03 logL: -9.52e+03 KL: 6.48e+01 MMD: 7.87e-01\n",
      "config 40, alpha = 0.0, lambda = 37.7, dropout = 0.00; 2 hidden layers with 53, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.05e+04 logL: -9.94e+03 KL: 4.83e+02 MMD: 9.93e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 7.15e+03 logL: -6.87e+03 KL: 2.47e+02 MMD: 1.03e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.79e+03 logL: -5.60e+03 KL: 1.54e+02 MMD: 1.01e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.79e+03 logL: -5.64e+03 KL: 1.18e+02 MMD: 9.93e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 5.57e+03 logL: -5.43e+03 KL: 9.75e+01 MMD: 1.07e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.33e+03 logL: -5.20e+03 KL: 8.98e+01 MMD: 1.07e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.19e+03 logL: -5.07e+03 KL: 8.25e+01 MMD: 1.03e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.13e+03 logL: -5.01e+03 KL: 7.95e+01 MMD: 1.08e+00\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 5.01e+03 logL: -4.90e+03 KL: 7.60e+01 MMD: 1.02e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.00e+03 logL: -4.88e+03 KL: 7.43e+01 MMD: 1.06e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 4.98e+03 logL: -4.87e+03 KL: 7.19e+01 MMD: 1.16e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.96e+03 logL: -4.85e+03 KL: 6.94e+01 MMD: 1.05e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 4.94e+03 logL: -4.84e+03 KL: 6.80e+01 MMD: 1.04e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 4.92e+03 logL: -4.82e+03 KL: 6.62e+01 MMD: 9.42e-01\n",
      "Epoch 00143: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 150 VALIDATION Loss: 4.91e+03 logL: -4.80e+03 KL: 6.53e+01 MMD: 1.13e+00\n",
      "Epoch 00153: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 157 VALIDATION Loss: 4.91e+03 logL: -4.80e+03 KL: 6.51e+01 MMD: 1.03e+00\n",
      "config 40, alpha = 0.0, lambda = 62740.8, dropout = 0.00; 2 hidden layers with 56, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.86e+04 logL: -1.35e+04 KL: 1.05e+01 MMD: 8.04e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 1.52e+04 logL: -1.12e+04 KL: 1.29e+01 MMD: 6.25e-02\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.34e+04 logL: -9.92e+03 KL: 1.40e+01 MMD: 5.58e-02\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.29e+04 logL: -9.77e+03 KL: 1.43e+01 MMD: 4.98e-02\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.23e+04 logL: -9.76e+03 KL: 1.45e+01 MMD: 4.11e-02\n",
      "Stopping\n",
      "====> Epoch: 53 VALIDATION Loss: 1.22e+04 logL: -9.75e+03 KL: 1.45e+01 MMD: 3.80e-02\n",
      "config 40, alpha = 0.0, lambda = 1.0, dropout = 0.00; 2 hidden layers with 95, 51 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 2.83e+03 logL: -2.57e+03 KL: 2.55e+02 MMD: 1.61e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.80e+03 logL: -1.64e+03 KL: 1.58e+02 MMD: 1.67e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.76e+03 logL: -1.64e+03 KL: 1.22e+02 MMD: 1.62e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.72e+03 logL: -1.61e+03 KL: 1.03e+02 MMD: 1.80e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.63e+03 logL: -1.54e+03 KL: 9.03e+01 MMD: 1.58e+00\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.57e+03 logL: -1.49e+03 KL: 8.50e+01 MMD: 1.75e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.57e+03 logL: -1.48e+03 KL: 8.18e+01 MMD: 1.62e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.56e+03 logL: -1.48e+03 KL: 7.81e+01 MMD: 1.69e+00\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.55e+03 logL: -1.48e+03 KL: 7.72e+01 MMD: 1.70e+00\n",
      "Stopping\n",
      "====> Epoch: 92 VALIDATION Loss: 1.55e+03 logL: -1.48e+03 KL: 7.73e+01 MMD: 1.67e+00\n",
      "config 40, alpha = 0.0, lambda = 233.2, dropout = 0.00; 2 hidden layers with 26, 24 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.14e+06 logL: -4.77e+05 KL: 2.66e+06 MMD: 1.97e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.31e+06 logL: -5.25e+05 KL: 1.79e+06 MMD: 1.94e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.81e+06 logL: -4.23e+05 KL: 1.39e+06 MMD: 2.06e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.39e+06 logL: -3.76e+05 KL: 1.02e+06 MMD: 1.90e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.02e+06 logL: -3.84e+05 KL: 6.38e+05 MMD: 1.93e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.68e+05 logL: -3.33e+05 KL: 3.35e+05 MMD: 1.96e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.70e+05 logL: -3.12e+05 KL: 1.57e+05 MMD: 1.86e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.67e+05 logL: -2.96e+05 KL: 6.96e+04 MMD: 1.84e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.07e+05 logL: -2.75e+05 KL: 3.15e+04 MMD: 1.84e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.70e+05 logL: -2.55e+05 KL: 1.49e+04 MMD: 1.98e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.45e+05 logL: -2.37e+05 KL: 7.85e+03 MMD: 2.07e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.13e+05 logL: -2.07e+05 KL: 4.81e+03 MMD: 1.89e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.47e+05 logL: -1.43e+05 KL: 3.79e+03 MMD: 1.86e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 5.32e+04 logL: -4.97e+04 KL: 3.14e+03 MMD: 1.90e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.68e+04 logL: -1.42e+04 KL: 2.20e+03 MMD: 1.84e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.47e+04 logL: -1.28e+04 KL: 1.39e+03 MMD: 1.82e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 1.36e+04 logL: -1.22e+04 KL: 9.19e+02 MMD: 1.97e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 1.23e+04 logL: -1.12e+04 KL: 6.27e+02 MMD: 1.82e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 1.04e+04 logL: -9.45e+03 KL: 4.40e+02 MMD: 2.03e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 8.33e+03 logL: -7.56e+03 KL: 3.19e+02 MMD: 1.95e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 8.33e+03 logL: -7.56e+03 KL: 3.19e+02 MMD: 1.95e+00\n",
      "config 41, alpha = 0.0, lambda = 1.0, dropout = 0.00; 2 hidden layers with 123, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.92e+03 logL: -9.55e+03 KL: 3.66e+02 MMD: 6.70e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.75e+03 logL: -9.58e+03 KL: 1.74e+02 MMD: 7.70e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.51e+03 logL: -9.39e+03 KL: 1.20e+02 MMD: 7.24e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.16e+03 logL: -9.05e+03 KL: 1.11e+02 MMD: 7.60e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.04e+03 logL: -8.94e+03 KL: 9.70e+01 MMD: 7.18e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 8.76e+03 logL: -8.68e+03 KL: 8.10e+01 MMD: 7.12e-01\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 8.71e+03 logL: -8.64e+03 KL: 7.11e+01 MMD: 7.09e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.66e+03 logL: -8.59e+03 KL: 6.67e+01 MMD: 7.95e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 8.62e+03 logL: -8.56e+03 KL: 6.40e+01 MMD: 7.42e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 8.56e+03 logL: -8.50e+03 KL: 6.22e+01 MMD: 7.77e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 8.56e+03 logL: -8.50e+03 KL: 6.01e+01 MMD: 7.71e-01\n",
      "Epoch 00113: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 8.53e+03 logL: -8.47e+03 KL: 5.88e+01 MMD: 7.83e-01\n",
      "Stopping\n",
      "====> Epoch: 125 VALIDATION Loss: 8.53e+03 logL: -8.47e+03 KL: 5.87e+01 MMD: 7.19e-01\n",
      "config 41, alpha = 0.0, lambda = 14144.9, dropout = 0.00; 2 hidden layers with 41, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.33e+04 logL: -1.55e+04 KL: 5.59e+02 MMD: 1.22e+00\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 3.03e+04 logL: -1.50e+04 KL: 5.13e+02 MMD: 1.05e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.07e+04 logL: -1.49e+04 KL: 4.96e+02 MMD: 1.08e+00\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 3.06e+04 logL: -1.48e+04 KL: 4.81e+02 MMD: 1.08e+00\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.96e+04 logL: -1.47e+04 KL: 4.78e+02 MMD: 1.02e+00\n",
      "Stopping\n",
      "====> Epoch: 52 VALIDATION Loss: 3.00e+04 logL: -1.47e+04 KL: 4.78e+02 MMD: 1.04e+00\n",
      "config 41, alpha = 0.0, lambda = 15596.2, dropout = 0.00; 2 hidden layers with 19, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.21e+04 logL: -9.82e+03 KL: 5.93e+02 MMD: 1.39e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.60e+04 logL: -6.04e+03 KL: 4.05e+02 MMD: 1.26e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.62e+04 logL: -5.60e+03 KL: 2.43e+02 MMD: 1.30e+00\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.36e+04 logL: -5.28e+03 KL: 2.02e+02 MMD: 1.16e+00\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.51e+04 logL: -4.42e+03 KL: 2.23e+02 MMD: 1.31e+00\n",
      "Stopping\n",
      "====> Epoch: 50 VALIDATION Loss: 2.51e+04 logL: -4.42e+03 KL: 2.23e+02 MMD: 1.31e+00\n",
      "config 41, alpha = 0.0, lambda = 27.2, dropout = 0.00; 2 hidden layers with 168, 106 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.03e+04 logL: -9.63e+03 KL: 5.96e+02 MMD: 1.65e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.92e+03 logL: -5.58e+03 KL: 3.02e+02 MMD: 1.68e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.89e+03 logL: -3.61e+03 KL: 2.35e+02 MMD: 1.63e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.26e+03 logL: -2.01e+03 KL: 2.10e+02 MMD: 1.54e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.99e+03 logL: -1.77e+03 KL: 1.81e+02 MMD: 1.66e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.94e+03 logL: -1.75e+03 KL: 1.61e+02 MMD: 1.44e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.78e+03 logL: -1.59e+03 KL: 1.47e+02 MMD: 1.72e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.71e+03 logL: -1.54e+03 KL: 1.30e+02 MMD: 1.60e+00\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.66e+03 logL: -1.49e+03 KL: 1.23e+02 MMD: 1.54e+00\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.65e+03 logL: -1.49e+03 KL: 1.20e+02 MMD: 1.61e+00\n",
      "Epoch 00108: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.65e+03 logL: -1.49e+03 KL: 1.19e+02 MMD: 1.58e+00\n",
      "Stopping\n",
      "====> Epoch: 112 VALIDATION Loss: 1.65e+03 logL: -1.49e+03 KL: 1.19e+02 MMD: 1.59e+00\n",
      "config 41, alpha = 0.0, lambda = 1936.0, dropout = 0.00; 2 hidden layers with 20, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.10e+04 logL: -6.91e+03 KL: 4.15e+02 MMD: 1.90e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.89e+03 logL: -3.11e+03 KL: 2.42e+02 MMD: 1.83e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.36e+03 logL: -2.65e+03 KL: 1.79e+02 MMD: 1.82e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.19e+03 logL: -2.22e+03 KL: 1.50e+02 MMD: 1.97e+00\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 6.02e+03 logL: -2.19e+03 KL: 1.28e+02 MMD: 1.91e+00\n",
      "Stopping\n",
      "====> Epoch: 51 VALIDATION Loss: 5.92e+03 logL: -2.19e+03 KL: 1.28e+02 MMD: 1.86e+00\n",
      "config 42, alpha = 0.0, lambda = 830.8, dropout = 0.00; 2 hidden layers with 10, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.49e+04 logL: -1.35e+04 KL: 7.62e+02 MMD: 7.40e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.11e+04 logL: -1.02e+04 KL: 4.19e+02 MMD: 6.73e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.04e+04 logL: -9.58e+03 KL: 2.04e+02 MMD: 7.60e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.03e+04 logL: -9.55e+03 KL: 1.29e+02 MMD: 7.12e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.02e+04 logL: -9.52e+03 KL: 9.90e+01 MMD: 7.20e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.02e+04 logL: -9.49e+03 KL: 9.01e+01 MMD: 7.83e-01\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: 1.02e+04 logL: -9.49e+03 KL: 8.91e+01 MMD: 7.23e-01\n",
      "config 42, alpha = 0.0, lambda = 22437.3, dropout = 0.00; 2 hidden layers with 54, 31 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.86e+04 logL: -5.30e+03 KL: 2.86e+02 MMD: 1.03e+00\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 2.88e+04 logL: -5.17e+03 KL: 2.31e+02 MMD: 1.04e+00\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 2.90e+04 logL: -5.15e+03 KL: 2.24e+02 MMD: 1.05e+00\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.90e+04 logL: -5.15e+03 KL: 2.23e+02 MMD: 1.05e+00\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 48 VALIDATION Loss: 2.69e+04 logL: -5.15e+03 KL: 2.23e+02 MMD: 9.59e-01\n",
      "config 42, alpha = 0.0, lambda = 395.2, dropout = 0.00; 2 hidden layers with 9, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.78e+04 logL: -1.54e+04 KL: 1.87e+03 MMD: 1.32e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.52e+04 logL: -1.40e+04 KL: 6.88e+02 MMD: 1.28e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.45e+04 logL: -1.36e+04 KL: 3.40e+02 MMD: 1.39e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.07e+04 logL: -9.89e+03 KL: 2.90e+02 MMD: 1.27e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.03e+04 logL: -9.61e+03 KL: 1.59e+02 MMD: 1.35e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.02e+04 logL: -9.55e+03 KL: 1.17e+02 MMD: 1.34e+00\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.01e+04 logL: -9.51e+03 KL: 9.76e+01 MMD: 1.29e+00\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.02e+04 logL: -9.50e+03 KL: 9.63e+01 MMD: 1.43e+00\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 87 VALIDATION Loss: 1.01e+04 logL: -9.50e+03 KL: 9.62e+01 MMD: 1.29e+00\n",
      "config 42, alpha = 0.0, lambda = 1242.8, dropout = 0.00; 2 hidden layers with 96, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.61e+04 logL: -1.33e+04 KL: 5.75e+02 MMD: 1.81e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.38e+03 logL: -6.76e+03 KL: 3.52e+02 MMD: 1.82e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.09e+03 logL: -3.72e+03 KL: 2.24e+02 MMD: 1.73e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.94e+03 logL: -2.82e+03 KL: 1.69e+02 MMD: 1.57e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.89e+03 logL: -2.81e+03 KL: 1.34e+02 MMD: 1.57e+00\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.83e+03 logL: -2.74e+03 KL: 1.22e+02 MMD: 1.58e+00\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 67 VALIDATION Loss: 4.83e+03 logL: -2.74e+03 KL: 1.20e+02 MMD: 1.59e+00\n",
      "config 42, alpha = 0.0, lambda = 1.9, dropout = 0.00; 2 hidden layers with 31, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.39e+04 logL: -1.36e+04 KL: 2.44e+02 MMD: 3.48e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 1.43e+02 MMD: 4.18e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.75e+03 logL: -9.62e+03 KL: 1.33e+02 MMD: 3.24e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.66e+03 logL: -9.55e+03 KL: 1.01e+02 MMD: 3.51e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.56e+03 logL: -9.47e+03 KL: 8.50e+01 MMD: 3.42e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.39e+03 logL: -9.30e+03 KL: 8.00e+01 MMD: 3.52e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.17e+03 logL: -7.08e+03 KL: 8.74e+01 MMD: 2.82e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.86e+03 logL: -6.78e+03 KL: 7.88e+01 MMD: 3.10e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.46e+03 logL: -5.38e+03 KL: 8.07e+01 MMD: 2.96e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.15e+03 logL: -5.08e+03 KL: 7.38e+01 MMD: 2.81e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.73e+03 logL: -3.65e+03 KL: 8.21e+01 MMD: 2.80e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.67e+03 logL: -3.59e+03 KL: 7.07e+01 MMD: 2.74e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.65e+03 logL: -3.59e+03 KL: 6.47e+01 MMD: 2.87e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.64e+03 logL: -3.58e+03 KL: 6.07e+01 MMD: 2.60e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 3.65e+03 logL: -3.59e+03 KL: 5.85e+01 MMD: 2.63e+00\n",
      "Epoch 00155: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 160 VALIDATION Loss: 3.62e+03 logL: -3.56e+03 KL: 5.70e+01 MMD: 2.51e+00\n",
      "Epoch 00166: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 166 VALIDATION Loss: 3.62e+03 logL: -3.56e+03 KL: 5.66e+01 MMD: 2.72e+00\n",
      "config 43, alpha = 0.0, lambda = 5.0, dropout = 0.00; 2 hidden layers with 38, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.52e+04 logL: -1.48e+04 KL: 3.87e+02 MMD: 7.11e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.44e+04 logL: -1.41e+04 KL: 2.91e+02 MMD: 7.59e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.13e+04 logL: -1.10e+04 KL: 2.84e+02 MMD: 6.36e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.02e+04 logL: -9.98e+03 KL: 2.21e+02 MMD: 6.55e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.98e+03 logL: -9.79e+03 KL: 1.81e+02 MMD: 7.26e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.86e+03 logL: -9.70e+03 KL: 1.49e+02 MMD: 6.98e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.74e+03 logL: -9.61e+03 KL: 1.26e+02 MMD: 7.23e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.67e+03 logL: -9.55e+03 KL: 1.09e+02 MMD: 7.01e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.54e+03 logL: -9.45e+03 KL: 9.15e+01 MMD: 6.68e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 9.49e+03 logL: -9.40e+03 KL: 7.90e+01 MMD: 7.26e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 9.53e+03 logL: -9.45e+03 KL: 8.09e+01 MMD: 7.84e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 9.20e+03 logL: -9.11e+03 KL: 8.76e+01 MMD: 6.77e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 9.11e+03 logL: -9.04e+03 KL: 7.00e+01 MMD: 6.99e-01\n",
      "Epoch 00134: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 8.87e+03 logL: -8.79e+03 KL: 7.75e+01 MMD: 7.50e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 8.77e+03 logL: -8.69e+03 KL: 7.16e+01 MMD: 7.31e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 8.67e+03 logL: -8.60e+03 KL: 6.53e+01 MMD: 7.28e-01\n",
      "Epoch 00165: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 170 VALIDATION Loss: 8.60e+03 logL: -8.53e+03 KL: 6.45e+01 MMD: 7.73e-01\n",
      "Stopping\n",
      "====> Epoch: 177 VALIDATION Loss: 8.60e+03 logL: -8.53e+03 KL: 6.45e+01 MMD: 7.29e-01\n",
      "config 43, alpha = 0.0, lambda = 19685.6, dropout = 0.00; 2 hidden layers with 43, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.55e+04 logL: -1.38e+04 KL: 6.00e+02 MMD: 1.07e+00\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 3.35e+04 logL: -1.36e+04 KL: 5.10e+02 MMD: 9.86e-01\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 24 VALIDATION Loss: 3.51e+04 logL: -1.36e+04 KL: 5.13e+02 MMD: 1.07e+00\n",
      "config 43, alpha = 0.0, lambda = 1028.2, dropout = 0.00; 2 hidden layers with 140, 109 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.31e+03 logL: -2.80e+03 KL: 2.07e+02 MMD: 1.27e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.19e+03 logL: -2.75e+03 KL: 1.21e+02 MMD: 1.28e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.23e+03 logL: -2.70e+03 KL: 9.08e+01 MMD: 1.40e+00\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 3.97e+03 logL: -2.52e+03 KL: 7.76e+01 MMD: 1.34e+00\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.03e+03 logL: -2.51e+03 KL: 7.38e+01 MMD: 1.41e+00\n",
      "Stopping\n",
      "====> Epoch: 52 VALIDATION Loss: 3.90e+03 logL: -2.51e+03 KL: 7.37e+01 MMD: 1.29e+00\n",
      "config 43, alpha = 0.0, lambda = 7.7, dropout = 0.00; 2 hidden layers with 11, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.43e+04 logL: -1.37e+04 KL: 5.87e+02 MMD: 1.90e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+04 logL: -1.36e+04 KL: 2.46e+02 MMD: 2.18e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 1.30e+02 MMD: 2.40e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.69e+03 logL: -9.56e+03 KL: 1.24e+02 MMD: 2.11e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.31e+03 logL: -7.19e+03 KL: 1.10e+02 MMD: 1.96e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.23e+03 logL: -7.13e+03 KL: 8.44e+01 MMD: 2.10e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.20e+03 logL: -7.11e+03 KL: 7.23e+01 MMD: 2.21e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 80 VALIDATION Loss: 7.21e+03 logL: -7.13e+03 KL: 6.47e+01 MMD: 2.20e+00\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 7.17e+03 logL: -7.09e+03 KL: 5.94e+01 MMD: 2.14e+00\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 7.17e+03 logL: -7.09e+03 KL: 5.88e+01 MMD: 2.12e+00\n",
      "Stopping\n",
      "====> Epoch: 100 VALIDATION Loss: 7.17e+03 logL: -7.09e+03 KL: 5.88e+01 MMD: 2.12e+00\n",
      "config 43, alpha = 0.0, lambda = 23.2, dropout = 0.00; 2 hidden layers with 52, 42 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.24e+03 logL: -2.92e+03 KL: 2.75e+02 MMD: 1.79e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.96e+03 logL: -1.75e+03 KL: 1.63e+02 MMD: 2.00e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.36e+03 logL: -1.19e+03 KL: 1.25e+02 MMD: 1.91e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.04e+03 logL: -8.89e+02 KL: 1.12e+02 MMD: 1.82e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.06e+03 logL: -9.16e+02 KL: 9.90e+01 MMD: 2.02e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.95e+02 logL: -8.59e+02 KL: 9.04e+01 MMD: 2.05e+00\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.64e+02 logL: -8.37e+02 KL: 8.40e+01 MMD: 1.89e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.60e+02 logL: -8.37e+02 KL: 8.20e+01 MMD: 1.81e+00\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 84 VALIDATION Loss: 9.59e+02 logL: -8.35e+02 KL: 8.21e+01 MMD: 1.85e+00\n",
      "config 44, alpha = 0.0, lambda = 18.5, dropout = 0.00; 2 hidden layers with 29, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.09e+04 logL: -1.06e+04 KL: 3.73e+02 MMD: 6.93e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.78e+03 logL: -9.59e+03 KL: 1.84e+02 MMD: 6.67e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.70e+03 logL: -9.54e+03 KL: 1.42e+02 MMD: 6.57e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.61e+03 logL: -9.48e+03 KL: 1.13e+02 MMD: 6.96e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.72e+03 logL: -9.61e+03 KL: 9.39e+01 MMD: 7.38e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.53e+03 logL: -9.43e+03 KL: 8.63e+01 MMD: 7.70e-01\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.49e+03 logL: -9.40e+03 KL: 8.26e+01 MMD: 6.85e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.46e+03 logL: -9.37e+03 KL: 7.82e+01 MMD: 7.75e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.45e+03 logL: -9.36e+03 KL: 7.38e+01 MMD: 7.59e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 9.44e+03 logL: -9.36e+03 KL: 7.13e+01 MMD: 7.65e-01\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 9.43e+03 logL: -9.34e+03 KL: 6.95e+01 MMD: 7.02e-01\n",
      "Stopping\n",
      "====> Epoch: 114 VALIDATION Loss: 9.42e+03 logL: -9.34e+03 KL: 6.92e+01 MMD: 7.48e-01\n",
      "config 44, alpha = 0.0, lambda = 1.9, dropout = 0.00; 2 hidden layers with 77, 57 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.45e+03 logL: -5.23e+03 KL: 2.16e+02 MMD: 1.07e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.08e+03 logL: -4.95e+03 KL: 1.25e+02 MMD: 1.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.85e+03 logL: -4.75e+03 KL: 9.83e+01 MMD: 1.02e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.64e+03 logL: -4.56e+03 KL: 7.89e+01 MMD: 1.02e+00\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.51e+03 logL: -4.44e+03 KL: 7.34e+01 MMD: 1.09e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.42e+03 logL: -4.34e+03 KL: 7.00e+01 MMD: 9.99e-01\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 4.40e+03 logL: -4.33e+03 KL: 6.75e+01 MMD: 1.06e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.40e+03 logL: -4.33e+03 KL: 6.73e+01 MMD: 1.05e+00\n",
      "Stopping\n",
      "====> Epoch: 80 VALIDATION Loss: 4.40e+03 logL: -4.33e+03 KL: 6.73e+01 MMD: 1.05e+00\n",
      "config 44, alpha = 0.0, lambda = 2668.0, dropout = 0.00; 2 hidden layers with 27, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.37e+04 logL: -9.74e+03 KL: 4.13e+02 MMD: 1.32e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.10e+03 logL: -5.24e+03 KL: 2.72e+02 MMD: 1.35e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.10e+03 logL: -3.70e+03 KL: 1.94e+02 MMD: 1.20e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.22e+03 logL: -3.62e+03 KL: 1.56e+02 MMD: 1.29e+00\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 7.05e+03 logL: -3.31e+03 KL: 1.55e+02 MMD: 1.35e+00\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 56 VALIDATION Loss: 6.81e+03 logL: -3.14e+03 KL: 1.58e+02 MMD: 1.32e+00\n",
      "config 44, alpha = 0.0, lambda = 22153.6, dropout = 0.00; 2 hidden layers with 136, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.81e+04 logL: -1.39e+04 KL: 3.20e+02 MMD: 1.53e+00\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 4.45e+04 logL: -1.02e+04 KL: 5.84e+02 MMD: 1.52e+00\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 4.83e+04 logL: -9.88e+03 KL: 5.56e+02 MMD: 1.71e+00\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 39 VALIDATION Loss: 4.40e+04 logL: -9.86e+03 KL: 5.51e+02 MMD: 1.52e+00\n",
      "config 44, alpha = 0.0, lambda = 8429.2, dropout = 0.00; 2 hidden layers with 26, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.35e+04 logL: -1.38e+04 KL: 2.65e+03 MMD: 2.02e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.05e+04 logL: -1.36e+04 KL: 8.50e+02 MMD: 1.91e+00\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.00e+04 logL: -1.32e+04 KL: 5.96e+02 MMD: 1.92e+00\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 3.05e+04 logL: -1.31e+04 KL: 5.80e+02 MMD: 1.99e+00\n",
      "config 45, alpha = 0.0, lambda = 3.1, dropout = 0.00; 2 hidden layers with 17, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.35e+04 logL: -1.29e+04 KL: 5.52e+02 MMD: 6.19e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.81e+03 logL: -9.58e+03 KL: 2.24e+02 MMD: 7.03e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.70e+03 logL: -9.57e+03 KL: 1.35e+02 MMD: 7.60e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.63e+03 logL: -9.53e+03 KL: 9.86e+01 MMD: 7.65e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.64e+03 logL: -9.56e+03 KL: 7.82e+01 MMD: 8.39e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.60e+03 logL: -9.54e+03 KL: 6.41e+01 MMD: 7.37e-01\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.57e+03 logL: -9.51e+03 KL: 5.99e+01 MMD: 7.62e-01\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 75 VALIDATION Loss: 9.57e+03 logL: -9.51e+03 KL: 5.90e+01 MMD: 7.72e-01\n",
      "config 45, alpha = 0.0, lambda = 2549.6, dropout = 0.00; 2 hidden layers with 7, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.89e+05 logL: -3.58e+05 KL: 2.86e+04 MMD: 8.42e-01\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 3.88e+05 logL: -3.58e+05 KL: 2.86e+04 MMD: 8.46e-01\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 26 VALIDATION Loss: 3.85e+05 logL: -3.55e+05 KL: 2.86e+04 MMD: 9.19e-01\n",
      "config 45, alpha = 0.0, lambda = 252.9, dropout = 0.00; 2 hidden layers with 124, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.99e+04 logL: -1.55e+04 KL: 4.12e+03 MMD: 1.32e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.75e+04 logL: -1.52e+04 KL: 2.02e+03 MMD: 1.31e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.59e+04 logL: -1.46e+04 KL: 9.24e+02 MMD: 1.34e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.46e+04 logL: -1.38e+04 KL: 4.20e+02 MMD: 1.39e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.19e+04 logL: -1.13e+04 KL: 2.85e+02 MMD: 1.39e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.15e+04 logL: -1.10e+04 KL: 1.86e+02 MMD: 1.43e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.61e+03 logL: -9.03e+03 KL: 2.26e+02 MMD: 1.42e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 7.92e+03 logL: -7.41e+03 KL: 1.68e+02 MMD: 1.38e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.51e+03 logL: -6.00e+03 KL: 1.59e+02 MMD: 1.40e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.01e+03 logL: -5.55e+03 KL: 1.32e+02 MMD: 1.32e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 4.17e+03 logL: -3.68e+03 KL: 1.36e+02 MMD: 1.38e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.02e+03 logL: -3.59e+03 KL: 1.11e+02 MMD: 1.25e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.20e+03 logL: -2.77e+03 KL: 1.04e+02 MMD: 1.27e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00137: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 3.15e+03 logL: -2.70e+03 KL: 1.13e+02 MMD: 1.35e+00\n",
      "Epoch 00145: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 149 VALIDATION Loss: 3.13e+03 logL: -2.69e+03 KL: 1.09e+02 MMD: 1.29e+00\n",
      "config 45, alpha = 0.0, lambda = 3585.7, dropout = 0.00; 2 hidden layers with 102, 27 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.17e+04 logL: -5.29e+03 KL: 4.24e+02 MMD: 1.66e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.13e+03 logL: -2.83e+03 KL: 2.33e+02 MMD: 1.69e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.15e+03 logL: -2.31e+03 KL: 1.83e+02 MMD: 1.58e+00\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 8.38e+03 logL: -2.20e+03 KL: 1.57e+02 MMD: 1.68e+00\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 8.12e+03 logL: -2.19e+03 KL: 1.50e+02 MMD: 1.61e+00\n",
      "Stopping\n",
      "====> Epoch: 52 VALIDATION Loss: 7.62e+03 logL: -2.19e+03 KL: 1.50e+02 MMD: 1.47e+00\n",
      "config 45, alpha = 0.0, lambda = 3092.4, dropout = 0.00; 2 hidden layers with 31, 31 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.22e+04 logL: -5.63e+03 KL: 4.15e+02 MMD: 2.00e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.97e+03 logL: -2.83e+03 KL: 2.42e+02 MMD: 1.91e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.83e+03 logL: -1.68e+03 KL: 1.94e+02 MMD: 1.93e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.45e+03 logL: -1.59e+03 KL: 1.52e+02 MMD: 1.85e+00\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 45 VALIDATION Loss: 7.54e+03 logL: -1.58e+03 KL: 1.43e+02 MMD: 1.88e+00\n",
      "config 46, alpha = 0.0, lambda = 93519.7, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.59e+04 logL: -1.37e+04 KL: 3.23e+02 MMD: 6.62e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 7.77e+04 logL: -1.35e+04 KL: 5.89e+02 MMD: 6.80e-01\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 7.65e+04 logL: -1.35e+04 KL: 6.50e+02 MMD: 6.67e-01\n",
      "Stopping\n",
      "====> Epoch: 31 VALIDATION Loss: 8.08e+04 logL: -1.35e+04 KL: 6.54e+02 MMD: 7.12e-01\n",
      "config 46, alpha = 0.0, lambda = 1100.5, dropout = 0.00; 2 hidden layers with 128, 66 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.51e+03 logL: -5.23e+03 KL: 2.00e+02 MMD: 9.82e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 6.33e+03 logL: -5.05e+03 KL: 1.25e+02 MMD: 1.05e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.11e+03 logL: -4.90e+03 KL: 1.01e+02 MMD: 1.01e+00\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 6.01e+03 logL: -4.73e+03 KL: 9.38e+01 MMD: 1.07e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.09e+03 logL: -4.70e+03 KL: 8.89e+01 MMD: 1.18e+00\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 54 VALIDATION Loss: 5.97e+03 logL: -4.69e+03 KL: 8.92e+01 MMD: 1.09e+00\n",
      "config 46, alpha = 0.0, lambda = 38537.3, dropout = 0.00; 2 hidden layers with 41, 37 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.77e+04 logL: -5.72e+03 KL: 4.66e+02 MMD: 1.34e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.81e+04 logL: -3.61e+03 KL: 2.65e+02 MMD: 1.41e+00\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 25 VALIDATION Loss: 5.30e+04 logL: -3.24e+03 KL: 2.52e+02 MMD: 1.29e+00\n",
      "config 46, alpha = 0.0, lambda = 75.7, dropout = 0.00; 2 hidden layers with 25, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.61e+04 logL: -1.39e+04 KL: 2.10e+03 MMD: 1.73e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.44e+04 logL: -1.36e+04 KL: 7.23e+02 MMD: 1.66e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.30e+04 logL: -1.24e+04 KL: 4.43e+02 MMD: 1.76e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.05e+04 logL: -1.01e+04 KL: 3.30e+02 MMD: 1.61e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.97e+03 logL: -9.61e+03 KL: 2.39e+02 MMD: 1.67e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.74e+03 logL: -7.40e+03 KL: 2.11e+02 MMD: 1.69e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.89e+03 logL: -5.60e+03 KL: 1.72e+02 MMD: 1.67e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.86e+03 logL: -4.59e+03 KL: 1.57e+02 MMD: 1.61e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.91e+03 logL: -3.64e+03 KL: 1.40e+02 MMD: 1.69e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.28e+03 logL: -3.03e+03 KL: 1.27e+02 MMD: 1.61e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.12e+03 logL: -2.87e+03 KL: 1.11e+02 MMD: 1.76e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.59e+03 logL: -2.36e+03 KL: 1.08e+02 MMD: 1.57e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.42e+03 logL: -2.20e+03 KL: 9.56e+01 MMD: 1.58e+00\n",
      "Epoch 00138: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 2.33e+03 logL: -2.12e+03 KL: 8.75e+01 MMD: 1.64e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.31e+03 logL: -2.09e+03 KL: 8.68e+01 MMD: 1.72e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 2.13e+03 logL: -1.93e+03 KL: 8.96e+01 MMD: 1.51e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 1.89e+03 logL: -1.68e+03 KL: 9.43e+01 MMD: 1.58e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 1.82e+03 logL: -1.61e+03 KL: 9.20e+01 MMD: 1.59e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 1.79e+03 logL: -1.58e+03 KL: 8.92e+01 MMD: 1.61e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 1.78e+03 logL: -1.57e+03 KL: 8.57e+01 MMD: 1.65e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 1.78e+03 logL: -1.57e+03 KL: 8.57e+01 MMD: 1.65e+00\n",
      "config 46, alpha = 0.0, lambda = 17782.9, dropout = 0.00; 2 hidden layers with 12, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.98e+04 logL: -1.38e+04 KL: 2.95e+02 MMD: 2.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.65e+04 logL: -1.36e+04 KL: 2.42e+02 MMD: 1.84e+00\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 4.88e+04 logL: -1.35e+04 KL: 1.74e+02 MMD: 1.98e+00\n",
      "Stopping\n",
      "====> Epoch: 33 VALIDATION Loss: 4.95e+04 logL: -1.35e+04 KL: 1.73e+02 MMD: 2.01e+00\n",
      "config 47, alpha = 0.0, lambda = 1.3, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.49e+04 logL: -1.40e+04 KL: 8.98e+02 MMD: 7.04e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.41e+04 logL: -1.37e+04 KL: 4.48e+02 MMD: 7.56e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 2.21e+02 MMD: 8.73e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.25e+02 MMD: 9.17e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.35e+04 logL: -1.35e+04 KL: 7.95e+01 MMD: 9.37e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 5.95e+01 MMD: 1.04e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 5.10e+01 MMD: 1.08e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.34e+04 logL: -1.34e+04 KL: 4.32e+01 MMD: 1.08e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.14e+04 logL: -1.13e+04 KL: 5.05e+01 MMD: 7.89e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 9.60e+03 logL: -9.54e+03 KL: 5.70e+01 MMD: 8.16e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 9.56e+03 logL: -9.51e+03 KL: 5.02e+01 MMD: 8.68e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 9.56e+03 logL: -9.52e+03 KL: 4.39e+01 MMD: 8.80e-01\n",
      "Stopping\n",
      "====> Epoch: 121 VALIDATION Loss: 9.56e+03 logL: -9.51e+03 KL: 4.32e+01 MMD: 8.45e-01\n",
      "config 47, alpha = 0.0, lambda = 12.5, dropout = 0.00; 2 hidden layers with 149, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.47e+04 logL: -1.38e+04 KL: 8.41e+02 MMD: 1.09e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+04 logL: -1.36e+04 KL: 2.96e+02 MMD: 1.27e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 1.56e+02 MMD: 1.30e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 9.93e+01 MMD: 1.28e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.74e+03 logL: -9.59e+03 KL: 1.38e+02 MMD: 1.21e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.58e+03 logL: -9.48e+03 KL: 9.00e+01 MMD: 1.20e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.51e+03 logL: -9.42e+03 KL: 7.43e+01 MMD: 1.28e+00\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.47e+03 logL: -9.38e+03 KL: 7.11e+01 MMD: 1.31e+00\n",
      "Stopping\n",
      "====> Epoch: 83 VALIDATION Loss: 9.46e+03 logL: -9.38e+03 KL: 6.98e+01 MMD: 1.29e+00\n",
      "config 47, alpha = 0.0, lambda = 1078.4, dropout = 0.00; 2 hidden layers with 83, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.36e+04 logL: -1.18e+04 KL: 3.94e+02 MMD: 1.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.00e+03 logL: -5.23e+03 KL: 2.04e+02 MMD: 1.46e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.67e+03 logL: -5.14e+03 KL: 1.38e+02 MMD: 1.29e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 6.89e+03 logL: -5.08e+03 KL: 1.24e+02 MMD: 1.57e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.09e+03 logL: -3.64e+03 KL: 1.15e+02 MMD: 1.24e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.02e+03 logL: -3.52e+03 KL: 9.81e+01 MMD: 1.31e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.39e+03 logL: -2.92e+03 KL: 9.05e+01 MMD: 1.28e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.16e+03 logL: -2.75e+03 KL: 7.00e+01 MMD: 1.24e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.99e+03 logL: -3.14e+03 KL: 4.16e+01 MMD: 7.57e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 3.20e+03 logL: -2.91e+03 KL: 4.00e+01 MMD: 2.30e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 3.02e+03 logL: -2.80e+03 KL: 3.98e+01 MMD: 1.63e-01\n",
      "Epoch 00113: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 2.82e+03 logL: -2.71e+03 KL: 3.94e+01 MMD: 7.04e-02\n",
      "Epoch 00124: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 2.82e+03 logL: -2.69e+03 KL: 3.95e+01 MMD: 8.69e-02\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 136 VALIDATION Loss: 2.84e+03 logL: -2.69e+03 KL: 3.96e+01 MMD: 1.05e-01\n",
      "config 47, alpha = 0.0, lambda = 11.1, dropout = 0.00; 2 hidden layers with 18, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.46e+04 logL: -1.39e+04 KL: 6.92e+02 MMD: 1.79e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.37e+04 logL: -1.34e+04 KL: 2.48e+02 MMD: 1.82e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.01e+04 logL: -9.88e+03 KL: 2.13e+02 MMD: 1.87e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.46e+03 logL: -7.26e+03 KL: 1.78e+02 MMD: 1.60e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.40e+03 logL: -5.23e+03 KL: 1.58e+02 MMD: 1.65e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.27e+03 logL: -3.11e+03 KL: 1.49e+02 MMD: 1.64e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.94e+03 logL: -2.81e+03 KL: 1.17e+02 MMD: 1.65e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.93e+03 logL: -2.81e+03 KL: 1.00e+02 MMD: 1.59e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.92e+03 logL: -2.81e+03 KL: 8.89e+01 MMD: 1.72e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.54e+03 logL: -2.43e+03 KL: 8.77e+01 MMD: 1.66e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.27e+03 logL: -2.17e+03 KL: 8.18e+01 MMD: 1.65e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.23e+03 logL: -2.14e+03 KL: 7.81e+01 MMD: 1.65e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.20e+03 logL: -2.11e+03 KL: 7.21e+01 MMD: 1.71e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.19e+03 logL: -2.11e+03 KL: 6.88e+01 MMD: 1.64e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.20e+03 logL: -2.12e+03 KL: 6.42e+01 MMD: 1.65e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 2.20e+03 logL: -2.12e+03 KL: 6.36e+01 MMD: 1.65e+00\n",
      "Epoch 00160: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 170 VALIDATION Loss: 2.15e+03 logL: -2.07e+03 KL: 6.27e+01 MMD: 1.46e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 2.14e+03 logL: -2.06e+03 KL: 6.25e+01 MMD: 1.61e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 2.09e+03 logL: -2.01e+03 KL: 6.52e+01 MMD: 1.51e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 1.81e+03 logL: -1.71e+03 KL: 8.04e+01 MMD: 1.65e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 1.81e+03 logL: -1.71e+03 KL: 8.04e+01 MMD: 1.65e+00\n",
      "config 47, alpha = 0.0, lambda = 4.5, dropout = 0.00; 2 hidden layers with 13, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.39e+04 logL: -1.35e+04 KL: 3.72e+02 MMD: 3.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.83e+03 logL: -9.62e+03 KL: 2.07e+02 MMD: 2.32e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.39e+03 logL: -7.24e+03 KL: 1.43e+02 MMD: 2.30e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.24e+03 logL: -7.13e+03 KL: 1.06e+02 MMD: 2.51e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.21e+03 logL: -7.12e+03 KL: 8.88e+01 MMD: 2.62e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.16e+03 logL: -7.07e+03 KL: 8.03e+01 MMD: 2.80e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.62e+03 logL: -5.53e+03 KL: 8.79e+01 MMD: 2.51e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.49e+03 logL: -5.40e+03 KL: 7.69e+01 MMD: 2.55e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.70e+03 logL: -3.61e+03 KL: 8.12e+01 MMD: 2.54e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.68e+03 logL: -3.60e+03 KL: 7.06e+01 MMD: 2.46e+00\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 3.66e+03 logL: -3.58e+03 KL: 7.00e+01 MMD: 2.62e+00\n",
      "Stopping\n",
      "====> Epoch: 111 VALIDATION Loss: 3.66e+03 logL: -3.58e+03 KL: 7.00e+01 MMD: 2.56e+00\n",
      "config 48, alpha = 0.0, lambda = 32.9, dropout = 0.00; 2 hidden layers with 5, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.44e+05 logL: -3.43e+05 KL: 1.50e+03 MMD: 5.38e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.31e+05 logL: -3.30e+05 KL: 1.06e+03 MMD: 5.50e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.13e+05 logL: -3.13e+05 KL: 7.47e+02 MMD: 6.83e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.20e+04 logL: -8.94e+04 KL: 2.54e+03 MMD: 6.08e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 3.90e+04 logL: -3.61e+04 KL: 2.84e+03 MMD: 7.32e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 3.16e+04 logL: -2.93e+04 KL: 2.23e+03 MMD: 6.54e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 2.70e+04 logL: -2.52e+04 KL: 1.79e+03 MMD: 6.53e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.81e+04 logL: -1.63e+04 KL: 1.79e+03 MMD: 6.37e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.66e+04 logL: -1.54e+04 KL: 1.15e+03 MMD: 6.64e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.45e+04 logL: -1.40e+04 KL: 5.59e+02 MMD: 7.17e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.35e+04 logL: -1.32e+04 KL: 3.16e+02 MMD: 6.58e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.13e+04 logL: -1.10e+04 KL: 2.69e+02 MMD: 6.72e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 1.04e+04 logL: -1.02e+04 KL: 1.86e+02 MMD: 6.79e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 9.73e+03 logL: -9.56e+03 KL: 1.45e+02 MMD: 6.80e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 9.65e+03 logL: -9.50e+03 KL: 1.18e+02 MMD: 7.23e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 9.65e+03 logL: -9.53e+03 KL: 9.77e+01 MMD: 6.74e-01\n",
      "====> Epoch: 170 VALIDATION Loss: 9.64e+03 logL: -9.53e+03 KL: 8.84e+01 MMD: 7.58e-01\n",
      "====> Epoch: 180 VALIDATION Loss: 9.61e+03 logL: -9.51e+03 KL: 7.47e+01 MMD: 6.70e-01\n",
      "Epoch 00181: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 190 VALIDATION Loss: 9.57e+03 logL: -9.47e+03 KL: 7.41e+01 MMD: 6.67e-01\n",
      "Stopping\n",
      "====> Epoch: 192 VALIDATION Loss: 9.57e+03 logL: -9.47e+03 KL: 7.32e+01 MMD: 7.03e-01\n",
      "config 48, alpha = 0.0, lambda = 67.5, dropout = 0.00; 2 hidden layers with 9, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.38e+04 logL: -1.34e+04 KL: 4.03e+02 MMD: 1.15e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.01e+04 logL: -9.74e+03 KL: 2.49e+02 MMD: 1.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.37e+03 logL: -7.13e+03 KL: 1.71e+02 MMD: 1.07e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.30e+03 logL: -7.12e+03 KL: 1.08e+02 MMD: 1.04e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.26e+03 logL: -7.11e+03 KL: 8.14e+01 MMD: 1.01e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.27e+03 logL: -7.12e+03 KL: 6.80e+01 MMD: 1.19e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.05e+03 logL: -5.90e+03 KL: 8.29e+01 MMD: 1.01e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.31e+03 logL: -5.17e+03 KL: 6.97e+01 MMD: 1.07e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.29e+03 logL: -5.16e+03 KL: 6.02e+01 MMD: 1.04e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.29e+03 logL: -5.16e+03 KL: 5.45e+01 MMD: 1.09e+00\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 5.26e+03 logL: -5.14e+03 KL: 5.25e+01 MMD: 1.01e+00\n",
      "Stopping\n",
      "====> Epoch: 113 VALIDATION Loss: 5.26e+03 logL: -5.14e+03 KL: 5.22e+01 MMD: 1.06e+00\n",
      "config 48, alpha = 0.0, lambda = 105.1, dropout = 0.00; 2 hidden layers with 74, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.02e+04 logL: -1.89e+04 KL: 1.14e+03 MMD: 1.37e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.71e+04 logL: -1.63e+04 KL: 6.87e+02 MMD: 1.65e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.55e+04 logL: -1.49e+04 KL: 4.46e+02 MMD: 1.55e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.31e+04 logL: -1.26e+04 KL: 2.83e+02 MMD: 1.46e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.30e+03 logL: -8.96e+03 KL: 2.06e+02 MMD: 1.30e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.58e+03 logL: -6.26e+03 KL: 1.91e+02 MMD: 1.26e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.00e+03 logL: -5.69e+03 KL: 1.69e+02 MMD: 1.34e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.72e+03 logL: -5.43e+03 KL: 1.41e+02 MMD: 1.37e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.55e+03 logL: -5.28e+03 KL: 1.21e+02 MMD: 1.40e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 100 VALIDATION Loss: 5.47e+03 logL: -5.22e+03 KL: 1.06e+02 MMD: 1.41e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.95e+03 logL: -3.67e+03 KL: 1.43e+02 MMD: 1.40e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.78e+03 logL: -3.54e+03 KL: 1.02e+02 MMD: 1.31e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.73e+03 logL: -3.48e+03 KL: 9.02e+01 MMD: 1.45e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.68e+03 logL: -3.47e+03 KL: 8.28e+01 MMD: 1.23e+00\n",
      "Epoch 00142: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 3.62e+03 logL: -3.40e+03 KL: 8.04e+01 MMD: 1.33e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 3.62e+03 logL: -3.40e+03 KL: 8.00e+01 MMD: 1.33e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 3.59e+03 logL: -3.37e+03 KL: 8.10e+01 MMD: 1.39e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 3.23e+03 logL: -2.96e+03 KL: 1.22e+02 MMD: 1.40e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 3.01e+03 logL: -2.77e+03 KL: 1.05e+02 MMD: 1.34e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 2.96e+03 logL: -2.73e+03 KL: 9.25e+01 MMD: 1.34e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 2.96e+03 logL: -2.73e+03 KL: 9.25e+01 MMD: 1.34e+00\n",
      "config 48, alpha = 0.0, lambda = 4.8, dropout = 0.00; 2 hidden layers with 159, 32 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.83e+03 logL: -5.47e+03 KL: 3.57e+02 MMD: 1.62e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.02e+03 logL: -2.82e+03 KL: 1.90e+02 MMD: 1.72e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.81e+03 logL: -1.65e+03 KL: 1.48e+02 MMD: 1.79e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.74e+03 logL: -1.61e+03 KL: 1.19e+02 MMD: 1.73e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.70e+03 logL: -1.59e+03 KL: 1.02e+02 MMD: 1.56e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.71e+03 logL: -1.61e+03 KL: 9.31e+01 MMD: 1.74e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.63e+03 logL: -1.54e+03 KL: 8.58e+01 MMD: 1.63e+00\n",
      "Epoch 00071: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.59e+03 logL: -1.50e+03 KL: 8.31e+01 MMD: 1.60e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.59e+03 logL: -1.50e+03 KL: 7.98e+01 MMD: 1.59e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.58e+03 logL: -1.49e+03 KL: 7.62e+01 MMD: 1.52e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.56e+03 logL: -1.49e+03 KL: 7.39e+01 MMD: 1.45e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.56e+03 logL: -1.48e+03 KL: 7.21e+01 MMD: 1.66e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.55e+03 logL: -1.48e+03 KL: 7.07e+01 MMD: 1.58e+00\n",
      "Epoch 00133: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 140 VALIDATION Loss: 1.54e+03 logL: -1.47e+03 KL: 6.97e+01 MMD: 1.56e+00\n",
      "Epoch 00142: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 144 VALIDATION Loss: 1.54e+03 logL: -1.46e+03 KL: 6.97e+01 MMD: 1.72e+00\n",
      "config 48, alpha = 0.0, lambda = 1631.2, dropout = 0.00; 2 hidden layers with 73, 72 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.93e+03 logL: -3.16e+03 KL: 3.48e+02 MMD: 2.10e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.28e+03 logL: -2.13e+03 KL: 2.05e+02 MMD: 1.81e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.45e+03 logL: -1.20e+03 KL: 1.61e+02 MMD: 1.90e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.07e+03 logL: -8.96e+02 KL: 1.38e+02 MMD: 1.86e+00\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.09e+03 logL: -8.46e+02 KL: 1.05e+02 MMD: 1.93e+00\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.14e+03 logL: -8.49e+02 KL: 9.55e+01 MMD: 1.96e+00\n",
      "Stopping\n",
      "====> Epoch: 61 VALIDATION Loss: 4.26e+03 logL: -8.48e+02 KL: 9.54e+01 MMD: 2.04e+00\n",
      "config 49, alpha = 0.0, lambda = 31.9, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.50e+04 logL: -1.37e+04 KL: 1.28e+03 MMD: 7.14e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.41e+04 logL: -1.35e+04 KL: 4.87e+02 MMD: 7.42e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 2.56e+02 MMD: 7.60e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.00e+04 logL: -9.68e+03 KL: 3.02e+02 MMD: 6.96e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.77e+03 logL: -9.57e+03 KL: 1.79e+02 MMD: 7.08e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.72e+03 logL: -9.57e+03 KL: 1.30e+02 MMD: 6.91e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.65e+03 logL: -9.53e+03 KL: 1.01e+02 MMD: 7.42e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.63e+03 logL: -9.52e+03 KL: 8.38e+01 MMD: 7.72e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.63e+03 logL: -9.54e+03 KL: 7.08e+01 MMD: 7.13e-01\n",
      "Stopping\n",
      "====> Epoch: 93 VALIDATION Loss: 9.61e+03 logL: -9.52e+03 KL: 6.79e+01 MMD: 6.98e-01\n",
      "config 49, alpha = 0.0, lambda = 748.9, dropout = 0.00; 2 hidden layers with 7, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.77e+04 logL: -1.51e+04 KL: 1.86e+03 MMD: 1.07e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.63e+04 logL: -1.46e+04 KL: 8.82e+02 MMD: 1.05e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.57e+04 logL: -1.45e+04 KL: 4.87e+02 MMD: 1.01e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.55e+04 logL: -1.44e+04 KL: 3.03e+02 MMD: 1.09e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.51e+04 logL: -1.42e+04 KL: 2.22e+02 MMD: 9.85e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.47e+04 logL: -1.37e+04 KL: 1.94e+02 MMD: 1.01e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.13e+04 logL: -1.03e+04 KL: 2.71e+02 MMD: 1.01e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.08e+04 logL: -9.83e+03 KL: 1.96e+02 MMD: 1.01e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.07e+04 logL: -9.65e+03 KL: 1.50e+02 MMD: 1.16e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.05e+03 logL: -8.17e+03 KL: 1.32e+02 MMD: 1.00e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 8.14e+03 logL: -7.22e+03 KL: 1.18e+02 MMD: 1.07e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 8.11e+03 logL: -7.11e+03 KL: 1.05e+02 MMD: 1.19e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 6.55e+03 logL: -5.59e+03 KL: 1.21e+02 MMD: 1.12e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 6.11e+03 logL: -5.27e+03 KL: 1.06e+02 MMD: 9.88e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 6.09e+03 logL: -5.17e+03 KL: 9.36e+01 MMD: 1.10e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 5.92e+03 logL: -5.15e+03 KL: 8.41e+01 MMD: 9.19e-01\n",
      "Epoch 00166: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 170 VALIDATION Loss: 6.02e+03 logL: -5.12e+03 KL: 7.88e+01 MMD: 1.10e+00\n",
      "Epoch 00179: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 180 VALIDATION Loss: 6.01e+03 logL: -5.11e+03 KL: 7.71e+01 MMD: 1.10e+00\n",
      "Stopping\n",
      "====> Epoch: 183 VALIDATION Loss: 5.92e+03 logL: -5.11e+03 KL: 7.71e+01 MMD: 9.81e-01\n",
      "config 49, alpha = 0.0, lambda = 1067.7, dropout = 0.00; 2 hidden layers with 107, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.61e+04 logL: -1.37e+04 KL: 8.19e+02 MMD: 1.53e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.26e+04 logL: -1.07e+04 KL: 4.95e+02 MMD: 1.39e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.13e+04 logL: -9.58e+03 KL: 2.51e+02 MMD: 1.33e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.83e+03 logL: -7.19e+03 KL: 2.18e+02 MMD: 1.34e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.73e+03 logL: -7.07e+03 KL: 1.66e+02 MMD: 1.40e+00\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 8.57e+03 logL: -7.01e+03 KL: 1.51e+02 MMD: 1.33e+00\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 69 VALIDATION Loss: 8.64e+03 logL: -7.00e+03 KL: 1.50e+02 MMD: 1.40e+00\n",
      "config 49, alpha = 0.0, lambda = 5.8, dropout = 0.00; 2 hidden layers with 19, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.50e+03 logL: -9.02e+03 KL: 4.73e+02 MMD: 1.58e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.36e+03 logL: -7.17e+03 KL: 1.81e+02 MMD: 1.98e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.25e+03 logL: -7.13e+03 KL: 1.16e+02 MMD: 1.91e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.46e+03 logL: -5.34e+03 KL: 1.07e+02 MMD: 1.69e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.25e+03 logL: -5.15e+03 KL: 8.69e+01 MMD: 1.78e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.72e+03 logL: -3.62e+03 KL: 9.31e+01 MMD: 1.68e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.66e+03 logL: -3.58e+03 KL: 7.58e+01 MMD: 1.86e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.64e+03 logL: -3.56e+03 KL: 6.90e+01 MMD: 1.99e+00\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.63e+03 logL: -3.55e+03 KL: 6.80e+01 MMD: 1.85e+00\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 94 VALIDATION Loss: 3.62e+03 logL: -3.54e+03 KL: 6.80e+01 MMD: 1.84e+00\n",
      "config 49, alpha = 0.0, lambda = 30.8, dropout = 0.00; 2 hidden layers with 95, 70 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 3.41e+03 logL: -3.03e+03 KL: 3.14e+02 MMD: 1.96e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.98e+03 logL: -1.72e+03 KL: 2.03e+02 MMD: 1.92e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.42e+03 logL: -1.21e+03 KL: 1.51e+02 MMD: 1.91e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.11e+03 logL: -9.25e+02 KL: 1.25e+02 MMD: 1.95e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.06e+03 logL: -8.96e+02 KL: 1.10e+02 MMD: 1.78e+00\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.97e+02 logL: -8.41e+02 KL: 1.01e+02 MMD: 1.83e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.93e+02 logL: -8.42e+02 KL: 9.55e+01 MMD: 1.88e+00\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.87e+02 logL: -8.36e+02 KL: 9.29e+01 MMD: 1.95e+00\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 89 VALIDATION Loss: 9.86e+02 logL: -8.35e+02 KL: 9.23e+01 MMD: 1.97e+00\n",
      "config 50, alpha = 0.0, lambda = 288.8, dropout = 0.00; 2 hidden layers with 11, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.51e+04 logL: -1.36e+04 KL: 1.23e+03 MMD: 7.68e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.42e+04 logL: -1.35e+04 KL: 4.47e+02 MMD: 6.98e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.40e+04 logL: -1.35e+04 KL: 2.28e+02 MMD: 7.59e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 1.40e+02 MMD: 6.81e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.38e+04 logL: -1.34e+04 KL: 1.02e+02 MMD: 7.91e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.37e+04 logL: -1.34e+04 KL: 8.48e+01 MMD: 6.56e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.22e+04 logL: -1.18e+04 KL: 1.65e+02 MMD: 7.35e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.00e+04 logL: -9.62e+03 KL: 1.84e+02 MMD: 7.54e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.84e+03 logL: -9.51e+03 KL: 1.26e+02 MMD: 7.19e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 9.75e+03 logL: -9.45e+03 KL: 9.28e+01 MMD: 7.26e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 9.71e+03 logL: -9.42e+03 KL: 8.18e+01 MMD: 7.28e-01\n",
      "Epoch 00114: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 9.69e+03 logL: -9.39e+03 KL: 7.82e+01 MMD: 7.73e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 9.68e+03 logL: -9.38e+03 KL: 7.59e+01 MMD: 7.68e-01\n",
      "Epoch 00135: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 137 VALIDATION Loss: 9.69e+03 logL: -9.38e+03 KL: 7.51e+01 MMD: 7.90e-01\n",
      "config 50, alpha = 0.0, lambda = 96.2, dropout = 0.00; 2 hidden layers with 25, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.93e+03 logL: -7.46e+03 KL: 3.75e+02 MMD: 9.22e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.89e+03 logL: -5.57e+03 KL: 2.18e+02 MMD: 1.00e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.71e+03 logL: -5.47e+03 KL: 1.41e+02 MMD: 1.06e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.49e+03 logL: -5.27e+03 KL: 1.08e+02 MMD: 1.09e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.36e+03 logL: -5.16e+03 KL: 9.33e+01 MMD: 1.12e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.33e+03 logL: -5.14e+03 KL: 8.53e+01 MMD: 1.07e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.30e+03 logL: -5.11e+03 KL: 7.68e+01 MMD: 1.17e+00\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 5.22e+03 logL: -5.05e+03 KL: 7.37e+01 MMD: 1.03e+00\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 86 VALIDATION Loss: 5.22e+03 logL: -5.05e+03 KL: 7.30e+01 MMD: 1.02e+00\n",
      "config 50, alpha = 0.0, lambda = 7415.8, dropout = 0.00; 2 hidden layers with 18, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.54e+04 logL: -1.39e+04 KL: 1.43e+03 MMD: 1.36e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.05e+04 logL: -9.94e+03 KL: 4.75e+02 MMD: 1.36e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.03e+04 logL: -9.68e+03 KL: 2.89e+02 MMD: 1.39e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.60e+04 logL: -6.03e+03 KL: 2.81e+02 MMD: 1.30e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.40e+04 logL: -4.17e+03 KL: 2.45e+02 MMD: 1.29e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+04 logL: -3.73e+03 KL: 1.89e+02 MMD: 1.29e+00\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: 1.33e+04 logL: -3.66e+03 KL: 1.73e+02 MMD: 1.27e+00\n",
      "config 50, alpha = 0.0, lambda = 195.4, dropout = 0.00; 2 hidden layers with 84, 27 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.45e+04 logL: -1.36e+04 KL: 6.65e+02 MMD: 1.57e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.05e+04 logL: -9.88e+03 KL: 3.74e+02 MMD: 1.50e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.31e+03 logL: -5.67e+03 KL: 3.21e+02 MMD: 1.67e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.44e+03 logL: -3.87e+03 KL: 2.45e+02 MMD: 1.62e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.11e+03 logL: -3.60e+03 KL: 1.87e+02 MMD: 1.63e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.89e+03 logL: -2.40e+03 KL: 1.70e+02 MMD: 1.63e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.74e+03 logL: -2.28e+03 KL: 1.43e+02 MMD: 1.63e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.11e+03 logL: -1.67e+03 KL: 1.27e+02 MMD: 1.59e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.05e+03 logL: -1.64e+03 KL: 1.06e+02 MMD: 1.57e+00\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 2.00e+03 logL: -1.59e+03 KL: 1.00e+02 MMD: 1.60e+00\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 109 VALIDATION Loss: 2.01e+03 logL: -1.58e+03 KL: 9.88e+01 MMD: 1.71e+00\n",
      "config 50, alpha = 0.0, lambda = 605.6, dropout = 0.00; 2 hidden layers with 46, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.56e+03 logL: -6.01e+03 KL: 3.75e+02 MMD: 1.94e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.35e+03 logL: -2.95e+03 KL: 2.13e+02 MMD: 1.96e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.62e+03 logL: -2.23e+03 KL: 1.59e+02 MMD: 2.03e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.07e+03 logL: -1.77e+03 KL: 1.31e+02 MMD: 1.93e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.87e+03 logL: -1.62e+03 KL: 1.05e+02 MMD: 1.90e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.40e+03 logL: -1.33e+03 KL: 7.79e+01 MMD: 1.64e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.75e+03 logL: -1.28e+03 KL: 6.13e+01 MMD: 6.65e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.61e+03 logL: -1.33e+03 KL: 6.00e+01 MMD: 3.64e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.32e+03 logL: -1.10e+03 KL: 6.10e+01 MMD: 2.61e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.14e+03 logL: -9.35e+02 KL: 5.87e+01 MMD: 2.36e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 1.11e+03 logL: -9.44e+02 KL: 5.92e+01 MMD: 1.83e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 1.13e+03 logL: -9.68e+02 KL: 5.95e+01 MMD: 1.76e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 1.12e+03 logL: -9.81e+02 KL: 5.40e+01 MMD: 1.33e-01\n",
      "Epoch 00130: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 1.00e+03 logL: -8.61e+02 KL: 5.69e+01 MMD: 1.36e-01\n",
      "Epoch 00141: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 145 VALIDATION Loss: 9.92e+02 logL: -8.57e+02 KL: 5.65e+01 MMD: 1.31e-01\n",
      "config 51, alpha = 0.0, lambda = 1437.8, dropout = 0.00; 2 hidden layers with 34, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.17e+04 logL: -1.03e+04 KL: 4.32e+02 MMD: 6.91e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.06e+04 logL: -9.38e+03 KL: 1.89e+02 MMD: 6.94e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.05e+04 logL: -9.34e+03 KL: 1.46e+02 MMD: 7.10e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.99e+03 logL: -8.87e+03 KL: 1.07e+02 MMD: 7.06e-01\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 9.89e+03 logL: -8.74e+03 KL: 9.78e+01 MMD: 7.30e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.00e+04 logL: -8.79e+03 KL: 9.42e+01 MMD: 7.84e-01\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: 9.80e+03 logL: -8.71e+03 KL: 9.35e+01 MMD: 6.94e-01\n",
      "config 51, alpha = 0.0, lambda = 5.4, dropout = 0.00; 2 hidden layers with 17, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 2.79e+02 MMD: 1.20e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.80e+03 logL: -9.59e+03 KL: 2.00e+02 MMD: 1.13e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.59e+03 logL: -7.41e+03 KL: 1.73e+02 MMD: 1.07e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.32e+03 logL: -7.21e+03 KL: 1.12e+02 MMD: 1.16e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.22e+03 logL: -7.13e+03 KL: 8.49e+01 MMD: 1.20e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.23e+03 logL: -7.16e+03 KL: 7.03e+01 MMD: 1.23e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 70 VALIDATION Loss: 7.18e+03 logL: -7.11e+03 KL: 6.11e+01 MMD: 1.27e+00\n",
      "Stopping\n",
      "====> Epoch: 71 VALIDATION Loss: 7.19e+03 logL: -7.12e+03 KL: 6.10e+01 MMD: 1.22e+00\n",
      "config 51, alpha = 0.0, lambda = 13.8, dropout = 0.00; 2 hidden layers with 74, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.83e+03 logL: -9.50e+03 KL: 3.20e+02 MMD: 1.36e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.46e+03 logL: -5.27e+03 KL: 1.74e+02 MMD: 1.36e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.79e+03 logL: -3.65e+03 KL: 1.30e+02 MMD: 1.36e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.75e+03 logL: -3.64e+03 KL: 9.83e+01 MMD: 1.33e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.69e+03 logL: -3.59e+03 KL: 8.32e+01 MMD: 1.33e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.58e+03 logL: -3.49e+03 KL: 7.82e+01 MMD: 1.37e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.55e+03 logL: -3.46e+03 KL: 7.49e+01 MMD: 1.37e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.84e+03 logL: -2.75e+03 KL: 7.64e+01 MMD: 1.25e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.85e+03 logL: -2.76e+03 KL: 6.91e+01 MMD: 1.30e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.90e+03 logL: -2.81e+03 KL: 6.45e+01 MMD: 1.36e+00\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.72e+03 logL: -2.64e+03 KL: 6.18e+01 MMD: 1.36e+00\n",
      "Epoch 00118: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 2.71e+03 logL: -2.63e+03 KL: 6.13e+01 MMD: 1.29e+00\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 129 VALIDATION Loss: 2.71e+03 logL: -2.63e+03 KL: 6.13e+01 MMD: 1.35e+00\n",
      "config 51, alpha = 0.0, lambda = 405.2, dropout = 0.00; 2 hidden layers with 35, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.55e+04 logL: -1.41e+04 KL: 7.27e+02 MMD: 1.71e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.48e+04 logL: -1.37e+04 KL: 4.19e+02 MMD: 1.74e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.01e+04 logL: -9.00e+03 KL: 3.52e+02 MMD: 1.74e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.13e+03 logL: -7.23e+03 KL: 2.19e+02 MMD: 1.68e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.02e+03 logL: -7.14e+03 KL: 1.60e+02 MMD: 1.77e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.92e+03 logL: -7.10e+03 KL: 1.38e+02 MMD: 1.70e+00\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 7.82e+03 logL: -7.04e+03 KL: 1.29e+02 MMD: 1.61e+00\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 7.91e+03 logL: -7.04e+03 KL: 1.28e+02 MMD: 1.84e+00\n",
      "Stopping\n",
      "====> Epoch: 80 VALIDATION Loss: 7.91e+03 logL: -7.04e+03 KL: 1.28e+02 MMD: 1.84e+00\n",
      "config 51, alpha = 0.0, lambda = 3.5, dropout = 0.00; 2 hidden layers with 17, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.72e+04 logL: -1.61e+04 KL: 1.13e+03 MMD: 2.26e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.63e+04 logL: -1.57e+04 KL: 5.79e+02 MMD: 2.44e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.57e+04 logL: -1.53e+04 KL: 3.58e+02 MMD: 2.64e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.34e+04 logL: -1.31e+04 KL: 2.74e+02 MMD: 2.54e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.12e+04 logL: -1.09e+04 KL: 2.32e+02 MMD: 2.27e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.03e+04 logL: -1.01e+04 KL: 1.82e+02 MMD: 2.04e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.48e+03 logL: -7.28e+03 KL: 1.90e+02 MMD: 1.96e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.38e+03 logL: -6.20e+03 KL: 1.78e+02 MMD: 2.21e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.80e+03 logL: -4.62e+03 KL: 1.78e+02 MMD: 1.91e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.17e+03 logL: -4.00e+03 KL: 1.66e+02 MMD: 2.01e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.65e+03 logL: -3.49e+03 KL: 1.53e+02 MMD: 1.90e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.88e+03 logL: -2.74e+03 KL: 1.39e+02 MMD: 1.89e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.37e+03 logL: -2.25e+03 KL: 1.23e+02 MMD: 1.93e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.29e+03 logL: -2.18e+03 KL: 1.09e+02 MMD: 1.85e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.25e+03 logL: -2.14e+03 KL: 9.64e+01 MMD: 2.01e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 2.21e+03 logL: -2.11e+03 KL: 8.89e+01 MMD: 2.05e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 2.20e+03 logL: -2.11e+03 KL: 8.26e+01 MMD: 2.10e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 2.17e+03 logL: -2.09e+03 KL: 7.98e+01 MMD: 1.93e+00\n",
      "Epoch 00186: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 190 VALIDATION Loss: 2.16e+03 logL: -2.07e+03 KL: 7.74e+01 MMD: 1.94e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 2.16e+03 logL: -2.07e+03 KL: 7.66e+01 MMD: 2.00e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 2.16e+03 logL: -2.07e+03 KL: 7.66e+01 MMD: 2.00e+00\n",
      "config 52, alpha = 0.0, lambda = 18806.0, dropout = 0.00; 2 hidden layers with 8, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.77e+04 logL: -1.36e+04 KL: 5.46e+02 MMD: 7.20e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 2.57e+04 logL: -1.21e+04 KL: 6.02e+02 MMD: 6.90e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.30e+04 logL: -9.61e+03 KL: 3.80e+02 MMD: 6.93e-01\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.20e+04 logL: -9.57e+03 KL: 3.53e+02 MMD: 6.40e-01\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.27e+04 logL: -9.56e+03 KL: 3.43e+02 MMD: 6.80e-01\n",
      "Stopping\n",
      "====> Epoch: 50 VALIDATION Loss: 2.27e+04 logL: -9.56e+03 KL: 3.43e+02 MMD: 6.80e-01\n",
      "config 52, alpha = 0.0, lambda = 3.4, dropout = 0.00; 2 hidden layers with 16, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.01e+04 logL: -9.64e+03 KL: 4.35e+02 MMD: 9.98e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 8.53e+03 logL: -8.27e+03 KL: 2.59e+02 MMD: 1.11e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.73e+03 logL: -5.55e+03 KL: 1.82e+02 MMD: 1.05e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.48e+03 logL: -5.35e+03 KL: 1.24e+02 MMD: 1.03e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.31e+03 logL: -5.21e+03 KL: 9.42e+01 MMD: 1.04e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.29e+03 logL: -5.21e+03 KL: 7.76e+01 MMD: 1.08e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.24e+03 logL: -5.17e+03 KL: 6.81e+01 MMD: 9.90e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 5.20e+03 logL: -5.14e+03 KL: 6.20e+01 MMD: 9.85e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 5.25e+03 logL: -5.19e+03 KL: 5.91e+01 MMD: 1.10e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.19e+03 logL: -5.13e+03 KL: 5.62e+01 MMD: 1.03e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.13e+03 logL: -5.07e+03 KL: 5.48e+01 MMD: 1.03e+00\n",
      "Epoch 00114: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 5.08e+03 logL: -5.02e+03 KL: 5.36e+01 MMD: 1.06e+00\n",
      "Epoch 00121: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 125 VALIDATION Loss: 5.08e+03 logL: -5.02e+03 KL: 5.35e+01 MMD: 1.03e+00\n",
      "config 52, alpha = 0.0, lambda = 4468.9, dropout = 0.00; 2 hidden layers with 41, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.13e+04 logL: -1.40e+04 KL: 7.68e+02 MMD: 1.45e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.83e+04 logL: -1.23e+04 KL: 4.08e+02 MMD: 1.25e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.61e+04 logL: -9.72e+03 KL: 3.31e+02 MMD: 1.35e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.59e+04 logL: -9.60e+03 KL: 2.46e+02 MMD: 1.35e+00\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 46 VALIDATION Loss: 1.62e+04 logL: -9.54e+03 KL: 2.26e+02 MMD: 1.45e+00\n",
      "config 52, alpha = 0.0, lambda = 4.4, dropout = 0.00; 2 hidden layers with 26, 21 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.68e+04 logL: -1.50e+04 KL: 1.84e+03 MMD: 1.63e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.51e+04 logL: -1.45e+04 KL: 5.56e+02 MMD: 1.73e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.45e+04 logL: -1.42e+04 KL: 2.84e+02 MMD: 1.92e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.25e+04 logL: -1.23e+04 KL: 2.11e+02 MMD: 1.78e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.10e+04 logL: -1.09e+04 KL: 1.67e+02 MMD: 1.82e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.65e+03 logL: -9.45e+03 KL: 1.96e+02 MMD: 1.71e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.24e+03 logL: -8.05e+03 KL: 1.77e+02 MMD: 1.66e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.36e+03 logL: -6.19e+03 KL: 1.65e+02 MMD: 1.83e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.94e+03 logL: -4.78e+03 KL: 1.56e+02 MMD: 1.62e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.17e+03 logL: -4.02e+03 KL: 1.40e+02 MMD: 1.57e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.92e+03 logL: -3.79e+03 KL: 1.27e+02 MMD: 1.64e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.20e+03 logL: -3.07e+03 KL: 1.22e+02 MMD: 1.59e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 130 VALIDATION Loss: 2.56e+03 logL: -2.44e+03 KL: 1.14e+02 MMD: 1.60e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.39e+03 logL: -2.28e+03 KL: 1.05e+02 MMD: 1.61e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.29e+03 logL: -2.19e+03 KL: 9.56e+01 MMD: 1.66e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 2.23e+03 logL: -2.14e+03 KL: 8.86e+01 MMD: 1.77e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 2.22e+03 logL: -2.13e+03 KL: 8.30e+01 MMD: 1.67e+00\n",
      "Epoch 00178: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 180 VALIDATION Loss: 2.17e+03 logL: -2.09e+03 KL: 7.75e+01 MMD: 1.63e+00\n",
      "Stopping\n",
      "====> Epoch: 189 VALIDATION Loss: 2.17e+03 logL: -2.09e+03 KL: 7.66e+01 MMD: 1.51e+00\n",
      "config 52, alpha = 0.0, lambda = 241.6, dropout = 0.00; 2 hidden layers with 44, 31 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.44e+05 logL: -3.38e+05 KL: 4.96e+03 MMD: 1.98e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.17e+05 logL: -3.11e+05 KL: 5.40e+03 MMD: 1.87e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.66e+05 logL: -2.61e+05 KL: 4.10e+03 MMD: 2.08e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.72e+04 logL: -8.16e+04 KL: 5.01e+03 MMD: 2.06e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.06e+04 logL: -1.66e+04 KL: 3.56e+03 MMD: 1.89e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.55e+04 logL: -1.31e+04 KL: 1.94e+03 MMD: 1.93e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.42e+04 logL: -1.26e+04 KL: 1.09e+03 MMD: 1.97e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.28e+04 logL: -1.17e+04 KL: 7.13e+02 MMD: 1.70e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.06e+04 logL: -9.60e+03 KL: 5.31e+02 MMD: 1.94e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 7.76e+03 logL: -6.81e+03 KL: 4.36e+02 MMD: 2.11e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.67e+03 logL: -4.84e+03 KL: 3.60e+02 MMD: 1.94e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.36e+03 logL: -3.60e+03 KL: 2.92e+02 MMD: 1.93e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.80e+03 logL: -3.11e+03 KL: 2.31e+02 MMD: 1.94e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.05e+03 logL: -2.43e+03 KL: 1.94e+02 MMD: 1.77e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.84e+03 logL: -2.19e+03 KL: 1.63e+02 MMD: 2.02e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 2.63e+03 logL: -2.04e+03 KL: 1.43e+02 MMD: 1.87e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 2.52e+03 logL: -1.93e+03 KL: 1.30e+02 MMD: 1.91e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 2.13e+03 logL: -1.54e+03 KL: 1.35e+02 MMD: 1.87e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 2.02e+03 logL: -1.43e+03 KL: 1.25e+02 MMD: 1.95e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 1.87e+03 logL: -1.30e+03 KL: 1.13e+02 MMD: 1.91e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 1.87e+03 logL: -1.30e+03 KL: 1.13e+02 MMD: 1.91e+00\n",
      "config 53, alpha = 0.0, lambda = 211.7, dropout = 0.00; 2 hidden layers with 177, 29 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.86e+03 logL: -9.51e+03 KL: 2.04e+02 MMD: 6.97e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.81e+03 logL: -9.54e+03 KL: 1.09e+02 MMD: 7.62e-01\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 9.60e+03 logL: -9.35e+03 KL: 9.07e+01 MMD: 7.22e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.58e+03 logL: -9.35e+03 KL: 8.35e+01 MMD: 7.24e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.51e+03 logL: -9.28e+03 KL: 7.78e+01 MMD: 7.49e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.44e+03 logL: -9.21e+03 KL: 7.13e+01 MMD: 7.95e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.37e+03 logL: -9.15e+03 KL: 6.88e+01 MMD: 7.43e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.19e+03 logL: -8.97e+03 KL: 6.61e+01 MMD: 7.49e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.07e+03 logL: -8.86e+03 KL: 6.61e+01 MMD: 6.84e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 8.88e+03 logL: -8.67e+03 KL: 6.64e+01 MMD: 6.99e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 8.85e+03 logL: -8.62e+03 KL: 6.21e+01 MMD: 7.91e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 8.72e+03 logL: -8.50e+03 KL: 6.32e+01 MMD: 7.42e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 8.67e+03 logL: -8.46e+03 KL: 6.23e+01 MMD: 7.29e-01\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00139: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 140 VALIDATION Loss: 8.61e+03 logL: -8.39e+03 KL: 6.02e+01 MMD: 7.79e-01\n",
      "Stopping\n",
      "====> Epoch: 143 VALIDATION Loss: 8.62e+03 logL: -8.39e+03 KL: 6.03e+01 MMD: 8.07e-01\n",
      "config 53, alpha = 0.0, lambda = 2188.1, dropout = 0.00; 2 hidden layers with 140, 30 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.66e+04 logL: -1.37e+04 KL: 7.17e+02 MMD: 9.96e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.24e+04 logL: -9.91e+03 KL: 3.49e+02 MMD: 1.00e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.01e+04 logL: -7.74e+03 KL: 2.64e+02 MMD: 9.53e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 7.84e+03 logL: -5.43e+03 KL: 2.31e+02 MMD: 9.96e-01\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 7.68e+03 logL: -5.31e+03 KL: 1.86e+02 MMD: 9.99e-01\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 7.63e+03 logL: -5.29e+03 KL: 1.81e+02 MMD: 9.85e-01\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: 7.74e+03 logL: -5.29e+03 KL: 1.80e+02 MMD: 1.04e+00\n",
      "config 53, alpha = 0.0, lambda = 56.0, dropout = 0.00; 2 hidden layers with 16, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.48e+04 logL: -1.38e+04 KL: 9.51e+02 MMD: 1.33e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.41e+04 logL: -1.36e+04 KL: 3.99e+02 MMD: 1.58e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.39e+04 logL: -1.36e+04 KL: 2.14e+02 MMD: 1.66e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 1.38e+02 MMD: 1.81e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.03e+04 logL: -1.01e+04 KL: 1.66e+02 MMD: 1.47e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.74e+03 logL: -9.54e+03 KL: 1.11e+02 MMD: 1.73e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.72e+03 logL: -9.54e+03 KL: 8.91e+01 MMD: 1.68e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 7.71e+03 logL: -7.53e+03 KL: 1.01e+02 MMD: 1.46e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 7.27e+03 logL: -7.09e+03 KL: 8.73e+01 MMD: 1.56e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 7.23e+03 logL: -7.06e+03 KL: 7.67e+01 MMD: 1.63e+00\n",
      "Epoch 00106: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 7.22e+03 logL: -7.06e+03 KL: 7.17e+01 MMD: 1.62e+00\n",
      "Epoch 00117: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 7.20e+03 logL: -7.05e+03 KL: 7.11e+01 MMD: 1.48e+00\n",
      "Stopping\n",
      "====> Epoch: 121 VALIDATION Loss: 7.21e+03 logL: -7.05e+03 KL: 7.12e+01 MMD: 1.65e+00\n",
      "config 53, alpha = 0.0, lambda = 1255.5, dropout = 0.00; 2 hidden layers with 100, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.65e+04 logL: -1.37e+04 KL: 3.56e+02 MMD: 1.96e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.60e+04 logL: -1.35e+04 KL: 2.51e+02 MMD: 1.76e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.20e+04 logL: -9.62e+03 KL: 1.76e+02 MMD: 1.73e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.18e+04 logL: -9.54e+03 KL: 1.37e+02 MMD: 1.71e+00\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.11e+04 logL: -8.84e+03 KL: 1.19e+02 MMD: 1.70e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.95e+03 logL: -7.65e+03 KL: 1.32e+02 MMD: 1.73e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.57e+03 logL: -7.40e+03 KL: 1.29e+02 MMD: 1.63e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.24e+03 logL: -7.19e+03 KL: 1.23e+02 MMD: 1.53e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.15e+03 logL: -7.06e+03 KL: 1.16e+02 MMD: 1.57e+00\n",
      "Epoch 00099: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 9.40e+03 logL: -7.02e+03 KL: 1.11e+02 MMD: 1.81e+00\n",
      "Stopping\n",
      "====> Epoch: 103 VALIDATION Loss: 9.21e+03 logL: -7.02e+03 KL: 1.10e+02 MMD: 1.66e+00\n",
      "config 53, alpha = 0.0, lambda = 1.2, dropout = 0.00; 2 hidden layers with 71, 27 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.89e+03 logL: -3.64e+03 KL: 2.55e+02 MMD: 1.86e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.47e+03 logL: -2.32e+03 KL: 1.54e+02 MMD: 2.01e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.08e+03 logL: -1.96e+03 KL: 1.22e+02 MMD: 1.87e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.88e+03 logL: -1.77e+03 KL: 1.04e+02 MMD: 2.05e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.67e+03 logL: -1.58e+03 KL: 9.37e+01 MMD: 2.16e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.24e+03 logL: -1.15e+03 KL: 9.10e+01 MMD: 2.08e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.19e+03 logL: -1.10e+03 KL: 8.35e+01 MMD: 1.82e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 80 VALIDATION Loss: 1.17e+03 logL: -1.09e+03 KL: 7.89e+01 MMD: 1.88e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.16e+03 logL: -1.09e+03 KL: 7.38e+01 MMD: 1.91e+00\n",
      "Epoch 00098: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.14e+03 logL: -1.07e+03 KL: 7.04e+01 MMD: 1.83e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.14e+03 logL: -1.07e+03 KL: 6.97e+01 MMD: 1.92e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.13e+03 logL: -1.07e+03 KL: 6.78e+01 MMD: 1.93e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.13e+03 logL: -1.06e+03 KL: 6.66e+01 MMD: 2.06e+00\n",
      "Epoch 00136: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 140 VALIDATION Loss: 1.13e+03 logL: -1.06e+03 KL: 6.57e+01 MMD: 1.95e+00\n",
      "Epoch 00144: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 147 VALIDATION Loss: 1.13e+03 logL: -1.06e+03 KL: 6.56e+01 MMD: 1.91e+00\n",
      "config 54, alpha = 0.0, lambda = 68.8, dropout = 0.00; 2 hidden layers with 36, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.40e+04 logL: -1.35e+04 KL: 4.47e+02 MMD: 7.67e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.94e+03 logL: -9.62e+03 KL: 2.78e+02 MMD: 6.73e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.78e+03 logL: -9.60e+03 KL: 1.30e+02 MMD: 7.35e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.68e+03 logL: -9.54e+03 KL: 9.19e+01 MMD: 7.19e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.94e+03 logL: -9.81e+03 KL: 7.82e+01 MMD: 7.82e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.56e+03 logL: -9.44e+03 KL: 6.42e+01 MMD: 8.53e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.54e+03 logL: -9.43e+03 KL: 5.90e+01 MMD: 7.73e-01\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.49e+03 logL: -9.38e+03 KL: 5.63e+01 MMD: 7.88e-01\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 83 VALIDATION Loss: 9.49e+03 logL: -9.38e+03 KL: 5.61e+01 MMD: 7.97e-01\n",
      "config 54, alpha = 0.0, lambda = 35.8, dropout = 0.00; 2 hidden layers with 13, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.03e+04 logL: -9.75e+03 KL: 4.78e+02 MMD: 9.79e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.77e+03 logL: -9.54e+03 KL: 1.85e+02 MMD: 1.10e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.71e+03 logL: -9.55e+03 KL: 1.18e+02 MMD: 1.18e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.41e+03 logL: -7.26e+03 KL: 1.10e+02 MMD: 1.06e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.21e+03 logL: -7.09e+03 KL: 8.47e+01 MMD: 1.04e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.68e+03 logL: -5.55e+03 KL: 9.76e+01 MMD: 1.06e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.60e+03 logL: -5.49e+03 KL: 7.50e+01 MMD: 1.16e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.52e+03 logL: -5.42e+03 KL: 6.53e+01 MMD: 1.17e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.39e+03 logL: -5.29e+03 KL: 5.93e+01 MMD: 1.06e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.30e+03 logL: -5.21e+03 KL: 5.36e+01 MMD: 1.05e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.20e+03 logL: -5.11e+03 KL: 5.12e+01 MMD: 1.06e+00\n",
      "Epoch 00119: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 5.16e+03 logL: -5.08e+03 KL: 4.77e+01 MMD: 1.09e+00\n",
      "Epoch 00127: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 5.16e+03 logL: -5.07e+03 KL: 4.72e+01 MMD: 1.13e+00\n",
      "Stopping\n",
      "====> Epoch: 130 VALIDATION Loss: 5.16e+03 logL: -5.07e+03 KL: 4.72e+01 MMD: 1.13e+00\n",
      "config 54, alpha = 0.0, lambda = 2973.7, dropout = 0.00; 2 hidden layers with 20, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.81e+04 logL: -1.33e+04 KL: 5.57e+02 MMD: 1.41e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.24e+04 logL: -8.14e+03 KL: 3.41e+02 MMD: 1.31e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.76e+03 logL: -5.64e+03 KL: 2.34e+02 MMD: 1.31e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.22e+03 logL: -3.73e+03 KL: 1.97e+02 MMD: 1.45e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.78e+03 logL: -3.62e+03 KL: 1.50e+02 MMD: 1.35e+00\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 54 VALIDATION Loss: 7.93e+03 logL: -3.60e+03 KL: 1.47e+02 MMD: 1.41e+00\n",
      "config 54, alpha = 0.0, lambda = 17.8, dropout = 0.00; 2 hidden layers with 11, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.95e+04 logL: -1.76e+04 KL: 1.94e+03 MMD: 1.71e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.79e+04 logL: -1.67e+04 KL: 1.13e+03 MMD: 1.80e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.65e+04 logL: -1.58e+04 KL: 6.58e+02 MMD: 1.80e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.54e+04 logL: -1.49e+04 KL: 4.45e+02 MMD: 1.76e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.44e+04 logL: -1.41e+04 KL: 2.71e+02 MMD: 2.02e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.40e+04 logL: -1.38e+04 KL: 1.95e+02 MMD: 2.06e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.38e+04 logL: -1.36e+04 KL: 1.50e+02 MMD: 2.09e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.23e+02 MMD: 1.93e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.91e+03 logL: -9.76e+03 KL: 1.18e+02 MMD: 1.92e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 7.73e+03 logL: -7.57e+03 KL: 1.29e+02 MMD: 1.81e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 7.38e+03 logL: -7.24e+03 KL: 1.09e+02 MMD: 1.76e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.71e+03 logL: -5.58e+03 KL: 1.01e+02 MMD: 1.78e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.99e+03 logL: -3.84e+03 KL: 1.26e+02 MMD: 1.58e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.79e+03 logL: -3.66e+03 KL: 1.03e+02 MMD: 1.50e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 3.75e+03 logL: -3.63e+03 KL: 8.91e+01 MMD: 1.64e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 3.62e+03 logL: -3.51e+03 KL: 8.12e+01 MMD: 1.92e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 3.12e+03 logL: -3.01e+03 KL: 8.54e+01 MMD: 1.58e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 2.98e+03 logL: -2.87e+03 KL: 8.05e+01 MMD: 1.82e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 2.95e+03 logL: -2.84e+03 KL: 7.42e+01 MMD: 1.71e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 2.86e+03 logL: -2.76e+03 KL: 7.02e+01 MMD: 1.80e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 2.86e+03 logL: -2.76e+03 KL: 7.02e+01 MMD: 1.80e+00\n",
      "config 54, alpha = 0.0, lambda = 1297.9, dropout = 0.00; 2 hidden layers with 83, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.02e+04 logL: -7.36e+03 KL: 4.01e+02 MMD: 1.91e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.53e+03 logL: -3.67e+03 KL: 2.32e+02 MMD: 2.03e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.36e+03 logL: -2.74e+03 KL: 1.80e+02 MMD: 1.88e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.36e+03 logL: -1.64e+03 KL: 1.53e+02 MMD: 1.98e+00\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.23e+03 logL: -1.58e+03 KL: 1.32e+02 MMD: 1.94e+00\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.13e+03 logL: -1.57e+03 KL: 1.26e+02 MMD: 1.88e+00\n",
      "Stopping\n",
      "====> Epoch: 61 VALIDATION Loss: 4.18e+03 logL: -1.57e+03 KL: 1.26e+02 MMD: 1.91e+00\n",
      "config 55, alpha = 0.0, lambda = 13.7, dropout = 0.00; 2 hidden layers with 139, 44 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.64e+03 logL: -9.45e+03 KL: 1.77e+02 MMD: 7.36e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.36e+03 logL: -9.25e+03 KL: 9.41e+01 MMD: 7.73e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.27e+03 logL: -9.19e+03 KL: 7.69e+01 MMD: 7.05e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.12e+03 logL: -9.06e+03 KL: 4.86e+01 MMD: 7.74e-01\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 8.71e+03 logL: -8.63e+03 KL: 6.42e+01 MMD: 7.64e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 8.68e+03 logL: -8.61e+03 KL: 5.78e+01 MMD: 7.96e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.66e+03 logL: -8.59e+03 KL: 6.03e+01 MMD: 7.82e-01\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 76 VALIDATION Loss: 8.68e+03 logL: -8.61e+03 KL: 6.08e+01 MMD: 7.96e-01\n",
      "config 55, alpha = 0.0, lambda = 65.6, dropout = 0.00; 2 hidden layers with 18, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.77e+03 logL: -9.36e+03 KL: 3.44e+02 MMD: 1.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.38e+03 logL: -7.15e+03 KL: 1.71e+02 MMD: 9.30e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.55e+03 logL: -5.36e+03 KL: 1.33e+02 MMD: 1.01e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.36e+03 logL: -5.19e+03 KL: 1.04e+02 MMD: 1.05e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.29e+03 logL: -5.14e+03 KL: 8.62e+01 MMD: 1.02e+00\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 5.25e+03 logL: -5.10e+03 KL: 8.13e+01 MMD: 1.06e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 70 VALIDATION Loss: 5.25e+03 logL: -5.10e+03 KL: 7.95e+01 MMD: 9.93e-01\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 5.24e+03 logL: -5.09e+03 KL: 7.85e+01 MMD: 1.05e+00\n",
      "config 55, alpha = 0.0, lambda = 14.3, dropout = 0.00; 2 hidden layers with 90, 67 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.02e+03 logL: -2.80e+03 KL: 2.01e+02 MMD: 1.41e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.87e+03 logL: -2.74e+03 KL: 1.12e+02 MMD: 1.49e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.79e+03 logL: -2.68e+03 KL: 8.69e+01 MMD: 1.38e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.68e+03 logL: -2.59e+03 KL: 7.47e+01 MMD: 1.35e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.66e+03 logL: -2.57e+03 KL: 6.56e+01 MMD: 1.27e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.61e+03 logL: -2.53e+03 KL: 5.99e+01 MMD: 1.37e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.61e+03 logL: -2.54e+03 KL: 5.58e+01 MMD: 1.33e+00\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 2.52e+03 logL: -2.44e+03 KL: 5.59e+01 MMD: 1.32e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.51e+03 logL: -2.44e+03 KL: 5.49e+01 MMD: 1.33e+00\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 99 VALIDATION Loss: 2.52e+03 logL: -2.45e+03 KL: 5.37e+01 MMD: 1.33e+00\n",
      "config 55, alpha = 0.0, lambda = 71.1, dropout = 0.00; 2 hidden layers with 133, 26 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.06e+03 logL: -7.53e+03 KL: 4.15e+02 MMD: 1.63e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.00e+03 logL: -3.67e+03 KL: 2.13e+02 MMD: 1.62e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.19e+03 logL: -2.93e+03 KL: 1.54e+02 MMD: 1.56e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.43e+03 logL: -2.19e+03 KL: 1.29e+02 MMD: 1.59e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.37e+03 logL: -2.14e+03 KL: 1.06e+02 MMD: 1.69e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.33e+03 logL: -2.13e+03 KL: 9.41e+01 MMD: 1.55e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.79e+03 logL: -1.59e+03 KL: 8.90e+01 MMD: 1.69e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.76e+03 logL: -1.56e+03 KL: 8.49e+01 MMD: 1.66e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.80e+03 logL: -1.61e+03 KL: 8.13e+01 MMD: 1.56e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.76e+03 logL: -1.57e+03 KL: 7.69e+01 MMD: 1.54e+00\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.65e+03 logL: -1.46e+03 KL: 7.30e+01 MMD: 1.55e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.64e+03 logL: -1.46e+03 KL: 7.03e+01 MMD: 1.50e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.64e+03 logL: -1.47e+03 KL: 6.79e+01 MMD: 1.48e+00\n",
      "Epoch 00131: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 140 VALIDATION Loss: 1.63e+03 logL: -1.46e+03 KL: 6.70e+01 MMD: 1.50e+00\n",
      "Stopping\n",
      "====> Epoch: 145 VALIDATION Loss: 1.64e+03 logL: -1.45e+03 KL: 6.69e+01 MMD: 1.64e+00\n",
      "config 55, alpha = 0.0, lambda = 9997.5, dropout = 0.00; 2 hidden layers with 17, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.11e+04 logL: -1.16e+04 KL: 7.90e+02 MMD: 1.87e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.57e+04 logL: -6.40e+03 KL: 4.19e+02 MMD: 1.88e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.36e+04 logL: -3.76e+03 KL: 2.85e+02 MMD: 1.96e+00\n",
      "Stopping\n",
      "====> Epoch: 38 VALIDATION Loss: 2.39e+04 logL: -3.19e+03 KL: 2.34e+02 MMD: 2.05e+00\n",
      "config 56, alpha = 0.0, lambda = 5.5, dropout = 0.00; 2 hidden layers with 9, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.41e+04 logL: -1.36e+04 KL: 5.05e+02 MMD: 7.46e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 1.92e+02 MMD: 8.21e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.97e+03 logL: -9.78e+03 KL: 1.86e+02 MMD: 6.42e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.65e+03 logL: -9.55e+03 KL: 1.00e+02 MMD: 7.38e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.62e+03 logL: -9.55e+03 KL: 6.54e+01 MMD: 7.24e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.58e+03 logL: -9.53e+03 KL: 5.06e+01 MMD: 7.17e-01\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.56e+03 logL: -9.51e+03 KL: 4.49e+01 MMD: 7.61e-01\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 76 VALIDATION Loss: 9.56e+03 logL: -9.51e+03 KL: 4.43e+01 MMD: 8.31e-01\n",
      "config 56, alpha = 0.0, lambda = 6.3, dropout = 0.00; 2 hidden layers with 12, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.44e+04 logL: -1.36e+04 KL: 7.84e+02 MMD: 1.15e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.15e+04 logL: -1.10e+04 KL: 5.23e+02 MMD: 1.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.78e+03 logL: -9.56e+03 KL: 2.11e+02 MMD: 1.09e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.66e+03 logL: -9.53e+03 KL: 1.31e+02 MMD: 1.11e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.34e+03 logL: -7.18e+03 KL: 1.49e+02 MMD: 1.04e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.38e+03 logL: -7.28e+03 KL: 9.89e+01 MMD: 1.09e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.21e+03 logL: -7.12e+03 KL: 8.25e+01 MMD: 1.05e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.79e+03 logL: -5.68e+03 KL: 1.04e+02 MMD: 9.73e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 5.63e+03 logL: -5.55e+03 KL: 7.52e+01 MMD: 1.19e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.58e+03 logL: -5.50e+03 KL: 6.59e+01 MMD: 1.07e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.50e+03 logL: -5.44e+03 KL: 5.81e+01 MMD: 1.08e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.32e+03 logL: -5.26e+03 KL: 5.58e+01 MMD: 1.11e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 5.26e+03 logL: -5.20e+03 KL: 5.14e+01 MMD: 1.06e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 5.23e+03 logL: -5.18e+03 KL: 4.89e+01 MMD: 1.07e+00\n",
      "Epoch 00140: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 5.21e+03 logL: -5.15e+03 KL: 4.81e+01 MMD: 1.12e+00\n",
      "Stopping\n",
      "====> Epoch: 151 VALIDATION Loss: 5.21e+03 logL: -5.15e+03 KL: 4.79e+01 MMD: 1.16e+00\n",
      "config 56, alpha = 0.0, lambda = 1604.7, dropout = 0.00; 2 hidden layers with 45, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.21e+03 logL: -6.83e+03 KL: 3.44e+02 MMD: 1.27e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.84e+03 logL: -5.26e+03 KL: 1.94e+02 MMD: 1.49e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.12e+03 logL: -3.83e+03 KL: 1.65e+02 MMD: 1.32e+00\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 38 VALIDATION Loss: 5.93e+03 logL: -3.58e+03 KL: 1.48e+02 MMD: 1.37e+00\n",
      "config 56, alpha = 0.0, lambda = 1.1, dropout = 0.00; 2 hidden layers with 155, 23 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.74e+03 logL: -3.49e+03 KL: 2.51e+02 MMD: 1.56e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.37e+03 logL: -2.23e+03 KL: 1.42e+02 MMD: 1.61e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.16e+03 logL: -2.05e+03 KL: 1.17e+02 MMD: 1.63e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.76e+03 logL: -1.65e+03 KL: 1.05e+02 MMD: 1.75e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.69e+03 logL: -1.59e+03 KL: 9.40e+01 MMD: 1.59e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.66e+03 logL: -1.57e+03 KL: 8.87e+01 MMD: 1.68e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.60e+03 logL: -1.52e+03 KL: 8.13e+01 MMD: 1.63e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.62e+03 logL: -1.54e+03 KL: 7.78e+01 MMD: 1.67e+00\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.54e+03 logL: -1.46e+03 KL: 7.25e+01 MMD: 1.64e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.53e+03 logL: -1.46e+03 KL: 7.10e+01 MMD: 1.69e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.53e+03 logL: -1.46e+03 KL: 6.86e+01 MMD: 1.71e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.52e+03 logL: -1.45e+03 KL: 6.64e+01 MMD: 1.56e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.52e+03 logL: -1.45e+03 KL: 6.50e+01 MMD: 1.68e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.51e+03 logL: -1.45e+03 KL: 6.35e+01 MMD: 1.59e+00\n",
      "Epoch 00149: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 150 VALIDATION Loss: 1.50e+03 logL: -1.44e+03 KL: 6.24e+01 MMD: 1.53e+00\n",
      "Epoch 00156: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 160 VALIDATION Loss: 1.50e+03 logL: -1.44e+03 KL: 6.23e+01 MMD: 1.61e+00\n",
      "Stopping\n",
      "====> Epoch: 160 VALIDATION Loss: 1.50e+03 logL: -1.44e+03 KL: 6.23e+01 MMD: 1.61e+00\n",
      "config 56, alpha = 0.0, lambda = 12389.9, dropout = 0.00; 2 hidden layers with 70, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.78e+04 logL: -1.38e+04 KL: 4.67e+02 MMD: 1.90e+00\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 18 VALIDATION Loss: 3.83e+04 logL: -1.36e+04 KL: 4.09e+02 MMD: 1.96e+00\n",
      "config 57, alpha = 0.0, lambda = 378.5, dropout = 0.00; 2 hidden layers with 155, 24 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 9.88e+03 logL: -9.46e+03 KL: 1.69e+02 MMD: 6.60e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.60e+03 logL: -9.23e+03 KL: 1.11e+02 MMD: 6.96e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.06e+03 logL: -8.68e+03 KL: 9.43e+01 MMD: 7.59e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 8.73e+03 logL: -8.36e+03 KL: 7.85e+01 MMD: 7.49e-01\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 8.63e+03 logL: -8.25e+03 KL: 6.42e+01 MMD: 8.28e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 8.55e+03 logL: -8.20e+03 KL: 5.96e+01 MMD: 7.85e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.48e+03 logL: -8.15e+03 KL: 5.90e+01 MMD: 7.36e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.45e+03 logL: -8.10e+03 KL: 5.76e+01 MMD: 7.59e-01\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 90 VALIDATION Loss: 8.44e+03 logL: -8.10e+03 KL: 5.65e+01 MMD: 7.65e-01\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 98 VALIDATION Loss: 8.44e+03 logL: -8.10e+03 KL: 5.65e+01 MMD: 7.64e-01\n",
      "config 57, alpha = 0.0, lambda = 142.9, dropout = 0.00; 2 hidden layers with 26, 24 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.01e+03 logL: -7.56e+03 KL: 2.92e+02 MMD: 1.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.66e+03 logL: -5.35e+03 KL: 1.70e+02 MMD: 1.00e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.47e+03 logL: -5.20e+03 KL: 1.17e+02 MMD: 1.06e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.37e+03 logL: -5.13e+03 KL: 9.28e+01 MMD: 1.03e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.36e+03 logL: -5.12e+03 KL: 7.71e+01 MMD: 1.10e+00\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 5.30e+03 logL: -5.08e+03 KL: 7.58e+01 MMD: 1.02e+00\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.31e+03 logL: -5.08e+03 KL: 7.50e+01 MMD: 1.09e+00\n",
      "Stopping\n",
      "====> Epoch: 72 VALIDATION Loss: 5.31e+03 logL: -5.08e+03 KL: 7.50e+01 MMD: 1.08e+00\n",
      "config 57, alpha = 0.0, lambda = 5474.4, dropout = 0.00; 2 hidden layers with 12, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.71e+04 logL: -9.63e+03 KL: 5.37e+02 MMD: 1.26e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.32e+04 logL: -5.59e+03 KL: 2.79e+02 MMD: 1.34e+00\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 29 VALIDATION Loss: 1.26e+04 logL: -5.54e+03 KL: 2.18e+02 MMD: 1.25e+00\n",
      "config 57, alpha = 0.0, lambda = 71.3, dropout = 0.00; 2 hidden layers with 10, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.49e+04 logL: -1.40e+04 KL: 7.87e+02 MMD: 1.77e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.40e+04 logL: -1.35e+04 KL: 3.51e+02 MMD: 1.64e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.01e+04 logL: -9.72e+03 KL: 2.65e+02 MMD: 1.86e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.88e+03 logL: -9.58e+03 KL: 1.56e+02 MMD: 2.06e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.78e+03 logL: -9.53e+03 KL: 1.14e+02 MMD: 1.92e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.52e+03 logL: -7.27e+03 KL: 1.28e+02 MMD: 1.72e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.35e+03 logL: -7.13e+03 KL: 9.57e+01 MMD: 1.87e+00\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 7.32e+03 logL: -7.10e+03 KL: 8.27e+01 MMD: 1.90e+00\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 7.32e+03 logL: -7.10e+03 KL: 8.09e+01 MMD: 1.92e+00\n",
      "Stopping\n",
      "====> Epoch: 93 VALIDATION Loss: 7.31e+03 logL: -7.10e+03 KL: 8.08e+01 MMD: 1.78e+00\n",
      "config 57, alpha = 0.0, lambda = 6390.9, dropout = 0.00; 2 hidden layers with 15, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.36e+04 logL: -1.08e+04 KL: 6.39e+02 MMD: 1.89e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.01e+04 logL: -7.30e+03 KL: 3.31e+02 MMD: 1.95e+00\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.99e+04 logL: -7.14e+03 KL: 2.47e+02 MMD: 1.95e+00\n",
      "Stopping\n",
      "====> Epoch: 31 VALIDATION Loss: 2.02e+04 logL: -7.14e+03 KL: 2.45e+02 MMD: 2.00e+00\n",
      "config 58, alpha = 0.0, lambda = 278.8, dropout = 0.00; 2 hidden layers with 33, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.39e+04 logL: -1.32e+04 KL: 5.53e+02 MMD: 6.42e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.01e+04 logL: -9.62e+03 KL: 2.81e+02 MMD: 7.04e-01\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 9.90e+03 logL: -9.54e+03 KL: 1.64e+02 MMD: 6.84e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.88e+03 logL: -9.54e+03 KL: 1.52e+02 MMD: 7.00e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.89e+03 logL: -9.54e+03 KL: 1.36e+02 MMD: 7.60e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.85e+03 logL: -9.54e+03 KL: 1.22e+02 MMD: 6.83e-01\n",
      "Stopping\n",
      "====> Epoch: 64 VALIDATION Loss: 9.85e+03 logL: -9.55e+03 KL: 1.18e+02 MMD: 6.71e-01\n",
      "config 58, alpha = 0.0, lambda = 28480.2, dropout = 0.00; 2 hidden layers with 19, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.68e+04 logL: -1.40e+04 KL: 1.21e+03 MMD: 1.11e+00\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 4.32e+04 logL: -1.38e+04 KL: 9.95e+02 MMD: 9.99e-01\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 28 VALIDATION Loss: 4.27e+04 logL: -1.37e+04 KL: 9.62e+02 MMD: 9.84e-01\n",
      "config 58, alpha = 0.0, lambda = 58678.4, dropout = 0.00; 2 hidden layers with 13, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.52e+04 logL: -5.17e+04 KL: 2.12e+01 MMD: 5.71e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.98e+04 logL: -1.65e+04 KL: 1.81e+01 MMD: 5.61e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 1.71e+04 logL: -1.46e+04 KL: 1.91e+01 MMD: 4.20e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.85e+04 logL: -1.35e+04 KL: 2.03e+01 MMD: 8.49e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.34e+04 logL: -1.14e+04 KL: 2.18e+01 MMD: 3.36e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.27e+04 logL: -1.03e+04 KL: 2.34e+01 MMD: 4.01e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 1.22e+04 logL: -9.07e+03 KL: 2.25e+01 MMD: 5.24e-02\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.06e+04 logL: -8.74e+03 KL: 2.30e+01 MMD: 3.08e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 1.07e+04 logL: -8.74e+03 KL: 2.31e+01 MMD: 3.26e-02\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 97 VALIDATION Loss: 1.02e+04 logL: -8.72e+03 KL: 2.30e+01 MMD: 2.41e-02\n",
      "config 58, alpha = 0.0, lambda = 83.0, dropout = 0.00; 2 hidden layers with 132, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.37e+04 KL: 3.50e+02 MMD: 1.86e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+04 logL: -1.35e+04 KL: 2.01e+02 MMD: 2.07e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.49e+03 logL: -8.10e+03 KL: 2.32e+02 MMD: 1.86e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.40e+03 logL: -7.11e+03 KL: 1.34e+02 MMD: 1.89e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.30e+03 logL: -7.04e+03 KL: 1.06e+02 MMD: 1.86e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.91e+03 logL: -5.63e+03 KL: 1.41e+02 MMD: 1.73e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.81e+03 logL: -5.56e+03 KL: 9.70e+01 MMD: 1.86e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.86e+03 logL: -3.61e+03 KL: 1.07e+02 MMD: 1.74e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.77e+03 logL: -3.55e+03 KL: 9.19e+01 MMD: 1.63e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.49e+03 logL: -3.18e+03 KL: 1.78e+02 MMD: 1.57e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.08e+03 logL: -2.82e+03 KL: 1.31e+02 MMD: 1.61e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.03e+03 logL: -2.77e+03 KL: 1.14e+02 MMD: 1.79e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.96e+03 logL: -2.72e+03 KL: 1.01e+02 MMD: 1.68e+00\n",
      "Epoch 00134: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 2.91e+03 logL: -2.68e+03 KL: 9.27e+01 MMD: 1.63e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.91e+03 logL: -2.68e+03 KL: 8.60e+01 MMD: 1.70e+00\n",
      "Epoch 00152: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 160 VALIDATION Loss: 2.88e+03 logL: -2.68e+03 KL: 8.45e+01 MMD: 1.45e+00\n",
      "Epoch 00166: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 170 VALIDATION Loss: 2.90e+03 logL: -2.68e+03 KL: 8.38e+01 MMD: 1.67e+00\n",
      "Stopping\n",
      "====> Epoch: 170 VALIDATION Loss: 2.90e+03 logL: -2.68e+03 KL: 8.38e+01 MMD: 1.67e+00\n",
      "config 58, alpha = 0.0, lambda = 5.1, dropout = 0.00; 2 hidden layers with 195, 69 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 3.06e+03 logL: -2.77e+03 KL: 2.80e+02 MMD: 1.99e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.42e+03 logL: -1.22e+03 KL: 1.88e+02 MMD: 1.99e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.11e+03 logL: -9.50e+02 KL: 1.50e+02 MMD: 1.84e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.02e+03 logL: -8.81e+02 KL: 1.26e+02 MMD: 2.04e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.90e+02 logL: -8.70e+02 KL: 1.12e+02 MMD: 1.90e+00\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.02e+02 logL: -7.91e+02 KL: 1.03e+02 MMD: 2.03e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.90e+02 logL: -7.86e+02 KL: 9.68e+01 MMD: 1.83e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 8.86e+02 logL: -7.87e+02 KL: 9.13e+01 MMD: 1.86e+00\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 8.71e+02 logL: -7.77e+02 KL: 8.66e+01 MMD: 1.84e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 8.71e+02 logL: -7.77e+02 KL: 8.61e+01 MMD: 1.91e+00\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 109 VALIDATION Loss: 8.69e+02 logL: -7.76e+02 KL: 8.57e+01 MMD: 1.87e+00\n",
      "config 59, alpha = 0.0, lambda = 393.0, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.49e+04 logL: -1.45e+04 KL: 9.31e+01 MMD: 7.63e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.41e+04 logL: -1.38e+04 KL: 8.40e+01 MMD: 7.43e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.39e+04 logL: -1.35e+04 KL: 7.50e+01 MMD: 7.24e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 7.01e+01 MMD: 7.09e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 6.26e+01 MMD: 7.66e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.38e+04 logL: -1.34e+04 KL: 5.58e+01 MMD: 7.73e-01\n",
      "Stopping\n",
      "====> Epoch: 62 VALIDATION Loss: 1.38e+04 logL: -1.34e+04 KL: 5.51e+01 MMD: 7.61e-01\n",
      "config 59, alpha = 0.0, lambda = 2.4, dropout = 0.00; 2 hidden layers with 47, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.86e+03 logL: -9.55e+03 KL: 3.14e+02 MMD: 1.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.13e+03 logL: -7.95e+03 KL: 1.79e+02 MMD: 9.73e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.74e+03 logL: -5.55e+03 KL: 1.82e+02 MMD: 1.11e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.36e+03 logL: -5.24e+03 KL: 1.13e+02 MMD: 9.92e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 5.22e+03 logL: -5.13e+03 KL: 9.26e+01 MMD: 1.12e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.15e+03 logL: -5.07e+03 KL: 7.95e+01 MMD: 1.15e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.01e+03 logL: -4.94e+03 KL: 7.26e+01 MMD: 1.01e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.94e+03 logL: -4.87e+03 KL: 6.75e+01 MMD: 1.09e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.87e+03 logL: -4.80e+03 KL: 6.71e+01 MMD: 1.11e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.91e+03 logL: -4.85e+03 KL: 6.24e+01 MMD: 1.16e+00\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 4.80e+03 logL: -4.73e+03 KL: 6.37e+01 MMD: 1.22e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.79e+03 logL: -4.73e+03 KL: 6.35e+01 MMD: 1.10e+00\n",
      "Epoch 00121: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 4.79e+03 logL: -4.72e+03 KL: 6.37e+01 MMD: 1.05e+00\n",
      "Epoch 00131: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 132 VALIDATION Loss: 4.79e+03 logL: -4.72e+03 KL: 6.37e+01 MMD: 1.14e+00\n",
      "config 59, alpha = 0.0, lambda = 111.8, dropout = 0.00; 2 hidden layers with 136, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.35e+04 logL: -1.29e+04 KL: 3.38e+02 MMD: 1.55e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.00e+04 logL: -9.64e+03 KL: 1.98e+02 MMD: 1.63e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.86e+03 logL: -9.54e+03 KL: 1.22e+02 MMD: 1.74e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.77e+03 logL: -9.47e+03 KL: 9.85e+01 MMD: 1.85e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.58e+03 logL: -7.27e+03 KL: 1.18e+02 MMD: 1.73e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.39e+03 logL: -7.14e+03 KL: 8.00e+01 MMD: 1.59e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.16e+03 logL: -3.86e+03 KL: 1.45e+02 MMD: 1.39e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.86e+03 logL: -3.59e+03 KL: 1.16e+02 MMD: 1.37e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.78e+03 logL: -3.53e+03 KL: 9.76e+01 MMD: 1.40e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.04e+03 logL: -2.80e+03 KL: 8.84e+01 MMD: 1.37e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.98e+03 logL: -2.75e+03 KL: 7.56e+01 MMD: 1.41e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.96e+03 logL: -2.75e+03 KL: 6.82e+01 MMD: 1.31e+00\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 2.87e+03 logL: -2.66e+03 KL: 6.66e+01 MMD: 1.33e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.85e+03 logL: -2.65e+03 KL: 6.19e+01 MMD: 1.30e+00\n",
      "Epoch 00143: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 147 VALIDATION Loss: 2.85e+03 logL: -2.64e+03 KL: 6.02e+01 MMD: 1.34e+00\n",
      "config 59, alpha = 0.0, lambda = 2473.0, dropout = 0.00; 2 hidden layers with 41, 29 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.99e+03 logL: -3.59e+03 KL: 3.10e+02 MMD: 1.65e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.59e+03 logL: -2.82e+03 KL: 1.85e+02 MMD: 1.45e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.50e+03 logL: -2.24e+03 KL: 1.52e+02 MMD: 1.66e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.84e+03 logL: -1.65e+03 KL: 1.33e+02 MMD: 1.64e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.77e+03 logL: -1.61e+03 KL: 1.04e+02 MMD: 1.64e+00\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 5.77e+03 logL: -1.62e+03 KL: 7.00e+01 MMD: 1.65e+00\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.15e+03 logL: -1.66e+03 KL: 6.31e+01 MMD: 1.39e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.09e+03 logL: -1.68e+03 KL: 6.06e+01 MMD: 1.36e+00\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 86 VALIDATION Loss: 5.04e+03 logL: -1.68e+03 KL: 5.99e+01 MMD: 1.33e+00\n",
      "config 59, alpha = 0.0, lambda = 1632.4, dropout = 0.00; 2 hidden layers with 15, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.77e+04 logL: -1.36e+04 KL: 7.07e+02 MMD: 2.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.35e+04 logL: -9.84e+03 KL: 4.16e+02 MMD: 1.99e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.02e+04 logL: -6.62e+03 KL: 3.49e+02 MMD: 1.99e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.94e+03 logL: -5.73e+03 KL: 2.37e+02 MMD: 1.82e+00\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 9.08e+03 logL: -5.62e+03 KL: 2.14e+02 MMD: 1.99e+00\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 58 VALIDATION Loss: 9.08e+03 logL: -5.60e+03 KL: 2.11e+02 MMD: 2.00e+00\n",
      "config 60, alpha = 0.0, lambda = 58118.6, dropout = 0.00; 2 hidden layers with 8, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.26e+04 logL: -1.32e+04 KL: 1.02e+03 MMD: 6.59e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.86e+04 logL: -9.63e+03 KL: 5.54e+02 MMD: 6.61e-01\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 28 VALIDATION Loss: 5.22e+04 logL: -9.55e+03 KL: 5.24e+02 MMD: 7.25e-01\n",
      "config 60, alpha = 0.0, lambda = 399.1, dropout = 0.00; 2 hidden layers with 161, 55 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.05e+03 logL: -7.27e+03 KL: 3.67e+02 MMD: 1.02e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.13e+03 logL: -5.49e+03 KL: 2.18e+02 MMD: 1.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.73e+03 logL: -5.14e+03 KL: 1.65e+02 MMD: 1.08e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.68e+03 logL: -5.11e+03 KL: 1.46e+02 MMD: 1.06e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.61e+03 logL: -5.03e+03 KL: 1.37e+02 MMD: 1.10e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.46e+03 logL: -4.97e+03 KL: 1.31e+02 MMD: 9.09e-01\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.26e+03 logL: -4.69e+03 KL: 1.21e+02 MMD: 1.13e+00\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 5.19e+03 logL: -4.67e+03 KL: 1.13e+02 MMD: 1.03e+00\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 90 VALIDATION Loss: 5.18e+03 logL: -4.66e+03 KL: 1.12e+02 MMD: 1.00e+00\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 100 VALIDATION Loss: 5.20e+03 logL: -4.66e+03 KL: 1.12e+02 MMD: 1.06e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping\n",
      "====> Epoch: 101 VALIDATION Loss: 5.20e+03 logL: -4.66e+03 KL: 1.11e+02 MMD: 1.06e+00\n",
      "config 60, alpha = 0.0, lambda = 39.5, dropout = 0.00; 2 hidden layers with 140, 43 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.09e+03 logL: -2.83e+03 KL: 2.10e+02 MMD: 1.36e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.96e+03 logL: -2.78e+03 KL: 1.25e+02 MMD: 1.42e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.81e+03 logL: -2.66e+03 KL: 9.85e+01 MMD: 1.34e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.82e+03 logL: -2.69e+03 KL: 8.58e+01 MMD: 1.32e+00\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.72e+03 logL: -2.59e+03 KL: 8.04e+01 MMD: 1.33e+00\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.72e+03 logL: -2.59e+03 KL: 7.86e+01 MMD: 1.34e+00\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 68 VALIDATION Loss: 2.71e+03 logL: -2.58e+03 KL: 7.83e+01 MMD: 1.32e+00\n",
      "config 60, alpha = 0.0, lambda = 206.3, dropout = 0.00; 2 hidden layers with 14, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.58e+04 logL: -1.37e+04 KL: 1.71e+03 MMD: 1.72e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.45e+04 logL: -1.37e+04 KL: 5.29e+02 MMD: 1.69e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.43e+04 logL: -1.36e+04 KL: 2.79e+02 MMD: 1.78e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.37e+04 logL: -1.31e+04 KL: 2.20e+02 MMD: 1.71e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.84e+03 logL: -9.27e+03 KL: 2.27e+02 MMD: 1.67e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.38e+03 logL: -7.86e+03 KL: 1.77e+02 MMD: 1.68e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.63e+03 logL: -6.10e+03 KL: 1.79e+02 MMD: 1.68e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.52e+03 logL: -4.00e+03 KL: 1.87e+02 MMD: 1.65e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.20e+03 logL: -3.68e+03 KL: 1.61e+02 MMD: 1.77e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.59e+03 logL: -3.11e+03 KL: 1.48e+02 MMD: 1.60e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.41e+03 logL: -2.94e+03 KL: 1.26e+02 MMD: 1.65e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.31e+03 logL: -2.84e+03 KL: 1.08e+02 MMD: 1.77e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.22e+03 logL: -2.78e+03 KL: 9.67e+01 MMD: 1.66e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.70e+03 logL: -2.25e+03 KL: 1.02e+02 MMD: 1.68e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.61e+03 logL: -2.17e+03 KL: 8.77e+01 MMD: 1.70e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 2.55e+03 logL: -2.14e+03 KL: 7.82e+01 MMD: 1.62e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 2.54e+03 logL: -2.14e+03 KL: 7.10e+01 MMD: 1.61e+00\n",
      "Epoch 00170: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00178: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 180 VALIDATION Loss: 2.53e+03 logL: -2.11e+03 KL: 6.97e+01 MMD: 1.68e+00\n",
      "Stopping\n",
      "====> Epoch: 182 VALIDATION Loss: 2.48e+03 logL: -2.11e+03 KL: 6.97e+01 MMD: 1.45e+00\n",
      "config 60, alpha = 0.0, lambda = 1170.3, dropout = 0.00; 2 hidden layers with 29, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.83e+03 logL: -7.31e+03 KL: 4.05e+02 MMD: 1.81e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.04e+03 logL: -4.62e+03 KL: 2.58e+02 MMD: 1.85e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.13e+03 logL: -3.63e+03 KL: 1.76e+02 MMD: 1.99e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.51e+03 logL: -3.14e+03 KL: 1.54e+02 MMD: 1.90e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.57e+03 logL: -2.29e+03 KL: 1.35e+02 MMD: 1.84e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.64e+03 logL: -2.23e+03 KL: 9.21e+01 MMD: 1.98e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.31e+03 logL: -2.42e+03 KL: 5.83e+01 MMD: 7.09e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 2.76e+03 logL: -2.33e+03 KL: 5.96e+01 MMD: 3.17e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 2.74e+03 logL: -2.32e+03 KL: 6.00e+01 MMD: 3.02e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 2.08e+03 logL: -1.75e+03 KL: 5.93e+01 MMD: 2.32e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 2.01e+03 logL: -1.73e+03 KL: 5.78e+01 MMD: 1.94e-01\n",
      "Epoch 00115: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 1.88e+03 logL: -1.62e+03 KL: 5.82e+01 MMD: 1.70e-01\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.83e+03 logL: -1.61e+03 KL: 5.81e+01 MMD: 1.38e-01\n",
      "Epoch 00135: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 139 VALIDATION Loss: 1.85e+03 logL: -1.61e+03 KL: 5.80e+01 MMD: 1.60e-01\n",
      "config 61, alpha = 0.0, lambda = 5.7, dropout = 0.00; 2 hidden layers with 168, 47 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.70e+03 logL: -9.52e+03 KL: 1.72e+02 MMD: 6.85e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.10e+03 logL: -8.99e+03 KL: 1.05e+02 MMD: 6.67e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 8.54e+03 logL: -8.46e+03 KL: 7.16e+01 MMD: 7.40e-01\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 8.99e+03 logL: -8.92e+03 KL: 6.30e+01 MMD: 7.64e-01\n",
      "Stopping\n",
      "====> Epoch: 43 VALIDATION Loss: 8.84e+03 logL: -8.78e+03 KL: 6.00e+01 MMD: 8.01e-01\n",
      "config 61, alpha = 0.0, lambda = 32105.1, dropout = 0.00; 2 hidden layers with 55, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.28e+04 logL: -6.73e+03 KL: 3.64e+02 MMD: 1.11e+00\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 3.86e+04 logL: -5.24e+03 KL: 2.74e+02 MMD: 1.03e+00\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.92e+04 logL: -5.22e+03 KL: 2.66e+02 MMD: 1.05e+00\n",
      "Stopping\n",
      "====> Epoch: 31 VALIDATION Loss: 3.85e+04 logL: -5.22e+03 KL: 2.66e+02 MMD: 1.03e+00\n",
      "config 61, alpha = 0.0, lambda = 33.7, dropout = 0.00; 2 hidden layers with 31, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.97e+03 logL: -6.58e+03 KL: 3.40e+02 MMD: 1.34e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.14e+03 logL: -3.85e+03 KL: 2.49e+02 MMD: 1.31e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.76e+03 logL: -3.60e+03 KL: 1.22e+02 MMD: 1.40e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.99e+03 logL: -2.82e+03 KL: 1.23e+02 MMD: 1.41e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.86e+03 logL: -2.73e+03 KL: 8.78e+01 MMD: 1.29e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.84e+03 logL: -2.73e+03 KL: 7.50e+01 MMD: 1.32e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.85e+03 logL: -2.74e+03 KL: 6.73e+01 MMD: 1.38e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.92e+03 logL: -2.82e+03 KL: 6.04e+01 MMD: 1.31e+00\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 2.80e+03 logL: -2.70e+03 KL: 5.86e+01 MMD: 1.38e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.80e+03 logL: -2.69e+03 KL: 5.76e+01 MMD: 1.42e+00\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 105 VALIDATION Loss: 2.80e+03 logL: -2.69e+03 KL: 5.74e+01 MMD: 1.40e+00\n",
      "config 61, alpha = 0.0, lambda = 42.5, dropout = 0.00; 2 hidden layers with 52, 48 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.09e+03 logL: -3.73e+03 KL: 2.91e+02 MMD: 1.69e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.28e+03 logL: -2.04e+03 KL: 1.78e+02 MMD: 1.64e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.88e+03 logL: -1.68e+03 KL: 1.35e+02 MMD: 1.64e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.83e+03 logL: -1.64e+03 KL: 1.12e+02 MMD: 1.71e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.75e+03 logL: -1.59e+03 KL: 9.82e+01 MMD: 1.55e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.75e+03 logL: -1.59e+03 KL: 8.68e+01 MMD: 1.71e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.72e+03 logL: -1.57e+03 KL: 7.99e+01 MMD: 1.74e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.69e+03 logL: -1.55e+03 KL: 7.37e+01 MMD: 1.67e+00\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.63e+03 logL: -1.49e+03 KL: 7.22e+01 MMD: 1.63e+00\n",
      "Stopping\n",
      "====> Epoch: 92 VALIDATION Loss: 1.63e+03 logL: -1.49e+03 KL: 7.21e+01 MMD: 1.69e+00\n",
      "config 61, alpha = 0.0, lambda = 53.1, dropout = 0.00; 2 hidden layers with 102, 89 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.10e+04 logL: -1.04e+04 KL: 5.24e+02 MMD: 1.81e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.25e+03 logL: -3.83e+03 KL: 3.20e+02 MMD: 1.84e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.80e+03 logL: -2.48e+03 KL: 2.26e+02 MMD: 1.87e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.05e+03 logL: -1.76e+03 KL: 1.89e+02 MMD: 1.96e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.44e+03 logL: -1.18e+03 KL: 1.64e+02 MMD: 1.88e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 60 VALIDATION Loss: 1.25e+03 logL: -1.01e+03 KL: 1.47e+02 MMD: 1.91e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.16e+03 logL: -9.43e+02 KL: 1.29e+02 MMD: 1.76e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.16e+03 logL: -9.33e+02 KL: 1.21e+02 MMD: 1.97e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.08e+03 logL: -8.80e+02 KL: 1.09e+02 MMD: 1.75e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.07e+03 logL: -8.68e+02 KL: 9.87e+01 MMD: 1.89e+00\n",
      "Epoch 00101: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.01e+03 logL: -8.11e+02 KL: 9.43e+01 MMD: 1.99e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 9.90e+02 logL: -8.05e+02 KL: 8.95e+01 MMD: 1.84e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 9.89e+02 logL: -8.07e+02 KL: 8.52e+01 MMD: 1.86e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 9.79e+02 logL: -8.01e+02 KL: 7.98e+01 MMD: 1.90e+00\n",
      "Epoch 00148: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 150 VALIDATION Loss: 9.68e+02 logL: -7.92e+02 KL: 7.77e+01 MMD: 1.90e+00\n",
      "Epoch 00157: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 160 VALIDATION Loss: 9.67e+02 logL: -7.91e+02 KL: 7.73e+01 MMD: 1.89e+00\n",
      "Epoch 00165: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 169 VALIDATION Loss: 9.59e+02 logL: -7.91e+02 KL: 7.73e+01 MMD: 1.74e+00\n",
      "config 62, alpha = 0.0, lambda = 3.1, dropout = 0.00; 2 hidden layers with 79, 65 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.90e+03 logL: -9.57e+03 KL: 3.28e+02 MMD: 7.01e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.69e+03 logL: -9.51e+03 KL: 1.77e+02 MMD: 6.80e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.64e+03 logL: -9.52e+03 KL: 1.21e+02 MMD: 6.98e-01\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 9.49e+03 logL: -9.39e+03 KL: 1.02e+02 MMD: 7.16e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.47e+03 logL: -9.37e+03 KL: 9.54e+01 MMD: 7.27e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.46e+03 logL: -9.37e+03 KL: 8.66e+01 MMD: 7.89e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.42e+03 logL: -9.34e+03 KL: 7.93e+01 MMD: 7.44e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.37e+03 logL: -9.30e+03 KL: 7.18e+01 MMD: 7.33e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.31e+03 logL: -9.24e+03 KL: 6.71e+01 MMD: 7.54e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 9.30e+03 logL: -9.24e+03 KL: 6.31e+01 MMD: 7.55e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 9.20e+03 logL: -9.14e+03 KL: 5.91e+01 MMD: 7.76e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 9.18e+03 logL: -9.12e+03 KL: 5.63e+01 MMD: 7.38e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 9.19e+03 logL: -9.13e+03 KL: 5.45e+01 MMD: 7.98e-01\n",
      "Epoch 00139: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 140 VALIDATION Loss: 9.08e+03 logL: -9.03e+03 KL: 5.06e+01 MMD: 7.20e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 9.06e+03 logL: -9.01e+03 KL: 5.04e+01 MMD: 8.08e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 9.06e+03 logL: -9.01e+03 KL: 4.98e+01 MMD: 7.68e-01\n",
      "Stopping\n",
      "====> Epoch: 161 VALIDATION Loss: 9.06e+03 logL: -9.01e+03 KL: 4.99e+01 MMD: 7.32e-01\n",
      "config 62, alpha = 0.0, lambda = 24.1, dropout = 0.00; 2 hidden layers with 12, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.36e+04 KL: 6.13e+02 MMD: 1.19e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.02e+04 logL: -9.77e+03 KL: 3.67e+02 MMD: 9.33e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.76e+03 logL: -9.57e+03 KL: 1.61e+02 MMD: 1.20e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.69e+03 logL: -9.56e+03 KL: 1.08e+02 MMD: 1.29e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.58e+03 logL: -9.47e+03 KL: 8.79e+01 MMD: 1.15e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.35e+03 logL: -7.20e+03 KL: 1.29e+02 MMD: 1.02e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.19e+03 logL: -7.07e+03 KL: 8.80e+01 MMD: 1.17e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.71e+03 logL: -5.57e+03 KL: 1.19e+02 MMD: 1.04e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.65e+03 logL: -5.54e+03 KL: 8.82e+01 MMD: 9.66e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 5.62e+03 logL: -5.53e+03 KL: 7.11e+01 MMD: 1.05e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.54e+03 logL: -5.45e+03 KL: 6.32e+01 MMD: 1.12e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.43e+03 logL: -5.35e+03 KL: 5.93e+01 MMD: 1.02e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 5.29e+03 logL: -5.20e+03 KL: 6.02e+01 MMD: 1.14e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 5.24e+03 logL: -5.16e+03 KL: 5.40e+01 MMD: 1.06e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 5.23e+03 logL: -5.15e+03 KL: 4.94e+01 MMD: 1.18e+00\n",
      "Epoch 00153: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 160 VALIDATION Loss: 5.20e+03 logL: -5.12e+03 KL: 4.79e+01 MMD: 1.14e+00\n",
      "Stopping\n",
      "====> Epoch: 164 VALIDATION Loss: 5.20e+03 logL: -5.12e+03 KL: 4.79e+01 MMD: 1.17e+00\n",
      "config 62, alpha = 0.0, lambda = 240.2, dropout = 0.00; 2 hidden layers with 33, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.26e+05 logL: -3.60e+05 KL: 1.66e+05 MMD: 1.32e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.80e+05 logL: -3.25e+05 KL: 5.43e+04 MMD: 1.25e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.54e+05 logL: -2.26e+05 KL: 2.77e+04 MMD: 1.34e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.45e+04 logL: -2.92e+04 KL: 1.50e+04 MMD: 1.35e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.27e+04 logL: -1.55e+04 KL: 6.92e+03 MMD: 1.32e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.85e+04 logL: -1.49e+04 KL: 3.25e+03 MMD: 1.31e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.64e+04 logL: -1.45e+04 KL: 1.53e+03 MMD: 1.40e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.53e+04 logL: -1.42e+04 KL: 7.16e+02 MMD: 1.39e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.47e+04 logL: -1.40e+04 KL: 3.55e+02 MMD: 1.40e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.43e+04 logL: -1.38e+04 KL: 2.10e+02 MMD: 1.33e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.28e+04 logL: -1.22e+04 KL: 2.03e+02 MMD: 1.26e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.04e+04 logL: -9.92e+03 KL: 1.79e+02 MMD: 1.21e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 8.66e+03 logL: -8.19e+03 KL: 1.51e+02 MMD: 1.32e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 6.97e+03 logL: -6.50e+03 KL: 1.41e+02 MMD: 1.39e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 6.55e+03 logL: -6.11e+03 KL: 1.35e+02 MMD: 1.30e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 6.19e+03 logL: -5.71e+03 KL: 1.41e+02 MMD: 1.40e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 4.97e+03 logL: -4.51e+03 KL: 1.69e+02 MMD: 1.24e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 4.48e+03 logL: -4.00e+03 KL: 1.50e+02 MMD: 1.38e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 4.23e+03 logL: -3.78e+03 KL: 1.27e+02 MMD: 1.38e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 4.09e+03 logL: -3.65e+03 KL: 1.05e+02 MMD: 1.43e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 4.09e+03 logL: -3.65e+03 KL: 1.05e+02 MMD: 1.43e+00\n",
      "config 62, alpha = 0.0, lambda = 5188.4, dropout = 0.00; 2 hidden layers with 49, 34 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.27e+04 logL: -3.63e+03 KL: 3.10e+02 MMD: 1.69e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.04e+04 logL: -2.23e+03 KL: 1.93e+02 MMD: 1.53e+00\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.08e+04 logL: -2.14e+03 KL: 1.55e+02 MMD: 1.63e+00\n",
      "Stopping\n",
      "====> Epoch: 30 VALIDATION Loss: 1.08e+04 logL: -2.14e+03 KL: 1.55e+02 MMD: 1.63e+00\n",
      "config 62, alpha = 0.0, lambda = 61690.7, dropout = 0.00; 2 hidden layers with 173, 102 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.67e+05 logL: -2.30e+05 KL: 2.46e+04 MMD: 1.82e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.01e+05 logL: -1.57e+05 KL: 2.14e+04 MMD: 1.97e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.96e+05 logL: -5.78e+04 KL: 1.76e+04 MMD: 1.96e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.42e+05 logL: -1.59e+04 KL: 9.82e+03 MMD: 1.88e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.36e+05 logL: -1.18e+04 KL: 4.94e+03 MMD: 1.93e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.32e+05 logL: -1.06e+04 KL: 2.67e+03 MMD: 1.92e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.22e+05 logL: -9.32e+03 KL: 1.59e+03 MMD: 1.80e+00\n",
      "Epoch 00071: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 75 VALIDATION Loss: 1.26e+05 logL: -9.08e+03 KL: 1.50e+03 MMD: 1.86e+00\n",
      "config 63, alpha = 0.0, lambda = 26809.9, dropout = 0.00; 2 hidden layers with 53, 24 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.04e+04 logL: -1.00e+04 KL: 6.55e+02 MMD: 7.37e-01\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 2.90e+04 logL: -9.60e+03 KL: 4.33e+02 MMD: 7.08e-01\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping\n",
      "====> Epoch: 29 VALIDATION Loss: 2.75e+04 logL: -9.60e+03 KL: 4.25e+02 MMD: 6.53e-01\n",
      "config 63, alpha = 0.0, lambda = 23895.3, dropout = 0.00; 2 hidden layers with 74, 37 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.23e+04 logL: -5.21e+03 KL: 2.85e+02 MMD: 1.12e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.00e+04 logL: -4.97e+03 KL: 2.15e+02 MMD: 1.04e+00\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 27 VALIDATION Loss: 2.68e+04 logL: -4.86e+03 KL: 2.04e+02 MMD: 9.10e-01\n",
      "config 63, alpha = 0.0, lambda = 6.9, dropout = 0.00; 2 hidden layers with 89, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.18e+04 logL: -1.12e+04 KL: 6.16e+02 MMD: 1.51e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.51e+03 logL: -6.16e+03 KL: 3.51e+02 MMD: 1.31e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.67e+03 logL: -5.45e+03 KL: 2.19e+02 MMD: 1.35e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.45e+03 logL: -5.29e+03 KL: 1.51e+02 MMD: 1.46e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.25e+03 logL: -5.13e+03 KL: 1.18e+02 MMD: 1.36e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.75e+03 logL: -3.63e+03 KL: 1.12e+02 MMD: 1.37e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.68e+03 logL: -3.58e+03 KL: 9.27e+01 MMD: 1.39e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.61e+03 logL: -3.52e+03 KL: 8.67e+01 MMD: 1.47e+00\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.55e+03 logL: -3.46e+03 KL: 8.31e+01 MMD: 1.30e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.55e+03 logL: -3.46e+03 KL: 8.01e+01 MMD: 1.45e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.54e+03 logL: -3.45e+03 KL: 7.83e+01 MMD: 1.43e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.52e+03 logL: -3.44e+03 KL: 7.59e+01 MMD: 1.34e+00\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 3.52e+03 logL: -3.43e+03 KL: 7.43e+01 MMD: 1.39e+00\n",
      "Stopping\n",
      "====> Epoch: 137 VALIDATION Loss: 3.52e+03 logL: -3.43e+03 KL: 7.41e+01 MMD: 1.33e+00\n",
      "config 63, alpha = 0.0, lambda = 2.0, dropout = 0.00; 2 hidden layers with 144, 95 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.85e+03 logL: -5.50e+03 KL: 3.44e+02 MMD: 1.50e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.02e+03 logL: -2.81e+03 KL: 2.18e+02 MMD: 1.58e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.99e+03 logL: -1.81e+03 KL: 1.78e+02 MMD: 1.60e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.89e+03 logL: -1.74e+03 KL: 1.51e+02 MMD: 1.67e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.86e+03 logL: -1.73e+03 KL: 1.36e+02 MMD: 1.60e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.75e+03 logL: -1.63e+03 KL: 1.19e+02 MMD: 1.62e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.73e+03 logL: -1.62e+03 KL: 1.11e+02 MMD: 1.61e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.87e+03 logL: -1.77e+03 KL: 9.73e+01 MMD: 1.55e+00\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.58e+03 logL: -1.48e+03 KL: 9.44e+01 MMD: 1.73e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.57e+03 logL: -1.47e+03 KL: 8.95e+01 MMD: 1.50e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.55e+03 logL: -1.47e+03 KL: 8.44e+01 MMD: 1.65e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.55e+03 logL: -1.47e+03 KL: 7.90e+01 MMD: 1.77e+00\n",
      "Epoch 00124: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.53e+03 logL: -1.45e+03 KL: 7.69e+01 MMD: 1.57e+00\n",
      "Epoch 00131: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 135 VALIDATION Loss: 1.53e+03 logL: -1.45e+03 KL: 7.69e+01 MMD: 1.69e+00\n",
      "config 63, alpha = 0.0, lambda = 546.9, dropout = 0.00; 2 hidden layers with 42, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.49e+05 logL: -3.36e+05 KL: 1.24e+04 MMD: 2.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.24e+05 logL: -3.18e+05 KL: 4.81e+03 MMD: 1.78e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.59e+05 logL: -2.55e+05 KL: 2.96e+03 MMD: 1.89e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.08e+05 logL: -1.05e+05 KL: 2.01e+03 MMD: 1.77e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.12e+04 logL: -1.87e+04 KL: 1.45e+03 MMD: 2.00e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.52e+04 logL: -1.33e+04 KL: 8.80e+02 MMD: 1.84e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.46e+04 logL: -1.30e+04 KL: 5.42e+02 MMD: 1.94e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.38e+04 logL: -1.24e+04 KL: 3.83e+02 MMD: 1.93e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.24e+04 logL: -1.09e+04 KL: 3.12e+02 MMD: 2.07e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.77e+03 logL: -8.49e+03 KL: 2.87e+02 MMD: 1.82e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 7.98e+03 logL: -6.66e+03 KL: 2.56e+02 MMD: 1.93e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 6.75e+03 logL: -5.40e+03 KL: 2.29e+02 MMD: 2.05e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 5.81e+03 logL: -4.64e+03 KL: 2.01e+02 MMD: 1.77e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 5.46e+03 logL: -4.25e+03 KL: 1.81e+02 MMD: 1.89e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 5.15e+03 logL: -3.95e+03 KL: 1.66e+02 MMD: 1.90e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 4.86e+03 logL: -3.60e+03 KL: 1.59e+02 MMD: 2.00e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 4.46e+03 logL: -3.28e+03 KL: 1.53e+02 MMD: 1.88e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 4.10e+03 logL: -2.89e+03 KL: 1.49e+02 MMD: 1.95e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 3.81e+03 logL: -2.66e+03 KL: 1.43e+02 MMD: 1.83e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 3.29e+03 logL: -2.16e+03 KL: 1.39e+02 MMD: 1.81e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 3.29e+03 logL: -2.16e+03 KL: 1.39e+02 MMD: 1.81e+00\n",
      "config 64, alpha = 0.0, lambda = 239.9, dropout = 0.00; 2 hidden layers with 78, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.91e+03 logL: -9.48e+03 KL: 2.70e+02 MMD: 6.64e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.80e+03 logL: -9.45e+03 KL: 1.60e+02 MMD: 7.65e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.70e+03 logL: -9.39e+03 KL: 1.32e+02 MMD: 7.52e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.64e+03 logL: -9.37e+03 KL: 1.04e+02 MMD: 7.09e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.21e+03 logL: -8.93e+03 KL: 9.57e+01 MMD: 7.94e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.44e+03 logL: -9.18e+03 KL: 8.57e+01 MMD: 7.38e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.98e+03 logL: -8.72e+03 KL: 6.99e+01 MMD: 7.72e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.61e+03 logL: -8.36e+03 KL: 5.62e+01 MMD: 8.26e-01\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 8.38e+03 logL: -8.14e+03 KL: 5.47e+01 MMD: 7.59e-01\n",
      "Epoch 00095: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 99 VALIDATION Loss: 8.35e+03 logL: -8.12e+03 KL: 5.37e+01 MMD: 7.34e-01\n",
      "config 64, alpha = 0.0, lambda = 4027.2, dropout = 0.00; 2 hidden layers with 17, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.46e+04 logL: -9.80e+03 KL: 6.01e+02 MMD: 1.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.15e+04 logL: -7.45e+03 KL: 2.91e+02 MMD: 9.27e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.02e+04 logL: -5.75e+03 KL: 2.17e+02 MMD: 1.04e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.46e+03 logL: -5.43e+03 KL: 1.68e+02 MMD: 9.61e-01\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 9.62e+03 logL: -5.35e+03 KL: 1.58e+02 MMD: 1.02e+00\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 59 VALIDATION Loss: 9.80e+03 logL: -5.34e+03 KL: 1.55e+02 MMD: 1.07e+00\n",
      "config 64, alpha = 0.0, lambda = 3124.7, dropout = 0.00; 2 hidden layers with 49, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.87e+04 logL: -1.38e+04 KL: 9.26e+02 MMD: 1.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.50e+04 logL: -1.05e+04 KL: 3.87e+02 MMD: 1.33e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.19e+04 logL: -7.88e+03 KL: 2.44e+02 MMD: 1.20e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.03e+04 logL: -5.78e+03 KL: 1.96e+02 MMD: 1.38e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.51e+03 logL: -4.17e+03 KL: 2.15e+02 MMD: 1.32e+00\n",
      "Epoch 00058: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 8.25e+03 logL: -3.86e+03 KL: 1.80e+02 MMD: 1.35e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.72e+03 logL: -3.83e+03 KL: 1.77e+02 MMD: 1.19e+00\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 77 VALIDATION Loss: 7.73e+03 logL: -3.82e+03 KL: 1.76e+02 MMD: 1.20e+00\n",
      "config 64, alpha = 0.0, lambda = 21207.8, dropout = 0.00; 2 hidden layers with 10, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.19e+04 logL: -1.51e+04 KL: 5.53e+02 MMD: 1.71e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.67e+04 logL: -1.41e+04 KL: 4.72e+02 MMD: 1.51e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 30 VALIDATION Loss: 4.70e+04 logL: -1.18e+04 KL: 3.69e+02 MMD: 1.64e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.52e+04 logL: -9.93e+03 KL: 2.88e+02 MMD: 1.65e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.53e+04 logL: -9.68e+03 KL: 2.19e+02 MMD: 1.67e+00\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.63e+04 logL: -9.65e+03 KL: 1.70e+02 MMD: 1.72e+00\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 65 VALIDATION Loss: 4.32e+04 logL: -9.60e+03 KL: 1.68e+02 MMD: 1.57e+00\n",
      "config 64, alpha = 0.0, lambda = 18806.5, dropout = 0.00; 2 hidden layers with 122, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.26e+04 logL: -5.55e+03 KL: 4.48e+02 MMD: 1.94e+00\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 4.02e+04 logL: -3.47e+03 KL: 3.51e+02 MMD: 1.93e+00\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 3.88e+04 logL: -2.97e+03 KL: 3.82e+02 MMD: 1.88e+00\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 3.92e+04 logL: -2.95e+03 KL: 3.77e+02 MMD: 1.91e+00\n",
      "config 65, alpha = 0.0, lambda = 208.0, dropout = 0.00; 2 hidden layers with 36, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.40e+04 logL: -1.33e+04 KL: 4.78e+02 MMD: 7.26e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.90e+03 logL: -9.54e+03 KL: 2.19e+02 MMD: 6.85e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.77e+03 logL: -9.50e+03 KL: 1.23e+02 MMD: 7.03e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.70e+03 logL: -9.47e+03 KL: 8.81e+01 MMD: 6.82e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.70e+03 logL: -9.47e+03 KL: 7.17e+01 MMD: 7.80e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.71e+03 logL: -9.49e+03 KL: 6.40e+01 MMD: 7.74e-01\n",
      "Stopping\n",
      "====> Epoch: 66 VALIDATION Loss: 9.66e+03 logL: -9.45e+03 KL: 5.88e+01 MMD: 7.47e-01\n",
      "config 65, alpha = 0.0, lambda = 165.8, dropout = 0.00; 2 hidden layers with 7, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.23e+04 logL: -2.11e+04 KL: 1.02e+03 MMD: 1.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.70e+04 logL: -1.63e+04 KL: 4.93e+02 MMD: 1.13e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.42e+04 logL: -1.38e+04 KL: 2.40e+02 MMD: 1.19e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.39e+04 logL: -1.35e+04 KL: 1.82e+02 MMD: 1.16e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.19e+04 logL: -1.15e+04 KL: 1.84e+02 MMD: 1.20e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.06e+04 logL: -1.03e+04 KL: 1.30e+02 MMD: 1.16e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.92e+03 logL: -9.62e+03 KL: 1.07e+02 MMD: 1.19e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.69e+03 logL: -9.40e+03 KL: 1.04e+02 MMD: 1.15e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 8.21e+03 logL: -7.90e+03 KL: 1.27e+02 MMD: 1.13e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 7.55e+03 logL: -7.25e+03 KL: 1.02e+02 MMD: 1.17e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 7.40e+03 logL: -7.13e+03 KL: 8.66e+01 MMD: 1.16e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 7.38e+03 logL: -7.11e+03 KL: 7.58e+01 MMD: 1.16e+00\n",
      "Epoch 00125: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 129 VALIDATION Loss: 7.36e+03 logL: -7.10e+03 KL: 7.13e+01 MMD: 1.14e+00\n",
      "config 65, alpha = 0.0, lambda = 5.4, dropout = 0.00; 2 hidden layers with 93, 76 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.53e+03 logL: -4.18e+03 KL: 3.42e+02 MMD: 1.36e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.04e+03 logL: -2.84e+03 KL: 1.96e+02 MMD: 1.23e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.95e+03 logL: -2.80e+03 KL: 1.47e+02 MMD: 1.30e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.89e+03 logL: -2.77e+03 KL: 1.22e+02 MMD: 1.22e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.87e+03 logL: -2.76e+03 KL: 1.09e+02 MMD: 1.44e+00\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.79e+03 logL: -2.68e+03 KL: 1.03e+02 MMD: 1.36e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.78e+03 logL: -2.67e+03 KL: 9.83e+01 MMD: 1.30e+00\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 2.76e+03 logL: -2.66e+03 KL: 9.52e+01 MMD: 1.39e+00\n",
      "Stopping\n",
      "====> Epoch: 85 VALIDATION Loss: 2.76e+03 logL: -2.66e+03 KL: 9.49e+01 MMD: 1.32e+00\n",
      "config 65, alpha = 0.0, lambda = 321.7, dropout = 0.00; 2 hidden layers with 34, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.57e+04 logL: -1.42e+04 KL: 9.71e+02 MMD: 1.55e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.20e+04 logL: -1.10e+04 KL: 5.06e+02 MMD: 1.59e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.98e+03 logL: -8.13e+03 KL: 3.28e+02 MMD: 1.61e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.00e+03 logL: -7.26e+03 KL: 2.13e+02 MMD: 1.64e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.99e+03 logL: -5.29e+03 KL: 1.77e+02 MMD: 1.65e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.86e+03 logL: -5.17e+03 KL: 1.33e+02 MMD: 1.73e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.76e+03 logL: -5.12e+03 KL: 1.14e+02 MMD: 1.63e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.70e+03 logL: -5.07e+03 KL: 1.03e+02 MMD: 1.64e+00\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 5.60e+03 logL: -5.01e+03 KL: 9.91e+01 MMD: 1.53e+00\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 5.62e+03 logL: -4.97e+03 KL: 9.77e+01 MMD: 1.72e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.60e+03 logL: -4.95e+03 KL: 9.73e+01 MMD: 1.71e+00\n",
      "Epoch 00113: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 117 VALIDATION Loss: 5.56e+03 logL: -4.95e+03 KL: 9.74e+01 MMD: 1.60e+00\n",
      "config 65, alpha = 0.0, lambda = 4.0, dropout = 0.00; 2 hidden layers with 18, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.72e+04 logL: -1.41e+04 KL: 3.11e+03 MMD: 2.15e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.51e+04 logL: -1.39e+04 KL: 1.18e+03 MMD: 1.87e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.44e+04 logL: -1.39e+04 KL: 5.90e+02 MMD: 2.03e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.40e+04 logL: -1.36e+04 KL: 3.70e+02 MMD: 1.95e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.22e+04 logL: -1.19e+04 KL: 2.85e+02 MMD: 2.09e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.09e+04 logL: -1.07e+04 KL: 2.12e+02 MMD: 2.03e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.65e+03 logL: -8.44e+03 KL: 2.03e+02 MMD: 1.97e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 7.22e+03 logL: -7.05e+03 KL: 1.62e+02 MMD: 1.87e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.00e+03 logL: -5.86e+03 KL: 1.34e+02 MMD: 1.96e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.58e+03 logL: -5.46e+03 KL: 1.18e+02 MMD: 1.93e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 4.01e+03 logL: -3.88e+03 KL: 1.25e+02 MMD: 1.97e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.83e+03 logL: -3.71e+03 KL: 1.14e+02 MMD: 1.92e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.15e+03 logL: -3.03e+03 KL: 1.16e+02 MMD: 2.03e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.95e+03 logL: -2.84e+03 KL: 1.07e+02 MMD: 1.92e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.88e+03 logL: -2.78e+03 KL: 9.66e+01 MMD: 1.91e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 2.39e+03 logL: -2.29e+03 KL: 9.37e+01 MMD: 1.93e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 2.26e+03 logL: -2.16e+03 KL: 8.70e+01 MMD: 2.08e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 2.20e+03 logL: -2.11e+03 KL: 8.18e+01 MMD: 1.92e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 2.19e+03 logL: -2.11e+03 KL: 7.62e+01 MMD: 1.86e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 2.19e+03 logL: -2.11e+03 KL: 7.32e+01 MMD: 1.94e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 2.19e+03 logL: -2.11e+03 KL: 7.32e+01 MMD: 1.94e+00\n",
      "config 66, alpha = 0.0, lambda = 3583.2, dropout = 0.00; 2 hidden layers with 34, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.33e+04 logL: -1.01e+04 KL: 6.38e+02 MMD: 7.24e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.21e+04 logL: -9.58e+03 KL: 2.39e+02 MMD: 6.49e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.21e+04 logL: -9.53e+03 KL: 1.61e+02 MMD: 6.84e-01\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 35 VALIDATION Loss: 1.22e+04 logL: -9.51e+03 KL: 1.54e+02 MMD: 6.96e-01\n",
      "config 66, alpha = 0.0, lambda = 4.5, dropout = 0.00; 2 hidden layers with 37, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.04e+04 logL: -9.92e+03 KL: 4.94e+02 MMD: 9.45e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 7.87e+03 logL: -7.64e+03 KL: 2.33e+02 MMD: 1.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.23e+03 logL: -6.08e+03 KL: 1.55e+02 MMD: 1.03e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.30e+03 logL: -5.19e+03 KL: 1.04e+02 MMD: 9.81e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 50 VALIDATION Loss: 5.22e+03 logL: -5.13e+03 KL: 8.41e+01 MMD: 1.03e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.18e+03 logL: -5.10e+03 KL: 7.27e+01 MMD: 9.75e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 5.18e+03 logL: -5.10e+03 KL: 6.87e+01 MMD: 1.12e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.01e+03 logL: -4.94e+03 KL: 6.87e+01 MMD: 9.67e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 4.90e+03 logL: -4.83e+03 KL: 7.04e+01 MMD: 1.03e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.87e+03 logL: -4.79e+03 KL: 7.02e+01 MMD: 1.06e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 4.87e+03 logL: -4.80e+03 KL: 6.72e+01 MMD: 1.13e+00\n",
      "Epoch 00114: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 4.67e+03 logL: -4.60e+03 KL: 6.81e+01 MMD: 9.09e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 4.68e+03 logL: -4.61e+03 KL: 6.84e+01 MMD: 1.01e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 4.64e+03 logL: -4.57e+03 KL: 6.72e+01 MMD: 1.09e+00\n",
      "Stopping\n",
      "====> Epoch: 143 VALIDATION Loss: 4.64e+03 logL: -4.57e+03 KL: 6.92e+01 MMD: 1.10e+00\n",
      "config 66, alpha = 0.0, lambda = 3.0, dropout = 0.00; 2 hidden layers with 54, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.70e+04 logL: -1.51e+04 KL: 1.91e+03 MMD: 1.46e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.52e+04 logL: -1.46e+04 KL: 5.81e+02 MMD: 1.80e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.45e+04 logL: -1.42e+04 KL: 3.11e+02 MMD: 1.86e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.39e+04 logL: -1.37e+04 KL: 1.91e+02 MMD: 2.00e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.14e+04 logL: -1.13e+04 KL: 1.36e+02 MMD: 1.63e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.06e+04 logL: -1.05e+04 KL: 1.07e+02 MMD: 1.93e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.39e+03 logL: -8.28e+03 KL: 1.05e+02 MMD: 1.66e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 7.85e+03 logL: -7.75e+03 KL: 9.72e+01 MMD: 1.70e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 7.48e+03 logL: -7.39e+03 KL: 8.61e+01 MMD: 1.68e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.39e+03 logL: -5.30e+03 KL: 8.85e+01 MMD: 1.62e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.28e+03 logL: -5.20e+03 KL: 7.87e+01 MMD: 1.47e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.18e+03 logL: -5.11e+03 KL: 7.09e+01 MMD: 1.59e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.65e+03 logL: -3.57e+03 KL: 7.67e+01 MMD: 1.56e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.59e+03 logL: -3.51e+03 KL: 7.02e+01 MMD: 1.49e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.98e+03 logL: -2.88e+03 KL: 9.56e+01 MMD: 1.51e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 2.80e+03 logL: -2.73e+03 KL: 7.05e+01 MMD: 1.43e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 2.75e+03 logL: -2.68e+03 KL: 6.30e+01 MMD: 1.41e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 2.74e+03 logL: -2.68e+03 KL: 5.88e+01 MMD: 1.42e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 2.72e+03 logL: -2.66e+03 KL: 5.48e+01 MMD: 1.52e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 2.71e+03 logL: -2.66e+03 KL: 5.27e+01 MMD: 1.45e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 2.71e+03 logL: -2.66e+03 KL: 5.27e+01 MMD: 1.45e+00\n",
      "config 66, alpha = 0.0, lambda = 20054.9, dropout = 0.00; 2 hidden layers with 13, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.41e+04 logL: -9.69e+03 KL: 5.65e+02 MMD: 1.69e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.85e+04 logL: -5.30e+03 KL: 3.38e+02 MMD: 1.64e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.62e+04 logL: -4.61e+03 KL: 2.39e+02 MMD: 1.56e+00\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 3.39e+04 logL: -3.62e+03 KL: 2.04e+02 MMD: 1.50e+00\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.04e+04 logL: -3.61e+03 KL: 1.95e+02 MMD: 1.82e+00\n",
      "Stopping\n",
      "====> Epoch: 50 VALIDATION Loss: 4.04e+04 logL: -3.61e+03 KL: 1.95e+02 MMD: 1.82e+00\n",
      "config 66, alpha = 0.0, lambda = 15.5, dropout = 0.00; 2 hidden layers with 15, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.41e+04 logL: -1.37e+04 KL: 3.73e+02 MMD: 2.24e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.00e+04 logL: -9.75e+03 KL: 2.50e+02 MMD: 2.30e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.75e+03 logL: -9.58e+03 KL: 1.30e+02 MMD: 2.62e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.71e+03 logL: -9.58e+03 KL: 9.37e+01 MMD: 2.85e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.42e+03 logL: -7.28e+03 KL: 1.05e+02 MMD: 2.48e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.27e+03 logL: -7.14e+03 KL: 8.56e+01 MMD: 2.58e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.63e+03 logL: -5.50e+03 KL: 9.60e+01 MMD: 2.17e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.39e+03 logL: -5.27e+03 KL: 8.05e+01 MMD: 2.23e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.28e+03 logL: -5.17e+03 KL: 7.25e+01 MMD: 2.40e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.15e+03 logL: -5.05e+03 KL: 6.90e+01 MMD: 2.31e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.67e+03 logL: -3.57e+03 KL: 7.35e+01 MMD: 2.18e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.67e+03 logL: -3.57e+03 KL: 6.82e+01 MMD: 2.24e+00\n",
      "Epoch 00121: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00129: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 3.64e+03 logL: -3.54e+03 KL: 6.74e+01 MMD: 2.27e+00\n",
      "Stopping\n",
      "====> Epoch: 132 VALIDATION Loss: 3.64e+03 logL: -3.54e+03 KL: 6.74e+01 MMD: 2.42e+00\n",
      "config 67, alpha = 0.0, lambda = 59.0, dropout = 0.00; 2 hidden layers with 18, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.38e+04 logL: -1.34e+04 KL: 3.14e+02 MMD: 7.24e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.78e+03 logL: -9.58e+03 KL: 1.56e+02 MMD: 6.93e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.67e+03 logL: -9.54e+03 KL: 8.89e+01 MMD: 7.40e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.68e+03 logL: -9.57e+03 KL: 6.39e+01 MMD: 7.12e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.62e+03 logL: -9.52e+03 KL: 5.21e+01 MMD: 8.44e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.55e+03 logL: -9.46e+03 KL: 4.56e+01 MMD: 7.53e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.56e+03 logL: -9.47e+03 KL: 4.15e+01 MMD: 8.62e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.45e+03 logL: -9.37e+03 KL: 4.08e+01 MMD: 7.92e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.51e+03 logL: -9.43e+03 KL: 4.05e+01 MMD: 7.95e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 9.03e+03 logL: -8.94e+03 KL: 4.10e+01 MMD: 7.37e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 8.91e+03 logL: -8.83e+03 KL: 3.45e+01 MMD: 8.21e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 8.93e+03 logL: -8.85e+03 KL: 3.01e+01 MMD: 8.25e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 8.89e+03 logL: -8.82e+03 KL: 2.95e+01 MMD: 7.62e-01\n",
      "Epoch 00130: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 8.70e+03 logL: -8.62e+03 KL: 2.90e+01 MMD: 8.44e-01\n",
      "Epoch 00144: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 148 VALIDATION Loss: 8.71e+03 logL: -8.64e+03 KL: 2.88e+01 MMD: 8.20e-01\n",
      "config 67, alpha = 0.0, lambda = 3.1, dropout = 0.00; 2 hidden layers with 165, 30 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.75e+03 logL: -5.53e+03 KL: 2.16e+02 MMD: 1.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.28e+03 logL: -5.14e+03 KL: 1.33e+02 MMD: 1.05e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.66e+03 logL: -4.57e+03 KL: 9.30e+01 MMD: 1.03e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.49e+03 logL: -4.42e+03 KL: 7.20e+01 MMD: 9.69e-01\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 4.39e+03 logL: -4.32e+03 KL: 6.60e+01 MMD: 1.07e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.32e+03 logL: -4.25e+03 KL: 6.24e+01 MMD: 9.74e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 4.32e+03 logL: -4.26e+03 KL: 5.94e+01 MMD: 1.04e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.28e+03 logL: -4.22e+03 KL: 5.77e+01 MMD: 1.02e+00\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 4.28e+03 logL: -4.22e+03 KL: 5.51e+01 MMD: 1.06e+00\n",
      "Stopping\n",
      "====> Epoch: 93 VALIDATION Loss: 4.28e+03 logL: -4.23e+03 KL: 5.51e+01 MMD: 9.65e-01\n",
      "config 67, alpha = 0.0, lambda = 95586.3, dropout = 0.00; 2 hidden layers with 8, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.18e+05 logL: -1.76e+05 KL: 1.51e+01 MMD: 4.37e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 2.41e+04 logL: -1.80e+04 KL: 2.14e+01 MMD: 6.41e-02\n",
      "====> Epoch: 30 VALIDATION Loss: 2.18e+04 logL: -1.53e+04 KL: 2.44e+01 MMD: 6.75e-02\n",
      "====> Epoch: 40 VALIDATION Loss: 1.70e+04 logL: -1.33e+04 KL: 2.39e+01 MMD: 3.80e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.74e+04 logL: -1.17e+04 KL: 2.38e+01 MMD: 5.89e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 1.61e+04 logL: -1.08e+04 KL: 2.27e+01 MMD: 5.52e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.39e+04 logL: -1.00e+04 KL: 2.32e+01 MMD: 3.99e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 1.41e+04 logL: -9.88e+03 KL: 2.30e+01 MMD: 4.37e-02\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.35e+04 logL: -9.72e+03 KL: 2.26e+01 MMD: 3.91e-02\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 100 VALIDATION Loss: 1.25e+04 logL: -9.68e+03 KL: 2.26e+01 MMD: 2.89e-02\n",
      "Stopping\n",
      "====> Epoch: 101 VALIDATION Loss: 1.40e+04 logL: -9.69e+03 KL: 2.26e+01 MMD: 4.45e-02\n",
      "config 67, alpha = 0.0, lambda = 45.6, dropout = 0.00; 2 hidden layers with 169, 112 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.69e+04 logL: -1.35e+04 KL: 3.27e+03 MMD: 1.52e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.38e+04 logL: -1.28e+04 KL: 1.00e+03 MMD: 1.64e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.17e+04 logL: -1.09e+04 KL: 7.11e+02 MMD: 1.60e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.58e+03 logL: -7.85e+03 KL: 6.55e+02 MMD: 1.65e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.39e+03 logL: -4.69e+03 KL: 6.32e+02 MMD: 1.63e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.67e+03 logL: -3.19e+03 KL: 4.15e+02 MMD: 1.45e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.00e+03 logL: -2.64e+03 KL: 2.96e+02 MMD: 1.65e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.73e+03 logL: -2.42e+03 KL: 2.35e+02 MMD: 1.59e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.52e+03 logL: -2.24e+03 KL: 2.08e+02 MMD: 1.60e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.04e+03 logL: -1.78e+03 KL: 1.94e+02 MMD: 1.46e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.01e+03 logL: -1.78e+03 KL: 1.69e+02 MMD: 1.51e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.04e+03 logL: -1.82e+03 KL: 1.56e+02 MMD: 1.48e+00\n",
      "Epoch 00124: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.82e+03 logL: -1.60e+03 KL: 1.47e+02 MMD: 1.66e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.81e+03 logL: -1.59e+03 KL: 1.44e+02 MMD: 1.61e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.80e+03 logL: -1.59e+03 KL: 1.39e+02 MMD: 1.58e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.79e+03 logL: -1.58e+03 KL: 1.34e+02 MMD: 1.61e+00\n",
      "Epoch 00160: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 170 VALIDATION Loss: 1.78e+03 logL: -1.57e+03 KL: 1.34e+02 MMD: 1.68e+00\n",
      "Epoch 00170: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 174 VALIDATION Loss: 1.77e+03 logL: -1.56e+03 KL: 1.34e+02 MMD: 1.66e+00\n",
      "config 67, alpha = 0.0, lambda = 33.6, dropout = 0.00; 2 hidden layers with 13, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.44e+05 logL: -3.43e+05 KL: 1.29e+03 MMD: 1.80e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.40e+05 logL: -3.39e+05 KL: 9.33e+02 MMD: 1.70e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.35e+05 logL: -3.34e+05 KL: 7.43e+02 MMD: 1.81e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.26e+05 logL: -3.25e+05 KL: 6.90e+02 MMD: 2.03e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.14e+05 logL: -3.13e+05 KL: 6.30e+02 MMD: 2.04e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.86e+05 logL: -2.85e+05 KL: 6.88e+02 MMD: 1.84e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.67e+05 logL: -1.66e+05 KL: 8.79e+02 MMD: 1.97e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.84e+04 logL: -2.75e+04 KL: 8.19e+02 MMD: 1.99e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.92e+04 logL: -1.85e+04 KL: 6.74e+02 MMD: 1.88e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.71e+04 logL: -1.65e+04 KL: 5.70e+02 MMD: 2.13e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.57e+04 logL: -1.52e+04 KL: 5.03e+02 MMD: 2.09e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.53e+04 logL: -1.48e+04 KL: 4.38e+02 MMD: 2.14e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.44e+04 logL: -1.40e+04 KL: 3.62e+02 MMD: 2.13e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.30e+04 logL: -1.26e+04 KL: 3.04e+02 MMD: 2.13e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.13e+04 logL: -1.10e+04 KL: 2.68e+02 MMD: 1.91e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 8.91e+03 logL: -8.62e+03 KL: 2.27e+02 MMD: 1.90e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 8.13e+03 logL: -7.87e+03 KL: 1.98e+02 MMD: 2.02e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 7.80e+03 logL: -7.56e+03 KL: 1.81e+02 MMD: 2.00e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 7.38e+03 logL: -7.14e+03 KL: 1.80e+02 MMD: 1.82e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 6.39e+03 logL: -6.13e+03 KL: 1.90e+02 MMD: 1.88e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 6.39e+03 logL: -6.13e+03 KL: 1.90e+02 MMD: 1.88e+00\n",
      "config 68, alpha = 0.0, lambda = 1.5, dropout = 0.00; 2 hidden layers with 8, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.43e+04 logL: -1.36e+04 KL: 6.34e+02 MMD: 7.23e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.38e+04 logL: -1.36e+04 KL: 2.34e+02 MMD: 8.90e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.37e+04 logL: -1.36e+04 KL: 1.29e+02 MMD: 9.46e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 7.88e+01 MMD: 9.88e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.71e+03 logL: -9.59e+03 KL: 1.14e+02 MMD: 7.08e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.62e+03 logL: -9.54e+03 KL: 7.32e+01 MMD: 7.88e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.58e+03 logL: -9.53e+03 KL: 5.64e+01 MMD: 7.81e-01\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.55e+03 logL: -9.51e+03 KL: 4.82e+01 MMD: 8.04e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.55e+03 logL: -9.51e+03 KL: 4.72e+01 MMD: 8.53e-01\n",
      "Epoch 00090: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 92 VALIDATION Loss: 9.55e+03 logL: -9.51e+03 KL: 4.72e+01 MMD: 8.73e-01\n",
      "config 68, alpha = 0.0, lambda = 1.8, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.17e+04 logL: -9.14e+04 KL: 3.37e+02 MMD: 1.10e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.30e+04 logL: -2.26e+04 KL: 3.81e+02 MMD: 1.11e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.86e+04 logL: -1.81e+04 KL: 4.28e+02 MMD: 1.15e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.63e+04 logL: -1.58e+04 KL: 4.82e+02 MMD: 1.16e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.51e+04 logL: -1.47e+04 KL: 4.54e+02 MMD: 1.14e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.44e+04 logL: -1.41e+04 KL: 3.55e+02 MMD: 1.18e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.40e+04 logL: -1.37e+04 KL: 2.44e+02 MMD: 1.24e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.21e+04 logL: -1.19e+04 KL: 2.44e+02 MMD: 1.20e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.14e+04 logL: -1.13e+04 KL: 1.64e+02 MMD: 1.17e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.12e+04 logL: -1.11e+04 KL: 1.48e+02 MMD: 1.25e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.09e+04 logL: -1.08e+04 KL: 1.37e+02 MMD: 1.04e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.06e+04 logL: -1.04e+04 KL: 1.28e+02 MMD: 1.07e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.02e+04 logL: -1.01e+04 KL: 1.21e+02 MMD: 1.18e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 9.90e+03 logL: -9.79e+03 KL: 1.06e+02 MMD: 1.32e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 9.69e+03 logL: -9.61e+03 KL: 8.66e+01 MMD: 1.19e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 9.61e+03 logL: -9.54e+03 KL: 6.92e+01 MMD: 1.37e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 9.58e+03 logL: -9.53e+03 KL: 5.67e+01 MMD: 1.45e+00\n",
      "Stopping\n",
      "====> Epoch: 179 VALIDATION Loss: 9.58e+03 logL: -9.53e+03 KL: 4.89e+01 MMD: 1.52e+00\n",
      "config 68, alpha = 0.0, lambda = 2.0, dropout = 0.00; 2 hidden layers with 11, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.57e+04 logL: -1.48e+04 KL: 9.44e+02 MMD: 1.38e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.40e+04 logL: -1.37e+04 KL: 3.46e+02 MMD: 1.37e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.09e+04 logL: -1.06e+04 KL: 2.75e+02 MMD: 1.46e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.89e+03 logL: -9.71e+03 KL: 1.70e+02 MMD: 1.39e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.02e+03 logL: -7.87e+03 KL: 1.49e+02 MMD: 1.30e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.36e+03 logL: -7.24e+03 KL: 1.26e+02 MMD: 1.38e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.54e+03 logL: -6.42e+03 KL: 1.22e+02 MMD: 1.42e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.36e+03 logL: -5.24e+03 KL: 1.15e+02 MMD: 1.37e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.29e+03 logL: -5.19e+03 KL: 9.55e+01 MMD: 1.45e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.07e+03 logL: -4.97e+03 KL: 9.27e+01 MMD: 1.39e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.70e+03 logL: -3.61e+03 KL: 8.78e+01 MMD: 1.47e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.66e+03 logL: -3.59e+03 KL: 7.46e+01 MMD: 1.40e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.69e+03 logL: -3.62e+03 KL: 6.66e+01 MMD: 1.36e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00138: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 3.63e+03 logL: -3.57e+03 KL: 6.21e+01 MMD: 1.26e+00\n",
      "Stopping\n",
      "====> Epoch: 149 VALIDATION Loss: 3.63e+03 logL: -3.57e+03 KL: 6.06e+01 MMD: 1.46e+00\n",
      "config 68, alpha = 0.0, lambda = 110.6, dropout = 0.00; 2 hidden layers with 17, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.36e+04 KL: 4.13e+02 MMD: 2.15e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.36e+04 logL: -1.32e+04 KL: 1.94e+02 MMD: 1.97e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.90e+03 logL: -7.52e+03 KL: 1.90e+02 MMD: 1.72e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.83e+03 logL: -5.49e+03 KL: 1.51e+02 MMD: 1.68e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.63e+03 logL: -5.33e+03 KL: 1.11e+02 MMD: 1.76e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.93e+03 logL: -3.63e+03 KL: 1.21e+02 MMD: 1.59e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.87e+03 logL: -3.59e+03 KL: 9.55e+01 MMD: 1.63e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.84e+03 logL: -3.56e+03 KL: 8.44e+01 MMD: 1.86e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.86e+03 logL: -3.59e+03 KL: 7.66e+01 MMD: 1.71e+00\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 3.80e+03 logL: -3.54e+03 KL: 7.37e+01 MMD: 1.72e+00\n",
      "Epoch 00103: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 107 VALIDATION Loss: 3.79e+03 logL: -3.54e+03 KL: 7.31e+01 MMD: 1.60e+00\n",
      "config 68, alpha = 0.0, lambda = 2611.5, dropout = 0.00; 2 hidden layers with 61, 41 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.53e+05 logL: -3.46e+05 KL: 1.78e+03 MMD: 1.93e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.40e+05 logL: -3.31e+05 KL: 3.60e+03 MMD: 2.09e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.21e+05 logL: -3.12e+05 KL: 3.67e+03 MMD: 1.94e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.97e+05 logL: -2.89e+05 KL: 3.02e+03 MMD: 1.83e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.24e+05 logL: -2.15e+05 KL: 4.80e+03 MMD: 1.90e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.94e+04 logL: -7.99e+04 KL: 4.63e+03 MMD: 1.87e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.30e+04 logL: -1.43e+04 KL: 3.28e+03 MMD: 2.06e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.91e+04 logL: -1.25e+04 KL: 1.65e+03 MMD: 1.89e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.78e+04 logL: -1.16e+04 KL: 9.56e+02 MMD: 2.01e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.62e+04 logL: -1.04e+04 KL: 6.60e+02 MMD: 1.98e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.40e+04 logL: -8.54e+03 KL: 5.26e+02 MMD: 1.89e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.18e+04 logL: -6.16e+03 KL: 4.50e+02 MMD: 2.00e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 9.88e+03 logL: -4.60e+03 KL: 3.63e+02 MMD: 1.89e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 8.43e+03 logL: -3.36e+03 KL: 3.02e+02 MMD: 1.83e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 7.81e+03 logL: -2.66e+03 KL: 2.52e+02 MMD: 1.88e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 7.66e+03 logL: -2.28e+03 KL: 2.09e+02 MMD: 1.98e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 7.23e+03 logL: -2.09e+03 KL: 1.80e+02 MMD: 1.90e+00\n",
      "Epoch 00171: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 180 VALIDATION Loss: 7.15e+03 logL: -2.02e+03 KL: 1.76e+02 MMD: 1.90e+00\n",
      "Epoch 00180: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 184 VALIDATION Loss: 7.72e+03 logL: -2.01e+03 KL: 1.75e+02 MMD: 2.12e+00\n",
      "config 69, alpha = 0.0, lambda = 165.7, dropout = 0.00; 2 hidden layers with 78, 77 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.71e+03 logL: -9.44e+03 KL: 1.49e+02 MMD: 7.19e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.64e+03 logL: -9.43e+03 KL: 8.82e+01 MMD: 7.41e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.31e+03 logL: -9.11e+03 KL: 8.68e+01 MMD: 7.29e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 8.83e+03 logL: -8.66e+03 KL: 5.47e+01 MMD: 7.40e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 8.75e+03 logL: -8.57e+03 KL: 4.90e+01 MMD: 7.92e-01\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 55 VALIDATION Loss: 8.54e+03 logL: -8.35e+03 KL: 5.39e+01 MMD: 8.11e-01\n",
      "config 69, alpha = 0.0, lambda = 28353.2, dropout = 0.00; 2 hidden layers with 17, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.46e+04 logL: -1.41e+04 KL: 1.46e+03 MMD: 1.02e+00\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 4.33e+04 logL: -1.40e+04 KL: 1.23e+03 MMD: 9.91e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 4.59e+04 logL: -1.39e+04 KL: 1.03e+03 MMD: 1.09e+00\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 38 VALIDATION Loss: 4.35e+04 logL: -1.38e+04 KL: 9.42e+02 MMD: 1.01e+00\n",
      "config 69, alpha = 0.0, lambda = 5.3, dropout = 0.00; 2 hidden layers with 67, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.21e+04 logL: -1.15e+04 KL: 5.77e+02 MMD: 1.46e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.40e+03 logL: -7.13e+03 KL: 2.64e+02 MMD: 1.34e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.71e+03 logL: -5.55e+03 KL: 1.60e+02 MMD: 1.30e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.58e+03 logL: -5.46e+03 KL: 1.11e+02 MMD: 1.45e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.31e+03 logL: -5.22e+03 KL: 8.83e+01 MMD: 1.39e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.24e+03 logL: -5.15e+03 KL: 7.66e+01 MMD: 1.43e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.17e+03 logL: -5.09e+03 KL: 7.08e+01 MMD: 1.51e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.47e+03 logL: -5.34e+03 KL: 1.24e+02 MMD: 1.35e+00\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 84 VALIDATION Loss: 5.37e+03 logL: -5.25e+03 KL: 1.20e+02 MMD: 1.37e+00\n",
      "config 69, alpha = 0.0, lambda = 5.7, dropout = 0.00; 2 hidden layers with 50, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.08e+05 logL: -3.70e+05 KL: 2.39e+05 MMD: 1.57e+00\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 6.17e+05 logL: -3.80e+05 KL: 2.37e+05 MMD: 1.62e+00\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 30 VALIDATION Loss: 6.06e+05 logL: -3.69e+05 KL: 2.36e+05 MMD: 1.64e+00\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 40 VALIDATION Loss: 5.98e+05 logL: -3.62e+05 KL: 2.36e+05 MMD: 1.61e+00\n",
      "Stopping\n",
      "====> Epoch: 42 VALIDATION Loss: 6.03e+05 logL: -3.67e+05 KL: 2.36e+05 MMD: 1.59e+00\n",
      "config 69, alpha = 0.0, lambda = 2740.7, dropout = 0.00; 2 hidden layers with 23, 21 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.72e+04 logL: -1.86e+04 KL: 3.28e+03 MMD: 1.96e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.07e+04 logL: -1.46e+04 KL: 9.45e+02 MMD: 1.88e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.94e+04 logL: -1.39e+04 KL: 5.12e+02 MMD: 1.85e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.86e+04 logL: -1.35e+04 KL: 3.85e+02 MMD: 1.75e+00\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.83e+04 logL: -1.28e+04 KL: 3.49e+02 MMD: 1.90e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.78e+04 logL: -1.24e+04 KL: 3.49e+02 MMD: 1.85e+00\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.76e+04 logL: -1.18e+04 KL: 3.49e+02 MMD: 1.98e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.80e+04 logL: -1.17e+04 KL: 3.50e+02 MMD: 2.17e+00\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 90 VALIDATION Loss: 1.73e+04 logL: -1.16e+04 KL: 3.50e+02 MMD: 1.94e+00\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 97 VALIDATION Loss: 1.70e+04 logL: -1.16e+04 KL: 3.51e+02 MMD: 1.82e+00\n",
      "config 70, alpha = 0.0, lambda = 180.2, dropout = 0.00; 2 hidden layers with 41, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.52e+04 logL: -1.37e+04 KL: 1.45e+03 MMD: 6.79e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.41e+04 logL: -1.36e+04 KL: 3.57e+02 MMD: 7.90e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.05e+04 logL: -1.01e+04 KL: 2.77e+02 MMD: 6.58e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.92e+03 logL: -9.61e+03 KL: 1.80e+02 MMD: 7.26e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.83e+03 logL: -9.58e+03 KL: 1.23e+02 MMD: 7.18e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.77e+03 logL: -9.55e+03 KL: 9.29e+01 MMD: 7.36e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.75e+03 logL: -9.54e+03 KL: 7.31e+01 MMD: 7.43e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.73e+03 logL: -9.52e+03 KL: 6.03e+01 MMD: 8.07e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.67e+03 logL: -9.48e+03 KL: 5.12e+01 MMD: 7.85e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 100 VALIDATION Loss: 9.71e+03 logL: -9.52e+03 KL: 4.65e+01 MMD: 7.50e-01\n",
      "Stopping\n",
      "====> Epoch: 103 VALIDATION Loss: 9.70e+03 logL: -9.52e+03 KL: 4.50e+01 MMD: 7.38e-01\n",
      "config 70, alpha = 0.0, lambda = 1442.7, dropout = 0.00; 2 hidden layers with 94, 76 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.81e+03 logL: -5.15e+03 KL: 2.08e+02 MMD: 1.00e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.53e+03 logL: -4.88e+03 KL: 1.37e+02 MMD: 1.05e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.02e+03 logL: -4.54e+03 KL: 1.11e+02 MMD: 9.55e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 5.96e+03 logL: -4.35e+03 KL: 9.03e+01 MMD: 1.05e+00\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 5.84e+03 logL: -4.19e+03 KL: 8.34e+01 MMD: 1.08e+00\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 5.69e+03 logL: -4.18e+03 KL: 8.24e+01 MMD: 9.86e-01\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 66 VALIDATION Loss: 5.81e+03 logL: -4.18e+03 KL: 8.24e+01 MMD: 1.07e+00\n",
      "config 70, alpha = 0.0, lambda = 187.2, dropout = 0.00; 2 hidden layers with 56, 31 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.07e+03 logL: -3.53e+03 KL: 2.95e+02 MMD: 1.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.26e+03 logL: -2.83e+03 KL: 1.63e+02 MMD: 1.41e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.19e+03 logL: -2.83e+03 KL: 1.20e+02 MMD: 1.31e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.09e+03 logL: -2.77e+03 KL: 9.55e+01 MMD: 1.25e+00\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.04e+03 logL: -2.72e+03 KL: 8.53e+01 MMD: 1.31e+00\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 3.03e+03 logL: -2.71e+03 KL: 8.36e+01 MMD: 1.25e+00\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.05e+03 logL: -2.71e+03 KL: 8.32e+01 MMD: 1.38e+00\n",
      "Stopping\n",
      "====> Epoch: 70 VALIDATION Loss: 3.05e+03 logL: -2.71e+03 KL: 8.32e+01 MMD: 1.38e+00\n",
      "config 70, alpha = 0.0, lambda = 64.3, dropout = 0.00; 2 hidden layers with 46, 29 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.26e+05 logL: -3.22e+05 KL: 3.54e+03 MMD: 1.67e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.89e+05 logL: -2.87e+05 KL: 2.21e+03 MMD: 1.68e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.13e+05 logL: -2.11e+05 KL: 1.87e+03 MMD: 1.67e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.45e+04 logL: -9.30e+04 KL: 1.38e+03 MMD: 1.55e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.07e+04 logL: -1.95e+04 KL: 1.07e+03 MMD: 1.63e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.42e+04 logL: -1.34e+04 KL: 7.07e+02 MMD: 1.67e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.34e+04 logL: -1.28e+04 KL: 4.98e+02 MMD: 1.59e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.24e+04 logL: -1.19e+04 KL: 3.95e+02 MMD: 1.73e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.94e+03 logL: -9.46e+03 KL: 3.82e+02 MMD: 1.51e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.91e+03 logL: -6.45e+03 KL: 3.57e+02 MMD: 1.62e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.57e+03 logL: -5.18e+03 KL: 2.91e+02 MMD: 1.68e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.93e+03 logL: -3.59e+03 KL: 2.37e+02 MMD: 1.58e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.42e+03 logL: -3.13e+03 KL: 1.87e+02 MMD: 1.75e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.78e+03 logL: -2.51e+03 KL: 1.58e+02 MMD: 1.66e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.60e+03 logL: -2.36e+03 KL: 1.38e+02 MMD: 1.60e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 2.05e+03 logL: -1.79e+03 KL: 1.53e+02 MMD: 1.59e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 1.97e+03 logL: -1.74e+03 KL: 1.25e+02 MMD: 1.71e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 1.91e+03 logL: -1.71e+03 KL: 1.07e+02 MMD: 1.59e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 1.84e+03 logL: -1.63e+03 KL: 9.46e+01 MMD: 1.72e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 1.80e+03 logL: -1.61e+03 KL: 8.35e+01 MMD: 1.74e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 1.80e+03 logL: -1.61e+03 KL: 8.35e+01 MMD: 1.74e+00\n",
      "config 70, alpha = 0.0, lambda = 152.9, dropout = 0.00; 2 hidden layers with 35, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.30e+03 logL: -5.63e+03 KL: 3.56e+02 MMD: 2.07e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.12e+03 logL: -3.63e+03 KL: 1.98e+02 MMD: 1.88e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.23e+03 logL: -2.78e+03 KL: 1.53e+02 MMD: 1.91e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.64e+03 logL: -2.23e+03 KL: 1.28e+02 MMD: 1.86e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.49e+03 logL: -2.10e+03 KL: 1.09e+02 MMD: 1.86e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.09e+03 logL: -1.69e+03 KL: 1.01e+02 MMD: 1.99e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.07e+03 logL: -1.66e+03 KL: 8.96e+01 MMD: 2.10e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.94e+03 logL: -1.59e+03 KL: 7.83e+01 MMD: 1.79e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.93e+03 logL: -1.59e+03 KL: 7.15e+01 MMD: 1.76e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.89e+03 logL: -1.59e+03 KL: 6.20e+01 MMD: 1.59e+00\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.78e+03 logL: -1.50e+03 KL: 5.93e+01 MMD: 1.48e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.55e+03 logL: -1.26e+03 KL: 6.09e+01 MMD: 1.51e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.47e+03 logL: -1.17e+03 KL: 5.95e+01 MMD: 1.55e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.40e+03 logL: -1.15e+03 KL: 5.79e+01 MMD: 1.28e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.43e+03 logL: -1.14e+03 KL: 5.64e+01 MMD: 1.48e+00\n",
      "Epoch 00150: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00158: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 160 VALIDATION Loss: 1.39e+03 logL: -1.14e+03 KL: 5.60e+01 MMD: 1.28e+00\n",
      "Stopping\n",
      "====> Epoch: 162 VALIDATION Loss: 1.39e+03 logL: -1.14e+03 KL: 5.60e+01 MMD: 1.28e+00\n",
      "config 71, alpha = 0.0, lambda = 7649.2, dropout = 0.00; 2 hidden layers with 12, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.98e+04 logL: -1.42e+04 KL: 7.18e+01 MMD: 7.27e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 2.00e+04 logL: -1.36e+04 KL: 8.91e+01 MMD: 8.19e-01\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.92e+04 logL: -1.35e+04 KL: 1.34e+02 MMD: 7.22e-01\n",
      "Stopping\n",
      "====> Epoch: 32 VALIDATION Loss: 1.90e+04 logL: -1.35e+04 KL: 1.36e+02 MMD: 7.07e-01\n",
      "config 71, alpha = 0.0, lambda = 13.1, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.43e+04 logL: -1.40e+04 KL: 3.08e+02 MMD: 1.07e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+04 logL: -1.36e+04 KL: 2.06e+02 MMD: 1.05e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.37e+04 logL: -1.36e+04 KL: 1.32e+02 MMD: 1.12e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 8.92e+01 MMD: 1.14e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.35e+04 logL: -1.35e+04 KL: 6.58e+01 MMD: 1.28e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 5.47e+01 MMD: 1.24e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 4.60e+01 MMD: 1.38e+00\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 73 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 4.29e+01 MMD: 1.48e+00\n",
      "config 71, alpha = 0.0, lambda = 73652.9, dropout = 0.00; 2 hidden layers with 18, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.14e+05 logL: -1.36e+04 KL: 7.44e+02 MMD: 1.36e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.04e+05 logL: -9.64e+03 KL: 5.05e+02 MMD: 1.27e+00\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.08e+05 logL: -9.43e+03 KL: 4.45e+02 MMD: 1.33e+00\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 34 VALIDATION Loss: 1.04e+05 logL: -9.43e+03 KL: 4.47e+02 MMD: 1.28e+00\n",
      "config 71, alpha = 0.0, lambda = 2042.1, dropout = 0.00; 2 hidden layers with 27, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.62e+05 logL: -3.51e+05 KL: 7.06e+03 MMD: 1.71e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.47e+05 logL: -3.41e+05 KL: 3.21e+03 MMD: 1.60e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.33e+05 logL: -3.26e+05 KL: 3.95e+03 MMD: 1.70e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.22e+05 logL: -3.15e+05 KL: 4.09e+03 MMD: 1.67e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.09e+05 logL: -3.03e+05 KL: 2.97e+03 MMD: 1.63e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.89e+05 logL: -2.84e+05 KL: 1.81e+03 MMD: 1.74e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.56e+05 logL: -2.52e+05 KL: 1.33e+03 MMD: 1.55e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 80 VALIDATION Loss: 1.97e+05 logL: -1.93e+05 KL: 1.34e+03 MMD: 1.67e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.55e+04 logL: -6.00e+04 KL: 2.27e+03 MMD: 1.57e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.24e+04 logL: -1.70e+04 KL: 1.70e+03 MMD: 1.80e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.91e+04 logL: -1.44e+04 KL: 1.22e+03 MMD: 1.67e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.82e+04 logL: -1.40e+04 KL: 8.77e+02 MMD: 1.61e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.75e+04 logL: -1.34e+04 KL: 5.99e+02 MMD: 1.74e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.59e+04 logL: -1.20e+04 KL: 4.09e+02 MMD: 1.70e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.37e+04 logL: -1.02e+04 KL: 2.85e+02 MMD: 1.55e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.16e+04 logL: -8.19e+03 KL: 2.44e+02 MMD: 1.56e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 9.23e+03 logL: -6.02e+03 KL: 2.41e+02 MMD: 1.45e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 8.05e+03 logL: -4.31e+03 KL: 2.48e+02 MMD: 1.71e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 7.22e+03 logL: -3.79e+03 KL: 2.27e+02 MMD: 1.57e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 6.94e+03 logL: -3.58e+03 KL: 2.00e+02 MMD: 1.55e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 6.94e+03 logL: -3.58e+03 KL: 2.00e+02 MMD: 1.55e+00\n",
      "config 71, alpha = 0.0, lambda = 21146.6, dropout = 0.00; 2 hidden layers with 30, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.67e+04 logL: -1.39e+04 KL: 9.70e+02 MMD: 1.98e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.76e+04 logL: -9.72e+03 KL: 7.06e+02 MMD: 1.76e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.49e+04 logL: -5.01e+03 KL: 4.48e+02 MMD: 1.87e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.33e+04 logL: -3.81e+03 KL: 3.45e+02 MMD: 1.85e+00\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 48 VALIDATION Loss: 4.61e+04 logL: -3.48e+03 KL: 3.15e+02 MMD: 2.00e+00\n",
      "config 72, alpha = 0.0, lambda = 71465.0, dropout = 0.00; 2 hidden layers with 33, 31 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.53e+04 logL: -1.16e+04 KL: 9.60e+02 MMD: 7.37e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.81e+04 logL: -9.81e+03 KL: 6.26e+02 MMD: 6.67e-01\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 27 VALIDATION Loss: 6.00e+04 logL: -9.63e+03 KL: 5.80e+02 MMD: 6.96e-01\n",
      "config 72, alpha = 0.0, lambda = 637.3, dropout = 0.00; 2 hidden layers with 55, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.52e+03 logL: -5.56e+03 KL: 2.95e+02 MMD: 1.04e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.36e+03 logL: -5.54e+03 KL: 1.56e+02 MMD: 1.03e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.06e+03 logL: -5.32e+03 KL: 1.12e+02 MMD: 9.95e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 5.83e+03 logL: -5.05e+03 KL: 1.01e+02 MMD: 1.06e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.81e+03 logL: -5.03e+03 KL: 8.98e+01 MMD: 1.09e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.82e+03 logL: -5.10e+03 KL: 8.35e+01 MMD: 1.00e+00\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.41e+03 logL: -4.67e+03 KL: 7.51e+01 MMD: 1.05e+00\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 5.33e+03 logL: -4.64e+03 KL: 7.40e+01 MMD: 9.63e-01\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 86 VALIDATION Loss: 5.36e+03 logL: -4.64e+03 KL: 7.38e+01 MMD: 1.02e+00\n",
      "config 72, alpha = 0.0, lambda = 1.7, dropout = 0.00; 2 hidden layers with 28, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.54e+03 logL: -8.18e+03 KL: 3.53e+02 MMD: 1.33e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.06e+03 logL: -3.84e+03 KL: 2.18e+02 MMD: 1.26e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.77e+03 logL: -3.63e+03 KL: 1.37e+02 MMD: 1.43e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.73e+03 logL: -3.62e+03 KL: 1.09e+02 MMD: 1.32e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.96e+03 logL: -2.85e+03 KL: 1.05e+02 MMD: 1.38e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.85e+03 logL: -2.76e+03 KL: 8.48e+01 MMD: 1.36e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.87e+03 logL: -2.79e+03 KL: 7.62e+01 MMD: 1.31e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.81e+03 logL: -2.74e+03 KL: 6.88e+01 MMD: 1.34e+00\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 2.78e+03 logL: -2.72e+03 KL: 6.38e+01 MMD: 1.40e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.78e+03 logL: -2.72e+03 KL: 6.29e+01 MMD: 1.27e+00\n",
      "Stopping\n",
      "====> Epoch: 101 VALIDATION Loss: 2.78e+03 logL: -2.72e+03 KL: 6.28e+01 MMD: 1.30e+00\n",
      "config 72, alpha = 0.0, lambda = 9434.6, dropout = 0.00; 2 hidden layers with 45, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.02e+04 logL: -1.37e+04 KL: 4.00e+02 MMD: 1.71e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.55e+04 logL: -9.75e+03 KL: 3.28e+02 MMD: 1.64e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.50e+04 logL: -9.58e+03 KL: 2.39e+02 MMD: 1.61e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.49e+04 logL: -9.57e+03 KL: 1.47e+02 MMD: 1.61e+00\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 45 VALIDATION Loss: 2.48e+04 logL: -9.55e+03 KL: 1.22e+02 MMD: 1.60e+00\n",
      "config 72, alpha = 0.0, lambda = 10.9, dropout = 0.00; 2 hidden layers with 53, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.85e+04 logL: -1.60e+04 KL: 2.42e+03 MMD: 2.00e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.58e+04 logL: -1.48e+04 KL: 1.01e+03 MMD: 1.89e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.48e+04 logL: -1.43e+04 KL: 5.22e+02 MMD: 2.04e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.33e+04 logL: -1.29e+04 KL: 4.05e+02 MMD: 2.01e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.69e+03 logL: -8.31e+03 KL: 3.60e+02 MMD: 1.95e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.54e+03 logL: -6.24e+03 KL: 2.85e+02 MMD: 1.84e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.30e+03 logL: -4.02e+03 KL: 2.57e+02 MMD: 2.00e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.44e+03 logL: -3.20e+03 KL: 2.26e+02 MMD: 1.89e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.87e+03 logL: -2.66e+03 KL: 1.95e+02 MMD: 2.03e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.53e+03 logL: -2.35e+03 KL: 1.57e+02 MMD: 1.83e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.89e+03 logL: -1.72e+03 KL: 1.41e+02 MMD: 2.10e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.42e+03 logL: -1.27e+03 KL: 1.36e+02 MMD: 1.82e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.34e+03 logL: -1.20e+03 KL: 1.21e+02 MMD: 1.78e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.27e+03 logL: -1.14e+03 KL: 1.11e+02 MMD: 1.88e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.23e+03 logL: -1.11e+03 KL: 1.01e+02 MMD: 1.87e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.02e+03 logL: -9.05e+02 KL: 9.84e+01 MMD: 1.83e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 9.49e+02 logL: -8.38e+02 KL: 9.18e+01 MMD: 1.95e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 9.38e+02 logL: -8.32e+02 KL: 8.71e+01 MMD: 1.89e+00\n",
      "Epoch 00181: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 190 VALIDATION Loss: 9.05e+02 logL: -8.01e+02 KL: 8.52e+01 MMD: 1.93e+00\n",
      "Stopping\n",
      "====> Epoch: 198 VALIDATION Loss: 9.02e+02 logL: -8.02e+02 KL: 8.29e+01 MMD: 1.74e+00\n",
      "config 73, alpha = 0.0, lambda = 1.9, dropout = 0.00; 2 hidden layers with 7, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.44e+04 logL: -1.35e+04 KL: 8.37e+02 MMD: 8.00e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 2.84e+02 MMD: 8.44e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.41e+02 MMD: 9.07e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.35e+04 logL: -1.35e+04 KL: 8.76e+01 MMD: 9.26e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.87e+03 logL: -9.67e+03 KL: 1.94e+02 MMD: 7.19e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.65e+03 logL: -9.56e+03 KL: 9.28e+01 MMD: 8.06e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.59e+03 logL: -9.52e+03 KL: 6.71e+01 MMD: 7.69e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.57e+03 logL: -9.52e+03 KL: 5.41e+01 MMD: 7.37e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.57e+03 logL: -9.52e+03 KL: 4.61e+01 MMD: 8.00e-01\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 9.58e+03 logL: -9.54e+03 KL: 4.51e+01 MMD: 8.25e-01\n",
      "config 73, alpha = 0.0, lambda = 160.8, dropout = 0.00; 2 hidden layers with 120, 51 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.11e+03 logL: -5.67e+03 KL: 2.69e+02 MMD: 1.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.43e+03 logL: -5.10e+03 KL: 1.58e+02 MMD: 1.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.22e+03 logL: -4.93e+03 KL: 1.29e+02 MMD: 9.88e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 5.11e+03 logL: -4.83e+03 KL: 1.15e+02 MMD: 1.06e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 50 VALIDATION Loss: 4.88e+03 logL: -4.59e+03 KL: 1.03e+02 MMD: 1.12e+00\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.67e+03 logL: -4.41e+03 KL: 9.33e+01 MMD: 9.95e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 4.64e+03 logL: -4.39e+03 KL: 8.81e+01 MMD: 1.01e+00\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 4.61e+03 logL: -4.36e+03 KL: 8.40e+01 MMD: 1.05e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.60e+03 logL: -4.36e+03 KL: 8.32e+01 MMD: 9.98e-01\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 100 VALIDATION Loss: 4.61e+03 logL: -4.35e+03 KL: 8.32e+01 MMD: 1.07e+00\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Stopping\n",
      "====> Epoch: 108 VALIDATION Loss: 4.60e+03 logL: -4.35e+03 KL: 8.31e+01 MMD: 1.01e+00\n",
      "config 73, alpha = 0.0, lambda = 3.3, dropout = 0.00; 2 hidden layers with 126, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.08e+03 logL: -5.74e+03 KL: 3.35e+02 MMD: 1.34e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.01e+03 logL: -2.83e+03 KL: 1.80e+02 MMD: 1.36e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.89e+03 logL: -2.76e+03 KL: 1.23e+02 MMD: 1.32e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.84e+03 logL: -2.74e+03 KL: 1.00e+02 MMD: 1.24e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.84e+03 logL: -2.75e+03 KL: 8.35e+01 MMD: 1.22e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.76e+03 logL: -2.68e+03 KL: 7.44e+01 MMD: 1.28e+00\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.72e+03 logL: -2.65e+03 KL: 6.99e+01 MMD: 1.38e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.72e+03 logL: -2.65e+03 KL: 6.81e+01 MMD: 1.33e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.72e+03 logL: -2.65e+03 KL: 6.57e+01 MMD: 1.36e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.70e+03 logL: -2.63e+03 KL: 6.31e+01 MMD: 1.52e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.70e+03 logL: -2.64e+03 KL: 6.11e+01 MMD: 1.32e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.69e+03 logL: -2.63e+03 KL: 5.96e+01 MMD: 1.37e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.69e+03 logL: -2.63e+03 KL: 5.84e+01 MMD: 1.49e+00\n",
      "Stopping\n",
      "====> Epoch: 132 VALIDATION Loss: 2.70e+03 logL: -2.64e+03 KL: 5.82e+01 MMD: 1.44e+00\n",
      "config 73, alpha = 0.0, lambda = 3591.7, dropout = 0.00; 2 hidden layers with 153, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.07e+04 logL: -1.38e+04 KL: 1.15e+03 MMD: 1.61e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.67e+04 logL: -1.04e+04 KL: 5.20e+02 MMD: 1.63e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.38e+04 logL: -7.79e+03 KL: 3.33e+02 MMD: 1.58e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.21e+04 logL: -5.46e+03 KL: 2.70e+02 MMD: 1.78e+00\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.15e+04 logL: -5.26e+03 KL: 2.23e+02 MMD: 1.67e+00\n",
      "Stopping\n",
      "====> Epoch: 51 VALIDATION Loss: 1.09e+04 logL: -5.26e+03 KL: 2.22e+02 MMD: 1.52e+00\n",
      "config 73, alpha = 0.0, lambda = 282.9, dropout = 0.00; 2 hidden layers with 13, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.54e+05 logL: -3.52e+05 KL: 1.46e+03 MMD: 2.04e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.51e+05 logL: -3.50e+05 KL: 7.36e+02 MMD: 1.80e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.46e+05 logL: -3.45e+05 KL: 7.14e+02 MMD: 1.81e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.40e+05 logL: -3.39e+05 KL: 4.34e+02 MMD: 2.02e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.32e+05 logL: -3.31e+05 KL: 2.68e+02 MMD: 1.90e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.21e+05 logL: -3.20e+05 KL: 1.86e+02 MMD: 1.83e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.06e+05 logL: -3.05e+05 KL: 2.27e+02 MMD: 1.88e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.58e+05 logL: -2.57e+05 KL: 5.12e+02 MMD: 1.93e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.92e+04 logL: -5.80e+04 KL: 6.67e+02 MMD: 2.03e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.57e+04 logL: -1.47e+04 KL: 4.96e+02 MMD: 1.87e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.51e+04 logL: -1.43e+04 KL: 3.34e+02 MMD: 1.95e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.48e+04 logL: -1.40e+04 KL: 2.35e+02 MMD: 1.95e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.46e+04 logL: -1.39e+04 KL: 1.94e+02 MMD: 1.90e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.46e+04 logL: -1.38e+04 KL: 1.73e+02 MMD: 2.00e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.40e+04 logL: -1.33e+04 KL: 1.81e+02 MMD: 2.01e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.18e+04 logL: -1.10e+04 KL: 1.77e+02 MMD: 1.96e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 1.15e+04 logL: -1.08e+04 KL: 1.60e+02 MMD: 1.92e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 1.02e+04 logL: -9.51e+03 KL: 1.48e+02 MMD: 1.92e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 9.89e+03 logL: -9.18e+03 KL: 1.40e+02 MMD: 2.02e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 9.45e+03 logL: -8.74e+03 KL: 1.31e+02 MMD: 2.05e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 9.45e+03 logL: -8.74e+03 KL: 1.31e+02 MMD: 2.05e+00\n",
      "config 74, alpha = 0.0, lambda = 4966.5, dropout = 0.00; 2 hidden layers with 88, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.32e+04 logL: -9.56e+03 KL: 2.66e+02 MMD: 6.75e-01\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 1.31e+04 logL: -9.45e+03 KL: 1.79e+02 MMD: 7.07e-01\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 28 VALIDATION Loss: 1.30e+04 logL: -9.44e+03 KL: 1.75e+02 MMD: 6.77e-01\n",
      "config 74, alpha = 0.0, lambda = 2.7, dropout = 0.00; 2 hidden layers with 104, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.70e+03 logL: -7.35e+03 KL: 3.44e+02 MMD: 1.02e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.69e+03 logL: -5.53e+03 KL: 1.52e+02 MMD: 1.08e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.26e+03 logL: -5.16e+03 KL: 9.52e+01 MMD: 1.03e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.12e+03 logL: -5.03e+03 KL: 8.62e+01 MMD: 9.65e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 5.14e+03 logL: -5.06e+03 KL: 8.50e+01 MMD: 1.01e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.94e+03 logL: -4.85e+03 KL: 8.55e+01 MMD: 1.03e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.84e+03 logL: -4.76e+03 KL: 7.57e+01 MMD: 1.01e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.75e+03 logL: -4.68e+03 KL: 7.09e+01 MMD: 1.09e+00\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 4.61e+03 logL: -4.54e+03 KL: 6.57e+01 MMD: 1.08e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.57e+03 logL: -4.50e+03 KL: 6.36e+01 MMD: 1.03e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 4.54e+03 logL: -4.48e+03 KL: 5.98e+01 MMD: 1.01e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.54e+03 logL: -4.47e+03 KL: 5.91e+01 MMD: 1.07e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 4.50e+03 logL: -4.44e+03 KL: 5.70e+01 MMD: 1.07e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 4.50e+03 logL: -4.44e+03 KL: 5.58e+01 MMD: 1.11e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 4.47e+03 logL: -4.42e+03 KL: 5.48e+01 MMD: 1.09e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 4.48e+03 logL: -4.42e+03 KL: 5.28e+01 MMD: 9.96e-01\n",
      "Epoch 00163: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 170 VALIDATION Loss: 4.45e+03 logL: -4.40e+03 KL: 5.32e+01 MMD: 1.13e+00\n",
      "Epoch 00174: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 175 VALIDATION Loss: 4.45e+03 logL: -4.40e+03 KL: 5.30e+01 MMD: 1.09e+00\n",
      "config 74, alpha = 0.0, lambda = 31.1, dropout = 0.00; 2 hidden layers with 139, 21 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.66e+03 logL: -3.35e+03 KL: 2.79e+02 MMD: 1.30e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.96e+03 logL: -2.76e+03 KL: 1.53e+02 MMD: 1.27e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.88e+03 logL: -2.73e+03 KL: 1.08e+02 MMD: 1.43e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.84e+03 logL: -2.72e+03 KL: 8.54e+01 MMD: 1.26e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.79e+03 logL: -2.68e+03 KL: 7.39e+01 MMD: 1.34e+00\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.72e+03 logL: -2.61e+03 KL: 7.13e+01 MMD: 1.35e+00\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 64 VALIDATION Loss: 2.71e+03 logL: -2.60e+03 KL: 7.10e+01 MMD: 1.33e+00\n",
      "config 74, alpha = 0.0, lambda = 22.2, dropout = 0.00; 2 hidden layers with 115, 81 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.01e+03 logL: -3.70e+03 KL: 2.72e+02 MMD: 1.72e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.02e+03 logL: -1.79e+03 KL: 1.92e+02 MMD: 1.62e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.85e+03 logL: -1.67e+03 KL: 1.46e+02 MMD: 1.60e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.72e+03 logL: -1.56e+03 KL: 1.26e+02 MMD: 1.53e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.71e+03 logL: -1.56e+03 KL: 1.17e+02 MMD: 1.58e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.69e+03 logL: -1.55e+03 KL: 1.06e+02 MMD: 1.67e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.68e+03 logL: -1.55e+03 KL: 9.73e+01 MMD: 1.62e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.67e+03 logL: -1.55e+03 KL: 9.18e+01 MMD: 1.52e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.66e+03 logL: -1.54e+03 KL: 8.81e+01 MMD: 1.70e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.64e+03 logL: -1.52e+03 KL: 8.48e+01 MMD: 1.69e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.63e+03 logL: -1.52e+03 KL: 8.23e+01 MMD: 1.49e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.62e+03 logL: -1.50e+03 KL: 8.02e+01 MMD: 1.50e+00\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.60e+03 logL: -1.49e+03 KL: 7.97e+01 MMD: 1.48e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.60e+03 logL: -1.49e+03 KL: 7.97e+01 MMD: 1.47e+00\n",
      "Stopping\n",
      "====> Epoch: 140 VALIDATION Loss: 1.60e+03 logL: -1.49e+03 KL: 7.97e+01 MMD: 1.47e+00\n",
      "config 74, alpha = 0.0, lambda = 521.2, dropout = 0.00; 2 hidden layers with 87, 29 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.11e+04 logL: -9.67e+03 KL: 4.24e+02 MMD: 1.89e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.18e+03 logL: -4.00e+03 KL: 2.37e+02 MMD: 1.81e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.59e+03 logL: -2.42e+03 KL: 1.80e+02 MMD: 1.90e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.32e+03 logL: -2.17e+03 KL: 1.44e+02 MMD: 1.92e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.83e+03 logL: -1.63e+03 KL: 1.21e+02 MMD: 2.09e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.65e+03 logL: -1.66e+03 KL: 9.07e+01 MMD: 1.74e+00\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.47e+03 logL: -1.58e+03 KL: 7.28e+01 MMD: 1.57e+00\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 2.48e+03 logL: -1.58e+03 KL: 7.02e+01 MMD: 1.59e+00\n",
      "Epoch 00085: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 89 VALIDATION Loss: 2.42e+03 logL: -1.58e+03 KL: 6.97e+01 MMD: 1.47e+00\n",
      "config 75, alpha = 0.0, lambda = 2.7, dropout = 0.00; 2 hidden layers with 63, 60 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.67e+03 logL: -9.50e+03 KL: 1.68e+02 MMD: 6.96e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.49e+03 logL: -9.40e+03 KL: 8.77e+01 MMD: 7.72e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.01e+03 logL: -8.93e+03 KL: 7.71e+01 MMD: 7.56e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 8.92e+03 logL: -8.86e+03 KL: 5.24e+01 MMD: 8.07e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 8.57e+03 logL: -8.51e+03 KL: 5.33e+01 MMD: 7.41e-01\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 58 VALIDATION Loss: 8.32e+03 logL: -8.27e+03 KL: 5.25e+01 MMD: 8.40e-01\n",
      "config 75, alpha = 0.0, lambda = 6464.5, dropout = 0.00; 2 hidden layers with 61, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.47e+05 logL: -3.45e+05 KL: 9.59e+00 MMD: 3.43e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.31e+05 logL: -3.30e+05 KL: 7.01e+00 MMD: 2.23e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.98e+05 logL: -2.97e+05 KL: 4.18e+00 MMD: 1.48e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.30e+05 logL: -2.30e+05 KL: 1.69e+00 MMD: 7.04e-02\n",
      "====> Epoch: 50 VALIDATION Loss: 1.38e+05 logL: -1.38e+05 KL: 7.91e-01 MMD: 2.46e-02\n",
      "====> Epoch: 60 VALIDATION Loss: 6.83e+04 logL: -6.81e+04 KL: 6.44e-01 MMD: 2.83e-02\n",
      "====> Epoch: 70 VALIDATION Loss: 4.52e+04 logL: -4.50e+04 KL: 6.03e-01 MMD: 2.98e-02\n",
      "====> Epoch: 80 VALIDATION Loss: 4.26e+04 logL: -4.25e+04 KL: 5.84e-01 MMD: 2.05e-02\n",
      "====> Epoch: 90 VALIDATION Loss: 4.26e+04 logL: -4.24e+04 KL: 5.21e-01 MMD: 2.44e-02\n",
      "Stopping\n",
      "====> Epoch: 96 VALIDATION Loss: 4.25e+04 logL: -4.24e+04 KL: 4.66e-01 MMD: 1.45e-02\n",
      "config 75, alpha = 0.0, lambda = 771.3, dropout = 0.00; 2 hidden layers with 114, 32 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.56e+04 logL: -1.37e+04 KL: 8.47e+02 MMD: 1.44e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.14e+04 logL: -9.98e+03 KL: 4.02e+02 MMD: 1.31e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.12e+03 logL: -7.79e+03 KL: 2.90e+02 MMD: 1.35e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.92e+03 logL: -5.73e+03 KL: 2.31e+02 MMD: 1.25e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.98e+03 logL: -3.81e+03 KL: 1.89e+02 MMD: 1.26e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.51e+03 logL: -3.41e+03 KL: 1.49e+02 MMD: 1.24e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.92e+03 logL: -2.85e+03 KL: 1.23e+02 MMD: 1.23e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.94e+03 logL: -2.75e+03 KL: 1.02e+02 MMD: 1.42e+00\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.69e+03 logL: -2.71e+03 KL: 9.35e+01 MMD: 1.15e+00\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 3.82e+03 logL: -2.70e+03 KL: 9.05e+01 MMD: 1.34e+00\n",
      "Stopping\n",
      "====> Epoch: 100 VALIDATION Loss: 3.82e+03 logL: -2.70e+03 KL: 9.05e+01 MMD: 1.34e+00\n",
      "config 75, alpha = 0.0, lambda = 1.6, dropout = 0.00; 2 hidden layers with 47, 45 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.39e+04 logL: -1.35e+04 KL: 4.28e+02 MMD: 1.62e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.33e+03 logL: -8.13e+03 KL: 2.08e+02 MMD: 1.66e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.29e+03 logL: -6.12e+03 KL: 1.63e+02 MMD: 1.67e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.80e+03 logL: -3.63e+03 KL: 1.66e+02 MMD: 1.67e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.97e+03 logL: -2.82e+03 KL: 1.52e+02 MMD: 1.60e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.28e+03 logL: -2.14e+03 KL: 1.45e+02 MMD: 1.47e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.08e+03 logL: -1.95e+03 KL: 1.29e+02 MMD: 1.67e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.95e+03 logL: -1.84e+03 KL: 1.13e+02 MMD: 1.70e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.85e+03 logL: -1.74e+03 KL: 1.03e+02 MMD: 1.66e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.80e+03 logL: -1.70e+03 KL: 9.52e+01 MMD: 1.59e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.72e+03 logL: -1.63e+03 KL: 8.92e+01 MMD: 1.60e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.69e+03 logL: -1.60e+03 KL: 8.34e+01 MMD: 1.57e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.65e+03 logL: -1.57e+03 KL: 7.84e+01 MMD: 1.69e+00\n",
      "Epoch 00131: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 1.61e+03 logL: -1.54e+03 KL: 7.78e+01 MMD: 1.57e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.61e+03 logL: -1.53e+03 KL: 7.67e+01 MMD: 1.53e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.60e+03 logL: -1.52e+03 KL: 7.46e+01 MMD: 1.41e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 1.59e+03 logL: -1.52e+03 KL: 7.33e+01 MMD: 1.56e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 1.59e+03 logL: -1.51e+03 KL: 7.18e+01 MMD: 1.59e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 1.59e+03 logL: -1.52e+03 KL: 7.06e+01 MMD: 1.54e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 1.57e+03 logL: -1.50e+03 KL: 6.96e+01 MMD: 1.58e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 1.57e+03 logL: -1.50e+03 KL: 6.96e+01 MMD: 1.58e+00\n",
      "config 75, alpha = 0.0, lambda = 51.7, dropout = 0.00; 2 hidden layers with 27, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.66e+03 logL: -6.25e+03 KL: 3.13e+02 MMD: 1.84e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.89e+03 logL: -3.62e+03 KL: 1.82e+02 MMD: 1.81e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.19e+03 logL: -2.95e+03 KL: 1.41e+02 MMD: 2.08e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.40e+03 logL: -2.17e+03 KL: 1.26e+02 MMD: 1.95e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.36e+03 logL: -2.16e+03 KL: 1.03e+02 MMD: 1.91e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.33e+03 logL: -2.13e+03 KL: 9.16e+01 MMD: 2.08e+00\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.31e+03 logL: -2.11e+03 KL: 9.01e+01 MMD: 2.08e+00\n",
      "Stopping\n",
      "====> Epoch: 72 VALIDATION Loss: 2.31e+03 logL: -2.11e+03 KL: 9.00e+01 MMD: 2.08e+00\n",
      "config 76, alpha = 0.0, lambda = 36147.0, dropout = 0.00; 2 hidden layers with 25, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.14e+04 logL: -1.53e+04 KL: 1.05e+02 MMD: 7.20e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 4.15e+04 logL: -1.40e+04 KL: 2.45e+02 MMD: 7.53e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 4.12e+04 logL: -1.36e+04 KL: 4.17e+02 MMD: 7.51e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 3.59e+04 logL: -9.69e+03 KL: 4.80e+02 MMD: 7.11e-01\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.48e+04 logL: -9.61e+03 KL: 4.31e+02 MMD: 6.84e-01\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 54 VALIDATION Loss: 3.45e+04 logL: -9.61e+03 KL: 4.32e+02 MMD: 6.76e-01\n",
      "config 76, alpha = 0.0, lambda = 90.4, dropout = 0.00; 2 hidden layers with 15, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.72e+03 logL: -8.06e+03 KL: 5.68e+02 MMD: 1.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.87e+03 logL: -5.55e+03 KL: 2.23e+02 MMD: 1.09e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.56e+03 logL: -5.34e+03 KL: 1.30e+02 MMD: 9.93e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 5.39e+03 logL: -5.19e+03 KL: 9.99e+01 MMD: 1.11e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.34e+03 logL: -5.17e+03 KL: 8.06e+01 MMD: 1.07e+00\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 5.30e+03 logL: -5.13e+03 KL: 7.51e+01 MMD: 1.04e+00\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.29e+03 logL: -5.12e+03 KL: 7.35e+01 MMD: 1.06e+00\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 78 VALIDATION Loss: 5.28e+03 logL: -5.12e+03 KL: 7.34e+01 MMD: 9.71e-01\n",
      "config 76, alpha = 0.0, lambda = 129.6, dropout = 0.00; 2 hidden layers with 60, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.34e+04 logL: -1.29e+04 KL: 2.51e+02 MMD: 1.55e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.00e+04 logL: -9.64e+03 KL: 1.93e+02 MMD: 1.50e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.97e+03 logL: -9.65e+03 KL: 1.30e+02 MMD: 1.46e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.61e+03 logL: -8.23e+03 KL: 1.96e+02 MMD: 1.45e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.38e+03 logL: -7.08e+03 KL: 1.05e+02 MMD: 1.52e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.28e+03 logL: -7.01e+03 KL: 8.94e+01 MMD: 1.46e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.25e+03 logL: -6.97e+03 KL: 8.25e+01 MMD: 1.55e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.77e+03 logL: -5.51e+03 KL: 8.71e+01 MMD: 1.37e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.59e+03 logL: -5.34e+03 KL: 8.11e+01 MMD: 1.33e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.94e+03 logL: -3.66e+03 KL: 1.21e+02 MMD: 1.30e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.80e+03 logL: -3.55e+03 KL: 8.19e+01 MMD: 1.33e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.83e+03 logL: -3.59e+03 KL: 6.89e+01 MMD: 1.34e+00\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 3.77e+03 logL: -3.52e+03 KL: 6.74e+01 MMD: 1.43e+00\n",
      "Epoch 00138: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 140 VALIDATION Loss: 3.75e+03 logL: -3.52e+03 KL: 6.71e+01 MMD: 1.25e+00\n",
      "Epoch 00146: reducing learning rate of group 0 to 1.0000e-07.\n",
      "====> Epoch: 150 VALIDATION Loss: 3.75e+03 logL: -3.52e+03 KL: 6.71e+01 MMD: 1.31e+00\n",
      "Stopping\n",
      "====> Epoch: 150 VALIDATION Loss: 3.75e+03 logL: -3.52e+03 KL: 6.71e+01 MMD: 1.31e+00\n",
      "config 76, alpha = 0.0, lambda = 7.9, dropout = 0.00; 2 hidden layers with 47, 36 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.43e+04 logL: -1.36e+04 KL: 7.28e+02 MMD: 1.62e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 2.67e+02 MMD: 1.69e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.05e+04 logL: -1.03e+04 KL: 2.18e+02 MMD: 1.70e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.63e+03 logL: -7.43e+03 KL: 1.81e+02 MMD: 1.70e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.92e+03 logL: -4.74e+03 KL: 1.73e+02 MMD: 1.56e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 4.12e+03 logL: -3.96e+03 KL: 1.49e+02 MMD: 1.78e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.89e+03 logL: -2.74e+03 KL: 1.42e+02 MMD: 1.69e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.65e+03 logL: -2.51e+03 KL: 1.25e+02 MMD: 1.61e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.31e+03 logL: -2.18e+03 KL: 1.18e+02 MMD: 1.68e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.92e+03 logL: -1.80e+03 KL: 1.15e+02 MMD: 1.71e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.81e+03 logL: -1.69e+03 KL: 1.11e+02 MMD: 1.65e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.75e+03 logL: -1.64e+03 KL: 1.02e+02 MMD: 1.48e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.69e+03 logL: -1.59e+03 KL: 9.25e+01 MMD: 1.55e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.66e+03 logL: -1.56e+03 KL: 8.63e+01 MMD: 1.62e+00\n",
      "Epoch 00144: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 1.62e+03 logL: -1.52e+03 KL: 8.26e+01 MMD: 1.68e+00\n",
      "Stopping\n",
      "====> Epoch: 159 VALIDATION Loss: 1.62e+03 logL: -1.52e+03 KL: 8.07e+01 MMD: 1.69e+00\n",
      "config 76, alpha = 0.0, lambda = 3.5, dropout = 0.00; 2 hidden layers with 23, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.48e+04 logL: -1.37e+04 KL: 1.07e+03 MMD: 1.98e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.35e+04 logL: -1.31e+04 KL: 4.06e+02 MMD: 2.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.90e+03 logL: -9.67e+03 KL: 2.26e+02 MMD: 2.08e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.09e+03 logL: -6.86e+03 KL: 2.28e+02 MMD: 1.96e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.20e+03 logL: -4.00e+03 KL: 1.89e+02 MMD: 1.98e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.94e+03 logL: -3.80e+03 KL: 1.42e+02 MMD: 1.96e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.19e+03 logL: -3.06e+03 KL: 1.25e+02 MMD: 1.90e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.01e+03 logL: -2.89e+03 KL: 1.10e+02 MMD: 1.99e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.50e+03 logL: -2.39e+03 KL: 1.05e+02 MMD: 2.01e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.29e+03 logL: -2.19e+03 KL: 9.57e+01 MMD: 1.95e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.73e+03 logL: -1.63e+03 KL: 9.63e+01 MMD: 1.87e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.28e+03 logL: -1.18e+03 KL: 9.40e+01 MMD: 1.77e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.25e+03 logL: -1.17e+03 KL: 8.49e+01 MMD: 1.77e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 1.29e+03 logL: -1.21e+03 KL: 7.87e+01 MMD: 1.90e+00\n",
      "Epoch 00142: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 1.20e+03 logL: -1.12e+03 KL: 7.75e+01 MMD: 1.85e+00\n",
      "Stopping\n",
      "====> Epoch: 154 VALIDATION Loss: 1.20e+03 logL: -1.12e+03 KL: 7.70e+01 MMD: 1.88e+00\n",
      "config 77, alpha = 0.0, lambda = 12846.8, dropout = 0.00; 2 hidden layers with 18, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.94e+04 logL: -9.64e+03 KL: 4.70e+02 MMD: 7.21e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.85e+04 logL: -9.57e+03 KL: 2.89e+02 MMD: 6.74e-01\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.81e+04 logL: -9.53e+03 KL: 2.75e+02 MMD: 6.42e-01\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.94e+04 logL: -9.53e+03 KL: 2.68e+02 MMD: 7.45e-01\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.90e+04 logL: -9.53e+03 KL: 2.66e+02 MMD: 7.20e-01\n",
      "Stopping\n",
      "====> Epoch: 53 VALIDATION Loss: 1.84e+04 logL: -9.53e+03 KL: 2.66e+02 MMD: 6.66e-01\n",
      "config 77, alpha = 0.0, lambda = 1.5, dropout = 0.00; 2 hidden layers with 60, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.34e+03 logL: -7.11e+03 KL: 2.34e+02 MMD: 9.69e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 5.25e+03 logL: -5.09e+03 KL: 1.55e+02 MMD: 1.01e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.22e+03 logL: -5.11e+03 KL: 1.14e+02 MMD: 1.06e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.11e+03 logL: -5.01e+03 KL: 9.77e+01 MMD: 9.08e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 4.97e+03 logL: -4.88e+03 KL: 8.79e+01 MMD: 9.26e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 4.93e+03 logL: -4.85e+03 KL: 8.06e+01 MMD: 1.01e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.95e+03 logL: -4.87e+03 KL: 7.41e+01 MMD: 1.14e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.96e+03 logL: -4.89e+03 KL: 6.71e+01 MMD: 1.10e+00\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 4.75e+03 logL: -4.68e+03 KL: 6.58e+01 MMD: 1.06e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.75e+03 logL: -4.69e+03 KL: 6.31e+01 MMD: 1.04e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 4.73e+03 logL: -4.67e+03 KL: 6.25e+01 MMD: 1.02e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.72e+03 logL: -4.66e+03 KL: 6.01e+01 MMD: 1.04e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 4.71e+03 logL: -4.65e+03 KL: 5.82e+01 MMD: 9.50e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 4.70e+03 logL: -4.64e+03 KL: 5.70e+01 MMD: 1.05e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00149: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 150 VALIDATION Loss: 4.69e+03 logL: -4.63e+03 KL: 5.64e+01 MMD: 1.12e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 4.69e+03 logL: -4.63e+03 KL: 5.64e+01 MMD: 9.99e-01\n",
      "Stopping\n",
      "====> Epoch: 160 VALIDATION Loss: 4.69e+03 logL: -4.63e+03 KL: 5.64e+01 MMD: 9.99e-01\n",
      "config 77, alpha = 0.0, lambda = 6.2, dropout = 0.00; 2 hidden layers with 195, 30 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.46e+05 logL: -3.29e+05 KL: 1.75e+04 MMD: 1.39e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.37e+04 logL: -3.00e+04 KL: 1.36e+04 MMD: 1.22e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.88e+04 logL: -1.43e+04 KL: 4.52e+03 MMD: 1.39e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.59e+04 logL: -1.39e+04 KL: 1.93e+03 MMD: 1.34e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.47e+04 logL: -1.38e+04 KL: 9.12e+02 MMD: 1.36e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.41e+04 logL: -1.36e+04 KL: 4.87e+02 MMD: 1.31e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.32e+04 logL: -1.29e+04 KL: 2.95e+02 MMD: 1.39e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.07e+04 logL: -1.05e+04 KL: 2.46e+02 MMD: 1.39e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.37e+03 logL: -9.16e+03 KL: 1.97e+02 MMD: 1.41e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 8.35e+03 logL: -8.19e+03 KL: 1.56e+02 MMD: 1.35e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 7.83e+03 logL: -7.70e+03 KL: 1.23e+02 MMD: 1.35e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.75e+03 logL: -4.47e+03 KL: 2.69e+02 MMD: 1.41e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 4.21e+03 logL: -4.00e+03 KL: 2.09e+02 MMD: 1.32e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.98e+03 logL: -3.81e+03 KL: 1.65e+02 MMD: 1.43e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 3.84e+03 logL: -3.70e+03 KL: 1.33e+02 MMD: 1.30e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 3.71e+03 logL: -3.59e+03 KL: 1.11e+02 MMD: 1.29e+00\n",
      "Epoch 00168: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 170 VALIDATION Loss: 3.62e+03 logL: -3.52e+03 KL: 9.94e+01 MMD: 1.30e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 3.60e+03 logL: -3.50e+03 KL: 9.47e+01 MMD: 1.27e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 3.60e+03 logL: -3.50e+03 KL: 8.98e+01 MMD: 1.32e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 3.57e+03 logL: -3.48e+03 KL: 8.55e+01 MMD: 1.37e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 3.57e+03 logL: -3.48e+03 KL: 8.55e+01 MMD: 1.37e+00\n",
      "config 77, alpha = 0.0, lambda = 8893.7, dropout = 0.00; 2 hidden layers with 69, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.27e+04 logL: -7.90e+03 KL: 5.52e+02 MMD: 1.60e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.89e+04 logL: -5.24e+03 KL: 3.38e+02 MMD: 1.50e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.94e+04 logL: -5.17e+03 KL: 2.79e+02 MMD: 1.57e+00\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 34 VALIDATION Loss: 1.97e+04 logL: -5.10e+03 KL: 2.77e+02 MMD: 1.61e+00\n",
      "config 77, alpha = 0.0, lambda = 2.2, dropout = 0.00; 2 hidden layers with 135, 52 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.72e+03 logL: -7.39e+03 KL: 3.34e+02 MMD: 1.99e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.08e+03 logL: -2.86e+03 KL: 2.10e+02 MMD: 1.91e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.59e+03 logL: -1.42e+03 KL: 1.76e+02 MMD: 1.85e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+03 logL: -1.21e+03 KL: 1.50e+02 MMD: 1.94e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.10e+03 logL: -9.67e+02 KL: 1.30e+02 MMD: 1.99e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.02e+03 logL: -8.94e+02 KL: 1.21e+02 MMD: 2.04e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.66e+02 logL: -8.54e+02 KL: 1.10e+02 MMD: 1.77e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.44e+02 logL: -8.40e+02 KL: 1.02e+02 MMD: 1.86e+00\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 8.97e+02 logL: -7.97e+02 KL: 9.81e+01 MMD: 2.02e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 8.92e+02 logL: -7.95e+02 KL: 9.44e+01 MMD: 1.95e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 8.87e+02 logL: -7.94e+02 KL: 9.03e+01 MMD: 2.02e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 8.75e+02 logL: -7.89e+02 KL: 8.41e+01 MMD: 1.88e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 8.64e+02 logL: -7.83e+02 KL: 7.84e+01 MMD: 1.86e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 8.59e+02 logL: -7.80e+02 KL: 7.67e+01 MMD: 1.92e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 8.59e+02 logL: -7.82e+02 KL: 7.54e+01 MMD: 1.81e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 8.51e+02 logL: -7.75e+02 KL: 7.39e+01 MMD: 1.69e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 8.48e+02 logL: -7.73e+02 KL: 7.28e+01 MMD: 1.87e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 8.42e+02 logL: -7.68e+02 KL: 7.09e+01 MMD: 1.83e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 8.42e+02 logL: -7.69e+02 KL: 7.03e+01 MMD: 1.81e+00\n",
      "Epoch 00192: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 200 VALIDATION Loss: 8.34e+02 logL: -7.62e+02 KL: 6.97e+01 MMD: 1.87e+00\n",
      "Epoch 00200: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 8.34e+02 logL: -7.62e+02 KL: 6.97e+01 MMD: 1.87e+00\n",
      "config 78, alpha = 0.0, lambda = 5382.6, dropout = 0.00; 2 hidden layers with 22, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.90e+04 logL: -1.40e+04 KL: 2.40e+02 MMD: 8.93e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.45e+04 logL: -1.04e+04 KL: 3.56e+02 MMD: 6.86e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.37e+04 logL: -9.60e+03 KL: 2.33e+02 MMD: 7.28e-01\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 38 VALIDATION Loss: 1.37e+04 logL: -9.55e+03 KL: 2.04e+02 MMD: 7.36e-01\n",
      "config 78, alpha = 0.0, lambda = 1752.8, dropout = 0.00; 2 hidden layers with 55, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.54e+05 logL: -3.52e+05 KL: 2.32e+02 MMD: 1.05e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.52e+05 logL: -3.50e+05 KL: 1.72e+02 MMD: 1.21e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.49e+05 logL: -3.46e+05 KL: 3.18e+02 MMD: 1.07e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.42e+05 logL: -3.39e+05 KL: 1.00e+03 MMD: 1.14e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.31e+05 logL: -3.27e+05 KL: 2.30e+03 MMD: 1.00e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.16e+05 logL: -3.12e+05 KL: 2.14e+03 MMD: 1.11e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.91e+05 logL: -2.87e+05 KL: 2.32e+03 MMD: 1.05e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.28e+05 logL: -2.24e+05 KL: 2.25e+03 MMD: 1.05e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.89e+04 logL: -6.54e+04 KL: 1.82e+03 MMD: 9.70e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 1.74e+04 logL: -1.44e+04 KL: 1.25e+03 MMD: 1.01e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.67e+04 logL: -1.40e+04 KL: 8.09e+02 MMD: 1.10e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.62e+04 logL: -1.37e+04 KL: 5.36e+02 MMD: 1.09e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.53e+04 logL: -1.33e+04 KL: 3.59e+02 MMD: 9.38e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 1.42e+04 logL: -1.22e+04 KL: 2.52e+02 MMD: 1.00e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 1.28e+04 logL: -1.09e+04 KL: 2.20e+02 MMD: 9.04e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 1.05e+04 logL: -8.51e+03 KL: 2.04e+02 MMD: 1.02e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 1.00e+04 logL: -7.97e+03 KL: 1.71e+02 MMD: 1.09e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 9.51e+03 logL: -7.51e+03 KL: 1.55e+02 MMD: 1.06e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 9.00e+03 logL: -6.88e+03 KL: 1.62e+02 MMD: 1.12e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 7.72e+03 logL: -5.65e+03 KL: 2.06e+02 MMD: 1.06e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 7.72e+03 logL: -5.65e+03 KL: 2.06e+02 MMD: 1.06e+00\n",
      "config 78, alpha = 0.0, lambda = 17589.7, dropout = 0.00; 2 hidden layers with 46, 43 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.79e+04 logL: -5.25e+03 KL: 3.29e+02 MMD: 1.27e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.67e+04 logL: -2.84e+03 KL: 2.03e+02 MMD: 1.35e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.56e+04 logL: -2.79e+03 KL: 1.65e+02 MMD: 1.29e+00\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 39 VALIDATION Loss: 2.83e+04 logL: -2.74e+03 KL: 1.53e+02 MMD: 1.45e+00\n",
      "config 78, alpha = 0.0, lambda = 1846.7, dropout = 0.00; 2 hidden layers with 113, 73 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.40e+03 logL: -2.25e+03 KL: 2.71e+02 MMD: 1.56e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.85e+03 logL: -1.69e+03 KL: 1.65e+02 MMD: 1.62e+00\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 28 VALIDATION Loss: 5.08e+03 logL: -1.57e+03 KL: 1.47e+02 MMD: 1.82e+00\n",
      "config 78, alpha = 0.0, lambda = 50.2, dropout = 0.00; 2 hidden layers with 19, 15 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.36e+04 KL: 5.16e+02 MMD: 2.17e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.17e+03 logL: -8.76e+03 KL: 3.12e+02 MMD: 2.00e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.15e+03 logL: -3.84e+03 KL: 2.18e+02 MMD: 1.83e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.26e+03 logL: -3.00e+03 KL: 1.70e+02 MMD: 1.85e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.69e+03 logL: -2.46e+03 KL: 1.44e+02 MMD: 1.80e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.45e+03 logL: -2.22e+03 KL: 1.25e+02 MMD: 2.01e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.94e+03 logL: -1.73e+03 KL: 1.21e+02 MMD: 1.81e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.92e+03 logL: -1.71e+03 KL: 1.06e+02 MMD: 1.98e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.80e+03 logL: -1.61e+03 KL: 9.71e+01 MMD: 1.89e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.79e+03 logL: -1.60e+03 KL: 8.89e+01 MMD: 1.92e+00\n",
      "Epoch 00107: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.75e+03 logL: -1.57e+03 KL: 8.41e+01 MMD: 1.99e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.74e+03 logL: -1.57e+03 KL: 8.26e+01 MMD: 1.80e+00\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 1.75e+03 logL: -1.57e+03 KL: 8.12e+01 MMD: 1.99e+00\n",
      "Stopping\n",
      "====> Epoch: 130 VALIDATION Loss: 1.75e+03 logL: -1.57e+03 KL: 8.12e+01 MMD: 1.99e+00\n",
      "config 79, alpha = 0.0, lambda = 4172.6, dropout = 0.00; 2 hidden layers with 5, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.80e+04 logL: -1.39e+04 KL: 1.24e+03 MMD: 6.73e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.70e+04 logL: -1.37e+04 KL: 5.76e+02 MMD: 6.53e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.67e+04 logL: -1.35e+04 KL: 3.07e+02 MMD: 6.93e-01\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.69e+04 logL: -1.35e+04 KL: 2.46e+02 MMD: 7.45e-01\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 48 VALIDATION Loss: 1.69e+04 logL: -1.35e+04 KL: 2.39e+02 MMD: 7.48e-01\n",
      "config 79, alpha = 0.0, lambda = 24012.9, dropout = 0.00; 2 hidden layers with 11, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.63e+04 logL: -1.29e+04 KL: 4.12e+02 MMD: 9.56e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.32e+04 logL: -9.18e+03 KL: 3.09e+02 MMD: 9.88e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.35e+04 logL: -7.14e+03 KL: 2.31e+02 MMD: 1.09e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.42e+04 logL: -7.10e+03 KL: 1.80e+02 MMD: 1.12e+00\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 45 VALIDATION Loss: 3.27e+04 logL: -7.08e+03 KL: 1.81e+02 MMD: 1.06e+00\n",
      "config 79, alpha = 0.0, lambda = 83.3, dropout = 0.00; 2 hidden layers with 65, 20 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.67e+04 logL: -1.44e+04 KL: 2.19e+03 MMD: 1.44e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.47e+04 logL: -1.40e+04 KL: 6.15e+02 MMD: 1.34e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.43e+04 logL: -1.38e+04 KL: 3.03e+02 MMD: 1.53e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.30e+04 logL: -1.26e+04 KL: 2.51e+02 MMD: 1.32e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.16e+04 logL: -1.13e+04 KL: 1.76e+02 MMD: 1.45e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.47e+03 logL: -9.19e+03 KL: 1.74e+02 MMD: 1.29e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.81e+03 logL: -7.54e+03 KL: 1.71e+02 MMD: 1.29e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.52e+03 logL: -6.24e+03 KL: 1.57e+02 MMD: 1.43e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 4.54e+03 logL: -4.28e+03 KL: 1.54e+02 MMD: 1.30e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 4.07e+03 logL: -3.82e+03 KL: 1.34e+02 MMD: 1.42e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.37e+03 logL: -3.13e+03 KL: 1.20e+02 MMD: 1.42e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.18e+03 logL: -2.97e+03 KL: 1.07e+02 MMD: 1.30e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.04e+03 logL: -2.84e+03 KL: 9.78e+01 MMD: 1.32e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.02e+03 logL: -2.81e+03 KL: 8.85e+01 MMD: 1.43e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.94e+03 logL: -2.76e+03 KL: 7.99e+01 MMD: 1.31e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 2.90e+03 logL: -2.71e+03 KL: 7.24e+01 MMD: 1.32e+00\n",
      "Epoch 00169: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 170 VALIDATION Loss: 2.86e+03 logL: -2.68e+03 KL: 6.71e+01 MMD: 1.42e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 2.85e+03 logL: -2.68e+03 KL: 6.55e+01 MMD: 1.24e+00\n",
      "Epoch 00182: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 190 VALIDATION Loss: 2.84e+03 logL: -2.67e+03 KL: 6.49e+01 MMD: 1.26e+00\n",
      "Epoch 00190: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 194 VALIDATION Loss: 2.84e+03 logL: -2.67e+03 KL: 6.49e+01 MMD: 1.28e+00\n",
      "config 79, alpha = 0.0, lambda = 39102.9, dropout = 0.00; 2 hidden layers with 34, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.77e+04 logL: -1.39e+04 KL: 9.81e+02 MMD: 1.61e+00\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 7.36e+04 logL: -1.10e+04 KL: 4.87e+02 MMD: 1.59e+00\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 7.62e+04 logL: -1.07e+04 KL: 4.70e+02 MMD: 1.66e+00\n",
      "Stopping\n",
      "====> Epoch: 32 VALIDATION Loss: 7.05e+04 logL: -1.07e+04 KL: 4.70e+02 MMD: 1.52e+00\n",
      "config 79, alpha = 0.0, lambda = 27588.9, dropout = 0.00; 2 hidden layers with 103, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.97e+04 logL: -7.16e+03 KL: 4.27e+02 MMD: 1.89e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.10e+04 logL: -3.16e+03 KL: 3.13e+02 MMD: 2.09e+00\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.71e+04 logL: -2.91e+03 KL: 3.10e+02 MMD: 1.95e+00\n",
      "Stopping\n",
      "====> Epoch: 32 VALIDATION Loss: 5.69e+04 logL: -2.91e+03 KL: 3.09e+02 MMD: 1.95e+00\n",
      "config 80, alpha = 0.0, lambda = 204.6, dropout = 0.00; 2 hidden layers with 9, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.44e+04 logL: -1.34e+04 KL: 8.45e+02 MMD: 6.61e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.99e+03 logL: -9.55e+03 KL: 2.91e+02 MMD: 7.53e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.80e+03 logL: -9.51e+03 KL: 1.48e+02 MMD: 6.86e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.73e+03 logL: -9.49e+03 KL: 1.01e+02 MMD: 7.02e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.67e+03 logL: -9.44e+03 KL: 7.99e+01 MMD: 7.16e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.65e+03 logL: -9.44e+03 KL: 6.67e+01 MMD: 6.93e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.63e+03 logL: -9.42e+03 KL: 5.79e+01 MMD: 7.64e-01\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 9.62e+03 logL: -9.41e+03 KL: 5.72e+01 MMD: 7.40e-01\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 85 VALIDATION Loss: 9.64e+03 logL: -9.41e+03 KL: 5.70e+01 MMD: 8.53e-01\n",
      "config 80, alpha = 0.0, lambda = 13678.7, dropout = 0.00; 2 hidden layers with 79, 21 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.83e+04 logL: -5.66e+03 KL: 3.07e+02 MMD: 9.01e-01\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 1.98e+04 logL: -5.32e+03 KL: 2.41e+02 MMD: 1.04e+00\n",
      "Stopping\n",
      "====> Epoch: 20 VALIDATION Loss: 1.98e+04 logL: -5.32e+03 KL: 2.41e+02 MMD: 1.04e+00\n",
      "config 80, alpha = 0.0, lambda = 4.7, dropout = 0.00; 2 hidden layers with 181, 27 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.04e+03 logL: -2.80e+03 KL: 2.35e+02 MMD: 1.37e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.92e+03 logL: -2.77e+03 KL: 1.45e+02 MMD: 1.35e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.85e+03 logL: -2.74e+03 KL: 1.04e+02 MMD: 1.30e+00\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.71e+03 logL: -2.61e+03 KL: 8.92e+01 MMD: 1.44e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.70e+03 logL: -2.61e+03 KL: 8.50e+01 MMD: 1.32e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.70e+03 logL: -2.62e+03 KL: 8.06e+01 MMD: 1.33e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.68e+03 logL: -2.60e+03 KL: 7.70e+01 MMD: 1.43e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.67e+03 logL: -2.59e+03 KL: 7.34e+01 MMD: 1.30e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.67e+03 logL: -2.59e+03 KL: 7.07e+01 MMD: 1.41e+00\n",
      "Stopping\n",
      "====> Epoch: 94 VALIDATION Loss: 2.66e+03 logL: -2.59e+03 KL: 7.07e+01 MMD: 1.34e+00\n",
      "config 80, alpha = 0.0, lambda = 2.3, dropout = 0.00; 2 hidden layers with 19, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 2.87e+02 MMD: 1.98e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 20 VALIDATION Loss: 5.91e+03 logL: -5.64e+03 KL: 2.71e+02 MMD: 1.74e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.04e+03 logL: -3.86e+03 KL: 1.83e+02 MMD: 1.69e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.74e+03 logL: -3.62e+03 KL: 1.19e+02 MMD: 1.51e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.72e+03 logL: -3.62e+03 KL: 9.47e+01 MMD: 1.71e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.65e+03 logL: -3.57e+03 KL: 8.16e+01 MMD: 1.63e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.88e+03 logL: -2.79e+03 KL: 8.24e+01 MMD: 1.67e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.84e+03 logL: -2.76e+03 KL: 7.23e+01 MMD: 1.60e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.82e+03 logL: -2.75e+03 KL: 6.64e+01 MMD: 1.78e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.81e+03 logL: -2.75e+03 KL: 6.18e+01 MMD: 1.66e+00\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.80e+03 logL: -2.73e+03 KL: 6.17e+01 MMD: 1.73e+00\n",
      "Stopping\n",
      "====> Epoch: 111 VALIDATION Loss: 2.80e+03 logL: -2.73e+03 KL: 6.15e+01 MMD: 1.68e+00\n",
      "config 80, alpha = 0.0, lambda = 700.6, dropout = 0.00; 2 hidden layers with 98, 36 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.49e+03 logL: -3.78e+03 KL: 3.71e+02 MMD: 1.92e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.90e+03 logL: -2.28e+03 KL: 1.93e+02 MMD: 2.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.24e+03 logL: -1.67e+03 KL: 1.46e+02 MMD: 2.04e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.20e+03 logL: -1.68e+03 KL: 1.20e+02 MMD: 2.01e+00\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.93e+03 logL: -1.53e+03 KL: 1.13e+02 MMD: 1.83e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.83e+03 logL: -1.39e+03 KL: 1.08e+02 MMD: 1.90e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.63e+03 logL: -1.22e+03 KL: 9.80e+01 MMD: 1.88e+00\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 2.52e+03 logL: -1.17e+03 KL: 8.96e+01 MMD: 1.80e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.63e+03 logL: -1.17e+03 KL: 8.82e+01 MMD: 1.95e+00\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 97 VALIDATION Loss: 2.52e+03 logL: -1.17e+03 KL: 8.74e+01 MMD: 1.80e+00\n",
      "config 81, alpha = 0.0, lambda = 8959.4, dropout = 0.00; 2 hidden layers with 6, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.01e+04 logL: -1.34e+04 KL: 6.95e+02 MMD: 6.69e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.58e+04 logL: -9.59e+03 KL: 3.36e+02 MMD: 6.51e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.59e+04 logL: -9.58e+03 KL: 2.24e+02 MMD: 6.79e-01\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 36 VALIDATION Loss: 1.60e+04 logL: -9.53e+03 KL: 2.19e+02 MMD: 6.95e-01\n",
      "config 81, alpha = 0.0, lambda = 2942.2, dropout = 0.00; 2 hidden layers with 72, 18 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.49e+04 logL: -1.14e+04 KL: 6.05e+02 MMD: 9.56e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 8.80e+03 logL: -5.38e+03 KL: 2.59e+02 MMD: 1.07e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.22e+03 logL: -5.29e+03 KL: 1.65e+02 MMD: 9.40e-01\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 8.30e+03 logL: -5.14e+03 KL: 1.40e+02 MMD: 1.02e+00\n",
      "Stopping\n",
      "====> Epoch: 41 VALIDATION Loss: 8.40e+03 logL: -5.14e+03 KL: 1.40e+02 MMD: 1.06e+00\n",
      "config 81, alpha = 0.0, lambda = 2.4, dropout = 0.00; 2 hidden layers with 96, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.62e+03 logL: -8.24e+03 KL: 3.76e+02 MMD: 1.31e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.33e+03 logL: -5.13e+03 KL: 2.02e+02 MMD: 1.41e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.07e+03 logL: -2.92e+03 KL: 1.51e+02 MMD: 1.30e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.89e+03 logL: -2.77e+03 KL: 1.13e+02 MMD: 1.34e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.84e+03 logL: -2.74e+03 KL: 9.42e+01 MMD: 1.28e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.79e+03 logL: -2.70e+03 KL: 8.27e+01 MMD: 1.34e+00\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.75e+03 logL: -2.67e+03 KL: 7.73e+01 MMD: 1.29e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.75e+03 logL: -2.67e+03 KL: 7.53e+01 MMD: 1.31e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.74e+03 logL: -2.66e+03 KL: 7.26e+01 MMD: 1.32e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.74e+03 logL: -2.66e+03 KL: 7.04e+01 MMD: 1.48e+00\n",
      "Stopping\n",
      "====> Epoch: 107 VALIDATION Loss: 2.73e+03 logL: -2.66e+03 KL: 6.89e+01 MMD: 1.28e+00\n",
      "config 81, alpha = 0.0, lambda = 3.7, dropout = 0.00; 2 hidden layers with 19, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.37e+04 logL: -1.34e+04 KL: 2.52e+02 MMD: 2.06e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.76e+03 logL: -7.59e+03 KL: 1.59e+02 MMD: 1.93e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.67e+03 logL: -5.54e+03 KL: 1.28e+02 MMD: 1.76e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.61e+03 logL: -5.52e+03 KL: 9.24e+01 MMD: 1.89e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.43e+03 logL: -5.34e+03 KL: 7.83e+01 MMD: 1.86e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.72e+03 logL: -3.63e+03 KL: 9.11e+01 MMD: 1.67e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.67e+03 logL: -3.59e+03 KL: 7.48e+01 MMD: 1.85e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.65e+03 logL: -3.58e+03 KL: 6.70e+01 MMD: 1.73e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.64e+03 logL: -3.57e+03 KL: 6.24e+01 MMD: 1.80e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.62e+03 logL: -3.55e+03 KL: 5.95e+01 MMD: 1.69e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.83e+03 logL: -2.76e+03 KL: 6.43e+01 MMD: 1.84e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.82e+03 logL: -2.75e+03 KL: 5.82e+01 MMD: 1.97e+00\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 2.79e+03 logL: -2.73e+03 KL: 5.74e+01 MMD: 1.68e+00\n",
      "Stopping\n",
      "====> Epoch: 135 VALIDATION Loss: 2.80e+03 logL: -2.74e+03 KL: 5.69e+01 MMD: 1.87e+00\n",
      "config 81, alpha = 0.0, lambda = 1217.2, dropout = 0.00; 2 hidden layers with 43, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.49e+05 logL: -3.48e+05 KL: 9.67e+01 MMD: 5.43e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.41e+05 logL: -3.40e+05 KL: 9.27e+01 MMD: 5.64e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.29e+05 logL: -3.28e+05 KL: 8.59e+01 MMD: 5.12e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 5.22e+04 logL: -4.78e+04 KL: 2.20e+03 MMD: 1.86e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.83e+04 logL: -1.48e+04 KL: 1.25e+03 MMD: 1.88e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.75e+04 logL: -1.45e+04 KL: 7.15e+02 MMD: 1.91e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.67e+04 logL: -1.41e+04 KL: 4.51e+02 MMD: 1.80e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.54e+04 logL: -1.28e+04 KL: 3.31e+02 MMD: 1.91e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.28e+04 logL: -1.01e+04 KL: 3.01e+02 MMD: 1.93e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.05e+04 logL: -7.86e+03 KL: 2.93e+02 MMD: 1.94e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 9.50e+03 logL: -7.00e+03 KL: 2.50e+02 MMD: 1.85e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 8.48e+03 logL: -6.12e+03 KL: 2.23e+02 MMD: 1.76e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 7.93e+03 logL: -5.39e+03 KL: 1.97e+02 MMD: 1.93e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 7.55e+03 logL: -4.88e+03 KL: 1.64e+02 MMD: 2.06e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 6.95e+03 logL: -4.36e+03 KL: 1.33e+02 MMD: 2.02e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 6.16e+03 logL: -4.01e+03 KL: 1.10e+02 MMD: 1.68e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 5.79e+03 logL: -3.88e+03 KL: 6.75e+01 MMD: 1.52e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 4.62e+03 logL: -3.92e+03 KL: 4.88e+01 MMD: 5.40e-01\n",
      "====> Epoch: 190 VALIDATION Loss: 4.22e+03 logL: -3.78e+03 KL: 4.62e+01 MMD: 3.21e-01\n",
      "====> Epoch: 200 VALIDATION Loss: 4.23e+03 logL: -3.83e+03 KL: 4.55e+01 MMD: 2.95e-01\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 4.23e+03 logL: -3.83e+03 KL: 4.55e+01 MMD: 2.95e-01\n",
      "config 82, alpha = 0.0, lambda = 173.6, dropout = 0.00; 2 hidden layers with 165, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.91e+03 logL: -9.58e+03 KL: 2.16e+02 MMD: 6.72e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.78e+03 logL: -9.53e+03 KL: 1.29e+02 MMD: 6.97e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.65e+03 logL: -9.41e+03 KL: 9.75e+01 MMD: 8.02e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.90e+03 logL: -9.68e+03 KL: 8.82e+01 MMD: 7.59e-01\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 9.35e+03 logL: -9.14e+03 KL: 8.58e+01 MMD: 7.33e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.27e+03 logL: -9.06e+03 KL: 8.11e+01 MMD: 7.59e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 70 VALIDATION Loss: 9.15e+03 logL: -8.96e+03 KL: 7.57e+01 MMD: 6.65e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.05e+03 logL: -8.85e+03 KL: 7.21e+01 MMD: 7.67e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 8.96e+03 logL: -8.75e+03 KL: 6.93e+01 MMD: 7.60e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 8.86e+03 logL: -8.66e+03 KL: 6.64e+01 MMD: 8.23e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 8.78e+03 logL: -8.59e+03 KL: 6.61e+01 MMD: 7.36e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 8.72e+03 logL: -8.53e+03 KL: 6.37e+01 MMD: 7.19e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 8.68e+03 logL: -8.48e+03 KL: 6.07e+01 MMD: 7.99e-01\n",
      "Epoch 00133: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 137 VALIDATION Loss: 8.61e+03 logL: -8.42e+03 KL: 6.15e+01 MMD: 7.10e-01\n",
      "config 82, alpha = 0.0, lambda = 22818.4, dropout = 0.00; 2 hidden layers with 7, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.73e+04 logL: -1.40e+04 KL: 5.06e+02 MMD: 9.98e-01\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 3.72e+04 logL: -1.35e+04 KL: 4.43e+02 MMD: 1.02e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.86e+04 logL: -1.32e+04 KL: 4.24e+02 MMD: 1.09e+00\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 3.62e+04 logL: -1.30e+04 KL: 4.37e+02 MMD: 9.98e-01\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 45 VALIDATION Loss: 3.65e+04 logL: -1.30e+04 KL: 4.36e+02 MMD: 1.01e+00\n",
      "config 82, alpha = 0.0, lambda = 942.9, dropout = 0.00; 2 hidden layers with 28, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.64e+04 logL: -1.39e+04 KL: 1.34e+03 MMD: 1.20e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.52e+04 logL: -1.37e+04 KL: 4.03e+02 MMD: 1.19e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.34e+04 logL: -1.19e+04 KL: 2.61e+02 MMD: 1.37e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.14e+03 logL: -6.63e+03 KL: 2.64e+02 MMD: 1.33e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.31e+03 logL: -4.83e+03 KL: 2.36e+02 MMD: 1.32e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.04e+03 logL: -3.70e+03 KL: 2.11e+02 MMD: 1.20e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.52e+03 logL: -3.11e+03 KL: 1.82e+02 MMD: 1.30e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 4.25e+03 logL: -2.92e+03 KL: 1.52e+02 MMD: 1.26e+00\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 4.21e+03 logL: -2.83e+03 KL: 1.34e+02 MMD: 1.32e+00\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 4.19e+03 logL: -2.83e+03 KL: 1.34e+02 MMD: 1.30e+00\n",
      "config 82, alpha = 0.0, lambda = 108.9, dropout = 0.00; 2 hidden layers with 39, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.48e+04 logL: -1.38e+04 KL: 7.52e+02 MMD: 2.09e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.06e+04 logL: -1.01e+04 KL: 3.72e+02 MMD: 1.73e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.60e+03 logL: -9.22e+03 KL: 1.72e+02 MMD: 1.92e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.36e+03 logL: -8.02e+03 KL: 1.27e+02 MMD: 1.93e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.52e+03 logL: -7.21e+03 KL: 1.15e+02 MMD: 1.80e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.32e+03 logL: -7.03e+03 KL: 1.01e+02 MMD: 1.73e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.69e+03 logL: -5.40e+03 KL: 9.94e+01 MMD: 1.73e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.38e+03 logL: -5.11e+03 KL: 8.43e+01 MMD: 1.69e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.37e+03 logL: -5.10e+03 KL: 7.86e+01 MMD: 1.81e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.84e+03 logL: -3.57e+03 KL: 9.27e+01 MMD: 1.64e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 3.82e+03 logL: -3.55e+03 KL: 7.51e+01 MMD: 1.78e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 3.74e+03 logL: -3.49e+03 KL: 7.12e+01 MMD: 1.58e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.70e+03 logL: -3.46e+03 KL: 6.55e+01 MMD: 1.56e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.67e+03 logL: -3.43e+03 KL: 6.25e+01 MMD: 1.65e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.98e+03 logL: -2.74e+03 KL: 6.30e+01 MMD: 1.63e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 2.94e+03 logL: -2.71e+03 KL: 5.92e+01 MMD: 1.54e+00\n",
      "Epoch 00164: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 170 VALIDATION Loss: 2.90e+03 logL: -2.69e+03 KL: 5.64e+01 MMD: 1.41e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 2.91e+03 logL: -2.69e+03 KL: 5.46e+01 MMD: 1.58e+00\n",
      "Epoch 00184: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 188 VALIDATION Loss: 2.89e+03 logL: -2.69e+03 KL: 5.36e+01 MMD: 1.35e+00\n",
      "config 82, alpha = 0.0, lambda = 3268.9, dropout = 0.00; 2 hidden layers with 15, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.16e+04 logL: -1.46e+04 KL: 4.10e+02 MMD: 2.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.00e+04 logL: -1.35e+04 KL: 3.82e+02 MMD: 1.88e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.61e+04 logL: -9.78e+03 KL: 3.38e+02 MMD: 1.84e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.63e+04 logL: -9.65e+03 KL: 2.24e+02 MMD: 1.96e+00\n",
      "Stopping\n",
      "====> Epoch: 43 VALIDATION Loss: 1.65e+04 logL: -9.59e+03 KL: 2.00e+02 MMD: 2.05e+00\n",
      "config 83, alpha = 0.0, lambda = 205.9, dropout = 0.00; 2 hidden layers with 53, 30 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.94e+03 logL: -9.53e+03 KL: 2.62e+02 MMD: 7.31e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.92e+03 logL: -9.66e+03 KL: 1.24e+02 MMD: 6.96e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.55e+03 logL: -9.31e+03 KL: 8.84e+01 MMD: 7.54e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.80e+03 logL: -9.55e+03 KL: 7.60e+01 MMD: 8.29e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.03e+03 logL: -8.80e+03 KL: 6.92e+01 MMD: 7.83e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 8.91e+03 logL: -8.70e+03 KL: 5.90e+01 MMD: 7.34e-01\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 8.83e+03 logL: -8.61e+03 KL: 5.62e+01 MMD: 7.96e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.65e+03 logL: -8.44e+03 KL: 5.41e+01 MMD: 7.71e-01\n",
      "Epoch 00086: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 8.67e+03 logL: -8.44e+03 KL: 5.39e+01 MMD: 8.36e-01\n",
      "Stopping\n",
      "====> Epoch: 90 VALIDATION Loss: 8.67e+03 logL: -8.44e+03 KL: 5.39e+01 MMD: 8.36e-01\n",
      "config 83, alpha = 0.0, lambda = 439.2, dropout = 0.00; 2 hidden layers with 20, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.35e+03 logL: -7.30e+03 KL: 5.70e+02 MMD: 1.09e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.03e+03 logL: -5.36e+03 KL: 2.34e+02 MMD: 9.83e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 5.77e+03 logL: -5.15e+03 KL: 1.59e+02 MMD: 1.05e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.66e+03 logL: -5.09e+03 KL: 1.24e+02 MMD: 1.01e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.59e+03 logL: -5.02e+03 KL: 9.90e+01 MMD: 1.09e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.56e+03 logL: -5.01e+03 KL: 8.97e+01 MMD: 1.06e+00\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.49e+03 logL: -4.95e+03 KL: 8.68e+01 MMD: 1.04e+00\n",
      "Stopping\n",
      "====> Epoch: 73 VALIDATION Loss: 5.49e+03 logL: -4.95e+03 KL: 8.68e+01 MMD: 1.05e+00\n",
      "config 83, alpha = 0.0, lambda = 480.3, dropout = 0.00; 2 hidden layers with 30, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.52e+04 logL: -1.36e+04 KL: 4.97e+02 MMD: 2.16e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.46e+04 logL: -1.36e+04 KL: 2.54e+02 MMD: 1.69e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.45e+04 logL: -1.35e+04 KL: 1.81e+02 MMD: 1.60e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.43e+04 logL: -1.35e+04 KL: 1.30e+02 MMD: 1.46e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.03e+04 logL: -9.55e+03 KL: 1.41e+02 MMD: 1.34e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.04e+04 logL: -9.52e+03 KL: 1.59e+02 MMD: 1.44e+00\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 65 VALIDATION Loss: 1.03e+04 logL: -9.46e+03 KL: 1.50e+02 MMD: 1.48e+00\n",
      "config 83, alpha = 0.0, lambda = 18946.8, dropout = 0.00; 2 hidden layers with 16, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.69e+04 logL: -1.38e+04 KL: 7.53e+02 MMD: 1.71e+00\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 15 VALIDATION Loss: 4.47e+04 logL: -1.39e+04 KL: 7.44e+02 MMD: 1.58e+00\n",
      "config 83, alpha = 0.0, lambda = 14669.9, dropout = 0.00; 2 hidden layers with 21, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.43e+04 logL: -1.47e+04 KL: 6.28e+02 MMD: 1.97e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 4.17e+04 logL: -1.37e+04 KL: 4.53e+02 MMD: 1.87e+00\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping\n",
      "====> Epoch: 25 VALIDATION Loss: 4.20e+04 logL: -1.36e+04 KL: 4.15e+02 MMD: 1.90e+00\n",
      "config 84, alpha = 0.0, lambda = 11.4, dropout = 0.00; 2 hidden layers with 130, 35 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.38e+04 logL: -1.36e+04 KL: 2.85e+02 MMD: 6.97e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.72e+03 logL: -9.53e+03 KL: 1.85e+02 MMD: 6.84e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.62e+03 logL: -9.48e+03 KL: 1.29e+02 MMD: 7.45e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.66e+03 logL: -9.54e+03 KL: 1.12e+02 MMD: 7.56e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.74e+03 logL: -9.62e+03 KL: 1.10e+02 MMD: 6.95e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.07e+03 logL: -8.97e+03 KL: 8.77e+01 MMD: 7.40e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.19e+03 logL: -9.11e+03 KL: 7.09e+01 MMD: 7.72e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.54e+03 logL: -8.47e+03 KL: 6.49e+01 MMD: 7.56e-01\n",
      "Epoch 00084: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 8.31e+03 logL: -8.24e+03 KL: 6.02e+01 MMD: 7.65e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 8.23e+03 logL: -8.17e+03 KL: 5.61e+01 MMD: 7.35e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 8.26e+03 logL: -8.20e+03 KL: 5.07e+01 MMD: 7.44e-01\n",
      "Epoch 00113: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 117 VALIDATION Loss: 8.20e+03 logL: -8.14e+03 KL: 4.99e+01 MMD: 8.23e-01\n",
      "config 84, alpha = 0.0, lambda = 16080.1, dropout = 0.00; 2 hidden layers with 23, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.63e+04 logL: -8.76e+03 KL: 6.45e+02 MMD: 1.05e+00\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 2.17e+04 logL: -5.66e+03 KL: 3.59e+02 MMD: 9.75e-01\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 2.18e+04 logL: -5.58e+03 KL: 3.29e+02 MMD: 9.89e-01\n",
      "Stopping\n",
      "====> Epoch: 32 VALIDATION Loss: 2.31e+04 logL: -5.58e+03 KL: 3.29e+02 MMD: 1.07e+00\n",
      "config 84, alpha = 0.0, lambda = 40685.3, dropout = 0.00; 2 hidden layers with 26, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.74e+04 logL: -1.40e+04 KL: 4.35e+02 MMD: 1.30e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.63e+04 logL: -9.84e+03 KL: 4.54e+02 MMD: 1.38e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.33e+04 logL: -7.20e+03 KL: 3.72e+02 MMD: 1.37e+00\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 6.47e+04 logL: -7.12e+03 KL: 3.33e+02 MMD: 1.41e+00\n",
      "config 84, alpha = 0.0, lambda = 1477.8, dropout = 0.00; 2 hidden layers with 146, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.92e+04 logL: -1.38e+04 KL: 2.96e+03 MMD: 1.65e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.70e+04 logL: -1.37e+04 KL: 9.49e+02 MMD: 1.65e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.65e+04 logL: -1.36e+04 KL: 4.63e+02 MMD: 1.69e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.57e+04 logL: -1.30e+04 KL: 3.09e+02 MMD: 1.65e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.27e+04 logL: -1.01e+04 KL: 2.73e+02 MMD: 1.58e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.01e+04 logL: -7.61e+03 KL: 2.97e+02 MMD: 1.50e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.59e+03 logL: -5.94e+03 KL: 2.51e+02 MMD: 1.62e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.89e+03 logL: -4.27e+03 KL: 2.31e+02 MMD: 1.62e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.00e+03 logL: -3.46e+03 KL: 2.15e+02 MMD: 1.58e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.75e+03 logL: -3.16e+03 KL: 1.88e+02 MMD: 1.63e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.35e+03 logL: -2.93e+03 KL: 1.62e+02 MMD: 1.52e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.68e+03 logL: -2.36e+03 KL: 1.46e+02 MMD: 1.47e+00\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 130 VALIDATION Loss: 4.78e+03 logL: -2.16e+03 KL: 1.24e+02 MMD: 1.69e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 4.32e+03 logL: -2.15e+03 KL: 1.18e+02 MMD: 1.39e+00\n",
      "Epoch 00146: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 150 VALIDATION Loss: 4.53e+03 logL: -2.15e+03 KL: 1.13e+02 MMD: 1.54e+00\n",
      "Stopping\n",
      "====> Epoch: 150 VALIDATION Loss: 4.53e+03 logL: -2.15e+03 KL: 1.13e+02 MMD: 1.54e+00\n",
      "config 84, alpha = 0.0, lambda = 4.3, dropout = 0.00; 2 hidden layers with 21, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.41e+04 logL: -1.36e+04 KL: 5.17e+02 MMD: 2.59e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.83e+03 logL: -9.59e+03 KL: 2.24e+02 MMD: 2.45e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.37e+03 logL: -7.20e+03 KL: 1.71e+02 MMD: 2.23e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.81e+03 logL: -3.65e+03 KL: 1.51e+02 MMD: 2.01e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.73e+03 logL: -3.61e+03 KL: 1.13e+02 MMD: 2.15e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.68e+03 logL: -3.58e+03 KL: 9.44e+01 MMD: 2.11e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.64e+03 logL: -3.55e+03 KL: 8.50e+01 MMD: 2.18e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.57e+03 logL: -3.48e+03 KL: 8.22e+01 MMD: 2.14e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.86e+03 logL: -2.77e+03 KL: 8.23e+01 MMD: 1.99e+00\n",
      "Epoch 00099: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 2.82e+03 logL: -2.74e+03 KL: 7.55e+01 MMD: 2.04e+00\n",
      "Epoch 00107: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.82e+03 logL: -2.74e+03 KL: 7.50e+01 MMD: 1.98e+00\n",
      "Stopping\n",
      "====> Epoch: 119 VALIDATION Loss: 2.82e+03 logL: -2.74e+03 KL: 7.49e+01 MMD: 2.08e+00\n",
      "config 85, alpha = 0.0, lambda = 1.9, dropout = 0.00; 2 hidden layers with 21, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.44e+04 logL: -1.36e+04 KL: 8.80e+02 MMD: 8.50e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.39e+04 logL: -1.35e+04 KL: 3.41e+02 MMD: 7.58e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 1.80e+02 MMD: 9.05e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.28e+02 MMD: 9.48e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 9.96e+01 MMD: 9.72e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 8.75e+01 MMD: 9.02e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.89e+03 logL: -9.72e+03 KL: 1.60e+02 MMD: 7.02e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.64e+03 logL: -9.53e+03 KL: 1.08e+02 MMD: 7.08e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.57e+03 logL: -9.48e+03 KL: 8.51e+01 MMD: 7.66e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 9.56e+03 logL: -9.49e+03 KL: 7.06e+01 MMD: 7.21e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 9.50e+03 logL: -9.44e+03 KL: 6.19e+01 MMD: 7.49e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 9.47e+03 logL: -9.42e+03 KL: 5.48e+01 MMD: 8.25e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 9.46e+03 logL: -9.41e+03 KL: 5.03e+01 MMD: 8.39e-01\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 9.43e+03 logL: -9.38e+03 KL: 4.86e+01 MMD: 7.41e-01\n",
      "Epoch 00140: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 143 VALIDATION Loss: 9.43e+03 logL: -9.38e+03 KL: 4.84e+01 MMD: 7.57e-01\n",
      "config 85, alpha = 0.0, lambda = 1098.2, dropout = 0.00; 2 hidden layers with 7, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.59e+04 logL: -1.37e+04 KL: 9.34e+02 MMD: 1.17e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.52e+04 logL: -1.36e+04 KL: 3.99e+02 MMD: 1.15e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.13e+04 logL: -9.82e+03 KL: 2.71e+02 MMD: 1.07e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.10e+04 logL: -9.57e+03 KL: 1.84e+02 MMD: 1.15e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.09e+04 logL: -9.55e+03 KL: 1.63e+02 MMD: 1.08e+00\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 54 VALIDATION Loss: 1.11e+04 logL: -9.53e+03 KL: 1.61e+02 MMD: 1.24e+00\n",
      "config 85, alpha = 0.0, lambda = 1.1, dropout = 0.00; 2 hidden layers with 24, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.92e+03 logL: -9.60e+03 KL: 3.20e+02 MMD: 1.52e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.69e+03 logL: -9.54e+03 KL: 1.46e+02 MMD: 1.81e+00\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 9.66e+03 logL: -9.54e+03 KL: 1.13e+02 MMD: 1.80e+00\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 9.65e+03 logL: -9.54e+03 KL: 1.08e+02 MMD: 1.96e+00\n",
      "config 85, alpha = 0.0, lambda = 60.6, dropout = 0.00; 2 hidden layers with 21, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.39e+04 logL: -1.35e+04 KL: 2.88e+02 MMD: 2.16e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.86e+03 logL: -9.57e+03 KL: 1.66e+02 MMD: 2.13e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.82e+03 logL: -9.57e+03 KL: 1.19e+02 MMD: 2.14e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 7.39e+03 logL: -7.15e+03 KL: 1.24e+02 MMD: 1.95e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.69e+03 logL: -5.47e+03 KL: 1.14e+02 MMD: 1.69e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.46e+03 logL: -5.26e+03 KL: 9.39e+01 MMD: 1.75e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.36e+03 logL: -5.17e+03 KL: 8.27e+01 MMD: 1.81e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.34e+03 logL: -5.16e+03 KL: 7.58e+01 MMD: 1.75e+00\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 5.32e+03 logL: -5.14e+03 KL: 7.29e+01 MMD: 1.93e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.31e+03 logL: -5.13e+03 KL: 7.15e+01 MMD: 1.85e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.28e+03 logL: -5.10e+03 KL: 7.02e+01 MMD: 1.79e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 4.89e+03 logL: -4.71e+03 KL: 7.28e+01 MMD: 1.87e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 3.96e+03 logL: -3.78e+03 KL: 8.03e+01 MMD: 1.65e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.83e+03 logL: -3.65e+03 KL: 7.97e+01 MMD: 1.70e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 3.78e+03 logL: -3.61e+03 KL: 7.80e+01 MMD: 1.62e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 3.77e+03 logL: -3.59e+03 KL: 7.56e+01 MMD: 1.79e+00\n",
      "Epoch 00165: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 169 VALIDATION Loss: 3.75e+03 logL: -3.58e+03 KL: 7.44e+01 MMD: 1.68e+00\n",
      "config 85, alpha = 0.0, lambda = 84199.5, dropout = 0.00; 2 hidden layers with 21, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.57e+05 logL: -3.40e+05 KL: 2.57e+02 MMD: 1.97e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.84e+05 logL: -1.57e+05 KL: 7.95e+01 MMD: 3.14e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 2.61e+04 logL: -1.60e+04 KL: 4.95e+01 MMD: 1.19e-01\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 2.17e+04 logL: -1.40e+04 KL: 4.40e+01 MMD: 9.07e-02\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.97e+04 logL: -1.40e+04 KL: 4.36e+01 MMD: 6.80e-02\n",
      "Stopping\n",
      "====> Epoch: 51 VALIDATION Loss: 2.12e+04 logL: -1.40e+04 KL: 4.36e+01 MMD: 8.54e-02\n",
      "config 86, alpha = 0.0, lambda = 1.1, dropout = 0.00; 2 hidden layers with 7, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.35e+04 KL: 7.43e+02 MMD: 7.28e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.89e+03 logL: -9.59e+03 KL: 2.93e+02 MMD: 7.17e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.72e+03 logL: -9.56e+03 KL: 1.60e+02 MMD: 6.80e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.67e+03 logL: -9.56e+03 KL: 1.07e+02 MMD: 7.34e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.62e+03 logL: -9.54e+03 KL: 7.82e+01 MMD: 7.98e-01\n",
      "Stopping\n",
      "====> Epoch: 59 VALIDATION Loss: 9.61e+03 logL: -9.55e+03 KL: 6.40e+01 MMD: 7.34e-01\n",
      "config 86, alpha = 0.0, lambda = 59872.9, dropout = 0.00; 2 hidden layers with 43, 43 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.89e+04 logL: -5.59e+03 KL: 3.37e+02 MMD: 1.05e+00\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 19 VALIDATION Loss: 6.72e+04 logL: -5.31e+03 KL: 2.77e+02 MMD: 1.03e+00\n",
      "config 86, alpha = 0.0, lambda = 2.1, dropout = 0.00; 2 hidden layers with 103, 38 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.95e+03 logL: -3.70e+03 KL: 2.46e+02 MMD: 1.21e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.98e+03 logL: -2.84e+03 KL: 1.42e+02 MMD: 1.45e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.86e+03 logL: -2.76e+03 KL: 1.08e+02 MMD: 1.25e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.83e+03 logL: -2.74e+03 KL: 8.74e+01 MMD: 1.40e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 2.92e+03 logL: -2.85e+03 KL: 7.68e+01 MMD: 1.28e+00\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.71e+03 logL: -2.63e+03 KL: 7.58e+01 MMD: 1.36e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.69e+03 logL: -2.62e+03 KL: 7.26e+01 MMD: 1.35e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.70e+03 logL: -2.63e+03 KL: 6.98e+01 MMD: 1.43e+00\n",
      "Stopping\n",
      "====> Epoch: 80 VALIDATION Loss: 2.70e+03 logL: -2.63e+03 KL: 6.98e+01 MMD: 1.43e+00\n",
      "config 86, alpha = 0.0, lambda = 28849.5, dropout = 0.00; 2 hidden layers with 118, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.06e+04 logL: -1.00e+04 KL: 6.88e+02 MMD: 1.73e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.82e+04 logL: -9.64e+03 KL: 4.08e+02 MMD: 1.67e+00\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.34e+04 logL: -7.96e+03 KL: 3.55e+02 MMD: 1.56e+00\n",
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 5.42e+04 logL: -7.85e+03 KL: 3.38e+02 MMD: 1.60e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.68e+04 logL: -7.81e+03 KL: 3.35e+02 MMD: 1.69e+00\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 54 VALIDATION Loss: 5.65e+04 logL: -7.81e+03 KL: 3.34e+02 MMD: 1.68e+00\n",
      "config 86, alpha = 0.0, lambda = 8550.4, dropout = 0.00; 2 hidden layers with 49, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.85e+04 logL: -1.01e+04 KL: 7.19e+02 MMD: 2.07e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.07e+04 logL: -5.05e+03 KL: 3.61e+02 MMD: 1.79e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.99e+04 logL: -2.95e+03 KL: 2.60e+02 MMD: 1.95e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.96e+04 logL: -2.03e+03 KL: 1.91e+02 MMD: 2.03e+00\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 45 VALIDATION Loss: 1.91e+04 logL: -1.95e+03 KL: 1.80e+02 MMD: 1.98e+00\n",
      "config 87, alpha = 0.0, lambda = 1.8, dropout = 0.00; 2 hidden layers with 10, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.52e+04 logL: -1.44e+04 KL: 7.97e+02 MMD: 6.91e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.44e+04 logL: -1.38e+04 KL: 6.00e+02 MMD: 7.30e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.40e+04 logL: -1.36e+04 KL: 3.57e+02 MMD: 7.80e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 2.02e+02 MMD: 7.86e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 1.12e+02 MMD: 8.18e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.32e+04 logL: -1.32e+04 KL: 7.36e+01 MMD: 8.64e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.01e+04 logL: -9.97e+03 KL: 1.46e+02 MMD: 6.93e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.00e+04 logL: -9.88e+03 KL: 1.40e+02 MMD: 6.98e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 1.00e+04 logL: -9.87e+03 KL: 1.22e+02 MMD: 7.00e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 9.96e+03 logL: -9.86e+03 KL: 1.01e+02 MMD: 8.52e-01\n",
      "====> Epoch: 110 VALIDATION Loss: 9.94e+03 logL: -9.86e+03 KL: 8.00e+01 MMD: 7.17e-01\n",
      "====> Epoch: 120 VALIDATION Loss: 9.90e+03 logL: -9.84e+03 KL: 6.26e+01 MMD: 8.11e-01\n",
      "====> Epoch: 130 VALIDATION Loss: 9.88e+03 logL: -9.83e+03 KL: 5.07e+01 MMD: 7.50e-01\n",
      "====> Epoch: 140 VALIDATION Loss: 9.84e+03 logL: -9.80e+03 KL: 4.24e+01 MMD: 7.94e-01\n",
      "====> Epoch: 150 VALIDATION Loss: 9.82e+03 logL: -9.79e+03 KL: 3.67e+01 MMD: 7.98e-01\n",
      "====> Epoch: 160 VALIDATION Loss: 9.79e+03 logL: -9.75e+03 KL: 3.22e+01 MMD: 8.56e-01\n",
      "====> Epoch: 170 VALIDATION Loss: 9.76e+03 logL: -9.73e+03 KL: 2.90e+01 MMD: 8.27e-01\n",
      "====> Epoch: 180 VALIDATION Loss: 9.70e+03 logL: -9.68e+03 KL: 2.74e+01 MMD: 8.72e-01\n",
      "====> Epoch: 190 VALIDATION Loss: 9.67e+03 logL: -9.65e+03 KL: 2.64e+01 MMD: 8.27e-01\n",
      "====> Epoch: 200 VALIDATION Loss: 9.64e+03 logL: -9.61e+03 KL: 2.50e+01 MMD: 8.05e-01\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 9.64e+03 logL: -9.61e+03 KL: 2.50e+01 MMD: 8.05e-01\n",
      "config 87, alpha = 0.0, lambda = 724.9, dropout = 0.00; 2 hidden layers with 8, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.56e+04 logL: -1.37e+04 KL: 1.19e+03 MMD: 1.03e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.47e+04 logL: -1.35e+04 KL: 3.90e+02 MMD: 1.07e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.18e+04 logL: -1.08e+04 KL: 2.86e+02 MMD: 1.03e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.07e+04 logL: -9.76e+03 KL: 1.85e+02 MMD: 1.09e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.94e+03 logL: -9.06e+03 KL: 1.83e+02 MMD: 9.76e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 8.02e+03 logL: -7.14e+03 KL: 1.42e+02 MMD: 1.02e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.07e+03 logL: -7.21e+03 KL: 1.14e+02 MMD: 1.02e+00\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 7.91e+03 logL: -7.06e+03 KL: 1.01e+02 MMD: 1.04e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 7.83e+03 logL: -7.05e+03 KL: 1.00e+02 MMD: 9.44e-01\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 7.92e+03 logL: -7.04e+03 KL: 9.90e+01 MMD: 1.09e+00\n",
      "Stopping\n",
      "====> Epoch: 100 VALIDATION Loss: 7.92e+03 logL: -7.04e+03 KL: 9.90e+01 MMD: 1.09e+00\n",
      "config 87, alpha = 0.0, lambda = 195.0, dropout = 0.00; 2 hidden layers with 43, 8 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 1.43e+04 logL: -1.37e+04 KL: 2.48e+02 MMD: 1.64e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.41e+04 logL: -1.35e+04 KL: 1.94e+02 MMD: 1.72e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.39e+04 logL: -1.34e+04 KL: 1.35e+02 MMD: 1.71e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.02e+04 logL: -9.69e+03 KL: 1.48e+02 MMD: 1.77e+00\n",
      "====> Epoch: 50 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 58 VALIDATION Loss: nan logL: nan KL: nan MMD: nan\n",
      "config 87, alpha = 0.0, lambda = 80489.0, dropout = 0.00; 2 hidden layers with 104, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.94e+04 logL: -1.42e+04 KL: 4.12e+00 MMD: 6.38e-02\n",
      "====> Epoch: 20 VALIDATION Loss: 1.83e+04 logL: -1.34e+04 KL: 5.02e+00 MMD: 6.14e-02\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 28 VALIDATION Loss: 1.74e+04 logL: -1.33e+04 KL: 5.32e+00 MMD: 5.10e-02\n",
      "config 87, alpha = 0.0, lambda = 3222.9, dropout = 0.00; 2 hidden layers with 62, 38 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.58e+05 logL: -1.35e+05 KL: 1.71e+04 MMD: 1.86e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.55e+04 logL: -1.40e+04 KL: 5.61e+03 MMD: 1.84e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.17e+04 logL: -1.30e+04 KL: 2.35e+03 MMD: 1.96e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.94e+04 logL: -1.19e+04 KL: 1.22e+03 MMD: 1.96e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.66e+04 logL: -9.25e+03 KL: 8.05e+02 MMD: 2.02e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.37e+04 logL: -7.00e+03 KL: 5.36e+02 MMD: 1.91e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.18e+04 logL: -4.87e+03 KL: 3.84e+02 MMD: 2.03e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.01e+04 logL: -3.63e+03 KL: 3.02e+02 MMD: 1.91e+00\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 9.80e+03 logL: -3.41e+03 KL: 2.81e+02 MMD: 1.90e+00\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 97 VALIDATION Loss: 9.50e+03 logL: -3.38e+03 KL: 2.77e+02 MMD: 1.81e+00\n",
      "config 88, alpha = 0.0, lambda = 2845.6, dropout = 0.00; 2 hidden layers with 139, 52 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.18e+04 logL: -9.61e+03 KL: 1.84e+02 MMD: 6.95e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.15e+04 logL: -9.38e+03 KL: 1.36e+02 MMD: 6.85e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.08e+04 logL: -8.67e+03 KL: 1.14e+02 MMD: 7.20e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.05e+04 logL: -8.28e+03 KL: 1.07e+02 MMD: 7.46e-01\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.04e+04 logL: -8.24e+03 KL: 1.06e+02 MMD: 7.35e-01\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.02e+04 logL: -8.18e+03 KL: 1.04e+02 MMD: 6.58e-01\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.04e+04 logL: -8.18e+03 KL: 1.04e+02 MMD: 7.56e-01\n",
      "Stopping\n",
      "====> Epoch: 70 VALIDATION Loss: 1.04e+04 logL: -8.18e+03 KL: 1.04e+02 MMD: 7.56e-01\n",
      "config 88, alpha = 0.0, lambda = 5677.1, dropout = 0.00; 2 hidden layers with 41, 17 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.34e+05 logL: -3.23e+05 KL: 4.64e+03 MMD: 1.12e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.96e+04 logL: -2.61e+04 KL: 7.49e+03 MMD: 1.07e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.36e+04 logL: -1.48e+04 KL: 2.98e+03 MMD: 1.04e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.12e+04 logL: -1.42e+04 KL: 1.41e+03 MMD: 9.88e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 2.04e+04 logL: -1.39e+04 KL: 7.31e+02 MMD: 1.02e+00\n",
      "Epoch 00058: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 2.05e+04 logL: -1.38e+04 KL: 4.85e+02 MMD: 1.09e+00\n",
      "Stopping\n",
      "====> Epoch: 62 VALIDATION Loss: 2.07e+04 logL: -1.38e+04 KL: 4.81e+02 MMD: 1.12e+00\n",
      "config 88, alpha = 0.0, lambda = 93850.8, dropout = 0.00; 2 hidden layers with 120, 26 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.36e+05 logL: -1.37e+04 KL: 1.04e+03 MMD: 1.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.37e+05 logL: -7.97e+03 KL: 5.89e+02 MMD: 1.37e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.30e+05 logL: -4.49e+03 KL: 4.47e+02 MMD: 1.34e+00\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 1.34e+05 logL: -4.18e+03 KL: 4.03e+02 MMD: 1.38e+00\n",
      "config 88, alpha = 0.0, lambda = 1.3, dropout = 0.00; 2 hidden layers with 11, 10 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.40e+04 logL: -1.36e+04 KL: 3.48e+02 MMD: 2.20e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.38e+04 logL: -1.36e+04 KL: 1.80e+02 MMD: 2.79e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.37e+04 logL: -1.35e+04 KL: 1.15e+02 MMD: 3.28e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 8.87e+01 MMD: 3.51e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.35e+04 logL: -1.34e+04 KL: 8.95e+01 MMD: 2.57e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.66e+03 logL: -9.55e+03 KL: 1.09e+02 MMD: 2.46e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.61e+03 logL: -9.53e+03 KL: 8.41e+01 MMD: 2.77e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 7.58e+03 logL: -7.49e+03 KL: 8.91e+01 MMD: 2.33e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 7.20e+03 logL: -7.12e+03 KL: 7.58e+01 MMD: 2.54e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 7.16e+03 logL: -7.10e+03 KL: 6.63e+01 MMD: 2.49e+00\n",
      "Epoch 00108: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 7.15e+03 logL: -7.09e+03 KL: 6.08e+01 MMD: 2.53e+00\n",
      "Epoch 00118: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 120 VALIDATION Loss: 7.15e+03 logL: -7.09e+03 KL: 5.97e+01 MMD: 2.69e+00\n",
      "Stopping\n",
      "====> Epoch: 122 VALIDATION Loss: 7.15e+03 logL: -7.09e+03 KL: 5.97e+01 MMD: 2.78e+00\n",
      "config 88, alpha = 0.0, lambda = 2299.2, dropout = 0.00; 2 hidden layers with 189, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.86e+04 logL: -1.36e+04 KL: 2.09e+02 MMD: 2.09e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.47e+04 logL: -9.65e+03 KL: 3.02e+02 MMD: 2.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.39e+04 logL: -9.55e+03 KL: 1.36e+02 MMD: 1.85e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.31e+05 logL: -1.54e+05 KL: 7.24e+04 MMD: 2.09e+00\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 47 VALIDATION Loss: 1.28e+05 logL: -9.87e+04 KL: 2.50e+04 MMD: 1.85e+00\n",
      "config 89, alpha = 0.0, lambda = 1.6, dropout = 0.00; 2 hidden layers with 75, 45 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.68e+03 logL: -9.51e+03 KL: 1.70e+02 MMD: 7.16e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.23e+03 logL: -9.14e+03 KL: 9.31e+01 MMD: 7.20e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 8.91e+03 logL: -8.85e+03 KL: 6.21e+01 MMD: 8.00e-01\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 9.64e+03 logL: -9.57e+03 KL: 6.54e+01 MMD: 8.58e-01\n",
      "Stopping\n",
      "====> Epoch: 40 VALIDATION Loss: 9.64e+03 logL: -9.57e+03 KL: 6.54e+01 MMD: 8.58e-01\n",
      "config 89, alpha = 0.0, lambda = 13682.6, dropout = 0.00; 2 hidden layers with 27, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.86e+04 logL: -1.45e+04 KL: 5.11e+02 MMD: 9.89e-01\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 2.82e+04 logL: -1.38e+04 KL: 4.99e+02 MMD: 1.01e+00\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 2.89e+04 logL: -1.37e+04 KL: 4.87e+02 MMD: 1.07e+00\n",
      "Stopping\n",
      "====> Epoch: 33 VALIDATION Loss: 3.07e+04 logL: -1.37e+04 KL: 4.85e+02 MMD: 1.20e+00\n",
      "config 89, alpha = 0.0, lambda = 19.7, dropout = 0.00; 2 hidden layers with 82, 31 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.41e+04 logL: -1.36e+04 KL: 4.94e+02 MMD: 1.40e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.64e+03 logL: -8.20e+03 KL: 4.21e+02 MMD: 1.33e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.33e+03 logL: -4.03e+03 KL: 2.78e+02 MMD: 1.29e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.01e+03 logL: -3.79e+03 KL: 1.99e+02 MMD: 1.35e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.78e+03 logL: -3.59e+03 KL: 1.66e+02 MMD: 1.39e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.02e+03 logL: -2.84e+03 KL: 1.54e+02 MMD: 1.46e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.94e+03 logL: -2.79e+03 KL: 1.29e+02 MMD: 1.35e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.87e+03 logL: -2.73e+03 KL: 1.14e+02 MMD: 1.38e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.87e+03 logL: -2.74e+03 KL: 1.01e+02 MMD: 1.34e+00\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 100 VALIDATION Loss: 2.81e+03 logL: -2.69e+03 KL: 9.81e+01 MMD: 1.28e+00\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 110 VALIDATION Loss: 2.80e+03 logL: -2.68e+03 KL: 9.70e+01 MMD: 1.25e+00\n",
      "Stopping\n",
      "====> Epoch: 113 VALIDATION Loss: 2.80e+03 logL: -2.68e+03 KL: 9.70e+01 MMD: 1.30e+00\n",
      "config 89, alpha = 0.0, lambda = 26649.2, dropout = 0.00; 2 hidden layers with 64, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.62e+04 logL: -1.16e+04 KL: 8.70e+02 MMD: 1.64e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.39e+04 logL: -8.82e+03 KL: 5.21e+02 MMD: 1.67e+00\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 27 VALIDATION Loss: 5.26e+04 logL: -8.12e+03 KL: 4.98e+02 MMD: 1.65e+00\n",
      "config 89, alpha = 0.0, lambda = 175.2, dropout = 0.00; 2 hidden layers with 25, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.93e+04 logL: -1.57e+04 KL: 3.26e+03 MMD: 1.77e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.65e+04 logL: -1.50e+04 KL: 1.20e+03 MMD: 1.86e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.50e+04 logL: -1.41e+04 KL: 5.45e+02 MMD: 2.07e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.25e+04 logL: -1.18e+04 KL: 3.81e+02 MMD: 1.80e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.29e+03 logL: -7.67e+03 KL: 2.90e+02 MMD: 1.88e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.25e+03 logL: -5.67e+03 KL: 2.66e+02 MMD: 1.79e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.44e+03 logL: -3.87e+03 KL: 2.41e+02 MMD: 1.89e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.61e+03 logL: -3.09e+03 KL: 2.01e+02 MMD: 1.84e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.28e+03 logL: -2.77e+03 KL: 1.64e+02 MMD: 1.98e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.76e+03 logL: -2.28e+03 KL: 1.52e+02 MMD: 1.87e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.64e+03 logL: -2.20e+03 KL: 1.31e+02 MMD: 1.78e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.61e+03 logL: -2.16e+03 KL: 1.18e+02 MMD: 1.91e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.61e+03 logL: -2.16e+03 KL: 1.05e+02 MMD: 1.98e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.56e+03 logL: -2.13e+03 KL: 9.95e+01 MMD: 1.87e+00\n",
      "Epoch 00145: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 2.47e+03 logL: -2.06e+03 KL: 9.42e+01 MMD: 1.85e+00\n",
      "Epoch 00157: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 160 VALIDATION Loss: 2.47e+03 logL: -2.05e+03 KL: 9.18e+01 MMD: 1.84e+00\n",
      "Epoch 00164: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 168 VALIDATION Loss: 2.46e+03 logL: -2.05e+03 KL: 9.16e+01 MMD: 1.83e+00\n",
      "config 90, alpha = 0.0, lambda = 754.3, dropout = 0.00; 2 hidden layers with 119, 35 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.01e+04 logL: -9.46e+03 KL: 1.60e+02 MMD: 6.82e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.01e+04 logL: -9.48e+03 KL: 1.17e+02 MMD: 7.06e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.63e+03 logL: -8.98e+03 KL: 9.69e+01 MMD: 7.38e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.18e+03 logL: -8.61e+03 KL: 7.43e+01 MMD: 6.67e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.49e+03 logL: -8.82e+03 KL: 8.13e+01 MMD: 7.82e-01\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 8.96e+03 logL: -8.33e+03 KL: 6.76e+01 MMD: 7.50e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.87e+03 logL: -8.26e+03 KL: 6.18e+01 MMD: 7.32e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.83e+03 logL: -8.22e+03 KL: 6.20e+01 MMD: 7.26e-01\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 90 VALIDATION Loss: 8.82e+03 logL: -8.21e+03 KL: 6.17e+01 MMD: 7.40e-01\n",
      "Stopping\n",
      "====> Epoch: 92 VALIDATION Loss: 8.88e+03 logL: -8.20e+03 KL: 6.13e+01 MMD: 8.27e-01\n",
      "config 90, alpha = 0.0, lambda = 16670.0, dropout = 0.00; 2 hidden layers with 23, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.90e+04 logL: -1.19e+04 KL: 7.53e+02 MMD: 9.82e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 2.81e+04 logL: -9.68e+03 KL: 4.89e+02 MMD: 1.08e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.42e+04 logL: -7.39e+03 KL: 3.20e+02 MMD: 9.87e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 2.39e+04 logL: -7.13e+03 KL: 2.27e+02 MMD: 9.95e-01\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 2.53e+04 logL: -7.07e+03 KL: 2.18e+02 MMD: 1.08e+00\n",
      "Stopping\n",
      "====> Epoch: 51 VALIDATION Loss: 2.48e+04 logL: -7.07e+03 KL: 2.18e+02 MMD: 1.05e+00\n",
      "config 90, alpha = 0.0, lambda = 3513.5, dropout = 0.00; 2 hidden layers with 166, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.52e+06 logL: -5.60e+05 KL: 9.52e+05 MMD: 1.40e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.03e+05 logL: -3.75e+05 KL: 2.24e+05 MMD: 1.25e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.34e+05 logL: -3.60e+05 KL: 6.98e+04 MMD: 1.21e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.81e+05 logL: -3.52e+05 KL: 2.40e+04 MMD: 1.31e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.64e+05 logL: -3.50e+05 KL: 9.07e+03 MMD: 1.34e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.51e+05 logL: -3.42e+05 KL: 3.51e+03 MMD: 1.38e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.42e+05 logL: -3.36e+05 KL: 1.46e+03 MMD: 1.33e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.33e+05 logL: -3.27e+05 KL: 6.99e+02 MMD: 1.39e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.20e+05 logL: -3.15e+05 KL: 3.92e+02 MMD: 1.40e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 3.05e+05 logL: -3.00e+05 KL: 2.93e+02 MMD: 1.33e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.85e+05 logL: -2.80e+05 KL: 3.16e+02 MMD: 1.36e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.44e+05 logL: -2.38e+05 KL: 7.67e+02 MMD: 1.36e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.48e+04 logL: -1.76e+04 KL: 2.34e+03 MMD: 1.40e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.24e+04 logL: -1.65e+04 KL: 1.38e+03 MMD: 1.29e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.13e+04 logL: -1.55e+04 KL: 8.48e+02 MMD: 1.40e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 1.96e+04 logL: -1.43e+04 KL: 5.05e+02 MMD: 1.36e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 1.80e+04 logL: -1.32e+04 KL: 2.93e+02 MMD: 1.30e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 1.67e+04 logL: -1.18e+04 KL: 2.26e+02 MMD: 1.33e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 1.42e+04 logL: -9.50e+03 KL: 2.42e+02 MMD: 1.27e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 1.22e+04 logL: -7.30e+03 KL: 2.67e+02 MMD: 1.32e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 1.22e+04 logL: -7.30e+03 KL: 2.67e+02 MMD: 1.32e+00\n",
      "config 90, alpha = 0.0, lambda = 24.2, dropout = 0.00; 2 hidden layers with 48, 25 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.50e+05 logL: -3.48e+05 KL: 1.72e+03 MMD: 1.78e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.35e+05 logL: -3.30e+05 KL: 4.99e+03 MMD: 1.60e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.07e+05 logL: -3.01e+05 KL: 6.19e+03 MMD: 1.72e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.36e+05 logL: -2.30e+05 KL: 5.72e+03 MMD: 1.69e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.56e+04 logL: -3.15e+04 KL: 4.09e+03 MMD: 1.50e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.56e+04 logL: -1.36e+04 KL: 1.92e+03 MMD: 1.60e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.42e+04 logL: -1.31e+04 KL: 1.04e+03 MMD: 1.76e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.30e+04 logL: -1.24e+04 KL: 6.20e+02 MMD: 1.64e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.04e+04 logL: -9.89e+03 KL: 4.32e+02 MMD: 1.66e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 8.12e+03 logL: -7.75e+03 KL: 3.31e+02 MMD: 1.57e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.83e+03 logL: -6.53e+03 KL: 2.56e+02 MMD: 1.59e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 6.05e+03 logL: -5.81e+03 KL: 2.01e+02 MMD: 1.64e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 5.04e+03 logL: -4.82e+03 KL: 1.86e+02 MMD: 1.67e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 3.63e+03 logL: -3.40e+03 KL: 1.86e+02 MMD: 1.67e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.96e+03 logL: -2.74e+03 KL: 1.80e+02 MMD: 1.65e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 2.79e+03 logL: -2.59e+03 KL: 1.56e+02 MMD: 1.77e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 2.68e+03 logL: -2.50e+03 KL: 1.40e+02 MMD: 1.71e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 2.59e+03 logL: -2.43e+03 KL: 1.29e+02 MMD: 1.54e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 2.01e+03 logL: -1.85e+03 KL: 1.27e+02 MMD: 1.60e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 1.94e+03 logL: -1.79e+03 KL: 1.11e+02 MMD: 1.63e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 1.94e+03 logL: -1.79e+03 KL: 1.11e+02 MMD: 1.63e+00\n",
      "config 90, alpha = 0.0, lambda = 24.1, dropout = 0.00; 2 hidden layers with 14, 13 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 1.54e+04 logL: -1.45e+04 KL: 8.05e+02 MMD: 2.07e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.44e+04 logL: -1.40e+04 KL: 3.76e+02 MMD: 2.15e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.08e+04 logL: -1.05e+04 KL: 2.46e+02 MMD: 2.15e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 8.78e+03 logL: -8.50e+03 KL: 2.34e+02 MMD: 2.15e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.78e+03 logL: -5.51e+03 KL: 2.18e+02 MMD: 2.09e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.96e+03 logL: -3.76e+03 KL: 1.58e+02 MMD: 1.87e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.85e+03 logL: -3.68e+03 KL: 1.28e+02 MMD: 2.06e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.78e+03 logL: -3.63e+03 KL: 1.07e+02 MMD: 2.03e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 3.70e+03 logL: -3.55e+03 KL: 9.70e+01 MMD: 2.02e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.92e+03 logL: -2.78e+03 KL: 9.43e+01 MMD: 1.95e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.89e+03 logL: -2.76e+03 KL: 8.66e+01 MMD: 1.94e+00\n",
      "Epoch 00111: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 120 VALIDATION Loss: 2.87e+03 logL: -2.74e+03 KL: 8.46e+01 MMD: 2.24e+00\n",
      "Epoch 00124: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 130 VALIDATION Loss: 2.86e+03 logL: -2.73e+03 KL: 8.39e+01 MMD: 1.98e+00\n",
      "Stopping\n",
      "====> Epoch: 138 VALIDATION Loss: 2.86e+03 logL: -2.73e+03 KL: 8.37e+01 MMD: 2.02e+00\n",
      "config 91, alpha = 0.0, lambda = 720.2, dropout = 0.00; 2 hidden layers with 33, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.51e+04 logL: -1.44e+04 KL: 1.47e+02 MMD: 8.14e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.43e+04 logL: -1.36e+04 KL: 1.87e+02 MMD: 7.87e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.41e+04 logL: -1.34e+04 KL: 1.54e+02 MMD: 8.28e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 1.39e+04 logL: -1.33e+04 KL: 1.06e+02 MMD: 7.39e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.03e+04 logL: -9.65e+03 KL: 1.54e+02 MMD: 7.44e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.01e+04 logL: -9.54e+03 KL: 1.01e+02 MMD: 6.92e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 1.01e+04 logL: -9.49e+03 KL: 8.07e+01 MMD: 6.78e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 1.00e+04 logL: -9.45e+03 KL: 7.63e+01 MMD: 7.15e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.99e+03 logL: -9.40e+03 KL: 7.37e+01 MMD: 7.13e-01\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 100 VALIDATION Loss: 9.94e+03 logL: -9.37e+03 KL: 7.06e+01 MMD: 6.95e-01\n",
      "Epoch 00106: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 110 VALIDATION Loss: 9.99e+03 logL: -9.37e+03 KL: 6.93e+01 MMD: 7.72e-01\n",
      "Stopping\n",
      "====> Epoch: 110 VALIDATION Loss: 9.99e+03 logL: -9.37e+03 KL: 6.93e+01 MMD: 7.72e-01\n",
      "config 91, alpha = 0.0, lambda = 2953.2, dropout = 0.00; 2 hidden layers with 31, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.99e+04 logL: -1.54e+04 KL: 1.25e+03 MMD: 1.10e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.91e+04 logL: -1.52e+04 KL: 9.09e+02 MMD: 1.03e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.84e+04 logL: -1.47e+04 KL: 6.05e+02 MMD: 1.05e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.70e+04 logL: -1.31e+04 KL: 4.34e+02 MMD: 1.18e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.47e+04 logL: -1.14e+04 KL: 3.56e+02 MMD: 9.99e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 1.39e+04 logL: -1.04e+04 KL: 3.13e+02 MMD: 1.09e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.17e+04 logL: -8.26e+03 KL: 2.69e+02 MMD: 1.09e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.09e+04 logL: -7.76e+03 KL: 2.13e+02 MMD: 1.00e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.12e+04 logL: -7.51e+03 KL: 1.76e+02 MMD: 1.19e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.04e+04 logL: -7.30e+03 KL: 1.47e+02 MMD: 1.01e+00\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 110 VALIDATION Loss: 1.01e+04 logL: -7.22e+03 KL: 1.34e+02 MMD: 9.42e-01\n",
      "Epoch 00114: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 118 VALIDATION Loss: 1.03e+04 logL: -7.21e+03 KL: 1.33e+02 MMD: 1.01e+00\n",
      "config 91, alpha = 0.0, lambda = 5239.1, dropout = 0.00; 2 hidden layers with 154, 76 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.01e+04 logL: -3.28e+03 KL: 2.69e+02 MMD: 1.25e+00\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 1.00e+04 logL: -2.76e+03 KL: 1.88e+02 MMD: 1.35e+00\n",
      "Stopping\n",
      "====> Epoch: 21 VALIDATION Loss: 1.02e+04 logL: -2.76e+03 KL: 1.87e+02 MMD: 1.38e+00\n",
      "config 91, alpha = 0.0, lambda = 31067.1, dropout = 0.00; 2 hidden layers with 14, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 6.75e+04 logL: -1.42e+04 KL: 1.56e+03 MMD: 1.67e+00\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 17 VALIDATION Loss: 6.53e+04 logL: -1.29e+04 KL: 1.04e+03 MMD: 1.65e+00\n",
      "config 91, alpha = 0.0, lambda = 43051.7, dropout = 0.00; 2 hidden layers with 61, 38 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 8.91e+04 logL: -5.40e+03 KL: 5.92e+02 MMD: 1.93e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.98e+04 logL: -2.41e+03 KL: 3.23e+02 MMD: 1.79e+00\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 25 VALIDATION Loss: 8.65e+04 logL: -2.33e+03 KL: 3.02e+02 MMD: 1.95e+00\n",
      "config 92, alpha = 0.0, lambda = 25620.6, dropout = 0.00; 2 hidden layers with 158, 65 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.73e+04 logL: -9.51e+03 KL: 3.45e+02 MMD: 6.82e-01\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 16 VALIDATION Loss: 2.77e+04 logL: -9.44e+03 KL: 3.28e+02 MMD: 7.00e-01\n",
      "config 92, alpha = 0.0, lambda = 34.9, dropout = 0.00; 2 hidden layers with 17, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.01e+04 logL: -9.60e+03 KL: 4.24e+02 MMD: 9.97e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 7.53e+03 logL: -7.27e+03 KL: 2.22e+02 MMD: 1.11e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.08e+03 logL: -5.87e+03 KL: 1.81e+02 MMD: 1.07e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 5.32e+03 logL: -5.16e+03 KL: 1.18e+02 MMD: 1.07e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.29e+03 logL: -5.16e+03 KL: 9.31e+01 MMD: 1.01e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.24e+03 logL: -5.12e+03 KL: 7.78e+01 MMD: 1.06e+00\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 5.21e+03 logL: -5.10e+03 KL: 7.22e+01 MMD: 1.06e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.20e+03 logL: -5.10e+03 KL: 7.05e+01 MMD: 1.01e+00\n",
      "Stopping\n",
      "====> Epoch: 83 VALIDATION Loss: 5.20e+03 logL: -5.10e+03 KL: 6.94e+01 MMD: 1.01e+00\n",
      "config 92, alpha = 0.0, lambda = 6863.6, dropout = 0.00; 2 hidden layers with 92, 40 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.54e+04 logL: -5.61e+03 KL: 4.06e+02 MMD: 1.36e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.23e+04 logL: -2.91e+03 KL: 2.23e+02 MMD: 1.34e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.22e+04 logL: -2.80e+03 KL: 1.54e+02 MMD: 1.35e+00\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.18e+04 logL: -2.77e+03 KL: 1.45e+02 MMD: 1.30e+00\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.11e+04 logL: -2.77e+03 KL: 1.43e+02 MMD: 1.20e+00\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 56 VALIDATION Loss: 1.19e+04 logL: -2.77e+03 KL: 1.42e+02 MMD: 1.30e+00\n",
      "config 92, alpha = 0.0, lambda = 2861.0, dropout = 0.00; 2 hidden layers with 194, 51 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.73e+03 logL: -2.89e+03 KL: 3.12e+02 MMD: 1.58e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.25e+03 logL: -1.73e+03 KL: 1.88e+02 MMD: 1.52e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.59e+03 logL: -1.59e+03 KL: 1.42e+02 MMD: 1.70e+00\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 38 VALIDATION Loss: 6.42e+03 logL: -1.53e+03 KL: 1.27e+02 MMD: 1.66e+00\n",
      "config 92, alpha = 0.0, lambda = 5597.0, dropout = 0.00; 2 hidden layers with 50, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.13e+04 logL: -1.00e+04 KL: 5.49e+02 MMD: 1.91e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.08e+04 logL: -9.62e+03 KL: 2.80e+02 MMD: 1.95e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.84e+04 logL: -7.16e+03 KL: 2.13e+02 MMD: 1.97e+00\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.72e+04 logL: -6.88e+03 KL: 1.80e+02 MMD: 1.81e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.59e+04 logL: -5.58e+03 KL: 2.01e+02 MMD: 1.81e+00\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.79e+04 logL: -5.45e+03 KL: 2.06e+02 MMD: 2.18e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00063: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 67 VALIDATION Loss: 1.62e+04 logL: -5.44e+03 KL: 2.06e+02 MMD: 1.89e+00\n",
      "config 93, alpha = 0.0, lambda = 8.4, dropout = 0.00; 2 hidden layers with 52, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.99e+03 logL: -9.70e+03 KL: 2.79e+02 MMD: 7.23e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.75e+03 logL: -9.61e+03 KL: 1.29e+02 MMD: 7.42e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.53e+03 logL: -9.44e+03 KL: 8.92e+01 MMD: 7.39e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.22e+03 logL: -9.13e+03 KL: 8.05e+01 MMD: 7.80e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 8.69e+03 logL: -8.61e+03 KL: 7.93e+01 MMD: 7.06e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 8.69e+03 logL: -8.62e+03 KL: 5.98e+01 MMD: 7.95e-01\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 8.10e+03 logL: -8.04e+03 KL: 5.28e+01 MMD: 7.42e-01\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 79 VALIDATION Loss: 8.22e+03 logL: -8.16e+03 KL: 5.10e+01 MMD: 7.38e-01\n",
      "config 93, alpha = 0.0, lambda = 1.2, dropout = 0.00; 2 hidden layers with 45, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.45e+04 logL: -1.41e+04 KL: 4.61e+02 MMD: 1.41e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.41e+04 logL: -1.37e+04 KL: 3.52e+02 MMD: 1.48e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.38e+04 logL: -1.36e+04 KL: 2.17e+02 MMD: 1.48e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.29e+02 MMD: 1.73e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.34e+04 logL: -1.33e+04 KL: 8.18e+01 MMD: 2.00e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.13e+04 logL: -1.10e+04 KL: 2.31e+02 MMD: 1.22e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.06e+04 logL: -1.04e+04 KL: 2.20e+02 MMD: 1.27e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.04e+04 logL: -1.02e+04 KL: 2.02e+02 MMD: 1.25e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.02e+04 logL: -1.00e+04 KL: 1.89e+02 MMD: 1.27e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 9.99e+03 logL: -9.81e+03 KL: 1.76e+02 MMD: 1.26e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 8.35e+03 logL: -8.18e+03 KL: 1.70e+02 MMD: 1.26e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 7.59e+03 logL: -7.44e+03 KL: 1.50e+02 MMD: 1.18e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 7.20e+03 logL: -7.08e+03 KL: 1.21e+02 MMD: 1.22e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 7.13e+03 logL: -7.03e+03 KL: 9.80e+01 MMD: 1.20e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 7.10e+03 logL: -7.02e+03 KL: 7.88e+01 MMD: 1.34e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 7.08e+03 logL: -7.02e+03 KL: 6.51e+01 MMD: 1.38e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 1.14e+04 logL: -1.12e+04 KL: 1.86e+02 MMD: 1.16e+00\n",
      "Epoch 00175: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 179 VALIDATION Loss: 6.85e+03 logL: -6.65e+03 KL: 1.97e+02 MMD: 1.19e+00\n",
      "config 93, alpha = 0.0, lambda = 479.8, dropout = 0.00; 2 hidden layers with 41, 15 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.09e+04 logL: -9.81e+03 KL: 4.22e+02 MMD: 1.31e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.77e+03 logL: -5.87e+03 KL: 2.48e+02 MMD: 1.36e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.71e+03 logL: -3.93e+03 KL: 1.81e+02 MMD: 1.24e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.46e+03 logL: -3.70e+03 KL: 1.41e+02 MMD: 1.29e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.37e+03 logL: -3.62e+03 KL: 1.22e+02 MMD: 1.31e+00\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 4.30e+03 logL: -3.56e+03 KL: 1.16e+02 MMD: 1.32e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 4.28e+03 logL: -3.54e+03 KL: 1.12e+02 MMD: 1.31e+00\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 76 VALIDATION Loss: 4.27e+03 logL: -3.50e+03 KL: 1.14e+02 MMD: 1.36e+00\n",
      "config 93, alpha = 0.0, lambda = 496.5, dropout = 0.00; 2 hidden layers with 37, 22 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.34e+03 logL: -8.03e+03 KL: 4.66e+02 MMD: 1.70e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.98e+03 logL: -2.94e+03 KL: 2.15e+02 MMD: 1.65e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.28e+03 logL: -2.28e+03 KL: 1.51e+02 MMD: 1.72e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.25e+03 logL: -2.28e+03 KL: 1.13e+02 MMD: 1.74e+00\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 3.11e+03 logL: -2.20e+03 KL: 1.02e+02 MMD: 1.64e+00\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 56 VALIDATION Loss: 3.08e+03 logL: -2.20e+03 KL: 1.01e+02 MMD: 1.58e+00\n",
      "config 93, alpha = 0.0, lambda = 1.6, dropout = 0.00; 2 hidden layers with 12, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.96e+03 logL: -9.64e+03 KL: 3.15e+02 MMD: 2.11e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 7.68e+03 logL: -7.49e+03 KL: 1.92e+02 MMD: 2.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.30e+03 logL: -4.09e+03 KL: 2.04e+02 MMD: 1.85e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.75e+03 logL: -3.61e+03 KL: 1.41e+02 MMD: 1.97e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.73e+03 logL: -3.61e+03 KL: 1.12e+02 MMD: 2.06e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.70e+03 logL: -3.60e+03 KL: 9.66e+01 MMD: 1.96e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 3.68e+03 logL: -3.59e+03 KL: 8.53e+01 MMD: 2.22e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 3.70e+03 logL: -3.62e+03 KL: 8.00e+01 MMD: 2.09e+00\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 3.66e+03 logL: -3.58e+03 KL: 7.74e+01 MMD: 2.24e+00\n",
      "Epoch 00091: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 3.66e+03 logL: -3.58e+03 KL: 7.77e+01 MMD: 2.12e+00\n",
      "config 94, alpha = 0.0, lambda = 39297.7, dropout = 0.00; 2 hidden layers with 16, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.99e+04 logL: -1.32e+04 KL: 2.46e+02 MMD: 6.73e-01\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 3.86e+04 logL: -9.69e+03 KL: 3.65e+02 MMD: 7.26e-01\n",
      "Stopping\n",
      "====> Epoch: 22 VALIDATION Loss: 3.89e+04 logL: -9.69e+03 KL: 3.66e+02 MMD: 7.35e-01\n",
      "config 94, alpha = 0.0, lambda = 5.2, dropout = 0.00; 2 hidden layers with 11, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.51e+04 logL: -1.41e+04 KL: 1.08e+03 MMD: 1.34e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.40e+04 logL: -1.36e+04 KL: 4.38e+02 MMD: 1.19e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.01e+04 logL: -9.82e+03 KL: 2.62e+02 MMD: 1.14e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.73e+03 logL: -9.59e+03 KL: 1.35e+02 MMD: 1.23e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.64e+03 logL: -9.55e+03 KL: 8.90e+01 MMD: 1.27e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.07e+03 logL: -7.97e+03 KL: 8.64e+01 MMD: 1.01e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.20e+03 logL: -7.12e+03 KL: 7.14e+01 MMD: 1.11e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 7.18e+03 logL: -7.11e+03 KL: 5.88e+01 MMD: 1.15e+00\n",
      "Stopping\n",
      "====> Epoch: 88 VALIDATION Loss: 7.18e+03 logL: -7.12e+03 KL: 5.33e+01 MMD: 1.35e+00\n",
      "config 94, alpha = 0.0, lambda = 7.3, dropout = 0.00; 2 hidden layers with 67, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.49e+05 logL: -3.49e+05 KL: 1.90e+00 MMD: 3.82e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 3.42e+05 logL: -3.42e+05 KL: 3.60e+00 MMD: 6.63e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 3.30e+05 logL: -3.30e+05 KL: 5.09e+00 MMD: 1.23e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.04e+05 logL: -3.04e+05 KL: 4.25e+01 MMD: 2.07e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.41e+04 logL: -3.37e+04 KL: 3.63e+02 MMD: 1.64e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.01e+04 logL: -1.97e+04 KL: 3.19e+02 MMD: 1.59e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.77e+04 logL: -1.73e+04 KL: 3.17e+02 MMD: 1.59e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.52e+04 logL: -1.49e+04 KL: 3.16e+02 MMD: 1.39e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 1.28e+04 logL: -1.25e+04 KL: 3.19e+02 MMD: 1.56e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 1.18e+04 logL: -1.15e+04 KL: 3.05e+02 MMD: 1.44e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 1.13e+04 logL: -1.10e+04 KL: 2.67e+02 MMD: 1.47e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 1.05e+04 logL: -1.03e+04 KL: 2.27e+02 MMD: 1.45e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 1.01e+04 logL: -9.92e+03 KL: 1.89e+02 MMD: 1.54e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 9.88e+03 logL: -9.72e+03 KL: 1.51e+02 MMD: 1.60e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 9.77e+03 logL: -9.64e+03 KL: 1.16e+02 MMD: 1.59e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 7.86e+03 logL: -7.73e+03 KL: 1.20e+02 MMD: 1.82e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 170 VALIDATION Loss: 7.37e+03 logL: -7.26e+03 KL: 1.04e+02 MMD: 1.65e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 7.27e+03 logL: -7.17e+03 KL: 8.77e+01 MMD: 1.64e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 7.19e+03 logL: -7.10e+03 KL: 7.98e+01 MMD: 1.73e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 6.10e+03 logL: -5.99e+03 KL: 9.68e+01 MMD: 1.52e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 6.10e+03 logL: -5.99e+03 KL: 9.68e+01 MMD: 1.52e+00\n",
      "config 94, alpha = 0.0, lambda = 114.7, dropout = 0.00; 2 hidden layers with 11, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.61e+04 logL: -5.02e+04 KL: 5.71e+03 MMD: 1.84e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.67e+04 logL: -1.43e+04 KL: 2.17e+03 MMD: 1.62e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.52e+04 logL: -1.41e+04 KL: 1.00e+03 MMD: 1.64e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.46e+04 logL: -1.39e+04 KL: 5.45e+02 MMD: 1.79e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.42e+04 logL: -1.36e+04 KL: 3.65e+02 MMD: 1.58e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.28e+04 logL: -1.23e+04 KL: 2.71e+02 MMD: 1.81e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.16e+04 logL: -1.12e+04 KL: 2.22e+02 MMD: 1.86e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.05e+04 logL: -1.01e+04 KL: 1.93e+02 MMD: 1.78e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 9.61e+03 logL: -9.24e+03 KL: 1.74e+02 MMD: 1.65e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 8.08e+03 logL: -7.71e+03 KL: 1.68e+02 MMD: 1.81e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 7.15e+03 logL: -6.78e+03 KL: 1.68e+02 MMD: 1.73e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.47e+03 logL: -5.12e+03 KL: 1.68e+02 MMD: 1.59e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 5.01e+03 logL: -4.67e+03 KL: 1.57e+02 MMD: 1.62e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 4.70e+03 logL: -4.37e+03 KL: 1.44e+02 MMD: 1.63e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 4.39e+03 logL: -4.07e+03 KL: 1.30e+02 MMD: 1.67e+00\n",
      "====> Epoch: 160 VALIDATION Loss: 3.55e+03 logL: -3.24e+03 KL: 1.20e+02 MMD: 1.67e+00\n",
      "====> Epoch: 170 VALIDATION Loss: 3.26e+03 logL: -2.95e+03 KL: 1.10e+02 MMD: 1.73e+00\n",
      "====> Epoch: 180 VALIDATION Loss: 2.78e+03 logL: -2.48e+03 KL: 1.08e+02 MMD: 1.65e+00\n",
      "====> Epoch: 190 VALIDATION Loss: 2.64e+03 logL: -2.36e+03 KL: 1.02e+02 MMD: 1.59e+00\n",
      "====> Epoch: 200 VALIDATION Loss: 2.54e+03 logL: -2.27e+03 KL: 9.61e+01 MMD: 1.60e+00\n",
      "Stopping\n",
      "====> Epoch: 200 VALIDATION Loss: 2.54e+03 logL: -2.27e+03 KL: 9.61e+01 MMD: 1.60e+00\n",
      "config 94, alpha = 0.0, lambda = 2.1, dropout = 0.00; 2 hidden layers with 48, 19 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.35e+04 logL: -1.33e+04 KL: 2.40e+02 MMD: 2.72e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.84e+03 logL: -5.65e+03 KL: 1.87e+02 MMD: 1.96e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.51e+03 logL: -5.39e+03 KL: 1.21e+02 MMD: 2.30e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.87e+03 logL: -3.74e+03 KL: 1.29e+02 MMD: 2.11e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.72e+03 logL: -3.62e+03 KL: 9.83e+01 MMD: 2.04e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.68e+03 logL: -3.59e+03 KL: 9.03e+01 MMD: 2.15e+00\n",
      "Epoch 00060: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 2.95e+03 logL: -2.83e+03 KL: 1.19e+02 MMD: 2.08e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.88e+03 logL: -2.78e+03 KL: 1.05e+02 MMD: 2.06e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.86e+03 logL: -2.76e+03 KL: 9.56e+01 MMD: 2.09e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.85e+03 logL: -2.76e+03 KL: 9.00e+01 MMD: 1.99e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.84e+03 logL: -2.75e+03 KL: 8.51e+01 MMD: 2.15e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.83e+03 logL: -2.75e+03 KL: 8.16e+01 MMD: 2.08e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.82e+03 logL: -2.74e+03 KL: 7.96e+01 MMD: 2.19e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 2.81e+03 logL: -2.73e+03 KL: 7.72e+01 MMD: 2.01e+00\n",
      "Epoch 00146: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 150 VALIDATION Loss: 2.81e+03 logL: -2.73e+03 KL: 7.56e+01 MMD: 2.07e+00\n",
      "Stopping\n",
      "====> Epoch: 157 VALIDATION Loss: 2.81e+03 logL: -2.73e+03 KL: 7.53e+01 MMD: 2.01e+00\n",
      "config 95, alpha = 0.0, lambda = 10.9, dropout = 0.00; 2 hidden layers with 4, 4 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.43e+04 logL: -1.35e+04 KL: 7.05e+02 MMD: 7.85e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 2.57e+02 MMD: 8.24e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 1.13e+04 logL: -1.11e+04 KL: 2.37e+02 MMD: 7.31e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.74e+03 logL: -9.58e+03 KL: 1.49e+02 MMD: 7.42e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.65e+03 logL: -9.54e+03 KL: 9.88e+01 MMD: 7.95e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.61e+03 logL: -9.52e+03 KL: 7.31e+01 MMD: 7.88e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.60e+03 logL: -9.53e+03 KL: 5.99e+01 MMD: 8.06e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.57e+03 logL: -9.52e+03 KL: 4.98e+01 MMD: 8.71e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 9.56e+03 logL: -9.51e+03 KL: 4.31e+01 MMD: 8.47e-01\n",
      "Stopping\n",
      "====> Epoch: 96 VALIDATION Loss: 9.57e+03 logL: -9.52e+03 KL: 3.98e+01 MMD: 8.18e-01\n",
      "config 95, alpha = 0.0, lambda = 9.8, dropout = 0.00; 2 hidden layers with 19, 7 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.40e+04 logL: -1.36e+04 KL: 4.13e+02 MMD: 1.14e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.24e+04 logL: -1.20e+04 KL: 3.66e+02 MMD: 1.14e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.60e+03 logL: -9.40e+03 KL: 1.96e+02 MMD: 9.93e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 5.84e+03 logL: -5.63e+03 KL: 1.97e+02 MMD: 1.03e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 5.69e+03 logL: -5.55e+03 KL: 1.34e+02 MMD: 1.07e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 5.62e+03 logL: -5.51e+03 KL: 1.07e+02 MMD: 1.09e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.55e+03 logL: -5.45e+03 KL: 9.12e+01 MMD: 1.07e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.49e+03 logL: -5.40e+03 KL: 8.19e+01 MMD: 1.09e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.34e+03 logL: -5.26e+03 KL: 7.43e+01 MMD: 1.05e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.28e+03 logL: -5.20e+03 KL: 6.97e+01 MMD: 1.10e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.23e+03 logL: -5.16e+03 KL: 6.46e+01 MMD: 1.10e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.21e+03 logL: -5.14e+03 KL: 6.08e+01 MMD: 1.05e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 5.15e+03 logL: -5.08e+03 KL: 5.72e+01 MMD: 1.15e+00\n",
      "====> Epoch: 140 VALIDATION Loss: 5.13e+03 logL: -5.07e+03 KL: 5.42e+01 MMD: 1.04e+00\n",
      "Epoch 00147: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 150 VALIDATION Loss: 5.08e+03 logL: -5.02e+03 KL: 5.23e+01 MMD: 1.09e+00\n",
      "Epoch 00156: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 158 VALIDATION Loss: 5.08e+03 logL: -5.01e+03 KL: 5.20e+01 MMD: 1.18e+00\n",
      "config 95, alpha = 0.0, lambda = 4369.1, dropout = 0.00; 2 hidden layers with 15, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.48e+04 logL: -8.11e+03 KL: 4.80e+02 MMD: 1.43e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.05e+04 logL: -4.02e+03 KL: 3.04e+02 MMD: 1.41e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.03e+03 logL: -3.60e+03 KL: 1.84e+02 MMD: 1.20e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.56e+03 logL: -3.61e+03 KL: 1.51e+02 MMD: 1.33e+00\n",
      "Epoch 00042: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 46 VALIDATION Loss: 9.55e+03 logL: -3.56e+03 KL: 1.44e+02 MMD: 1.34e+00\n",
      "config 95, alpha = 0.0, lambda = 2037.2, dropout = 0.00; 2 hidden layers with 20, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.07e+04 logL: -1.46e+04 KL: 2.82e+03 MMD: 1.62e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.81e+04 logL: -1.39e+04 KL: 7.60e+02 MMD: 1.66e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.73e+04 logL: -1.37e+04 KL: 4.04e+02 MMD: 1.57e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.63e+04 logL: -1.26e+04 KL: 3.06e+02 MMD: 1.66e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.26e+04 logL: -9.33e+03 KL: 2.74e+02 MMD: 1.47e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.00e+04 logL: -6.44e+03 KL: 2.61e+02 MMD: 1.63e+00\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 9.61e+03 logL: -6.08e+03 KL: 2.20e+02 MMD: 1.63e+00\n",
      "Stopping\n",
      "====> Epoch: 72 VALIDATION Loss: 9.86e+03 logL: -6.07e+03 KL: 2.19e+02 MMD: 1.75e+00\n",
      "config 95, alpha = 0.0, lambda = 15.9, dropout = 0.00; 2 hidden layers with 48, 37 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.32e+03 logL: -3.93e+03 KL: 3.62e+02 MMD: 2.01e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.97e+03 logL: -2.75e+03 KL: 1.90e+02 MMD: 1.79e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.95e+03 logL: -1.77e+03 KL: 1.51e+02 MMD: 2.01e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 VALIDATION Loss: 1.72e+03 logL: -1.57e+03 KL: 1.21e+02 MMD: 2.03e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.28e+03 logL: -1.14e+03 KL: 1.10e+02 MMD: 1.85e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.22e+03 logL: -1.09e+03 KL: 9.93e+01 MMD: 2.04e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.74e+02 logL: -8.51e+02 KL: 9.42e+01 MMD: 1.87e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.04e+03 logL: -9.29e+02 KL: 8.62e+01 MMD: 1.95e+00\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 90 VALIDATION Loss: 9.38e+02 logL: -8.28e+02 KL: 8.21e+01 MMD: 1.87e+00\n",
      "Stopping\n",
      "====> Epoch: 94 VALIDATION Loss: 9.39e+02 logL: -8.30e+02 KL: 8.03e+01 MMD: 1.94e+00\n",
      "config 96, alpha = 0.0, lambda = 78.6, dropout = 0.00; 2 hidden layers with 62, 5 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.02e+04 logL: -9.65e+03 KL: 4.47e+02 MMD: 6.77e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.79e+03 logL: -9.55e+03 KL: 1.89e+02 MMD: 7.10e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.77e+03 logL: -9.60e+03 KL: 1.19e+02 MMD: 7.23e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.67e+03 logL: -9.53e+03 KL: 8.69e+01 MMD: 7.13e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.62e+03 logL: -9.49e+03 KL: 7.50e+01 MMD: 7.13e-01\n",
      "Epoch 00058: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 60 VALIDATION Loss: 9.52e+03 logL: -9.39e+03 KL: 6.69e+01 MMD: 7.83e-01\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 69 VALIDATION Loss: 9.51e+03 logL: -9.39e+03 KL: 6.55e+01 MMD: 7.06e-01\n",
      "config 96, alpha = 0.0, lambda = 142.2, dropout = 0.00; 2 hidden layers with 8, 6 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.45e+04 logL: -1.36e+04 KL: 8.10e+02 MMD: 1.19e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.40e+04 logL: -1.35e+04 KL: 2.77e+02 MMD: 1.24e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.38e+04 logL: -1.35e+04 KL: 1.49e+02 MMD: 1.11e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.99e+03 logL: -9.69e+03 KL: 1.46e+02 MMD: 1.06e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.77e+03 logL: -9.50e+03 KL: 1.03e+02 MMD: 1.15e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.20e+03 logL: -7.96e+03 KL: 1.00e+02 MMD: 1.01e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.41e+03 logL: -7.17e+03 KL: 7.80e+01 MMD: 1.14e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 7.34e+03 logL: -7.11e+03 KL: 6.83e+01 MMD: 1.12e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 7.31e+03 logL: -7.10e+03 KL: 6.22e+01 MMD: 1.04e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 7.30e+03 logL: -7.08e+03 KL: 5.71e+01 MMD: 1.10e+00\n",
      "Stopping\n",
      "====> Epoch: 108 VALIDATION Loss: 7.30e+03 logL: -7.08e+03 KL: 5.40e+01 MMD: 1.19e+00\n",
      "config 96, alpha = 0.0, lambda = 18705.4, dropout = 0.00; 2 hidden layers with 11, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.71e+04 logL: -1.06e+04 KL: 5.88e+02 MMD: 1.39e+00\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 3.39e+04 logL: -9.78e+03 KL: 5.07e+02 MMD: 1.26e+00\n",
      "Stopping\n",
      "====> Epoch: 22 VALIDATION Loss: 3.56e+04 logL: -9.74e+03 KL: 5.06e+02 MMD: 1.35e+00\n",
      "config 96, alpha = 0.0, lambda = 1.7, dropout = 0.00; 2 hidden layers with 50, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.01e+04 logL: -9.64e+03 KL: 4.42e+02 MMD: 1.79e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 5.92e+03 logL: -5.70e+03 KL: 2.13e+02 MMD: 1.74e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 4.12e+03 logL: -3.97e+03 KL: 1.50e+02 MMD: 1.72e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.80e+03 logL: -3.69e+03 KL: 1.13e+02 MMD: 1.79e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.11e+03 logL: -3.01e+03 KL: 1.08e+02 MMD: 1.67e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.90e+03 logL: -2.80e+03 KL: 9.54e+01 MMD: 1.72e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.87e+03 logL: -2.78e+03 KL: 8.60e+01 MMD: 1.75e+00\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 2.82e+03 logL: -2.74e+03 KL: 8.03e+01 MMD: 1.62e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.82e+03 logL: -2.74e+03 KL: 7.81e+01 MMD: 1.72e+00\n",
      "Stopping\n",
      "====> Epoch: 97 VALIDATION Loss: 2.82e+03 logL: -2.74e+03 KL: 7.56e+01 MMD: 1.81e+00\n",
      "config 96, alpha = 0.0, lambda = 21645.9, dropout = 0.00; 2 hidden layers with 16, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.57e+04 logL: -1.63e+04 KL: 4.86e+02 MMD: 1.80e+00\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 5.78e+04 logL: -1.34e+04 KL: 5.14e+02 MMD: 2.03e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 5.38e+04 logL: -1.27e+04 KL: 4.92e+02 MMD: 1.88e+00\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 37 VALIDATION Loss: 5.50e+04 logL: -1.22e+04 KL: 4.79e+02 MMD: 1.96e+00\n",
      "config 97, alpha = 0.0, lambda = 14.0, dropout = 0.00; 2 hidden layers with 45, 13 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 9.92e+03 logL: -9.63e+03 KL: 2.72e+02 MMD: 7.33e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 9.72e+03 logL: -9.56e+03 KL: 1.43e+02 MMD: 6.99e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.60e+03 logL: -9.49e+03 KL: 1.02e+02 MMD: 7.51e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.60e+03 logL: -9.51e+03 KL: 7.94e+01 MMD: 7.45e-01\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 9.50e+03 logL: -9.41e+03 KL: 7.61e+01 MMD: 7.99e-01\n",
      "Stopping\n",
      "====> Epoch: 51 VALIDATION Loss: 9.49e+03 logL: -9.41e+03 KL: 7.59e+01 MMD: 7.48e-01\n",
      "config 97, alpha = 0.0, lambda = 47289.8, dropout = 0.00; 2 hidden layers with 71, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.35e+04 logL: -7.24e+03 KL: 5.71e+02 MMD: 9.65e-01\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 6.12e+04 logL: -5.55e+03 KL: 3.31e+02 MMD: 1.17e+00\n",
      "Stopping\n",
      "====> Epoch: 22 VALIDATION Loss: 5.21e+04 logL: -5.55e+03 KL: 3.28e+02 MMD: 9.77e-01\n",
      "config 97, alpha = 0.0, lambda = 511.3, dropout = 0.00; 2 hidden layers with 25, 14 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.16e+04 logL: -1.03e+04 KL: 5.98e+02 MMD: 1.37e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 6.67e+03 logL: -5.68e+03 KL: 3.41e+02 MMD: 1.27e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.17e+03 logL: -5.27e+03 KL: 1.89e+02 MMD: 1.39e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 4.44e+03 logL: -3.65e+03 KL: 1.43e+02 MMD: 1.27e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 4.29e+03 logL: -3.48e+03 KL: 1.14e+02 MMD: 1.36e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.61e+03 logL: -2.79e+03 KL: 1.06e+02 MMD: 1.41e+00\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 3.40e+03 logL: -2.75e+03 KL: 9.49e+01 MMD: 1.08e+00\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 3.49e+03 logL: -2.75e+03 KL: 9.28e+01 MMD: 1.27e+00\n",
      "Stopping\n",
      "====> Epoch: 80 VALIDATION Loss: 3.49e+03 logL: -2.75e+03 KL: 9.28e+01 MMD: 1.27e+00\n",
      "config 97, alpha = 0.0, lambda = 132.3, dropout = 0.00; 2 hidden layers with 80, 35 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 7.86e+03 logL: -7.34e+03 KL: 2.87e+02 MMD: 1.72e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 3.48e+03 logL: -3.09e+03 KL: 1.89e+02 MMD: 1.54e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.04e+03 logL: -1.66e+03 KL: 1.62e+02 MMD: 1.66e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.95e+03 logL: -1.59e+03 KL: 1.24e+02 MMD: 1.80e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.91e+03 logL: -1.59e+03 KL: 1.10e+02 MMD: 1.62e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.98e+03 logL: -1.66e+03 KL: 9.92e+01 MMD: 1.62e+00\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.81e+03 logL: -1.51e+03 KL: 9.39e+01 MMD: 1.56e+00\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Stopping\n",
      "====> Epoch: 73 VALIDATION Loss: 1.82e+03 logL: -1.51e+03 KL: 9.41e+01 MMD: 1.62e+00\n",
      "config 97, alpha = 0.0, lambda = 4840.3, dropout = 0.00; 2 hidden layers with 71, 38 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.14e+04 logL: -1.16e+04 KL: 6.40e+02 MMD: 1.89e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.63e+04 logL: -6.35e+03 KL: 3.31e+02 MMD: 1.99e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.33e+04 logL: -3.46e+03 KL: 2.66e+02 MMD: 1.97e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.10e+04 logL: -2.42e+03 KL: 2.12e+02 MMD: 1.73e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.13e+04 logL: -2.09e+03 KL: 1.51e+02 MMD: 1.86e+00\n",
      "Epoch 00051: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 55 VALIDATION Loss: 1.12e+04 logL: -1.85e+03 KL: 1.35e+02 MMD: 1.90e+00\n",
      "config 98, alpha = 0.0, lambda = 6.0, dropout = 0.00; 2 hidden layers with 7, 6 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.35e+04 KL: 6.16e+02 MMD: 7.06e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.06e+04 logL: -1.02e+04 KL: 3.15e+02 MMD: 7.13e-01\n",
      "====> Epoch: 30 VALIDATION Loss: 9.66e+03 logL: -9.50e+03 KL: 1.53e+02 MMD: 6.77e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.52e+03 logL: -9.42e+03 KL: 9.90e+01 MMD: 6.63e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.40e+03 logL: -9.31e+03 KL: 7.85e+01 MMD: 7.57e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.09e+03 logL: -9.02e+03 KL: 6.52e+01 MMD: 7.68e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.05e+03 logL: -8.98e+03 KL: 6.48e+01 MMD: 7.86e-01\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 8.88e+03 logL: -8.82e+03 KL: 6.19e+01 MMD: 7.79e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 8.80e+03 logL: -8.73e+03 KL: 5.85e+01 MMD: 7.09e-01\n",
      "Epoch 00099: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 100 VALIDATION Loss: 8.80e+03 logL: -8.73e+03 KL: 5.78e+01 MMD: 7.93e-01\n",
      "Stopping\n",
      "====> Epoch: 103 VALIDATION Loss: 8.80e+03 logL: -8.73e+03 KL: 5.79e+01 MMD: 7.51e-01\n",
      "config 98, alpha = 0.0, lambda = 2538.4, dropout = 0.00; 2 hidden layers with 11, 9 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.70e+04 logL: -1.36e+04 KL: 6.24e+02 MMD: 1.09e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.66e+04 logL: -1.36e+04 KL: 3.08e+02 MMD: 1.06e+00\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 30 VALIDATION Loss: 1.64e+04 logL: -1.35e+04 KL: 2.62e+02 MMD: 1.02e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.61e+04 logL: -1.34e+04 KL: 2.43e+02 MMD: 9.58e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 1.51e+04 logL: -1.21e+04 KL: 2.90e+02 MMD: 1.05e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 1.35e+04 logL: -1.03e+04 KL: 3.04e+02 MMD: 1.11e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.28e+04 logL: -9.79e+03 KL: 2.66e+02 MMD: 1.07e+00\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 80 VALIDATION Loss: 1.26e+04 logL: -9.71e+03 KL: 2.54e+02 MMD: 1.02e+00\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Stopping\n",
      "====> Epoch: 87 VALIDATION Loss: 1.26e+04 logL: -9.70e+03 KL: 2.52e+02 MMD: 1.06e+00\n",
      "config 98, alpha = 0.0, lambda = 18889.0, dropout = 0.00; 2 hidden layers with 66, 16 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.67e+04 logL: -3.68e+03 KL: 2.96e+02 MMD: 1.20e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.87e+04 logL: -2.84e+03 KL: 1.92e+02 MMD: 1.36e+00\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 24 VALIDATION Loss: 2.71e+04 logL: -2.79e+03 KL: 1.90e+02 MMD: 1.28e+00\n",
      "config 98, alpha = 0.0, lambda = 18.5, dropout = 0.00; 2 hidden layers with 101, 11 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.45e+04 logL: -1.37e+04 KL: 7.71e+02 MMD: 1.83e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 8.34e+03 logL: -7.87e+03 KL: 4.41e+02 MMD: 1.68e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 6.00e+03 logL: -5.65e+03 KL: 3.17e+02 MMD: 1.78e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 3.92e+03 logL: -3.65e+03 KL: 2.48e+02 MMD: 1.65e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 3.22e+03 logL: -3.01e+03 KL: 1.90e+02 MMD: 1.55e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 3.07e+03 logL: -2.89e+03 KL: 1.56e+02 MMD: 1.63e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 2.98e+03 logL: -2.81e+03 KL: 1.33e+02 MMD: 1.61e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 2.38e+03 logL: -2.22e+03 KL: 1.34e+02 MMD: 1.66e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 2.24e+03 logL: -2.10e+03 KL: 1.12e+02 MMD: 1.63e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 2.22e+03 logL: -2.09e+03 KL: 9.99e+01 MMD: 1.61e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 2.17e+03 logL: -2.06e+03 KL: 8.90e+01 MMD: 1.51e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 2.16e+03 logL: -2.05e+03 KL: 8.27e+01 MMD: 1.64e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 2.15e+03 logL: -2.04e+03 KL: 8.10e+01 MMD: 1.64e+00\n",
      "Epoch 00139: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 140 VALIDATION Loss: 2.07e+03 logL: -1.97e+03 KL: 7.85e+01 MMD: 1.61e+00\n",
      "====> Epoch: 150 VALIDATION Loss: 2.07e+03 logL: -1.97e+03 KL: 7.64e+01 MMD: 1.44e+00\n",
      "Epoch 00152: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 160 VALIDATION Loss: 2.07e+03 logL: -1.96e+03 KL: 7.55e+01 MMD: 1.61e+00\n",
      "Stopping\n",
      "====> Epoch: 166 VALIDATION Loss: 2.06e+03 logL: -1.96e+03 KL: 7.54e+01 MMD: 1.53e+00\n",
      "config 98, alpha = 0.0, lambda = 523.0, dropout = 0.00; 2 hidden layers with 159, 76 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 3.18e+03 logL: -1.99e+03 KL: 2.83e+02 MMD: 1.74e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.10e+03 logL: -9.07e+02 KL: 1.81e+02 MMD: 1.94e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.97e+03 logL: -8.99e+02 KL: 1.42e+02 MMD: 1.78e+00\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 40 VALIDATION Loss: 1.99e+03 logL: -8.31e+02 KL: 1.32e+02 MMD: 1.96e+00\n",
      "Stopping\n",
      "====> Epoch: 42 VALIDATION Loss: 1.92e+03 logL: -8.30e+02 KL: 1.31e+02 MMD: 1.83e+00\n",
      "config 99, alpha = 0.0, lambda = 56678.5, dropout = 0.00; 2 hidden layers with 47, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 5.47e+04 logL: -1.51e+04 KL: 1.63e+02 MMD: 6.96e-01\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 20 VALIDATION Loss: 5.47e+04 logL: -1.39e+04 KL: 4.44e+02 MMD: 7.11e-01\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 30 VALIDATION Loss: 5.67e+04 logL: -1.38e+04 KL: 4.81e+02 MMD: 7.49e-01\n",
      "Stopping\n",
      "====> Epoch: 33 VALIDATION Loss: 5.38e+04 logL: -1.38e+04 KL: 4.83e+02 MMD: 6.98e-01\n",
      "config 99, alpha = 0.0, lambda = 12813.3, dropout = 0.00; 2 hidden layers with 35, 33 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.18e+04 logL: -8.71e+03 KL: 4.72e+02 MMD: 9.88e-01\n",
      "====> Epoch: 20 VALIDATION Loss: 1.92e+04 logL: -5.41e+03 KL: 2.62e+02 MMD: 1.06e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.83e+04 logL: -5.18e+03 KL: 1.85e+02 MMD: 1.01e+00\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 38 VALIDATION Loss: 1.75e+04 logL: -5.14e+03 KL: 1.72e+02 MMD: 9.49e-01\n",
      "config 99, alpha = 0.0, lambda = 1112.0, dropout = 0.00; 2 hidden layers with 9, 8 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 1.60e+04 logL: -1.37e+04 KL: 7.19e+02 MMD: 1.43e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.55e+04 logL: -1.36e+04 KL: 3.66e+02 MMD: 1.39e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.34e+04 logL: -1.15e+04 KL: 3.19e+02 MMD: 1.40e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.13e+04 logL: -9.58e+03 KL: 2.09e+02 MMD: 1.37e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 1.12e+04 logL: -9.53e+03 KL: 1.50e+02 MMD: 1.38e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.73e+03 logL: -7.21e+03 KL: 1.42e+02 MMD: 1.23e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.71e+03 logL: -7.12e+03 KL: 1.13e+02 MMD: 1.33e+00\n",
      "Epoch 00079: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 80 VALIDATION Loss: 8.75e+03 logL: -7.10e+03 KL: 9.84e+01 MMD: 1.39e+00\n",
      "Stopping\n",
      "====> Epoch: 83 VALIDATION Loss: 8.71e+03 logL: -7.10e+03 KL: 9.77e+01 MMD: 1.37e+00\n",
      "config 99, alpha = 0.0, lambda = 281.3, dropout = 0.00; 2 hidden layers with 170, 120 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 2.48e+03 logL: -1.76e+03 KL: 2.40e+02 MMD: 1.72e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.26e+03 logL: -1.67e+03 KL: 1.46e+02 MMD: 1.61e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 2.09e+03 logL: -1.53e+03 KL: 1.08e+02 MMD: 1.62e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.07e+03 logL: -1.55e+03 KL: 9.58e+01 MMD: 1.50e+00\n",
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-04.\n",
      "====> Epoch: 50 VALIDATION Loss: 1.98e+03 logL: -1.43e+03 KL: 8.66e+01 MMD: 1.67e+00\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-05.\n",
      "====> Epoch: 60 VALIDATION Loss: 1.92e+03 logL: -1.42e+03 KL: 8.51e+01 MMD: 1.47e+00\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-06.\n",
      "====> Epoch: 70 VALIDATION Loss: 1.94e+03 logL: -1.42e+03 KL: 8.44e+01 MMD: 1.56e+00\n",
      "Stopping\n",
      "====> Epoch: 70 VALIDATION Loss: 1.94e+03 logL: -1.42e+03 KL: 8.44e+01 MMD: 1.56e+00\n",
      "config 99, alpha = 0.0, lambda = 3775.9, dropout = 0.00; 2 hidden layers with 35, 12 nodes\n",
      "====> Epoch: 10 VALIDATION Loss: 4.35e+05 logL: -3.45e+05 KL: 8.26e+04 MMD: 1.97e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 2.55e+05 logL: -2.22e+05 KL: 2.46e+04 MMD: 2.08e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 3.16e+04 logL: -1.53e+04 KL: 9.08e+03 MMD: 1.92e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 2.60e+04 logL: -1.49e+04 KL: 3.79e+03 MMD: 1.94e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 50 VALIDATION Loss: 2.34e+04 logL: -1.45e+04 KL: 1.70e+03 MMD: 1.92e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 2.06e+04 logL: -1.26e+04 KL: 8.30e+02 MMD: 1.89e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 1.83e+04 logL: -1.07e+04 KL: 4.33e+02 MMD: 1.91e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 1.76e+04 logL: -1.04e+04 KL: 2.64e+02 MMD: 1.83e+00\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Stopping\n",
      "====> Epoch: 85 VALIDATION Loss: 1.80e+04 logL: -1.04e+04 KL: 2.52e+02 MMD: 1.94e+00\n"
     ]
    }
   ],
   "source": [
    "for config, mdl_ncode in itertools.product(n_config_l,mdl_ncode_l):\n",
    "    alpha = 0\n",
    "    lambd = np.exp(np.random.uniform(0, np.log(1e5)))\n",
    "    dropout = 0#0.9*np.random.uniform()\n",
    "    dfac = 1./(1.-dropout)\n",
    "    nhidden = int(np.ceil(np.exp(np.random.uniform(np.log(dfac*mdl_ncode+1), np.log(dfac*2*nfeat)))))\n",
    "    nhidden2 = int(np.ceil(np.exp(np.random.uniform(np.log(dfac*mdl_ncode+1), np.log(nhidden)))))\n",
    "    print('config %i, alpha = %0.1f, lambda = %0.1f, dropout = %0.2f; 2 hidden layers with %i, %i nodes' % (config, alpha, lambd, dropout, nhidden, nhidden2))\n",
    "    model = InfoVAE(alpha=alpha, lambd=lambd, nfeat=nfeat, nhidden=nhidden, nhidden2=nhidden2, ncode=mdl_ncode, dropout=dropout)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5)\n",
    "    stopper = EarlyStopper(patience=10)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        valid_loss, valid_logL, valid_KLD, valid_MMD = train()\n",
    "        if epoch % log_interval == 0:\n",
    "            print('====> Epoch: {} VALIDATION Loss: {:.2e} logL: {:.2e} KL: {:.2e} MMD: {:.2e}'.format(\n",
    "                  epoch, valid_loss, valid_logL, valid_KLD, valid_MMD))\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "        if (not stopper.step(valid_loss)) or (epoch == epochs):\n",
    "            \n",
    "            if mdl_ncode == 2:\n",
    "                p_i = 0\n",
    "            elif mdl_ncode == 4:\n",
    "                p_i = 1\n",
    "            elif mdl_ncode == 6:\n",
    "                p_i = 2\n",
    "            elif mdl_ncode == 8:\n",
    "                p_i = 3\n",
    "            elif mdl_ncode == 10:\n",
    "                p_i = 4\n",
    "                \n",
    "            print('Stopping')\n",
    "            print('====> Epoch: {} VALIDATION Loss: {:.2e} logL: {:.2e} KL: {:.2e} MMD: {:.2e}'.format(\n",
    "                  epoch, valid_loss, valid_logL, valid_KLD, valid_MMD))\n",
    "            model.MSE = -valid_logL\n",
    "            model.KLD = valid_KLD\n",
    "            model.MMD = valid_MMD\n",
    "            mdl_MSE[config,p_i] = model.MSE\n",
    "            mdl_KLD[config,p_i] = model.KLD\n",
    "            mdl_MMD[config,p_i] = model.MMD\n",
    "            torch.save(model, tag +'/%i' % mdl_ncode +'/%04i.pth' % config  )\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad9a46f",
   "metadata": {},
   "source": [
    "# Saving Training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69553efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(tag+'/metrics.npz', MSE=mdl_MSE, KLD=mdl_KLD, MMD=mdl_MMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5af10",
   "metadata": {},
   "source": [
    "# Identifying top models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2ef8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = np.load(tag+'/metrics.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17b5932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_models(tag):\n",
    "    metrics = np.load(tag+'/metrics.npz')\n",
    "    model_paths = []\n",
    "    for j in range(2,11,2):\n",
    "        if j == 2:\n",
    "            p_i = 0\n",
    "        elif j == 4:\n",
    "            p_i = 1\n",
    "        elif j == 6:\n",
    "            p_i = 2\n",
    "        elif j == 8:\n",
    "            p_i = 3\n",
    "        elif j == 10:\n",
    "            p_i = 4\n",
    "        order = np.argsort(metrics['MSE'][:,p_i])\n",
    "        i = order[0]\n",
    "        m = torch.load(tag +'/%i' % j +'/%04i.pth' % i)\n",
    "        model_paths.append(tag +'/%i' % j +'/%04i.pth' % i)\n",
    "        print('Dims %i: Model %04i MSE %0.3e KLD %0.2e MMD %0.2e lambda %0.2e nhidden %i %i' % (j, i, metrics['MSE'][i,p_i], metrics['KLD'][i,p_i], metrics['MMD'][i,p_i], m.lambd, m.encd.out_features, m.enc2.out_features))\n",
    "    return model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f608095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims 2: Model 0010 MSE 8.060e+03 KLD 5.46e+01 MMD 7.43e-01 lambda 1.88e+01 nhidden 120 18\n",
      "Dims 4: Model 0034 MSE 4.155e+03 KLD 5.72e+01 MMD 1.09e+00 lambda 2.47e+01 nhidden 118 100\n",
      "Dims 6: Model 0002 MSE 2.419e+03 KLD 5.40e+01 MMD 1.38e+00 lambda 4.09e+00 nhidden 133 54\n",
      "Dims 8: Model 0099 MSE 1.418e+03 KLD 8.44e+01 MMD 1.56e+00 lambda 2.81e+02 nhidden 170 120\n",
      "Dims 10: Model 0077 MSE 7.622e+02 KLD 6.97e+01 MMD 1.87e+00 lambda 2.20e+00 nhidden 135 52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/GRFy/2/0010.pth',\n",
       " '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/GRFy/4/0034.pth',\n",
       " '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/GRFy/6/0002.pth',\n",
       " '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/GRFy/8/0099.pth',\n",
       " '/Users/pruthvibharadwaj/Desktop/Spring 22/MA679 - ML/Project/Models/GRFy/10/0077.pth']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_top_models(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a788cf54",
   "metadata": {},
   "source": [
    "# Storing the top models in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a3a689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims 2: Model 0010 MSE 8.060e+03 KLD 5.46e+01 MMD 7.43e-01 lambda 1.88e+01 nhidden 120 18\n",
      "Dims 4: Model 0034 MSE 4.155e+03 KLD 5.72e+01 MMD 1.09e+00 lambda 2.47e+01 nhidden 118 100\n",
      "Dims 6: Model 0002 MSE 2.419e+03 KLD 5.40e+01 MMD 1.38e+00 lambda 4.09e+00 nhidden 133 54\n",
      "Dims 8: Model 0099 MSE 1.418e+03 KLD 8.44e+01 MMD 1.56e+00 lambda 2.81e+02 nhidden 170 120\n",
      "Dims 10: Model 0077 MSE 7.622e+02 KLD 6.97e+01 MMD 1.87e+00 lambda 2.20e+00 nhidden 135 52\n"
     ]
    }
   ],
   "source": [
    "npars = [2,4,6,8,10]\n",
    "models_npars = []\n",
    "models_paths = show_top_models(tag)\n",
    "for path in models_paths:\n",
    "    modelp = torch.load(path)\n",
    "    modelp.eval()\n",
    "    models_npars.append(modelp)\n",
    "models_npars.append(modelp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd6f188",
   "metadata": {},
   "source": [
    "# Reconstructing the dataset and calculating MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98e79bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    MSE_VAE = np.zeros((1,len(npars)))\n",
    "    TSS_VAE = np.zeros((1,len(npars)))\n",
    "    R2 = np.zeros((1,len(npars)))\n",
    "    recon_VAE = np.zeros((len(npars), AP_GRF_stance_N_matrix.shape[0], AP_GRF_stance_N_matrix.shape[1]))\n",
    "    \n",
    "    for i in range(len(npars)):\n",
    "        modeli = models_npars[i]\n",
    "        _, mu_batch, _ = modeli(torch.tensor(AP_GRF_stance_N_matrix, dtype=torch.float32)) \n",
    "        recon_VAE[i,:,:] = modeli.decode(mu_batch).numpy()\n",
    "        MSE_VAE[0,i] = np.mean((recon_VAE[i,:,:] - AP_GRF_stance_N_matrix[:,:])**2)\n",
    "        TSS_VAE[0,i] = np.mean((AP_GRF_stance_N_matrix[:,:] - AP_GRF_stance_N_mean_array)**2)\n",
    "        R2[0,i] = (1-MSE_VAE[0,i]/TSS_VAE[0,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4acdc1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80997335, 0.90398512, 0.94431778, 0.96756252, 0.98247983]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6a343d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[160.67345875,  81.18357688,  47.08104972,  27.42689998,\n",
       "         14.81385146]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783242ab",
   "metadata": {},
   "source": [
    "# Reconstruction Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0679e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "APGRF_recon = []\n",
    "recon = []\n",
    "comb_fd = []\n",
    "orig = AP_GRF_stance_N_fd[10].data_matrix[0,:,:]\n",
    "for i in range(5):\n",
    "    APGRF_recon.append(skfda.FDataGrid(data_matrix=recon_VAE[i,:,:],grid_points=grid_points_100))\n",
    "    recon.append(APGRF_recon[i][10].data_matrix[0,:,:])\n",
    "    new = np.zeros((5,2,100,1))\n",
    "    new[i,0,:,:] = recon[i]\n",
    "    new[i,1,:,:] = orig\n",
    "    comb_temp = skfda.FDataGrid(data_matrix=new[i,:,:,:],grid_points=grid_points_100)\n",
    "    comb_fd.append(comb_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a46e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.zeros(5)\n",
    "tss = np.zeros(5)\n",
    "r2 = np.zeros(5)\n",
    "for i in range(5):\n",
    "    mse[i] = np.mean((recon_VAE[i,10,:] - AP_GRF_stance_N_matrix[10,:])**2)\n",
    "    tss[i] = np.mean((AP_GRF_stance_N_matrix[10,:] - AP_GRF_stance_N_mean_array[0,:])**2)\n",
    "    r2[i] = 1-mse[i]/tss[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b1be1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38.46679458, 24.7180982 , 10.64328922,  9.571944  ,  3.61519674])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d32ebf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95937231, 0.97389335, 0.98875882, 0.98989035, 0.99618172])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "880d9bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/pruthvibharadwaj/opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning:Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      " /Users/pruthvibharadwaj/opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning:Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      " /Users/pruthvibharadwaj/opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning:Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      " /Users/pruthvibharadwaj/opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning:Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      " /Users/pruthvibharadwaj/opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning:Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "titles = ['APGRF: 2 dims, MSE = 38.46, R2 = 95.9%','APGRF: 4 dims, MSE = 24.72, R2 = 97.4%','APGRF: 6 dims, MSE = 10.64, R2 = 98.9%','APGRF: 8 dims, MSE = 9.57, R2 = 99.0%','APGRF: 10 dims, MSE = 3.62, R2 = 99.6%']\n",
    "\n",
    "legend_elements = [Line2D([0], [0], label='Reconstructed'), \n",
    "                   Line2D([0], [0], color='orange', label='Original')]\n",
    "\n",
    "for i in range(5):\n",
    "    fig = comb_fd[i].plot()\n",
    "    fig.show()\n",
    "    plt.title(titles[i])\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"AP_GRF\")\n",
    "    plt.legend(handles = legend_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736a04d",
   "metadata": {},
   "source": [
    "# Latent Space Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5a52778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    recondata, latentmu, latentlv = models_npars[2](torch.tensor(AP_GRF_stance_N_matrix, dtype=torch.float32))\n",
    "    recondata = recondata.numpy()\n",
    "    latentmu = latentmu.numpy()\n",
    "    latentlv = latentlv.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4c41b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_PCA = PCA()\n",
    "latent_PCA.fit(latentmu[trainidx])\n",
    "latentmu_PCA = latent_PCA.transform(latentmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "78e4b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor()\n",
    "lof.fit(latentmu_PCA)\n",
    "outidx = np.argsort(lof.negative_outlier_factor_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c89f51cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O1 | ID : 136 | Trial: 3 | Leg: LEFT | Trial Length: 778\n",
      "O2 | ID : 1664 | Trial: 4 | Leg: RIGHT | Trial Length: 772\n",
      "O3 | ID : 1487 | Trial: 2 | Leg: LEFT | Trial Length: 688\n",
      "O4 | ID : 1865 | Trial: 2 | Leg: RIGHT | Trial Length: 783\n",
      "O5 | ID : 1210 | Trial: 5 | Leg: RIGHT | Trial Length: 757\n",
      "O6 | ID : 78 | Trial: 3 | Leg: RIGHT | Trial Length: 771\n",
      "O7 | ID : 89 | Trial: 5 | Leg: RIGHT | Trial Length: 708\n",
      "O8 | ID : 148 | Trial: 3 | Leg: LEFT | Trial Length: 821\n",
      "O9 | ID : 679 | Trial: 1 | Leg: RIGHT | Trial Length: 976\n",
      "O10 | ID : 1614 | Trial: 3 | Leg: LEFT | Trial Length: 783\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    j = outidx[i]\n",
    "    print('O%i | ID : %i | Trial: %i | Leg: %s | Trial Length: %i' % (i+1, int(ID_info_matrix[j,0]), int(ID_info_matrix[j,2]), ID_info_matrix[j,1] , int(ID_info_matrix[j,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d7a631b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(subsel, offset=3):\n",
    "    AP_GRF_stance_N_out = AP_GRF_stance_N_matrix[outidx[subsel]].copy()\n",
    "#    weigout = spec_weig[outidx[subsel]]\n",
    "\n",
    "    AP_GRF_stance_N_out /= np.mean(np.abs(AP_GRF_stance_N_out), axis=1)[:,None]\n",
    "    AP_GRF_stance_N_outfil = AP_GRF_stance_N_out.copy()\n",
    "#    specoutfil[weigout == 0] = float('nan')\n",
    "\n",
    "    prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "    colors = prop_cycle.by_key()['color']\n",
    "    for i in range(len(subsel)):\n",
    "        plt.plot(grid_points_100, AP_GRF_stance_N_out[i] + offset*i, zorder=-i-0.01, c=colors[subsel[i]], alpha=0.5)\n",
    "        plt.plot(grid_points_100, AP_GRF_stance_N_outfil[i] + offset*i, zorder=-i, c=colors[subsel[i]])\n",
    "        plt.text(grid_points_100[-1], AP_GRF_stance_N_out[i,-1] + i*offset, 'O%i' % (subsel[i]+1), color=colors[subsel[i]], fontsize='small')\n",
    "#    plt.xlim((3250,8800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dacc344f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1350 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7.2,4.5), dpi=300)\n",
    "fig.subplots_adjust(hspace=0, wspace=0)\n",
    "plt.subplot(1,2,1)\n",
    "plt.ylabel('APGRF + Offset')\n",
    "plt.yticks([])\n",
    "plt.xlabel('Time')\n",
    "plot_outliers(range(0,5))\n",
    "plt.subplot(1,2,2)\n",
    "plot_outliers(range(5,10))\n",
    "plt.yticks([])\n",
    "plt.xlabel('Time')\n",
    "#plt.savefig('figures/outliers.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c2e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
